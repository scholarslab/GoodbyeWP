[{"id":"2014-15-scholars-lab-graduate-fellows-panel","title":"2014-15 Scholars' Lab Graduate Fellows Panel","author":"jeremy-boggs","date":null,"categories":null,"url":"2014-15-scholars-lab-graduate-fellows-panel","layout":"events","content":"Please join us in welcoming our new Graduate Fellows in Digital Humanities! James Ambuske (History), Jennifer Foy (English), and Emily Senfeld’s (History) projects converge around questions of social networks and mapping. [gallery size=”medium” ids=”10208,10202,11009”] James Ambuske ’s (History) dissertation is titled “Scotland’s American Revolution: Emigration and Imperial Crisis, 1763-1803.” Jennifer Foy ’s (English) dissertation is titled “Mapping Sympathy: Sensibility, Stigma, and Space in the Long Eighteenth Century” Emily Senefeld ’s (History) project is titled “The Cultural Programs of the Highlander Folk School, 1932-1964.” All three fellows will share their research questions and digital project proposal.  Please come for the discussion and stay for the lunch!"},{"id":"2018-19-digital-humanities-fellowship-application-deadline","title":"2018-19 Digital Humanities Fellowship Application Deadline","author":"laura-miller","date":null,"categories":null,"url":"2018-19-digital-humanities-fellowship-application-deadline","layout":"events","content":"We are now accepting applications for the 2018-2019 DH Fellows Cohort. Please visit https://tinyurl.com/SLabDHFellows2018-2019  for complete information. Applications are due  Wednesday, November 1st, 2017. The Digital Humanities Fellowship supports advanced doctoral students doing innovative work in the digital humanities at the University of Virginia. The Scholars’ Lab offers Grad Fellows advice and assistance with the creation and analysis of digital content, as well as consultation on intellectual property issues and best practices in digital scholarship and DH software development. Fellows join our vibrant community, have a voice in intellectual programming for the Scholars’ Lab, make use of our dedicated grad office, and participate in one formal colloquium at the Library per fellowship year. As such, students are expected to be in residence on Grounds for the duration of the fellowship. Supported by the Jeffrey C. Walker Library Fund for Technology in the Humanities, the Matthew &amp; Nancy Walker Library Fund, and a challenge grant from the National Endowment for the Humanities, the highly competitive Graduate Fellowship in Digital Humanities is designed to advance the humanities and provide emerging digital scholars with an opportunity for growth."},{"id":"3d-modeling-printing-meetup-2016-02-23","title":"3D Modeling & Printing Meetup","author":"laura-miller","date":null,"categories":null,"url":"3d-modeling-printing-meetup-2016-02-23","layout":"events","content":"Never printed before? Have a model that will not print correctly? Or do you have a multi-part project in mind?  Come by, bring your ideas, your special filaments, your model files, your items to digitize.  And leave with shared knowledge, a bit of practice, and even some inspiration."},{"id":"3d-modeling-printing-meetup-2016-03-08","title":"3D Modeling & Printing Meetup","author":"laura-miller","date":null,"categories":null,"url":"3d-modeling-printing-meetup-2016-03-08","layout":"events","content":"Never printed before? Have a model that will not print correctly? Or do you have a multi-part project in mind?  Come by, bring your ideas, your special filaments, your model files, your items to digitize.  And leave with shared knowledge, a bit of practice, and even some inspiration."},{"id":"3d-modeling-printing-meetup-2016-03-22","title":"3D Modeling & Printing Meetup","author":"laura-miller","date":null,"categories":null,"url":"3d-modeling-printing-meetup-2016-03-22","layout":"events","content":"Never printed before? Have a model that will not print correctly? Or do you have a multi-part project in mind?  Come by, bring your ideas, your special filaments, your model files, your items to digitize.  And leave with shared knowledge, a bit of practice, and even some inspiration."},{"id":"3d-modeling-printing-meetup-2016-04-05","title":"3D Modeling & Printing Meetup","author":"laura-miller","date":null,"categories":null,"url":"3d-modeling-printing-meetup-2016-04-05","layout":"events","content":"Never printed before? Have a model that will not print correctly? Or do you have a multi-part project in mind?  Come by, bring your ideas, your special filaments, your model files, your items to digitize.  And leave with shared knowledge, a bit of practice, and even some inspiration."},{"id":"3d-modeling-printing-meetup-2016-04-19","title":"3D Modeling & Printing Meetup","author":"laura-miller","date":null,"categories":null,"url":"3d-modeling-printing-meetup-2016-04-19","layout":"events","content":"Never printed before? Have a model that will not print correctly? Or do you have a multi-part project in mind?  Come by, bring your ideas, your special filaments, your model files, your items to digitize.  And leave with shared knowledge, a bit of practice, and even some inspiration."},{"id":"access-evolving-methods-in-making-cultural-heritage-accessible-neh50","title":"ACCESS: Evolving Methods in Making Cultural Heritage Accessible (NEH@50)","author":"ronda-grizzle","date":null,"categories":null,"url":"access-evolving-methods-in-making-cultural-heritage-accessible-neh50","layout":"events","content":"A fundamental mission of the humanities as an academic discipline is making cultural heritage accessible to the public, student and scholars. Classic examples of this are the herculean efforts of the several Founding Fathers Papers projects. The documentary editing methods used to present cultural heritage embodied in the primary source documents have evolved over the long tenure of those efforts to generate a series of projects focused on specific clusters of documents. The Washington Papers, which first began in 1968 and has been strongly supported by the NEH, has to a expanded to the development of the George Washington Barbados Diary and the George Washington Financial Papers projects. Part of that evolution has been in making explicit interconnections between the materials and in providing interpretative layers to aid in the understanding of the cultural heritage, both in its original context and in today’s dynamic society. The Piers Plowman Project provides a variation on the presentation of cultural heritage as expressed in numerous versions of a fourteenth-century allegorical dream vision. The methods by which interconnections between the versions and interpretative layers has changed over the years with ever richer materials being made accessible. Consider also the cultural heritage inherent in the temporal and spatial associations among the more than 40 works (novels and short stories) that Faulkner set in his imaginary county of Yoknapatawph. The efforts of over 25 scholars are being focused through the Digital Yoknapatawph Project in making those associations accessible via interactive displays. Another project, Collective Biographies of Women, looks at 19th and 20th century mass-market short biographies of women to expose the social, literary, and historical networks that connected a surprisingly diverse and wide-range collection of famous, infamous, and now-forgotten women in the public mind. All of three of these projects are beneficiaries of NEH funding at key moments in their development. This extended workshop will provide hands-on experience with these evolving methods through more than a dozen specific humanities projects that have been undertaken, completed or are ongoing at the University of Virginia through collaborations with the Center for Digital Editing, the Institute for Advanced Technology in the Humanities, and Scholars’ Lab."},{"id":"african-studies-colloquium-series-angel-nieves-henry-lovejoy","title":"African Studies + DH Colloquium: Angel Nieves & Henry Lovejoy","author":"laura-miller","date":null,"categories":null,"url":"african-studies-colloquium-series-angel-nieves-henry-lovejoy","layout":"events","content":"Lunch and roundtable discussion to follow at noon Angel Nieves Digital Humanities and Difficult Narratives : Recovering Human Rights Violations During the 1976 Soweto Student Uprisings Over the past decade, scholars and community leaders have experimented with the use of new digital technologies to tell the history of the anti-apartheid movement in South Africa. Technologies now at our disposal allow us to layer victim testimony in hypertexts using multiple tools for mapping, text mining, and 3D visualizations.  Digital humanities (DH) may also help analyze documentation so as to reconstruct and recover an alternative historical narrative in the face of conventional wisdom or officializing histories for the foreign tourist market.  The potential layering of the many narratives also helps lay bare the messiness of archive making, the methodologies of digital ethnography, and, in particular, the endangered nature of those archives across South Africa related to the Soweto Uprisings of June, 1976.  My work on the Soweto Historical GIS Project’s Social Justice History Platform, builds on the early work of Soweto ’76, by providing a new software platform designed to represent geographic and spatial data within an enhanced interface that contextualizes locations and objects alongside the historical narrative of the primary source documents.  As a 3D and virtual reality enabled platform (built atop the Unity engine), the Social Justice History Platform is able to represent both 2D geospatial information (such as maps, photographs, and records) and 3D representations of landscapes, locations, and 3D models of historical buildings and objects.  This talk combines traditional ethnographic and oral history fieldwork with 3D technologies in the pursuit of documenting past human rights violations by the former apartheid regime.  It poses the question: “Can digital reconstructions of difficult histories be used to harness the tools of restorative social justice while also bringing to light those yet untold stories of past human rights violations?” Angel David Nieves, Ph.D. is an Associate Professor at Hamilton College, Clinton, N.Y and is Director of the American Studies and Cinema &amp; Media Studies Programs there. He is also Co-Director of Hamilton’s Digital Humanities Initiative (DHi) which is recognized as a leader among small-liberal arts colleges in the Northeast.  As Co-Director, he has raised over $2.7 million dollars in foundation and institutional support for digital humanities scholarship at Hamilton.  He is also Research Associate Professor in the Department of History at the University of the Witwatersrand, Johannesburg, South Africa. Nieves’s scholarly work and community-based activism critically engage with issues of race and the built environment in cities across the Global South.  His digital research and scholarship have been featured on MSNBC.com and in Newsweek International .  His work can be found at http://www.apartheidheritages.org Henry Lovejoy Crowd Sourcing Digital History Metadata: The Liberated Africans Project in Global Perspective After 1808, over 200,000 enslaved Africans were emancipated in an international effort to suppress and then abolish the trans-Atlantic slave trade. Intervention and prevention mostly involved the British Royal Navy (along with Portuguese, Brazilian, and Spanish naval ships) capturing slave ships and raiding coastal prisons. They would then escort the people responsible for engaging in the illegal slave trade, along with the rescued captives, into naval courts and bilateral commissions located in Africa and the Americas. The Liberated Africans Project reconstructs on a case-by-case basis widely dispersed archival evidence from an exceedingly rich, transnational collection of primary sources made by some of the world’s earliest international human rights courts. The long-term outcome of this Digital History project will be an interactive website that scholars, students and the general public can utilize to explore the history of antislavery and international human rights law, as well as the demography and ethnolinguistic composition of the post-1807 trans-Atlantic slave trade, principally from the perspective of the Africans involved. This talk focuses on how the design of liberatedafricans.org involves a content-driven database network (KORA) to re-organize, store, manage and assess available archival data based on events, places, objects and the people involved. Henry Lovejoy is an Assistant Professor at the University of Colorado – Boulder. His research focuses on in the history of West Africa, African Diaspora and Atlantic World. He is also heavily involved in advancing methodologies in the Digital Humanities and was recently awarded a NEH-Mellon Fellowship for Digital Publication for 2017-2018. He is principal investigator for The Liberated Africans Project, which received funding from the Hutchins Center at Harvard University to work with programmers at MATRIX, the center for digital humanities and social sciences at Michigan State University. This event, co-sponsored by the Carter G. Woodson Institute and the Scholars’ Lab, is part Woodson Institute’s African Studies Colloquium Series.  It is  free and open to the public."},{"id":"applications-due-for-praxis-and-prototyping-fellowships","title":"Applications Due for Praxis and DH Prototyping Fellowships","author":"laura-miller","date":null,"categories":null,"url":"applications-due-for-praxis-and-prototyping-fellowships","layout":"events","content":"Call for Praxis Fellowship Applications The Praxis Program is a radical re-imagining of the annual teaching and training we offer in the Scholars’ Lab. Its fellowships support a team of six University of Virginia PhD students from a variety of disciplines, who work collaboratively on a shared digital humanities project. Under the guidance of Scholars’ Lab faculty and staff, Praxis fellows conceive, develop, publish, and promote a digital project over the course of an academic year. Applications are due February 15th, 2018 for the 2018-2019 cohort. Call for DH Prototyping Fellowship Applications The Digital Humanities Prototyping Fellowship allows UVa graduate students to work with our R&amp;D team for a single semester to rapidly develop their digital project ideas and position themselves for future work in digital humanities. The Scholars’ Lab offers these fellows advice and assistance with the creation and analysis of digital content, as well as consultation on best practices in digital scholarship and DH software development. This program offers support to projects that might not fit the normal scope of the other fellowship programs: proposed projects may be collaborative, offer unusual outcomes, or represent significant departures from previous research. The fellowship is meant to help students rapidly acquire skills, to wireframe or prototype a project, and perhaps implement the first stages of a plan for further study.  Applications are due February 15th, 2018 for the 2018-2019 cohort."},{"id":"arduino-and-raspberry-pi-meetup-2016-02-17","title":"Arduino and Raspberry Pi Meetup","author":"laura-miller","date":null,"categories":null,"url":"arduino-and-raspberry-pi-meetup-2016-02-17","layout":"events","content":"None"},{"id":"arduino-and-raspberry-pi-meetup-2016-03-02","title":"Arduino and Raspberry Pi Meetup","author":"laura-miller","date":null,"categories":null,"url":"arduino-and-raspberry-pi-meetup-2016-03-02","layout":"events","content":"None"},{"id":"arduino-and-raspberry-pi-meetup-2016-03-16","title":"Arduino and Raspberry Pi Meetup","author":"laura-miller","date":null,"categories":null,"url":"arduino-and-raspberry-pi-meetup-2016-03-16","layout":"events","content":"None"},{"id":"arduino-and-raspberry-pi-meetup-2016-03-30","title":"Arduino and Raspberry Pi Meetup","author":"laura-miller","date":null,"categories":null,"url":"arduino-and-raspberry-pi-meetup-2016-03-30","layout":"events","content":"None"},{"id":"arduino-and-raspberry-pi-meetup-2016-04-13","title":"Arduino and Raspberry Pi Meetup","author":"laura-miller","date":null,"categories":null,"url":"arduino-and-raspberry-pi-meetup-2016-04-13","layout":"events","content":"None"},{"id":"arduino-and-raspberry-pi-meetup-2016-04-27","title":"Arduino and Raspberry Pi Meetup","author":"laura-miller","date":null,"categories":null,"url":"arduino-and-raspberry-pi-meetup-2016-04-27","layout":"events","content":"None"},{"id":"arduino-workshop-2019-01-23","title":"Arduino Workshop","author":"ammon-shepherd","date":null,"categories":null,"url":"arduino-workshop-2019-01-23","layout":"events","content":"Come learn about the Arduino micro-controller for fun or profit. No experience or equipment needed. Arduino kits and laptops provided to use in class, but you are welcome to bring your own. This workshop will go through the very basics of electricity, how to setup the Arduino, and building a first circuit; an LED nightlight. You can then spend the rest of the time working through the exercises at your own pace. Register here: https://cal.lib.virginia.edu/event/5010205"},{"id":"arduino-workshop-2019-02-20","title":"Arduino Workshop","author":"ammon-shepherd","date":null,"categories":null,"url":"arduino-workshop-2019-02-20","layout":"events","content":"Come learn about the Arduino micro-controller for fun or profit. No experience or equipment needed. Arduino kits and laptops provided to use in class, but you are welcome to bring your own. This workshop will go through the very basics of electricity, how to setup the Arduino, and building a first circuit; an LED nightlight. You can then spend the rest of the time working through the exercises at your own pace. Register here: https://cal.lib.virginia.edu/event/5010213"},{"id":"arduino-workshop-2019-03-18","title":"Arduino Workshop","author":"ammon-shepherd","date":null,"categories":null,"url":"arduino-workshop-2019-03-18","layout":"events","content":"Come learn about the Arduino micro-controller for fun or profit. No experience or equipment needed. Arduino kits and laptops provided to use in class, but you are welcome to bring your own. This workshop will go through the very basics of electricity, how to setup the Arduino, and building a first circuit; an LED nightlight. You can then spend the rest of the time working through the exercises at your own pace. Register here: https://cal.lib.virginia.edu/event/5010254"},{"id":"build-a-website-for-beginners-the-very-basics-of-html-and-css-2","title":"Workshop: Websites for Beginners - The Very Basics of HTML and CSS","author":"ammon-shepherd","date":null,"categories":null,"url":"build-a-website-for-beginners-the-very-basics-of-html-and-css-2","layout":"events","content":"This workshop will walk you through creating a very basic personal website with programs you already have on your laptop computer. If you don’t have a laptop, we can supply one for you. This is the wading pool of website building, no experience necessary."},{"id":"build-a-website-for-beginners-the-very-basics-of-html-and-css","title":"Workshop: Build a Website for Beginners - The Very Basics of HTML and CSS","author":"ammon-shepherd","date":null,"categories":null,"url":"build-a-website-for-beginners-the-very-basics-of-html-and-css","layout":"events","content":"This workshop will walk you through creating a very basic personal website with programs you already have on your laptop computer. If you don’t have a laptop, we can supply one for you. This is the wading pool of website building, no experience necessary."},{"id":"candace-barrington-jonathan-hsy-on-digital-hospitality","title":"Candace Barrington & Jonathan Hsy on Digital Hospitality","author":"laura-miller","date":null,"categories":null,"url":"candace-barrington-jonathan-hsy-on-digital-hospitality","layout":"events","content":"The Medieval Studies Lecture Series and the **UVA Interdisciplinary Graduate Medieval Colloquium **present: Candace Barrington &amp; Jonathan Hsy Digital Hospitality Barrington and Hsy’s joint presentation will outline some basic principles of archive and database creation that have been integral to the way their Global Chaucers project (with its international collective of scholars, translators, and enthusiasts) has developed and the shape it has taken.  This public event is co-sponsored by UVA’s Program in Medieval Studies, the Department of English, the Scholars’ Lab, and the Classics Department. Candace Barrington, a Professor at Central Connecticut State University, pursues two research interests. The first examines the intersection of legal and literary discourse, leading to several articles and co-edited volumes. Currently, she is co-editing the Cambridge Companion to Medieval English Law and Literature with Sebastian Sobecki (University of Groningen). Barrington’s second research interest examines Chaucer’s popular reception.  In this vein, she has written American Chaucers (2007) and contributed articles to Sex and Sexuality in a Feminist World (2009), American Literary History (2009), European Journal of English Studies (2011), Dark Chaucer: An Assortment (2012), Medieval Afterlives in Popular Culture (2012), Digital Gaming Re-imagines the Middle Ages (2013), Educational Theory (2014), Screening Chaucer: Absence, Presence, and Adapting the Canterbury Tales (2016), and Cambridge Companion to Medievalism (2016). In a broader context, she and Jonathan Hsy collaborate on Global Chaucers a project focusing on non-Anglophone adaptations and translations. With Hsy, she maintains an active blog and has written articles for Medieval Afterlives in Contemporary Culture (2015), Accessus (2015), and postmedieval (2015). Together they are co-editing an issue for the Global Circulation Project at Literature Compass. Because of her interest in teaching and Chaucer’s global reception, she is a founding member of the Editorial Collective for the Open Access Companion to The Canterbury Tales, a project developing a free, high-quality, open-access introductory volume reaching Chaucer’s global audience of English readers from a wide diversity of institutions. Jonathan Hsy is Associate Professor of English at The George Washington University and (with Alexa Huang) founding Co-Director of the GW Digital Humanities Institute . He specializes in medieval literature and culture with interests in translation theory, digital media, and disability studies. Author of Trading Tongues: Merchants, Multilingualism, and Medieval Literature (2013), he is founder (with Candace Barrington) of Global Chaucers an international network and online community that explores Chaucer’s legacy in contemporary non-Anglophone contexts; together with Barrington, he is co-editing a collection on global appropriations of Chaucer for Literature Compass. Among Hsy’s current book projects in the field of disability studies are a book on life writing by medieval authors who self-identified as blind or deaf, and a “minigraph” on the cultural history of eyeglasses (from the ambivalence of late-medieval poets upon the emergence of rivet spectacles to the techno-utopianism of “early adopters” of Google Glass). Hsy currently serves on the Modern Language Association’s Committee for Disability Issues in the Profession, and his publications on disability and digital media have appeared in Accessus, Cambridge Companion to the Body in Literature, New Medieval Literatures, Journal of Literary and Cultural Disability Studies, PMLA, and postmedieval. He serves on the Steering Committee of the BABEL Working Group and blogs at In the Middle, a group medieval studies blog."},{"id":"cartographers-still-exist-with-riley-d-champine","title":"GIS Speaker: Cartographers Still Exist with Riley D. Champine","author":"laura-miller","date":null,"categories":null,"url":"cartographers-still-exist-with-riley-d-champine","layout":"events","content":"The Department of Urban + Environmental Planning of the UVa School of Architecture together with the UVa Library’s Scholars’ Lab present a talk by Riley Champine, a Graphics Editor at National Geographic. Mr. Champine works with GIS and data visualization to support storytelling in the National Geographic magazine, see some of his work here .  Mr. Champine graduated with a B.S. in Geography and a minor in Planning, Public Policy &amp; Management from the University of Oregon and is based in Washington DC. Open to all.  Please contact Chris Gist ( cgist@virginia.edu ) with any questions.  Light refreshments will be served."},{"id":"code-workshop-basics-of-version-control-with-git-2","title":"Code Workshop: Basics of Version Control with Git","author":"laura-miller","date":null,"categories":null,"url":"code-workshop-basics-of-version-control-with-git-2","layout":"events","content":"Do you have file names like: Paper.doc, Paper-revision1.doc, Paper-v2.doc, Paper-Final.doc, Paper-Final2.doc? Have you heard of version control, but not sure how to get it working for you? This workshop will give you an overview of how to use Git and GitHub to version your documents (word processing, coding document, or any type of files). This is a workshop for absolute beginners, so no experience is required. We will mostly be talking about version control from a theoretical standpoint, and won’t be installing or using git on your own computers. For the full schedule of the Scholars’ Lab  Fall 2018 Maker &amp; Code Workshop Series, see  here ."},{"id":"code-workshop-basics-of-version-control-with-git-3","title":"Code Workshop: Basics of Version Control with Git","author":"laura-miller","date":null,"categories":null,"url":"code-workshop-basics-of-version-control-with-git-3","layout":"events","content":"Do you have file names like: Paper.doc, Paper-revision1.doc, Paper-v2.doc, Paper-Final.doc, Paper-Final2.doc? Have you heard of version control, but not sure how to get it working for you? This workshop will give you an overview of how to use Git and GitHub to version your documents (word processing, coding document, or any type of files). This is a workshop for absolute beginners, so no experience is required. We will mostly be talking about version control from a theoretical standpoint, and won’t be installing or using git on your own computers. For the full schedule of the Scholars’ Lab  Fall 2018 Maker &amp; Code Workshop Series, see  here ."},{"id":"code-workshop-basics-of-version-control-with-git-4","title":"Code Workshop: Basics of Version Control with Git","author":"laura-miller","date":null,"categories":null,"url":"code-workshop-basics-of-version-control-with-git-4","layout":"events","content":"Do you have file names like: Paper.doc, Paper-revision1.doc, Paper-v2.doc, Paper-Final.doc, Paper-Final2.doc? Have you heard of version control, but not sure how to get it working for you? This workshop will give you an overview of how to use Git and GitHub to version your documents (word processing, coding document, or any type of files). This is a workshop for absolute beginners, so no experience is required. We will mostly be talking about version control from a theoretical standpoint, and won’t be installing or using git on your own computers. For the full schedule of the Scholars’ Lab  Fall 2018 Maker &amp; Code Workshop Series, see  here ."},{"id":"code-workshop-basics-of-version-control-with-git","title":"Code Workshop: Basics of Version Control with Git","author":"laura-miller","date":null,"categories":null,"url":"code-workshop-basics-of-version-control-with-git","layout":"events","content":"Do you have file names like: Paper.doc, Paper-revision1.doc, Paper-v2.doc, Paper-Final.doc, Paper-Final2.doc? Have you heard of version control, but not sure how to get it working for you? This workshop will give you an overview of how to use Git and GitHub to version your documents (word processing, coding document, or any type of files). This is a workshop for absolute beginners, so no experience is required. We will mostly be talking about version control from a theoretical standpoint, and won’t be installing or using git on your own computers. For the full schedule of the Scholars’ Lab  Fall 2018 Maker &amp; Code Workshop Series, see  here ."},{"id":"code-workshop-build-a-website-for-beginners-the-very-basics-of-html-and-css-2","title":"Code Workshop: Build a Website for Beginners – The Very Basics of HTML and CSS","author":"laura-miller","date":null,"categories":null,"url":"code-workshop-build-a-website-for-beginners-the-very-basics-of-html-and-css-2","layout":"events","content":"This workshop will walk you through creating a very basic personal website with programs you already have on your laptop computer. If you don’t have a laptop, we can supply one for you. This is the wading pool of website building, no experience necessary. For the full schedule of the Scholars’ Lab  Fall 2018 Maker &amp; Code Workshop Series, see  here ."},{"id":"code-workshop-build-a-website-for-beginners-the-very-basics-of-html-and-css-3","title":"Code Workshop: Build a Website for Beginners – The Very Basics of HTML and CSS","author":"laura-miller","date":null,"categories":null,"url":"code-workshop-build-a-website-for-beginners-the-very-basics-of-html-and-css-3","layout":"events","content":"This workshop will walk you through creating a very basic personal website with programs you already have on your laptop computer. If you don’t have a laptop, we can supply one for you. This is the wading pool of website building, no experience necessary. For the full schedule of the Scholars’ Lab  Fall 2018 Maker &amp; Code Workshop Series, see  here ."},{"id":"code-workshop-build-a-website-for-beginners-the-very-basics-of-html-and-css-4","title":"Code Workshop: Build a Website for Beginners – The Very Basics of HTML and CSS","author":"laura-miller","date":null,"categories":null,"url":"code-workshop-build-a-website-for-beginners-the-very-basics-of-html-and-css-4","layout":"events","content":"This workshop will walk you through creating a very basic personal website with programs you already have on your laptop computer. If you don’t have a laptop, we can supply one for you. This is the wading pool of website building, no experience necessary. For the full schedule of the Scholars’ Lab  Fall 2018 Maker &amp; Code Workshop Series, see  here ."},{"id":"code-workshop-build-a-website-for-beginners-the-very-basics-of-html-and-css","title":"Code Workshop: Build a Website for Beginners – The Very Basics of HTML and CSS","author":"laura-miller","date":null,"categories":null,"url":"code-workshop-build-a-website-for-beginners-the-very-basics-of-html-and-css","layout":"events","content":"This workshop will walk you through creating a very basic personal website with programs you already have on your laptop computer. If you don’t have a laptop, we can supply one for you. This is the wading pool of website building, no experience necessary. For the full schedule of the Scholars’ Lab  Fall 2018 Maker &amp; Code Workshop Series, see  here ."},{"id":"conference-enduring-questions-new-methods-haitian-studies-in-the-21st-century","title":"Conference: Enduring Questions, New Methods: Haitian Studies in the 21st Century","author":"laura-miller","date":null,"categories":null,"url":"conference-enduring-questions-new-methods-haitian-studies-in-the-21st-century","layout":"events","content":"Please join our colleagues in English, French, Haitian Kreyòl, History, and Latin American Studies as we rethink challenges and opportunities for research and teaching in Haitian Studies. The full program is below. For more information on sessions and topics, please visit  http://enduringquestions.net . THURSDAY, 12 APRIL 4:00-5:30PM, Minor Hall 110 –  Haiti and the Digital Humanities Nathan H. Dize, Julia Gaffield,  Marlene L. Daut\n– moderated by Kaiama L. Glover 5:45PM, Clark 107 – Welcome Remarks\nDeborah McDowell 6:00-7:30PM, Clark 107 –  Building Programs for Haitian Studies in the United States Cécile Accilien, Laurent Dubois, Claudine Michel, Jean Eddy Saint-Paul\n– moderated by Robert Fatton, Jr. 7:30PM, Clark Hall – Reception FRIDAY, 13 APRIL 9:30-11:00AM, Minor Hall 110 –  Politics and Intellectual History Jean Casimir, Sara Johnson, Délide Joseph, Matthew Smith\n– moderated by Marlene L. Daut COFFEE 11:15AM-12:30PM, Minor Hall 110 –  Translating Haitian Literature Kaiama L. Glover, Deborah Jenson, Nadève Ménard\n– moderated by Njelle Hamilton LUNCH 2:00-3:30PM, Minor Hall 110 –  Thinking Vodou: Faith, the Archive, the Law Kyrah Malika Daniels, Colin Dayan, Christina Mobley, Kate Ramsey\n–moderated by Gina Athena Ulysse COFFEE 3:45-5:00PM, , Minor Hall 110 –  Haitian Kreyòl Michel DeGraff, Mariana Past, Jacques Pierre\n– moderated by Christina Mobley 5:00-5:30PM Closing Remarks"},{"id":"daniel-shore-on-cultural-constructicography-dh-discussion-and-workshop","title":"Daniel Shore on Cultural Constructicography: DH Discussion and Workshop","author":"laura-miller","date":null,"categories":null,"url":"daniel-shore-on-cultural-constructicography-dh-discussion-and-workshop","layout":"events","content":"Please join us for “Cultural Constructicography,” a discussion and workshop with Daniel Shore. Come learn how digital archives and advanced search tools can help us revise our understanding of the history of language, the nature of the sign, and the “structure” of Structuralism. The session will begin with a short talk about the arguments of the recent book, Cyberformalism: Histories of Linguistic Forms in the Digital Archive, move to a demonstration of corpus-based research methods, and end with a workshop format where students can try out corpus methods on their own research topics and get coaching. Daniel Shore, Provost’s Distinguished Associate Professor of English at Georgetown University, is the author of Cyberformalism: Histories of Linguistic Forms in the Digital Archive (Johns Hopkins University Press, 2018) and Milton and the Art of Rhetoric (Cambridge University Press, 2012), and of articles in journals including PMLA, Critical Inquiry, Modern Philology, Shakespeare Quarterly, Milton Studies and others. His research has been supported by the American Council of Learned Societies, the Andrew W. Mellon Foundation, and the Folger Shakespeare Library, and he is the co-founder of the Six Degrees of Francis Bacon project. RSVP is requested, but not mandatory. Please email Julie Gronlund (gronlund@virginia.edu) with your RSVP and any questions. This event is hosted by the Humanities Informatics Lab, at the Institute of the Humanities and Global Cultures."},{"id":"dh-fellows-final-presentation-spring-2019-05-01","title":"Dh Fellows Final Presentation - Spring 2019","author":"first-last","date":null,"categories":null,"url":"dh-fellows-final-presentation-spring-2019-05-01","layout":"events","content":"Final presentation of the 2018-2019 digital humanities. Presentations by Kelli Shermeyer and Sean Tennant from 10:30-12:00 with lunch provided following the talks. Kelli Shermeyer’s Abstract: This presentation will discuss the performance possibilities, theoretical cruxes, and technical difficulties of using motion capture and augmented reality technologies in theatrical performance. My project, Digital Skriker, explores how these increasingly accessible technologies might change both the ways we think about documenting and archiving movement and the kinds of immersive and interactive performances theater artists are able to create. This talk traces the process of rendering a scene from Caryl Churchill’s The Skriker (1994) in Unity, relating the various challenges that arose to the kinds of technical and aesthetic questions frequently considered by theater artists. How do the capabilities of motion capture enable us to blur boundaries between performance, film, archive, and game? Sean Tennant’s Abstract: Sean’s project is an extension of his dissertation work in Mediterranean Art and Archaeology, titled Domestic Spaces in the Roman West: Architectural Adaptation in Gaul, Britannia, and Germania . Employing network analysis methodologies, Sean’s research examines the spatial arrangements of Roman domestic structures as networks of adjacent spaces, quantifying those spaces and looking for broader patterns across the entire area of study. Those emergent patterns are then considered in the context of ongoing discussions about cultural assimilation and change in the frontier provinces of the Roman Empire."},{"id":"dh-fellows-presentation","title":"Digital Humanities Fellows Presentations & Lunch: Ethan Reed & Julia Haines","author":"brandon-walsh","date":null,"categories":null,"url":"dh-fellows-presentation","layout":"events","content":"Join Julia Haines and Ethan Reed as they give presentations on the work they have done during the course of their Graduate Fellowships in Digital Humanities in collaboration with the Scholars’ Lab. The event will run from 10:30-12:00 with lunch provided to follow. Ethan Reed Ethan is a PhD candidate in Department of English. His dissertation, “The Radical Sensibility: Cultures of Discontent in American Literature after 1934,” investigates the articulation of feelings associated with injustice—such as frustration or anger—as they relate to race, class, and gender in recent US literary history. As a Graduate Fellow, Ethan has worked on “Measured Unrest in the Poetry of the Black Arts Movement,” a project that uses quantitative methods—a machine reading technique called sentiment analysis—to examine a corpus of revolutionary poetry from the Black Arts Movement. A body of work famous for tying heightened affects to an explicitly political quest for racial justice in America, this poetry was also written in the shadow of government surveillance programs, active FBI counterintelligence operations, and a larger culture fearful of radical thought. In this sense, Ethan’s project explores the fraught methodological implications of using distanced, potentially decontextualizing computational text analysis techniques to think through BAM poetry, and how these methods might best be used to pursue questions, problems, and lines of inquiry centered around black thought and experience. Julia Jong Haines I am currently a PhD candidate in the Department of Anthropology at UVa. Since 2012 I have been conducting archeological research in Mauritius with the Mauritius Archaeology and Cultural Heritage (MACH) project. My dissertation research investigates the ruins of a 19th century sugar estate, the material culture excavated from the laborers’ village on the plantation, and colonial archival records inside Bras d’Eau National park, Mauritius. Through archaeology and the material world, I am interested in understanding expressions of immigrant identities within global colonial contexts. My dissertation research focuses on the archaeology of Asian indentured laborers who migrated to the small SW Indian Ocean island of Mauritius during the 19th century to work on sugar plantations in the wake of the abolition of slavery throughout the British Empire. I from 2015 to 2016 conducted 17 months of archaeological survey, excavations and archival fieldwork in Bras d’Eau National Park, a well-preserved colonial sugar estate.  During my DH fellowship I focused on reworking the data I collected, including maps of archaeological ruins, sketches from excavations and of artifacts collected, and extensive artifact catalogues. Recreating the material world through digital mediums such as ArcGIS, Adobe Illustrator and 3D printing, allowed me to better understand how the daily activities of indentured women, men, and children shaped the cultural and environmental landscape in colonial Mauritius and fit more broadly in Indian Ocean World networks."},{"id":"dh-incubation-meetup-2016-02-24","title":"DH Project Incubation Meetup","author":"laura-miller","date":null,"categories":null,"url":"dh-incubation-meetup-2016-02-24","layout":"events","content":"Interested in starting a DH project?  Curious what other digital humanists are doing at UVA? Wondering which tools might help you further your research?  Drop in for our biweekly DH Incubation hour! (Or if you are not available at this time, feel free to email scholarslab@virginia.edu to set up a one-on-one consultation.)"},{"id":"dh-incubation-meetup-2016-03-09","title":"DH Project Incubation Meetup","author":"laura-miller","date":null,"categories":null,"url":"dh-incubation-meetup-2016-03-09","layout":"events","content":"Interested in starting a DH project?  Curious what other digital humanists are doing at UVA? Wondering which tools might help you further your research?  Drop in for our biweekly DH Incubation hour! (Or if you are not available at this time, feel free to email scholarslab@virginia.edu to set up a one-on-one consultation.)"},{"id":"dh-incubation-meetup-2016-03-23","title":"DH Project Incubation Meetup","author":"laura-miller","date":null,"categories":null,"url":"dh-incubation-meetup-2016-03-23","layout":"events","content":"Interested in starting a DH project?  Curious what other digital humanists are doing at UVA? Wondering which tools might help you further your research?  Drop in for our biweekly DH Incubation hour! (Or if you are not available at this time, feel free to email scholarslab@virginia.edu to set up a one-on-one consultation.)"},{"id":"dh-incubation-meetup-2016-04-06","title":"DH Project Incubation Meetup","author":"laura-miller","date":null,"categories":null,"url":"dh-incubation-meetup-2016-04-06","layout":"events","content":"Interested in starting a DH project?  Curious what other digital humanists are doing at UVA? Wondering which tools might help you further your research?  Drop in for our biweekly DH Incubation hour! (Or if you are not available at this time, feel free to email scholarslab@virginia.edu to set up a one-on-one consultation.)"},{"id":"dh-incubation-meetup-2016-04-20","title":"DH Project Incubation Meetup","author":"laura-miller","date":null,"categories":null,"url":"dh-incubation-meetup-2016-04-20","layout":"events","content":"Interested in starting a DH project?  Curious what other digital humanists are doing at UVA? Wondering which tools might help you further your research?  Drop in for our biweekly DH Incubation hour! (Or if you are not available at this time, feel free to email scholarslab@virginia.edu to set up a one-on-one consultation.)"},{"id":"dh-methods-collaborators-fair-2019-02-01","title":"Dh Methods & Collaborators Fair","author":"first-last","date":null,"categories":null,"url":"dh-methods-collaborators-fair-2019-02-01","layout":"events","content":"How did they make that?\nWhat might a digital method do for my research?\nWhat are the opportunities to learn more, or to begin a project?\nWho can collaborate on my kind of research? 1:30 - 2:30: Short presentations of recent digital projects, spanning a variety of disciplines and methods, by faculty, graduate fellows, library staff, and others.\n2:30 - 3:30: Open session to meet practitioners and scholars Join us for an informal open house, hosted by the Scholars’ Lab, and featuring IATH, SHANTI, and other members of the DH community. Meet fellow faculty, students, and library staff, find out about opportunities for fellowships or internal grants, chat with specialists in different kinds of research, and explore digital projects on screen.  You could also schedule a follow up consultation or find out how to apply for the various DH opportunities for graduate students and faculty at UVA. Open to members of the University community.  Light refreshments will be served."},{"id":"dh-panel-fair-use-and-the-future-of-digital-scholarship","title":"DH Panel: Fair Use and the Future of Digital Scholarship","author":"laura-miller","date":null,"categories":null,"url":"dh-panel-fair-use-and-the-future-of-digital-scholarship","layout":"events","content":"Can fair use extend beyond the selection?  What if your research could include copyrighted works? What questions could you ask of a corpus of millions of digitized texts or other data? Explore the rich tradition of digital research at UVA, and learn how recent fair use decisions in the courts are opening up new vistas for textual scholarship. Panelists will include: Andrew Stauffer, UVA Associate Professor of English and Director of NINES and the Book Traces project; Jonathan Band, an expert on the Google Books litigation; and Stephen Downie, Co-Director of the HathiTrust Research Center, which facilitates digital research on a massive corpus of mostly in-copyright books. Moderated by Brandon Butler, the Library’s new Director of Information Policy. Jonathan Band helps shape the laws governing intellectual property and the Internet through a combination of legislative and appellate advocacy.  He has represented clients with respect to the drafting of the Digital Millennium Copyright Act (DMCA), the PRO-IP Act, the Stop Online Piracy Act (SOPA), and other federal and state statutes relating to intellectual property and the Internet.  Mr. Band is a professor at the Georgetown University Law Center, and has written extensively on intellectual property and the Internet, including the books Interfaces on Trial and_Interfaces on Trial 2.0_. J. Stephen Downie is the Associate Dean for Research and a Professor at the Graduate School of Library and Information Science at the University of Illinois at Urbana-Champaign. He is the Illinois Co-Director of the HathiTrust Research Center (HTRC). He was the Principal Investigator on the Networked Environment for Music Analysis (NEMA) project, funded by the Andrew W. Mellon Foundation. He was Co-PI on the Structural Analysis of Large Amounts of Music Information (SALAMI) project, jointly funded by the National Science Foundation (NSF), the Canadian Social Science and Humanities Research Council (SSHRC), and the UK’s Joint Information Systems Committee (JISC). Dr. Downie is the leader of the Hathitrust + Bookworm (HT+BW) text analysis project that is creating tools to visualize the evolution of term usage over time.  All of these aforementioned projects share a common thread of striving to provide large-scale analytic access to copyright-restricted cultural data."},{"id":"dh-speaker-casey-boyle-on-making-sense-in-calamitous-times","title":"DH Speaker: Casey Boyle on Making Sense in Calamitous Times","author":"laura-miller","date":null,"categories":null,"url":"dh-speaker-casey-boyle-on-making-sense-in-calamitous-times","layout":"events","content":"Lunch reception to follow Join us for a talk by professor of digital rhetoric, Casey Boyle, On Making Sense in Calamitous Times From carcinogens at the molecular level to climate collapse at a global scale, the space of “everyday” life is now beset–every day–by extraordinary calamities. Given the onslaught of these multiple and unending disasters, it is difficult to come to terms not only with resolutions to these many problems but also to develop a sense of the problems themselves. In short, we fail, repeatedly, to make sense. What senses might we develop, which senseabilities might we cultivate by repeatedly engaging multiple mediations that are not intended to represent but to make sense? In response to these questions, this presentation will make a methodological intervention into our ways of making sense by surveying recent\ndevelopments of empirical research methods and proposing a renewal of rhetorical methods with and through digital media technologies. Casey Boyle is an Assistant Professor in the Department of Rhetoric and Writing at the University of Texas at Austin where he researches and teaches digital rhetoric, composition theory, and rhetorical history. His forthcoming book, Rhetoric as a Posthuman Practice, explores the role of practice and ethics in digital rhetoric. You can read more about his work at: http://caseyboyle.net /. This talk is co-sponsored by the Academic and Professional Writing Program and the Scholars’ Lab."},{"id":"dh-speaker-erin-rose-glass","title":"Erin Rose Glass: Writing in the Age of Alienated Intelligence: How to Hack the Hidden Value of our Words","author":"laura-miller","date":null,"categories":null,"url":"dh-speaker-erin-rose-glass","layout":"events","content":"DH Speaker: Erin Rose Glass **\nWriting in the Age of Alienated Intelligence: How to Hack the Hidden Value of our Words** Surveillance capitalism has entered the academy! As this new economic logic permeates our writing and communication technologies, digitally-mediated academic writing represents at once two forms of intelligence: the academic intelligence as intended by the author, and an alienated form of intelligence about that author to be mined, analyzed, instrumentalized, and indefinitely archived by IT companies in their quest for the accumulation of capital. While this alienation may seem to have no bearing on our intellectual processes, Erin Rose Glass argues that it constitutes an “invisible discipline” that “teaches” the academic community to passively accept digital technology as neutral and inevitable. Such passivity not only contributes to the broader normalization of surveillance capitalism but also freezes our capacity, as academics, to explore exciting new intellectual, social, and political possibilities for our academic writing technologies and practices. In this talk, Glass offers a framework for thinking about writing in an age of alienated intelligence and discuss a variety of ways academics can hack our writing tools, practices, and information capital to generate new forms of intellectual community and processes. To demonstrate this approach, she will discuss experimental projects including Social Paper, #SocialDiss, and KNIT R&amp;D that attempt to demonstrate the exciting potential of reclaiming and redirecting the alienated intelligence of our academic communities. Erin Rose Glass  is the Digital Scholarship Librarian at UC San Diego, where she facilitates the  Digital Humanities Research Group  and directs  KNIT, a digital commons for UC San Diego, the San Diego Community College District, and San Diego State University. Her work, including #SocialDiss, which received the 2018 Emerging Open Scholarship Award, focuses on using digital tools and social practices to make education and knowledge production more democratic, collaborative, and publicly engaged."},{"id":"dh-speaker-event-greg-crane-on-open-philology","title":"DH Speaker Event: Greg Crane on Open Philology","author":"laura-miller","date":null,"categories":null,"url":"dh-speaker-event-greg-crane-on-open-philology","layout":"events","content":"Gregory Crane, editor-in-chief of the Perseus Digital Library, will be visiting the University of Virginia on Thursday, April 12.  Dr. Crane is a Professor of Classics and Computer Science at Tufts University and is the Alexander von Humboldt Professor of Digital Humanities at Leipzig University. During his visit, he will be participating in an open dialogue on Digital Humanities with faculty and students at UVA and talking about his newest digital endeavor, the Open Greek and Latin - First 1000 Years of Greek Project . Dr. Crane will be giving a public talk on “Open Philology” on Wednesday, April 12 at 3pm in Alderman 421 with a reception to follow sponsored by the UVA Library, the Classics Department, and the Scholars’ Lab."},{"id":"dh-speaker-series-david-mimno-on-hunting-for-topics","title":"DH Speaker Series: David Mimno on Hunting for Topics","author":"laura-miller","date":null,"categories":null,"url":"dh-speaker-series-david-mimno-on-hunting-for-topics","layout":"events","content":"Please join us for a public talk by David Mimno, Hunting for Topics: How we Search and What We Find . David Mimno is Assistant Professor of Computer Science at Cornell University and chief maintainer for MALLET, a Machine Learning for Language Toolkit. He is one of the principal architects behind the widespread use of topic modeling in the digital humanities. In his visit, he will speak on both the critical and practical dimensions of topic modeling. Sponsored by the Data Science Institute, the Scholars’ Lab, IATH, and SHANTI."},{"id":"dh-speaker-series-jentery-sayers","title":"DH Speaker Series: Jentery Sayers on Remaking the Past","author":"laura-miller","date":null,"categories":null,"url":"dh-speaker-series-jentery-sayers","layout":"events","content":"Remaking Victorian Miniatures: The Speculative Stitches between 2D and 3D In both digital humanities and popular culture, there is a rapidly growing interest in big data. How not to read a million books? How to wrangle petabytes of data? How to discover and express patterns across thousands of images? Frequently, this research is framed as a response to an historically unprecedented abundance of information. For instance, over 40 million photos are posted to Instagram each day, and roughly 6,000 tweets appear on Twitter each second. But in this talk I shift the focus away from amounts of data and toward ways of seeing with computers. In this sense, contemporary computing is more about mediation than media, or more about vision than the visible. As a case in point, I draw material from the Kits for Cultural History project at the University of Victoria’s Maker Lab in the Humanities in order to outline how digital humanities methods also help scholars think small. More specifically, I explain how the Kits use computer vision, photogrammetry, and fabrication techniques to remake Victorian miniatures (such as Gustave Trouvé’s electric jewelry) in 3D, based on extant 2D drawings, diagrams, photographs, and research notes from the second half of the nineteenth century. These miniatures pose a number of curious problems for humanities scholars: How to historicize the minutiae of their manufacture? How to understand their undocumented uses and reception? And how to reconstruct them when they no longer exist, or parts of them are simply not accessible? Comparable to big data projects, these acts of remaking involve some serious speculation where archival gaps are concerned. They also rely quite heavily on computation to stitch together evidence in the presence of absences. In short, they are matters not of “how many” but of “how this becomes that.” Jentery Sayers is Assistant Professor of English and Director of the Maker Lab in the Humanities at the University of Victoria. His research interests include comparative media studies, critical theories of technology, and cultural histories of dead devices. He is currently working on a book about the “digging condition” of digital humanities, or how new media, data, and computing were embedded in materialist metaphors and methods during the 2000s. With William J. Turkel (Western University), he is the PI of the SSHRC-supported Kits for Cultural History project, which is reconstructing pre-digital technologies using physical computing and fabrication techniques."},{"id":"dh-speaker-series-kari-kraus-on-humanistic-design","title":"DH Speaker Series: Kari Kraus on Humanistic Design","author":"laura-miller","date":null,"categories":null,"url":"dh-speaker-series-kari-kraus-on-humanistic-design","layout":"events","content":"F inding Faultlines: An Approach to Humanistic Design Historically we know that many new technologies have inherited parts from prior technologies. The skateboard remediated the surfboard; the camera pilfered from the gun; the telephone harvested batteries and wires from the telegraph; and early models of the phonograph borrowed the treadle mechanism of the sewing machine. In each of these instances, the logic of part-whole relationships governs the design. “Many of a technology’s parts are shared by other technologies,” notes Brian Arthur in The Nature of Technology, “so a great deal of development happens automatically as components improve in other uses ‘outside’ the host technology.” [1] Drawing inspiration from this assemblage model of design, I’ll report on research I recently conducted at the University of Maryland investigating how individuals locate the fault lines in objects and analyze them into their component parts. I’ll discuss several potential application domains, including the design of fragmented or non-finito products: intentionally unfinished things [2], such as a sketch, the torso of a statue, or (in the case of my own work) an alternate reality game that incorporates mobile and web apps. Kari Kraus is an associate professor in the College of Information Studies and the Department of English at the University of Maryland. Her research and teaching interests focus on new media and the digital humanities, digital preservation, game studies and transmedia storytelling, and speculative design. She is writing  a book  about how artists, designers, and humanities researchers think about, model, and design possible futures.  In addition, she is collaborating on DUST, a multiplayer collaborative online adventure for students in middle and high school, created through a partnership between Brigham Young University, University of Maryland, College Park, and NASA, with funding from the National Science Foundation. [1] Qtd. in Kevin Kelly, What Technology Wants (New York: Viking, 2010) 45.\n[2] Jin-min Seok, et al., \"Non-Finito Products: A New Design Space of User Creativity for Personal User Experience,\" CHI 2014, Toronto Canada."},{"id":"dh-speaker-series-rob-nelson-on-topic-modeling-in-the-humanities","title":"DH Speaker Series: Rob Nelson on Topic Modeling in the Humanities","author":"laura-miller","date":null,"categories":null,"url":"dh-speaker-series-rob-nelson-on-topic-modeling-in-the-humanities","layout":"events","content":"The Potential and Pitfalls of Topic Modeling for Humanities Research This talk will introduce the text-mining technique called topic modeling, briefly explaining what it is and how it’s done. It will then turn to more substantial questions: what does this technique offer humanities researchers and what are its methodological limitations and problems? Both the potential and the pitfalls of topic modeling will be illustrated through research that uses topic models of newspapers to explore Civil War nationalism. Dr. Robert K. Nelson is the director of the Digital Scholarship Lab at the University of Richmond. His current research uses a text-mining technique called topic modeling to uncover themes and reveal historical patterns in massive amounts of text from the Civil War era.  He is currently completing two projects from this research.  One is a digital project that will publish and analyze multiple topic models of Civil War-era archives including the Richmond Daily Dispatch and the New York Times .  The other is an essay that analyzes these models to produce a comparative analysis of Union and Confederate nationalism and patriotism. This event is co-sponsored by the Institute for Advanced Technology in the Humanities, the Center for the Study of Data and Knowledge, and the Scholars’ Lab."},{"id":"dh-speaker-series-sarah-bond","title":"Linking the Ancient World: A Pleiades Workshop With Sarah Bond","author":"laura-miller","date":null,"categories":null,"url":"dh-speaker-series-sarah-bond","layout":"events","content":"What can you do with linked data for over 35,000 ancient places, locations, and names?  Pleiades.stoa.org  has a few answers. The gazetteer, which began as a means of digitizing the  Barrington Atlas of the Greek and Roman World (2000), grows daily and provides extensive coverage for the Greek and Roman world. It is also expanding into Ancient Near Eastern, Byzantine, Celtic, and Early Medieval geography. At this workshop, Associate Editor Sarah Bond will introduce the  Pleiades  community to participants. She will walk them through the history and layout of the gazetteer, discuss the popular contribution and review of our linked geodata, and then help participants make a map of sites within the ancient Mediterranean. Persons at all levels of experience (from “interested” to “expert”) are welcome to participate and to contribute at no charge… just like the Pleiades project itself. This event is co-sponsored by the Corcoran Department of History, the Department of Classics and the Scholars’ Lab."},{"id":"dh-speaker-series-toniesha-taylor","title":"DH Speaker Series: Toniesha Taylor on Afrofuturism as DH","author":"ronda-grizzle","date":null,"categories":null,"url":"dh-speaker-series-toniesha-taylor","layout":"events","content":"We Speak, We Make, We Tinker: Afrofuturism as Applied Digital Humanities Afrofuturism is the manifestation of digital humanities. The goals of the thinkers, makers, creatives, and speakers involved in both digital humanities and afrofuturism are so much the same—yet—impossibly different. Afrofuturists focus on the future past were Black peoples are people—more than flesh—they are humanities. They have powers and abilities so present and ancient they are as unreadable script as the 3D maker spaces of digital humanist are to the CD ROMs of your average humanist. We are the hidden code. Don’t look for the gatekeepers to lock us out, our imaginations know no gates. We don’t wait for the dust to settle on old debates. We’ve already made new ideas and new dust. Digital Humanities is about making a past future where all of human knowledge and creation is understood to benefit human future past. To achieve this goal it can’t duplicate the failure of humanities by excluding human women, Black humans, Gay humans, Transgender humans, Asian humans, Queer humans, differently able humans. Digital humanities must first recognize humanness before it can code and create visualizations of a past future or future past. Afrofuturism can help. This talk focuses on the ways in which Afrofuturism and Digital Humanities can come together to bring their inherent creative theories, methods and applications together to engage in an equitable discourse to changes the future of the humanities. I argue that born out of these collaborations could be new approaches which will engage knowledge productions in the areas of the humanities often relegated to “area studies” and recenter those contributions within their equitable portion of human knowledge. Toniesha Taylor is an Associate Professor of Communication in the Department Languages and Communication at Prairie View A &amp; M University. She engages with discussions on womanist rhetoric as method and theory; practical social justice pedagogy for faculty and students; critical engagement in popular cultural critique; digital humanities methods implications for activist recovery projects; African American women’s sermons and conversion discourses both historic and contemporary. Her research projects include the Prairie View Women’s Oral History Project, designed to collect, preserve, curate and display the oral histories of women at Prairie View A&amp;M University, and “White Violence, Black Resistance,” with Dr. Amy Earhart, which seeks to digitize a broad set of primary documents related to interactions of race and power. Both projects can be found at https://sites.google.com/site/bkresist/."},{"id":"dh-workshop-introduction-to-docnow","title":"DH Workshop: Introduction to DocNow","author":"laura-miller","date":null,"categories":null,"url":"dh-workshop-introduction-to-docnow","layout":"events","content":"In honor of Endangered Data Week, join us for an introduction to DocNow, a tool and community developed around supporting the ethical collection, use, and preservation of social media content.  Scholars’ Lab staff and fellows will be on hand to provide a brief introduction to the tool, a discussion of data collection, and a consideration of the complex, often fraught, stories that can be told through social media data. All skill levels are welcome and encouraged to attend. Refreshments will be served!"},{"id":"dh-workshop-pelagios-commons","title":"DH Workshop: Pelagios Commons","author":"laura-miller","date":null,"categories":null,"url":"dh-workshop-pelagios-commons","layout":"events","content":"In conjunction with the Pelagios Commons, the UVA Library will be offering a two-day workshop on tools and strategies for working with geospatial data in the historical past.  This Cultural Heritage Moment is co-sponsored by the Library, the Scholars’ Lab, and IATH.  Please register in advance to ensure your seat."},{"id":"dhuva-conference-2016","title":"DH@UVA Conference 2016","author":"laura-miller","date":null,"categories":null,"url":"dhuva-conference-2016","layout":"events","content":"Please join us for two days of conversation and insights with the digital humanities community at UVa. On October 14 and 15, the DH community at UVa will come together for a multiplicity of conversations about what innovative digital work is happening here now. As our community of practice grows and evolves, we are taking a moment to reflect and exchange ideas about what can happen in the future. Join us for lighting talks, roundtables, and ongoing public dialogue. Invited speakers include Lauren Klein, Tanya Clement, and Chris Johanson. To learn more, please visit dh.virginia.edu/conference . DH@UVa 2016 is free and open to the public, but registration is required. It is initiated and sponsored by Ron Hutchins, Vice President for Information Technology, Archie Holmes, Associate Provost for Academic Affairs, Francesca Fiorani, Associate Dean for the Arts and Humanities, and John Unsworth, Dean of Libraries and University Librarian. It is organized by Alison Booth, Academic Director of the Scholars’ Lab; David Germano, Director of SHANTI; and Worthy Martin, Director of IATH, with the support of Judy Thomas, Senior Director of Academic Engagement, UVa Library."},{"id":"dhuva-speaker-series-matt-kirschenbaum","title":"DH@UVA Speaker Series: Matt Kirschenbaum","author":"laura-miller","date":null,"categories":null,"url":"dhuva-speaker-series-matt-kirschenbaum","layout":"events","content":"Greenscreeners: Locating the Literary History of Word Processing This event is free and open to the public, but please register to attend the DH@UVA reception to follow at 5:00 pm. Matthe w G. Kirschenbaum is Associate Professor in the Department of English at the University of Maryland and Associate Director of the Maryland Institute for Technology in the Humanities (MITH). He is also an affiliated faculty member with the College of Information Studies at Maryland, and a member of the teaching faculty at the University of Virginia’s Rare Book School. His most recent book, Track Changes: A Literary History of Word Processing   (Harvard UP, 2016), examines how the interests and ideals of creative authorship came to coexist with the computer revolution. It balances the stories of individual writers with a consideration of how the seemingly ineffable act of writing is always grounded in particular instruments and media, from quills to keyboards. Along the way, we discover the candidates for the first novel written on a word processor, explore the surprisingly varied reasons why writers of both popular and serious literature adopted the technology, trace the spread of new metaphors and ideas from word processing in fiction and poetry, and consider the fate of literary scholarship and memory in an era when the final remnants of authorship may consist of folders on a hard drive or documents in the cloud.  With Pat Harrigan, he has also co-edited the Zones of Control: Perspectives on Wargaming for the MIT Press, a volume containing nearly seventy contributions (2016).  In 2010 he co-authored (with Richard Ovenden and Gabriela Redwine) Digital Forensics and Born-Digital Content in Cultural Heritage Collections, a report published by the Council on Library and Information Resources and recognized with a commendation from the Society of American Archivists. His first book, Mechanisms: New Media and the Forensic Imagination, was published by the MIT Press in 2008 and won the 2009 Richard J. Finneran Award from the Society for Textual Scholarship (STS), the 2009 George A. and Jean S. DeLong Prize from the Society for the History of Authorship, Reading, and Publishing (SHARP), and the 16th annual Prize for a First Book from the Modern Language Association (MLA).  Kirschenbaum speaks and writes often on topics in the digital humanities and new media; his work has received coverage in the Atlantic, Slate, New York Times, The Guardian, National Public Radio, Wired, Boing Boing, Slashdot, and the Chronicle of Higher Education .  He is a 2011 Guggenheim Fellow . See http://www.mkirschenbaum.net for more."},{"id":"dhuva-workshop-network-analysis-with-scott-weingart","title":"DH@UVA Workshop: Network Analysis with Scott Weingart","author":"laura-miller","date":null,"categories":null,"url":"dhuva-workshop-network-analysis-with-scott-weingart","layout":"events","content":"Network Analysis Workshop with Scott Weingart (Carnegie Mellon University) Scott Weingart is a Digital Humanities Specialist at Carnegie Mellon University and a historian of science. Weingart will be leading a three-hour workshop on network analysis in Wilson 133 on Thursday, August 31st from 9am-12pm . Please bring your own laptop and, if at all possible, be sure Gephi is already installed. Weingart will introduce key concepts in network analysis and then lead us through a very hands-on workshop. Please RSVP with Chad Wellmon (mcw9d@virginia.edu) in order to ensure you spot and your food. No prior knowledge is assumed or necessary."},{"id":"digital-humanities-fellows-panel-luncheon","title":"Digital Humanities Fellows Panel & Luncheon","author":"laura-miller","date":null,"categories":null,"url":"digital-humanities-fellows-panel-luncheon","layout":"events","content":"As the semester winds down, we’d like to invite you to join us for our year-end Digital Humanities Graduate Student Fellows panel. Lunch will follow the presentation. Please plan to come for the presentation and stay for the refreshments! Nora Benedict is a PhD candidate (defended in February!) in the Department of Spanish, Italian &amp; Portuguese. Her dissertation, “The Fashioning of Jorge Luis Borges: Magazines, Books, and Print Culture in Argentina (1930-1951),” focuses on the marked presence of books (and book production) in this Argentine writer’s life by examining the physical features of his works, which she reads through the lens of analytical bibliography and material studies. As a Digital Humanities Fellow in the Scholars’ Lab, Nora has worked with Leaflet (JavaScript, GeoJSON), Jekyll (KML, CSV, YAML), and a few other out-of-the-box tools to develop a mapping and data visualization project that traces the role of publishers and printers in Jorge Luis Borges’s Argentina (1930-1951) and provides a digital archive of the records of the physical features of books from this moment and time in history. In light of the fact that there are virtually no extant publishers’ archives from the Argentine firms she has researched, she created her DH project as a way to give future scholars access to much of the raw data she created throughout the dissertation process and, hopefully, provide them with a resource to create projects of their own. Shane Lin is a Ph.D. candidate in the Corcoran Department of History. In support of his dissertation on the interconnected development of civilian encryption technology and digital privacy rights, Shane’s project analyzes Usenet conversations about cryptography and privacy between 1981 and 2000. Usenet was an early digital communications network, a key precursor to Web discussion forums and later online communities and social networks. In its heyday, it offered a genuinely vast sampling of public, potentially pseudonymous discussion organized neatly into hierarchies of “newsgroups” as diverse as alt.sex, rec.drugs, and alt.rock-n-roll. Over the course of his Digital Humanities Fellowship, Shane has written a set of software tools to scrape and process the raw data from select newsgroups, discover adjacent groups and key figures of influence, and map the flow of ideas and networks of interaction across domains and time. Leif Fredrickson is a PhD candidate in history. His dissertation, titled the “The Age of Lead,” examines the relationship between metropolitan development and environmental inequality. He is the Ambrose Monell Fellow in Technology and Democracy, Miller Center of Public Affairs. He was previously a Mellon Pre-Doctoral Fellow at the Digital Scholarship Lab at the University of Richmond, where he worked on the interactive historical mapping project American Panorama. As a fellow for the Scholar’s Lab, Leif has been developing a website, enviro-history.com, that will be host visualizations, writing and multi-media projects related to environmental history. He is using jekyll to the build the website, and has been working on several map visualizations for the website using ArcGIS and javascript map visualization libraries."},{"id":"digital-humanities-graduate-student-fellows-panel-luncheon","title":"Digital Humanities Graduate Student Fellows Panel & Luncheon","author":"laura-miller","date":null,"categories":null,"url":"digital-humanities-graduate-student-fellows-panel-luncheon","layout":"events","content":"Please join us for our Digital Humanities Graduate Student Fellows Panel and Luncheon! Panelists James Ambuske (History), Jennifer Foy (English), and Emily Senefeld (History) will briefly present their projects, discuss both the future directions of their projects and how their digital methodologies have shaped their analyses. Fellows will also share the ways digital humanities has influenced their current and future research and teaching. Come for the presentations and stay for the lunch!"},{"id":"digital-humanities-mixer-at-cafe-caturra","title":"Digital Humanities Mixer at Cafe Caturra","author":"laura-miller","date":null,"categories":null,"url":"digital-humanities-mixer-at-cafe-caturra","layout":"events","content":"Please join the Digital Humanities organizations on Grounds for delicious snacks, drinks, and fascinating conversation at our first DH@UVa Mixer.  This event is free, of course, but we ask that you register so that we can plan accordingly. When: Thursday, January 26, from 4-6\nWhere: Café Caturra on the University Corner We hope you’ll take this convivial occasion as an opportunity to extend your DH network and learn about others’ projects, methods, and resources. This mixer is sponsored by the Office of the Vice President for Information Technology ."},{"id":"digital-humanities-prototyping-fellows-presentation-2018-2019","title":"Digital Humanities Prototyping Fellows Presentation","author":"brandon-walsh","date":null,"categories":null,"url":"digital-humanities-prototyping-fellows-presentation-2018-2019","layout":"events","content":"Hope you had a restful holiday season! The Scholars’ Lab is getting back into the swing of things, and we wanted to invite you all to an upcoming presentation by our 2018-2019 Digital Humanities Prototyping Fellows. These students each received funding from us to work with on building out a proof-of-concept digital humanities project related to their research. The event will take place on Tuesday, January 22nd from 10:30-1:00PM in Alderman 421. The presentations will take place roughly from 10:30-12:00PM and lunch will follow for attendees. You’re free to come late or leave early as your schedule demands. Our presenters will be Alyssa Collins, Christian Howard, and Sarah McEleney . Alyssa and Christian are both PhD students in the English department, and they will be presenting on a joint project related to using Twitter data in literary research. Sarah is a PhD student in Slavic Languages and Literatures, and she will be presenting on a text analysis project analyzing socialist realism literature. Full abstracts and bios can be found below. We hope to see you there! Alyssa Collins and Christian Howard Our project, titled Twitterature: Methods and Metadata, seeks to outline a methodology for practically using Twitter data in literary research. Our project is interested in two specific aspects of Twitter: the use of Twitter as a publication platform and the collection of tweets into corpora based on keywords [based on hashtag communities?]. As a publication platform, Twitter unsettles traditional publication methods and reaches a wider audience that is not bound by national and linguistic borders. Twitter is thus creating a new kind of ephemeral, digitally-global literature. Additionally, we understand hashtags as metadata that allows us to collect various corpora related to specific places, events, or communities. We’re then able to analyze these communities for linguistic tricks influences. Our project uses Python to scrape Twitter, Twarc tools developed by DocNow to filter and organize the results, and tools such as ArcGIS and visual networking platforms to visualize the results. Alyssa Collins is a Ph.D. candidate in the English department of the University of Virginia and a 2016-17 Praxis fellow in the digital humanities. Her dissertation “Racing the Posthuman: Examining Representations of Technological and Virtual Embodiment” looks at the intersections of race and technology as depicted in 20th century and contemporary African American literature, digital culture, and new media. When she’s not writing her dissertation she writes about race, superheroes, and embodiment around the Internet. Christian Howard is a PhD Candidate in English at the University of Virginia and the current project manager for the DH@UVA Curriculum Development Team. Her dissertation, titled “Radical Translation: The Ethics of World Literature,” reconceptualizes world literature through advancements in digital humanities and explores how we can ethically connect to one another through online spaces. Sarah McEleney The state-prescribed genre of socialist realism defined, to a certain extent, mainstream literature in the Soviet Union, especially during the Stalin era.  Based on the idea that the main purpose of literature was to serve the state and promote communist ideology, Soviet writers were expected to embed the principles of ‘ideological commitment’ (ideinost’), ‘party-mindedness’ (partiinost’), and ‘popular spirit’ (narodnost’) into their socialist realist literary works.  In the harsh Stalinist political climate in the 1930s, adhering to socialist realist doctrine became almost mandatory for writers working with the state publishing houses and journals, particularly after the First Congress of Soviet Writers in 1934, which declared socialist realism as the official genre for Soviet writers.  However, definitions of socialist realism were often vague and varied, with interpretations differing among assorted critics and authorities.  Thus, pinpointing the distinct characteristics of socialist realist is not an easy task, as it was a dynamic genre that changed over Soviet history and was manifested differently in the work of different Soviet authors. With this is mind, can a modern text analysis/natural language process approach to socialist realism retrospectively shed new light on the attributes of this uniquely Soviet genre? What new information can topic modeling and text analysis approaches offer regarding the particularities of socialist realist prose in the early Soviet Union, and can this tell us something about the nature of ideologically-motivated literature and early Soviet cultural history? This project utilized Python and various available libraries within it to conduct an analysis of the linguistic and topical attributes of early Soviet socialist realist literature, with particular emphasis on Stalin-era literature. Sarah McEleney is Ph.D. candidate in Slavic Languages and Literatures at The University of Virginia. Her interests include the history of literature in the Soviet Union, digital humanities and data science approaches to the study of literature, and the Polish, Russian, and Ukrainian languages.  She is a prior Praxis fellow in the Scholars’ Lab and recipient of the Presidential Fellowship in Data Science at The University of Virginia."},{"id":"digital-yoknapatawpha-collaboratively-recreating-faulkners-county","title":"Digital Yoknapatawpha: Collaboratively Recreating Faulkner's County","author":"laura-miller","date":null,"categories":null,"url":"digital-yoknapatawpha-collaboratively-recreating-faulkners-county","layout":"events","content":"Over three dozen Faulknerians and technologists are collaborating on Digital Yoknapatawpha, a project to provide scholars and students with new modes of access to Faulkner’s mythical kingdom.  This panel discussion with project directors Stephen Railton and Worthy Martin, moderated by Alison Booth of the English Department and the Scholars’ Lab, will briefly look at the project and explore the various issues that arise with doing digital humanities. Refreshments will be available."},{"id":"easy-electronics-arduino-for-the-beginner","title":"Makerspace Workshop: Easy Electronics - Arduino for the Beginner","author":"laura-miller","date":null,"categories":null,"url":"easy-electronics-arduino-for-the-beginner","layout":"events","content":"Come learn the basics of using the Arduino for fun or profit. No experience or equipment needed. Arduino kits and laptops provided to use in class, but you are welcome to bring your own. This workshop will go through the very basics of electricity, how to setup the Arduino, and building a first circuit; an LED nightlight."},{"id":"fall-prototyping-fellows-presentation-and-lunch","title":"Fall Prototyping Fellows Presentation and Lunch","author":"laura-miller","date":null,"categories":null,"url":"fall-prototyping-fellows-presentation-and-lunch","layout":"events","content":"Please join us for presentations by our Fall 2017 DH Prototyping Fellows! During the course of their fellowships, these graduate students worked with the Scholars’ Lab R&amp;D team to rapidly develop their project ideas and position themselves for future work in digital humanities. Their cutting-edge work draws together approaches in GIS, sound and spatial analysis, towards the creation of digitally immersive experiences.  Lunch will follow. R. Benjamin Gorham, DH Prototyping Fellow Ben is a doctoral candidate in the Mediterranean Art and Archaeology program of the McIntire Department of Art and Architectural History, completing his dissertation on non-elite houses and urbanism at Pompeii under the guidance of Professor John Dobbins. A lifelong student of Classical languages, history, culture, and art, Ben took his BA in Classics from UNC-Chapel Hill in 2008 and his MA in Classical Archaeology from the University of Arizona in 2012. Over the past decade Ben has worked on excavations throughout Italy, including the Etruscan site of Poggio Civitate, the Roman cities of Pompeii and Ostia, as well as the Hellenistic city of Morgantina in Sicily. He is also one of the founding members of the Ostia Connectivity Project, examining social connectivity of the cult landscape of Rome’s port city, and has worked with the Harvard Yard Archaeological Project, building 3D models of active trenches to be used by undergraduates both in the classroom and in Augmented Reality platforms on campus. At UVA, Ben is a Jefferson Fellow of Art History, and helps represents the University of Virginia at the Archaeological Institute of America as a member of the Geospatial Interest Group. Ben’s Project Incubator Fellowship is a two-parter. In pursuit of rounding out his dissertation work, Ben has transformed his GIS data and cartography of Pompeii into a webmap through consultation with our GIS specialists. This map details the long-overlooked number of non-elite, or middle- and lower-class houses which made up the lion’s share of the city’s urban composition, and identifies never-before-seen patterns of clustering and dispersal that indicate discrete neighborhoods throughout the ancient city. In pursuit of his excavation work and aided by the expertise of the Scholars’ Lab’s own 3D specialists, Ben has also built a Virtual Reality experience of the agora at Morgantina, drawing on the work of multiple teams on site to recreate the city’s marketplace as it would have been in the 4th Century BCE. This platform provides an innovative space in which to visualize the ancient world and embed short lessons about archaeology and Greek life for students and amateurs alike. Joseph Thompson, DH Prototyping Fellow Joey Thompson is a PhD candidate in 20th Century U.S. history. His dissertation, “Sounding Southern: Music, Militarism, and the Making of the Sunbelt,” uses popular music to analyze the cultural and political impact of the military-industrial complex on the U.S. South after World War II. He argues that music offers a way to hear people working out the meanings of race, politics, and region in response to the South’s economic reliance on defense spending as it transformed pockets of the Deep South and Southwest into islands of white Sunbelt prosperity. “Sounding Southern” follows a variety of performers, from the nationalistic themes of Cold War country music singers to the revolutionary politics of Gil Scott-Heron, to reveal how different artists and genres have voiced ways for audiences to imagine their relationship to the defense spending strategies that remade the nation’s political culture in the late twentieth century. Through the Digital Humanities Prototyping Fellowship, he has experimented with ways to integrate sound and spatial analysis for public history projects. This work links visualizations of sound wave files to specific points on a map in order to direct users’ attention to the geographies of popular song. The project also integrates images of archival materials in order to triangulate between sound, space, and the material record and create an immerse experience through a digital platform."},{"id":"feminist-dhuva-collaboration","title":"Feminist DH@UVA: A Collaboration","author":"laura-miller","date":null,"categories":null,"url":"feminist-dhuva-collaboration","layout":"events","content":"The Scholars’ Lab and UVA Library are pleased to bring to Grounds four leaders of literary digital projects focusing on a diverse range of women writers across history. These four different digital projects share innovative ways to study networks of biographical records, including little-known early modern or eighteenth-century women writers, or all women writers associated with Britain, or women of many occupations across history, as represented circa 1830-1940. The projects have theorized and reflected on the argument and scholarship that go into ontologies, editorial practice, and database and visualization designs. Research collaborators and users of the projects can follow different paths into the archives of text or the personographies. Data visualization; the Text Encoding Initiative in a publishing and repository context; documenting the circulation of reviews, citations, and texts; tagging trends in gender ideology and typologies within the changing features of women’s biographies; and textual analysis are among the approaches we demonstrate and question in our practice. Please join us for a full day of presentations as well as frank discussion of the challenges and rewards of four different approaches. All are welcome. Susan Brown, Professor of English, University of Guelph, Canada Research Chair in Collaborative Digital Scholarship; Director of Orlando and Canadian Writing Research Collaboratory Julia Flanders, Professor of the Practice of English, Northeastern University; Director, Digital Scholarship Group; Director, Women Writers Project Laura Mandell, Professor of English, Texas A &amp; M University; Director, Initiative for Digital Humanities, Media, and Culture; ARC (Applied Research Consortium); The Poetess Archive Alison Booth, Professor of English, University of Virginia, and Academic Director of the Scholars’ Lab; Director of Collective Biographies of Women  in collaboration with Rennie Mapp, Worthy Martin, Daniel Pitti, Jeremy Boggs, and many others Schedule:\n8:30-9:30    Coffee and pastries\n9:30-10:15   Susan Brown, Linking: Feminism, DH and Infrastructure 10:30-11:15  Julia Flanders, Open-Access Data from the Women Writers Project 11:30-12:15  Laura Mandell, Visualization and Gender Discovery LUNCH\n2:00-2:45    Alison Booth, Can the Typologies Speak?: Problems for Feminist Biographical Networks 3:00-4:30   Discussion Reception to follow from 5:00-7:00 in the Rotunda, Multipurpose Room."},{"id":"feminist-dhuva-symposium-2","title":"Feminist DH@UVA II: Intersectionality and Doing Justice to/with Data","author":"elizabeth-mitchell","date":null,"categories":null,"url":"feminist-dhuva-symposium-2","layout":"events","content":"A symposium at the Scholars’ Lab, University of Virginia Library 8:30-9:30am  Coffee, Light Breakfast 9:30  Introductions 10:00-11:00 Marlene Daut, Associate Director, Carter G. Woodson Institute &amp; Associate Professor, African and African American Studies, University of Virginia; “Digital Print Culture and Accessibility: The Case for Haiti” Moderator: Deborah McDowell, Alice Griffin Professor of English; Director, Carter G. Woodson Institute Break 11:30-12:30 Elizabeth Losh, Associate Professor of English and American Studies, William &amp; Mary; “Little Data and the Trump Administration: Feminist Close Reading and Game Design.” Moderator: Kevin Driscoll, Assistant Professor, Media Studies 12:30-1:30  Lunch (buffet) 1:30-2:30 Catherine Knight Steele, Assistant Professor, Department of Communication, University of Maryland; Project Director of African American History, Culture and Digital Humanities (AADHum) ; “My Feminism is Black and Digital” Moderator: Meredith Clark, Assistant Professor, Media Studies Break 2:30-3:30 Jacqueline Wernimont, Distinguished Chair of Digital Humanities and Social Engagement &amp; Associate Professor of Women’s, Gender, and Sexuality Studies, Dartmouth College ; “Early Data Bodies and Anglo-American Empire” Moderator: TBD 3:30-4:30 Roundtable: Alison Booth, Academic Director, Scholars’ Lab, and Professor of English, UVA, with previous speakers and others 4:30pm Reception Please contact Alison Booth or Laura Miller in the Scholars’ Lab for more information. This event is sponsored by the UVA Library’s Scholars‘ Lab, with generous support by the Carter G. Woodson Institute, Citizen Justice Initiative; Women, Gender, and Sexuality Department; the English Department; IHGC’s Humanities Informatics Lab and Network/Corpus Research Group."},{"id":"feminist-dhuva-working-session-on-project-collaboration","title":"Feminist DH@UVA: Working Session on Project Collaboration","author":"laura-miller","date":null,"categories":null,"url":"feminist-dhuva-working-session-on-project-collaboration","layout":"events","content":"Morning refreshments and catered lunch will be served. Please RSVP to scholarslab@virginia.edu to reserve your seat."},{"id":"gis-day-2017","title":"GIS Day 2017 Celebration","author":"laura-miller","date":null,"categories":null,"url":"gis-day-2017","layout":"events","content":"  Mark your calendars! Wednesday, Nov. 15 is GIS Day ( http://www.gisday.com/ ). To celebrate, all are invited to the University of Virginia Library’s Scholars’ Lab for an afternoon of events. 1PM – Presentation: ArcGIS Pro for ArcMap Users\n2PM – Lightning Round Talks\n3PM – GIS Day Cake Location: Alderman Library Electronic Classroom (ALD 421, just off Scholars’ Lab) Presentation: ArcGIS Pro for ArcMap Users At 1PM, join us for a session on making the switch to ArcGIS Pro by our own Drew Macqueen. This is a quick and dirty overview of the major differences between ArcMap and ArcGIS Pro. Accept your fate as we delve into the future of desktop GIS. Spoiler alert, it’s totally worth it! Lighting Talks Starting at 2PM, our annual tradition of lightning talks continues. If you have never seen lightning round talks, they can be pretty entertaining: a rapid fire succession of speakers given a set, short amount of time and PowerPoint slides. In previous years, many great presenters have shown the incredible breadth of disciplines and fields in which GIS is used in meaningful ways. We encourage everyone, including students (UVa, PVCC and high school), researchers and practitioners in the greater Charlottesville community to contribute. In this year’s round, each speaker will be given five to ten minutes (depending on number of presenters) with a maximum of ten slides. It is a fairly easy task to create and give a lighting round talk. Help make this year’s event special by participating in the talks. You can present on anything spatially related you like. It could be about a project you have worked on, things going on at your office or just something of personal interest. If you have any interest in participating in the lightning round talks, please email us at uvagis@virginia.edu as soon as possible. GIS Day Cake Another great tradition continues. Please join us for the GIS Day cake unveiling and partake in the feeding frenzy."},{"id":"gis-day","title":"GIS Day 2014","author":"laura-miller","date":null,"categories":null,"url":"gis-day","layout":"events","content":" The Scholars’ Lab celebrates GIS Day 2014!  We’ll host lighting round talks - showcasing a variety of GIS projects from across Grounds and around Charlottesville - from 2:00 - 4:00 in the Scholars’ Lab Common Room.  The reveal of our annual GIS Day cake will follow. All are welcome. Please join us for the festivities!"},{"id":"gis-workshop-2","title":"GIS Workshop - Points on Your Map","author":"chris-gist","date":null,"categories":null,"url":"gis-workshop-2","layout":"events","content":"Points on Your Map: Street Addresses and More Spatial Things Do you have a list of street addresses crying out to be mapped?  Have a list of zip codes or census tracts you wish to associate with other data?  We’ll start with addresses and other things spatial and end with points on a map, ready for visualization and analysis."},{"id":"gis-workshop-2019-spring-arcgis-online-data-collection-tools","title":"Using ArcGIS Online Data Collection Tools","author":"amanda-visconti","date":null,"categories":null,"url":"gis-workshop-2019-spring-arcgis-online-data-collection-tools","layout":"events","content":"Whether you are crowd sourcing spatial data or performing survey work, having applications that automatically record location and upload data directly to a mapping application is incredibly useful. All sessions are one hour and assume participants have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials and expert assistance. All sessions will be taught on Wednesdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community. No registration, just show up!"},{"id":"gis-workshop-2019-spring-arcgis-online-story-maps","title":"Using ArcGIS Online Story Maps","author":"amanda-visconti","date":null,"categories":null,"url":"gis-workshop-2019-spring-arcgis-online-story-maps","layout":"events","content":"Story Maps are templates that allow authors to give context to their ArcGIS Online maps.  Whether telling a story, giving a tour or comparing historic maps, Esri Story Maps are easy-to-use applications that create polished presentations. All sessions are one hour and assume participants have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials and expert assistance. All sessions will be taught on Wednesdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community. No registration, just show up!"},{"id":"gis-workshop-2019-spring-arcgis-online-web-appbuilder","title":"Using ArcGIS Online Web AppBuilder ","author":"amanda-visconti","date":null,"categories":null,"url":"gis-workshop-2019-spring-arcgis-online-web-appbuilder","layout":"events","content":"Would you like to make a custom online mapping application without have to code?  Us too. ArcGIS Web AppBuilder allows developers to use drag and drop tools to create responsive mapping applications. All sessions are one hour and assume participants have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials and expert assistance. All sessions will be taught on Wednesdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community. No registration, just show up!"},{"id":"gis-workshop-2019-spring-creating-editing-spatial-data","title":"Creating and Editing Spatial Data (ArcGIS Pro)","author":"amanda-visconti","date":null,"categories":null,"url":"gis-workshop-2019-spring-creating-editing-spatial-data","layout":"events","content":"Until we perfect that magic “extract all those lines from this paper map” button we’re stuck using editor tools to get that job done.  If you’re lucky, someone else has done the work to create your points, lines, and polygons but maybe they need your magic touch to make them better.  This session shows you how to create and modify vector features in ArcGIS Pro.  We’ll explore tools to create new points, lines, and polygons and to edit existing datasets. All sessions are one hour and assume participants have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials and expert assistance. All sessions will be taught on Wednesdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community. No registration, just show up!"},{"id":"gis-workshop-2019-spring-georeferencing-a-map","title":"Georeferencing a Map: Putting Old Maps and Aerial Photos on Your Map (ArcGIS Pro)","author":"amanda-visconti","date":null,"categories":null,"url":"gis-workshop-2019-spring-georeferencing-a-map","layout":"events","content":"Would you like to see historic map overlaid on modern aerial photography?  Do you need to extract features of a map for use in GIS?  Georeferencing is the first step.  We will show you how to take a scan of a paper map and align in it in ArcGIS. All sessions are one hour and assume participants have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials and expert assistance. All sessions will be taught on Wednesdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community. No registration, just show up!"},{"id":"gis-workshop-2019-spring-intro-arcgis-online-makeup1","title":"Introduction to ArcGIS Online: Snow Day Makeup","author":"drew-macqueen","date":null,"categories":null,"url":"gis-workshop-2019-spring-intro-arcgis-online-makeup1","layout":"events","content":"With ArcGIS Online, you can use and create maps and scenes, access ready-to-use maps, layers and analytics, publish data as web layers, collaborate and share, access maps from any device, make maps with your Microsoft Excel data, and customize the ArcGIS Online website. All sessions are one hour and assume participants have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials and expert assistance. All sessions will be taught on Wednesdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community. No registration, just show up!"},{"id":"gis-workshop-2019-spring-intro-arcgis-online-makeup2","title":"Introduction to ArcGIS Online: 2nd Snow Day Makeup","author":"drew-macqueen","date":null,"categories":null,"url":"gis-workshop-2019-spring-intro-arcgis-online-makeup2","layout":"events","content":"With ArcGIS Online, you can use and create maps and scenes, access ready-to-use maps, layers and analytics, publish data as web layers, collaborate and share, access maps from any device, make maps with your Microsoft Excel data, and customize the ArcGIS Online website. All sessions are one hour and assume participants have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials and expert assistance. All sessions will be taught on Wednesdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community. No registration, just show up!"},{"id":"gis-workshop-2019-spring-intro-arcgis-online","title":"Introduction to ArcGIS Online","author":"amanda-visconti","date":null,"categories":null,"url":"gis-workshop-2019-spring-intro-arcgis-online","layout":"events","content":"With ArcGIS Online, you can use and create maps and scenes, access ready-to-use maps, layers and analytics, publish data as web layers, collaborate and share, access maps from any device, make maps with your Microsoft Excel data, and customize the ArcGIS Online website. All sessions are one hour and assume participants have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials and expert assistance. All sessions will be taught on Wednesdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community. No registration, just show up!"},{"id":"gis-workshop-2019-spring-street-addresses-on-map","title":"Getting Street Addresses on a Map (ArcGIS Pro)","author":"amanda-visconti","date":null,"categories":null,"url":"gis-workshop-2019-spring-street-addresses-on-map","layout":"events","content":"Do you have a list of street addresses crying out to be mapped?  Have a list of zip codes or census tracts you wish to associate with other data?  We’ll start with addresses and other things spatial and end with points on a map, ready for visualization and analysis. All sessions are one hour and assume participants have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials and expert assistance. All sessions will be taught on Wednesdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community. No registration, just show up!"},{"id":"gis-workshop-arcgis-online-data-collection","title":"GIS Workshop: ArcGIS Online: Data Collection","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-arcgis-online-data-collection","layout":"events","content":"Whether you are crowd sourcing spatial data or performing survey work, having applications that automatically record location and upload data directly to a mapping application is incredibly useful. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be held on  Tuesdays from 10AM to 11AM in the Alderman Electronic Classroom, ALD 421  (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up!"},{"id":"gis-workshop-arcgis-online-introduction","title":"GIS Workshop: ArcGIS Online: Introduction","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-arcgis-online-introduction","layout":"events","content":"With ArcGIS Online, you can use and create maps and scenes, access ready-to-use maps, layers and analytics, publish data as web layers, collaborate and share, access maps from any device, make maps with your Microsoft Excel data, and customize the ArcGIS Online website. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be held on  Tuesdays from 10AM to 11AM in the Alderman Electronic Classroom, ALD 421  (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up!"},{"id":"gis-workshop-arcgis-online-spatial-analysis","title":"GIS Workshop: ArcGIS Online: Spatial Analysis","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-arcgis-online-spatial-analysis","layout":"events","content":"ArcGIS Online now has spatial analysis tools that can be easier to use than similar desktop GIS tools.  Come learn how to use the simple yet powerful analysis tools available through ArcGIS Online. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be held on  Tuesdays from 10AM to 11AM in the Alderman Electronic Classroom, ALD 421  (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up!"},{"id":"gis-workshop-arcgis-online-story-maps","title":"GIS Workshop: ArcGIS Online: Story Maps","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-arcgis-online-story-maps","layout":"events","content":"Story Maps are templates that allow authors to give context to their ArcGIS Online maps.  Whether telling a story, giving a tour or comparing historic maps, Esri Story Maps are easy-to-use applications that create polished presentations. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be held on  Tuesdays from 10AM to 11AM in the Alderman Electronic Classroom, ALD 421  (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up!"},{"id":"gis-workshop-collecting-your-own-spatial-data","title":"GIS Workshop: Collecting Your Own Spatial Data","author":"jeremy-boggs","date":null,"categories":null,"url":"gis-workshop-collecting-your-own-spatial-data","layout":"events","content":"Research projects often rely on fieldwork to build new datasets. In this workshop we’ll focus on tools for spatial data collection. First we’ll take a quick look behind the curtain to see how GPS really works and how to use that knowledge to our advantage. Then we’ll evaluate free or low-cost options to gather locations and associated attributes using handheld GPS devices, smartphones, and apps. This workshop will introduce you to a range of devices and methods for mobile spatial data collection. All sessions are one hour and assume attendees have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials with expert assistance. All sessions will be taught in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community."},{"id":"gis-workshop-diy-aerial-photography-ii","title":"GIS Workshop: DIY Aerial Photography II","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-diy-aerial-photography-ii","layout":"events","content":"In this special session, we will spend a couple hours outside collecting aerial data from various platforms. All sessions are one hour and assume attendees have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials with expert assistance.  All sessions, except where noted, will be taught on Wednesdays from 1PM to 2PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community."},{"id":"gis-workshop-diy-aerial-photography","title":"GIS Workshop: DIY Aerial Photography","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-diy-aerial-photography","layout":"events","content":"Want to do your own aerial photography?  We will discuss techniques, equipment, and issues of capturing your own data. All sessions are one hour and assume attendees have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials with expert assistance.  All sessions, except where noted, will be taught on Wednesdays from 1PM to 2PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community."},{"id":"gis-workshop-easy-demographics-2","title":"GIS Workshop: Easy Demographics","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-easy-demographics-2","layout":"events","content":"Need to make a quick demographic map or religious adherence? This workshop will show you how easily navigate Social Explorer. This powerful online application makes it easy to create maps with contemporary and historic census data and religious information. All sessions are one hour and assume participants have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials and expert assistance. All sessions will be taught on Thursdays from 3PM to 4PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  "},{"id":"gis-workshop-easy-demographics-3","title":"GIS Workshop: Easy Demographics","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-easy-demographics-3","layout":"events","content":"Need to make a quick demographic map or religious adherence? This workshop will show you how easily navigate Social Explorer. This powerful online application makes it easy to create maps with contemporary and historic census data and religious information. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be held on Tuesdays from 3PM to 4PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up!  "},{"id":"gis-workshop-easy-demographics-4","title":"GIS Workshop: Easy Demographics","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-easy-demographics-4","layout":"events","content":"Need to make a quick demographic map or religious adherence?  This workshop will show you how easily navigate Social Explorer.  This powerful online application makes it easy to create maps with contemporary and historic census data and religious information. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be taught on  Wednesdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421  (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up! For the full schedule of the Fall 2018 UVa Library GIS Workshop Series, see here ."},{"id":"gis-workshop-easy-demographics","title":"GIS Workshop: Easy Demographics","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-easy-demographics","layout":"events","content":"** Need to make a quick demographic map or religious adherence?  This workshop will show you how easily navigate Social Explorer.  This powerful online application makes it easy to create maps with contemporary and historic census data and religious information. All sessions are one hour and assume attendees have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials with expert assistance.  All sessions, except where noted, will be taught on Wednesdays from 1PM to 2PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community."},{"id":"gis-workshop-expanding-content-in-arcgis-online","title":"GIS Workshop: Expanding Content in ArcGIS Online","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-expanding-content-in-arcgis-online","layout":"events","content":"Expanding Content in ArcGIS Online You can also use ArcGIS Online as a platform to build custom location-based apps.  You can create stories and context around online maps for things like storytelling, tours or map comparisons.   Many of these applications have templates that make for easy viewing on mobile devices. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be held on Tuesdays from 3PM to 4PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up!"},{"id":"gis-workshop-georeferencing-a-map-2","title":"GIS Workshop - Georeferencing a Map","author":"chris-gist","date":null,"categories":null,"url":"gis-workshop-georeferencing-a-map-2","layout":"events","content":"Georeferencing a Map - Putting Old maps and Aerial Photos on Your Map Would you like to see historic map overlaid on modern aerial photography?  Do you need to extract features of a map for use in GIS?  Georeferencing is the first step.  We will show you how to take a scan of a paper map and align in it in ArcGIS."},{"id":"gis-workshop-georeferencing-a-map-putting-old-maps-and-aerial-photos-on-your-map-2","title":"GIS Workshop:  Georeferencing a Map - Putting Old Maps and Aerial Photos on Your Map","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-georeferencing-a-map-putting-old-maps-and-aerial-photos-on-your-map-2","layout":"events","content":"Would you like to see historic map overlaid on modern aerial photography? Do you need to extract features of a map for use in GIS? Georeferencing is the first step. We will show you how to take a scan of a paper map and align in it in ArcGIS. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be held on Tuesdays from 3PM to 4PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up!"},{"id":"gis-workshop-georeferencing-a-map-putting-old-maps-and-aerial-photos-on-your-map-3","title":"GIS Workshop: Georeferencing a Map – Putting Old Maps and Aerial Photos on Your Map","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-georeferencing-a-map-putting-old-maps-and-aerial-photos-on-your-map-3","layout":"events","content":"Would you like to see historic map overlaid on modern aerial photography?  Do you need to extract features of a map for use in GIS?  Georeferencing is the first step.  We will show you how to take a scan of a paper map and align in it in ArcGIS. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be taught on  Wednesdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421  (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up! For the full schedule of the Fall 2018 UVa Library GIS Workshop Series, see here ."},{"id":"gis-workshop-georeferencing-a-map-putting-old-maps-and-aerial-photos-on-your-map","title":"GIS Workshop:  Georeferencing a Map - Putting Old Maps and Aerial Photos on Your Map","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-georeferencing-a-map-putting-old-maps-and-aerial-photos-on-your-map","layout":"events","content":"Would you like to see historic map overlaid on modern aerial photography? Do you need to extract features of a map for use in GIS? Georeferencing is the first step. We will show you how to take a scan of a paper map and align in it in ArcGIS. All sessions are one hour and assume participants have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials and expert assistance. All sessions will be taught on Thursdays from 3PM to 4PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community."},{"id":"gis-workshop-georeferencing-a-map","title":"GIS Workshop: Georeferencing a Map","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-georeferencing-a-map","layout":"events","content":"Would you like to see historic map overlaid on modern aerial photography?  Do you need to extract features of a map for use in GIS?  Georeferencing is the first step.  We will show you how to take a scan of a paper map and align in it in ArcGIS. All sessions are one hour and assume attendees have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials with expert assistance. All sessions will be taught in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community."},{"id":"gis-workshop-georeferencing-putting-old-maps-and-aerial-photos-on-your-map","title":"GIS Workshop: Georeferencing – Putting Old Maps and Aerial Photos on Your Map","author":"jeremy-boggs","date":null,"categories":null,"url":"gis-workshop-georeferencing-putting-old-maps-and-aerial-photos-on-your-map","layout":"events","content":"Have an old map or an aerial photograph that you would like to use as a spatial layer? This session will teach you techniques to properly place your data and make it useable in GIS software. We will also demo similar techniques for Google Earth. All sessions are one hour and assume attendees have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials with expert assistance. All sessions will be taught in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community."},{"id":"gis-workshop-getting-your-data-on-a-map-2","title":"GIS Workshop: Getting Your Data on a Map","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-getting-your-data-on-a-map-2","layout":"events","content":"Do you have a list of Lat/Lon coordinates or addresses you would like to see on a map?  We will show you how to do just that.  Through ArcGIS’s Add XY data tool and Geocoding (address matching), it is easy to take your tabular lists and generate points on a map. All sessions are one hour and assume attendees have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials with expert assistance. All sessions will be taught in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community."},{"id":"gis-workshop-getting-your-data-on-a-map-3","title":"GIS Workshop: Getting Your Data on a Map","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-getting-your-data-on-a-map-3","layout":"events","content":"Do you have a list of Lat/Lon coordinates or addresses you would like to see on a map? We will show you how to do just that. Through ArcGIS’s Add XY data tool and Geocoding (address matching), it is easy to take your tabular lists and generate points on a map. All sessions are one hour and assume participants have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials and expert assistance. All sessions will be taught on Thursdays from 3PM to 4PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community."},{"id":"gis-workshop-getting-your-data-on-a-map-4","title":"GIS Workshop - Getting Your Data on a Map","author":"chris-gist","date":null,"categories":null,"url":"gis-workshop-getting-your-data-on-a-map-4","layout":"events","content":"Getting Your Data on a Map Do you have a list of Lat/Lon coordinates or addresses you would like to see on a map?  We will show you how to do just that.  Through ArcGIS’s Add XY data tool and Geocoding (address matching), it is easy to take your tabular lists and generate points on a map."},{"id":"gis-workshop-getting-your-data-on-a-map-5","title":"GIS Workshop: Getting Your Data on a Map","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-getting-your-data-on-a-map-5","layout":"events","content":"Getting Your Data on a Map Do you have a list of Lat/Lon coordinates or addresses you would like to see on a map?  We will show you how to do just that.  Through ArcGIS’s Add XY data tool and Geocoding (address matching), it is easy to take your tabular lists and generate points on a map. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be held on Tuesdays from 3PM to 4PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up!"},{"id":"gis-workshop-getting-your-data-on-a-map-6","title":"GIS Workshop: Getting Your Data on a Map","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-getting-your-data-on-a-map-6","layout":"events","content":"Do you have a list of Lat/Lon coordinates or addresses you would like to see on a map?  We will show you how to do just that.  Through ArcGIS’s Add XY data tool and Geocoding (address matching), it is easy to take your tabular lists and generate points on a map. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be taught on  Wednesdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421  (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up! For the full schedule of the Fall 2018 UVa Library GIS Workshop Series, see here ."},{"id":"gis-workshop-getting-your-data-on-a-map","title":"GIS Workshop: Getting Your Data on a Map","author":"jeremy-boggs","date":null,"categories":null,"url":"gis-workshop-getting-your-data-on-a-map","layout":"events","content":"Do you have a list of Lat/Lon coordinates or addresses you would like to see on a map?  We will show you how to do just that.  Through ArcGIS’s Add XY data tool and Geocoding (address matching), it is easy to take your tabular lists and generate points on a map. All sessions are one hour and assume attendees have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials with expert assistance. All sessions will be taught in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community."},{"id":"gis-workshop-historic-census-data-2","title":"GIS Workshop: Historic Census Data","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-historic-census-data-2","layout":"events","content":"Would you like to map the poverty in Philadelphia around the turn of the 20th Century? How about a racial breakdown by state in the 1860s? This workshop will focus on how to download historic census boundary and tabular data to make historic demographic maps. All sessions are one hour and assume participants have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials and expert assistance. All sessions will be taught on Thursdays from 3PM to 4PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community."},{"id":"gis-workshop-historic-census-data","title":"GIS  Workshop: Historic Census Data","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-historic-census-data","layout":"events","content":"Would you like to map the poverty in Philadelphia around the turn of the 20th Century?  How about a racial breakdown by state in the 1860s?  This workshop will focus on how to download historic census boundary and tabular data to make historic demographic maps. All sessions are one hour and assume attendees have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials with expert assistance. All sessions will be taught in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community."},{"id":"gis-workshop-introduction-to-arcgis-online-2","title":"GIS Workshop: Introduction to ArcGIS Online","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-introduction-to-arcgis-online-2","layout":"events","content":"With ArcGIS Online, you can use and create maps and scenes, access ready-to-use maps, layers and analytics, publish data as web layers, collaborate and share, access maps from any device, make maps with your Microsoft Excel data, customize the ArcGIS Online website, and view status reports. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be held on Tuesdays from 3PM to 4PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up!"},{"id":"gis-workshop-introduction-to-arcgis-online-3","title":"GIS Workshop: Introduction to ArcGIS Online","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-introduction-to-arcgis-online-3","layout":"events","content":"With ArcGIS Online, you can use and create maps and scenes, access ready-to-use maps, layers and analytics, publish data as web layers, collaborate and share, access maps from any device, make maps with your Microsoft Excel data, customize the ArcGIS Online website, and view status reports. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be taught on  Wednesdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421  (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up! For the full schedule of the Fall 2018 UVa Library GIS Workshop Series, see here ."},{"id":"gis-workshop-introduction-to-arcgis-online","title":"GIS Workshop: Introduction to ArcGIS Online","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-introduction-to-arcgis-online","layout":"events","content":"With ArcGIS Online, you can use and create maps and scenes, access ready-to-use maps, layers and analytics, publish data as web layers, collaborate and share, access maps from any device, make maps with your Microsoft Excel data, customize the ArcGIS Online website, and view status reports. You can also use ArcGIS Online as a platform to build custom location-based apps. All sessions are one hour and assume participants have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials and expert assistance. All sessions will be taught on Thursdays from 3PM to 4PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community."},{"id":"gis-workshop-introduction-to-arcgis-pro","title":"GIS Workshop: Introduction to ArcGIS Pro","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-introduction-to-arcgis-pro","layout":"events","content":"The handwriting is on the wall.  ArcGIS Pro will be replacing ArcMap as the desktop GIS in the near future.  Come learn what’s the same and what’s new, and how this powerful new tool incorporates seamlessly with ArcGIS Online and ESRI’s cloud-based mapping tools. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be taught on  Wednesdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421  (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up! For the full schedule of the Fall 2018 UVa Library GIS Workshop Series, see here ."},{"id":"gis-workshop-introduction-to-arcgis-pro2","title":"GIS Workshop: Introduction To ArcGIS Pro","author":"Laura-Miller","date":null,"categories":null,"url":"gis-workshop-introduction-to-arcgis-pro2","layout":"events","content":"Here’s your chance to get started with geographic information systems software in a friendly, jargon-free environment.  This workshop introduces the skills you need to make your own maps.  Along the way, you’ll get a taste of the latest software suite from Earth’s most popular GIS company and a gentle introduction to cartography. You’ll leave with your own cartographic masterpieces and tips for learning more in your pursuit of mappiness. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be taught on  Wednesdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421  (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up!"},{"id":"gis-workshop-introduction-to-qgis","title":"GIS Workshop: Introduction to QGIS","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-introduction-to-qgis","layout":"events","content":"ArcGIS isn’t the only game in town.  The best and most popular open source GIS application is QGIS.  It runs on most platforms and does some things better than ArcGIS.  Come learn more about another tool in the GIS toolbox. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be held on  Tuesdays from 10AM to 11AM in the Alderman Electronic Classroom, ALD 421  (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up!"},{"id":"gis-workshop-learning-old-school-mapping-techniques","title":"GIS Workshop: Learning Old-School Mapping Techniques","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-learning-old-school-mapping-techniques","layout":"events","content":"How did folks make maps before GPS and satellite imagery?  In this workshop we’ll focus on plane table mapping.  Using just a flat surface, a sheet of paper, a straight edge, and a pencil we’ll learn techniques to create accurate maps for large geographic areas.   With plane table mapping, if you can see it, you can map it.  This technique can be particularly helpful to researchers such a biologists working in small areas under heavy tree canopy where GPS fails. All sessions are one hour and assume attendees have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials with expert assistance. All sessions will be taught in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community."},{"id":"gis-workshop-making-your-first-map-2","title":"GIS Workshop: Making Your First Map with ArcGIS","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-making-your-first-map-2","layout":"events","content":"Here’s your chance to get started with geographic information systems software in a friendly, jargon-free environment.  This workshop introduces the skills you need to make your own maps.  Along the way you’ll get a taste of Earth’s most popular GIS software (ArcGIS) and a gentle introduction to cartography. You’ll leave with your own cartographic masterpieces and tips for learning more in your pursuit of mappiness at UVa. All sessions are one hour and assume attendees have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials with expert assistance. All sessions will be taught in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community."},{"id":"gis-workshop-making-your-first-map-with-arcgis-2","title":"GIS Workshop: Making Your First Map with ArcGIS","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-making-your-first-map-with-arcgis-2","layout":"events","content":"Making Your First Map with ArcGIS Here’s your chance to get started with geographic information systems software in a friendly, jargon-free environment.  This workshop introduces the skills you need to make your own maps.  Along the way you’ll get a taste of Earth’s most popular GIS software (ArcGIS) and a gentle introduction to cartography. You’ll leave with your own cartographic masterpieces and tips for learning more in your pursuit of mappiness at UVa. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be held on Tuesdays from 3PM to 4PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up!"},{"id":"gis-workshop-making-your-first-map-with-arcgis-3","title":"GIS Workshop: Making Your First Map with ArcGIS","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-making-your-first-map-with-arcgis-3","layout":"events","content":"Here’s your chance to get started with geographic information systems software in a friendly, jargon-free environment.  This workshop introduces the skills you need to make your own maps.  Along the way you’ll get a taste of Earth’s most popular GIS software (ArcGIS) and a gentle introduction to cartography. You’ll leave with your own cartographic masterpieces and tips for learning more in your pursuit of mappiness at UVa. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be held on  Tuesdays from 10AM to 11AM in the Alderman Electronic Classroom, ALD 421  (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up!"},{"id":"gis-workshop-making-your-first-map-with-arcgis-4","title":"GIS Workshop: Making Your First Map with ArcGIS","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-making-your-first-map-with-arcgis-4","layout":"events","content":"Here’s your chance to get started with geographic information systems software in a friendly, jargon-free environment.  This workshop introduces the skills you need to make your own maps.  Along the way you’ll get a taste of Earth’s most popular GIS software (ArcGIS) and a gentle introduction to cartography. You’ll leave with your own cartographic masterpieces and tips for learning more in your pursuit of mappiness at UVa. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be taught on  Wednesdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421  (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up! For the full schedule of the Fall 2018 UVa Library GIS Workshop Series, see here ."},{"id":"gis-workshop-making-your-first-map-with-arcgis","title":"GIS Workshop: Making Your First Map with ArcGIS","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-making-your-first-map-with-arcgis","layout":"events","content":"Here’s your chance to get started with geographic information systems software in a friendly, jargon-free environment.  This workshop introduces the skills you need to make your own maps.  Along the way you’ll get a taste of Earth’s most popular GIS software (ArcGIS) and a gentle introduction to cartography. You’ll leave with your own cartographic masterpieces and tips for learning more in your pursuit of mappiness at UVa. All sessions are one hour and assume participants have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials and expert assistance. All sessions will be taught on Thursdays from 3PM to 4PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community."},{"id":"gis-workshop-making-your-first-map","title":"GIS Workshop: Making Your First Map","author":"jeremy-boggs","date":null,"categories":null,"url":"gis-workshop-making-your-first-map","layout":"events","content":"Getting started with new software can be intimidating. This workshop introduces the skills you need to work with spatial goodness. Along the way you’ll get a taste of Earth’s most popular geographic software and a gentle introduction to map making. You’ll leave with your own cartographic masterpiece and tips for learning more in your pursuit of mappiness at UVa. All sessions are one hour and assume attendees have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials with expert assistance. All sessions will be taught in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community."},{"id":"gis-workshop-points-on-your-map-street-addresses-and-more-spatial-things-2","title":"GIS Workshop: Points on Your Map: Street Addresses and More Spatial Things","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-points-on-your-map-street-addresses-and-more-spatial-things-2","layout":"events","content":"Do you have a list of street addresses crying out to be mapped?  Have a list of zip codes or census tracts you wish to associate with other data? We’ll start with addresses and other things spatial and end with points on a map, ready for visualization and analysis. All sessions are one hour and assume participants have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials and expert assistance. All sessions will be taught on Thursdays from 3PM to 4PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community."},{"id":"gis-workshop-points-on-your-map-street-addresses-and-more-spatial-things-3","title":"GIS Workshop: Points on Your Map","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-points-on-your-map-street-addresses-and-more-spatial-things-3","layout":"events","content":"Points on Your Map: Street Addresses and More Spatial Things Do you have a list of street addresses crying out to be mapped?  Have a list of zip codes or census tracts you wish to associate with other data? We’ll start with addresses and other things spatial and end with points on a map, ready for visualization and analysis. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be held on Tuesdays from 3PM to 4PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up!"},{"id":"gis-workshop-points-on-your-map-street-addresses-and-more-spatial-things-4","title":"GIS Workshop: Points on Your Map – Street Addresses and More Spatial Things","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-points-on-your-map-street-addresses-and-more-spatial-things-4","layout":"events","content":"Do you have a list of street addresses crying out to be mapped?  Have a list of zip codes or census tracts you wish to associate with other data?  We’ll start with addresses and other things spatial and end with points on a map, ready for visualization and analysis. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be taught on  Wednesdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421  (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up! For the full schedule of the Fall 2018 UVa Library GIS Workshop Series, see here ."},{"id":"gis-workshop-points-on-your-map-street-addresses-and-more-spatial-things","title":"GIS Workshop: Points on Your Map—Street Addresses and More Spatial Things","author":"jeremy-boggs","date":null,"categories":null,"url":"gis-workshop-points-on-your-map-street-addresses-and-more-spatial-things","layout":"events","content":"Do you have a list of street addresses crying out to be mapped? Have a list of zip codes or census tracts you wish to associate with other data? We’ll start with addresses and other things spatial and end with points on a map, ready for visualization and analysis. All sessions are one hour and assume attendees have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials with expert assistance. All sessions will be taught in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community."},{"id":"gis-workshop-taking-control-of-your-spatial-data-editing-in-arcgis-2","title":"GIS Workshop: Taking Control of Your Spatial Data: Editing in ArcGIS","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-taking-control-of-your-spatial-data-editing-in-arcgis-2","layout":"events","content":"Until we perfect that magic “extract all those lines from this paper map” button we’re stuck using editor tools to get that job done.  If you’re lucky, someone else has done the work to create your points, lines, and polygons but maybe they need your magic touch to make them better.  This session shows you how to create and modify vector features in ArcMap, the world’s most popular geographic information systems software.  We’ll explore tools to create new points, lines, and polygons and to edit existing datasets.  At version 10, ArcMap’s editor was revamped introducing new templates, but we’ll keep calm and carry on. All sessions are one hour and assume participants have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials and expert assistance. All sessions will be taught on Thursdays from 3PM to 4PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community."},{"id":"gis-workshop-taking-control-of-your-spatial-data-editing-in-arcgis-3","title":"GIS Workshop: Taking Control of Your Spatial Data - Editing in ArcGIS","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-taking-control-of-your-spatial-data-editing-in-arcgis-3","layout":"events","content":"Until we perfect that magic “extract all those lines from this paper map” button we’re stuck using editor tools to get that job done.  If you’re lucky, someone else has done the work to create your points, lines, and polygons but maybe they need your magic touch to make them better.  This session shows you how to create and modify vector features in ArcMap, the world’s most popular geographic information systems software.  We’ll explore tools to create new points, lines, and polygons and to edit existing datasets.  At version 10, ArcMap’s editor was revamped introducing new templates, but we’ll keep calm and carry on. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be held on Tuesdays from 3PM to 4PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up!"},{"id":"gis-workshop-taking-control-of-your-spatial-data-editing-in-arcgis-4","title":"GIS Workshop: Taking Control of Your Spatial Data: Editing in ArcGIS","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-taking-control-of-your-spatial-data-editing-in-arcgis-4","layout":"events","content":"Until we perfect that magic “extract all those lines from this paper map” button we’re stuck using editor tools to get that job done.  If you’re lucky, someone else has done the work to create your points, lines, and polygons but maybe they need your magic touch to make them better.  This session shows you how to create and modify vector features in ArcMap, the world’s most popular geographic information systems software.  We’ll explore tools to create new points, lines, and polygons and to edit existing datasets.  At version 10, ArcMap’s editor was revamped introducing new templates, but we’ll keep calm and carry on. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be taught on  Wednesdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421  (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up! For the full schedule of the Fall 2018 UVa Library GIS Workshop Series, see here ."},{"id":"gis-workshop-taking-control-of-your-spatial-data-editing-in-arcgis","title":"GIS Workshop: Taking Control of Your Spatial Data -- Editing in ArcGIS","author":"jeremy-boggs","date":null,"categories":null,"url":"gis-workshop-taking-control-of-your-spatial-data-editing-in-arcgis","layout":"events","content":"Until we perfect that magic “extract all those lines from this paper map” button we’re stuck using editor tools to get that job done. If you’re lucky, someone else has done the work to create your points, lines, and polygons but maybe they need your magic touch to make them better. This session shows you how to create and modify vector features in ArcMap, the world’s most popular geographic information systems software. We’ll explore tools to create new points, lines, and polygons and to edit existing datasets. At version 10, ArcMap’s editor was revamped introducing new templates, but we’ll keep calm and carry on. All sessions are one hour and assume attendees have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials with expert assistance. All sessions will be taught in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community."},{"id":"gis-workshop-whats-new-with-arcgis-pro","title":"GIS Workshop: What’s New with ArcGIS Pro","author":"laura-miller","date":null,"categories":null,"url":"gis-workshop-whats-new-with-arcgis-pro","layout":"events","content":"The handwriting is on the wall.  ArcGIS Pro will be replacing ArcMap as the desktop GIS in the near future.  Come learn about the changes and quirks of ArcGIS Pro from an ArcMap user prospective. All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be held on  Tuesdays from 10AM to 11AM in the Alderman Electronic Classroom, ALD 421  (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up!"},{"id":"gis-workshop","title":"GIS Workshop - Making Your First Map with ArcGIS","author":"chris-gist","date":null,"categories":null,"url":"gis-workshop","layout":"events","content":"Making Your First Map with ArcGIS Here’s your chance to get started with geographic information systems software in a friendly, jargon-free environment.  This workshop introduces the skills you need to make your own maps.  Along the way you’ll get a taste of Earth’s most popular GIS software (ArcGIS) and a gentle introduction to cartography. You’ll leave with your own cartographic masterpieces and tips for learning more in your pursuit of mappiness at UVa."},{"id":"hack-your-pants-a-thon-2019-04-17","title":"Hack-your-pants-a-thon","author":"ammon-shepherd","date":null,"categories":null,"url":"hack-your-pants-a-thon-2019-04-17","layout":"events","content":"Are you tired of your phone poking out of your pocket? Did you get a new Phablet and now you have nowhere to stick it? This workshop will show you how to take an old t-shirt and make your pockets bigger. No experience necessary, but you will need to bring your pants and an old t-shirt (or one can be provided. Warning: you will be cutting your pants pockets, and there is a possibility that the pocket will not be to your liking in the end. Drop in anytime during the workshop hours to hack your pants! Note: It usually takes 20 minutes for each pocket. No registration required."},{"id":"hack-your-pants-make-your-pockets-fit-your-phone","title":"Makerspace Workshop: Hack Your Pants - Make your Pockets Fit your Phone","author":"ammon-shepherd","date":null,"categories":null,"url":"hack-your-pants-make-your-pockets-fit-your-phone","layout":"events","content":"Are you tired of your phone poking out of your pocket? Did you get a new Phablet and now you have nowhere to stick it? This workshop will show you how to take an old tshirt and make your pockets bigger. No experience necessary, but you will need to bring your pants and an old tshirt. Warning: you will be cutting your pants pockets, and there is a possibility that the pocket will not be to your liking in the end."},{"id":"human-centered-design-workshop-digital-mock-up","title":"Human Centered Design Workshop: Digital Mock-up","author":"laura-miller","date":null,"categories":null,"url":"human-centered-design-workshop-digital-mock-up","layout":"events","content":"This workshop series is for everyone who wants to “create something that is useful and will make people happy.” You will develop your own small project over four weeks, culminating in a prototype that makes you proud.  The sessions will build on each other, and you can count on a few out-of-class hours of work per week, but we promise to have a lot of fun! In session 3, you will learn how to create a more concrete prototype of your idea (using PowerPoint or Impress) and continue to refine your design. This workshop series is for everyone who wants to “create something that is useful and will make people happy.” You will develop your own small project over four weeks, culminating in a prototype that makes you proud.  The sessions will build on each other, and you can count on a few out-of-class hours of work per week, but we promise to have a lot of fun! If you have questions, visit the Scholars’ Lab or email Marcus Kossatz at marcus.kossatz@uni-weimar.de ."},{"id":"human-centered-design-workshop-introduction","title":"Human Centered Design Workshop: Introduction","author":"laura-miller","date":null,"categories":null,"url":"human-centered-design-workshop-introduction","layout":"events","content":"This workshop series is for everyone who wants to “create something that is useful and will make people happy.” You will develop your own small project over four weeks, culminating in a prototype that makes you proud.  The sessions will build on each other, and you can count on a few out-of-class hours of work per week, but we promise to have a lot of fun! In session 1, students will learn the basics of usability and human-computer interaction and get inspiration for your own project idea. Marcus Kossatz studies Computer Science and Media at Bauhaus-Universität Weimar, Germany. He is a visiting scholar in the Scholars’ Lab, working on Histoglobe, his Master’s Thesis in Digital Humanities. Driven by the desire to create something useful and fueled by a passion for usability and human-machine interaction, he will share his experiences with you. If you have questions, visit the Scholars’ Lab or email Marcus Kossatz at marcus.kossatz@uni-weimar.de ."},{"id":"human-centered-design-workshop-paper-prototyping","title":"Human Centered Design Workshop: Paper Prototyping","author":"laura-miller","date":null,"categories":null,"url":"human-centered-design-workshop-paper-prototyping","layout":"events","content":"This workshop series is for everyone who wants to “create something that is useful and will make people happy.” You will develop your own small project over four weeks, culminating in a prototype that makes you proud.  The sessions will build on each other, and you can count on a few out-of-class hours of work per week, but we promise to have a lot of fun! In session 2, you will learn how to rapidly design your idea using only pen and paper, and how to determine if your design is effective. Marcus Kossatz studies Computer Science and Media at Bauhaus-Universität Weimar, Germany. He is a visiting scholar in the Scholars’ Lab, working on Histoglobe, his Master’s Thesis in Digital Humanities. Driven by the desire to create something useful and fueled by a passion for usability and human-machine interaction, he will share his experiences with you. If you have questions, visit the Scholars’ Lab or email Marcus Kossatz at marcus.kossatz@uni-weimar.de ."},{"id":"human-centered-design-workshop","title":"Human Centered Design Workshop: Next Steps","author":"laura-miller","date":null,"categories":null,"url":"human-centered-design-workshop","layout":"events","content":"This workshop series is for everyone who wants to “create something that is useful and will make people happy.” You will develop your own small project over four weeks, culminating in a prototype that makes you proud.  The sessions will build on each other, and you can count on a few out-of-class hours of work per week, but we promise to have a lot of fun! In session 4, we will celebrate at your useful prototype, discuss next steps, and you will receive your reward. Marcus Kossatz studies Computer Science and Media at Bauhaus-Universität Weimar, Germany. He is a visiting scholar in the Scholars’ Lab, working on Histoglobe, his Master’s Thesis in Digital Humanities. Driven by the desire to create something useful and fueled by a passion for usability and human-machine interaction, he will share his experiences with you. If you have questions, visit the Scholars’ Lab or email Marcus Kossatz at marcus.kossatz@uni-weimar.de ."},{"id":"info-session-landscape-studies-initiative-summer-opportunities","title":"Info Session: Landscape Studies Initiative - Summer Opportunities","author":"laura-miller","date":null,"categories":null,"url":"info-session-landscape-studies-initiative-summer-opportunities","layout":"events","content":"Please join us to learn about the Landscape Studies Initiative and its’  summer opportunities for employment - in research, prototyping, and fieldwork - as well as travel . The info session will: introduce the Landscape Studies Initiative, the research team, and their 3-year project to develop a digital platform for the teaching of landscape from a transdisciplinary perspective, which emphasizes the entanglements of places, texts, images, and on-going environmental and anthropocene issues. describe the employment opportunities this summer (with potential academic year extension) including roles prototyping archival api apps and 3d-4d media viewers, developing digital fieldwork protocols, and updating the project bibliography. elaborate on the opportunity to join the Initiative’s first fieldwork trip, to Germany (Bad Muskau, Dessau, Potsdam, and Berlin) from August 6th-17th, 2018. address application details - for employment and travel. Meg Studer, the program manager, will also take questions. The initiative welcomes students from any discipline and department with skills in digital visualization, computation as well as research skills in the humanities. More about the  Landscape Studies Initiative : Led by Beth Meyer and Michael Lee, in the A-School, the Mellon-funded Landscape Studies Initiative is creating a digital platform for the teaching of landscape studies. The platform will bring together primary sources as well as historic and contemporary field observations so that the history of the designed landscape is understood as a material as well as a cultural practice. During this 3 yr planning project, the Initiative will test our digital platform in several UVA courses taught in the A-School as well as Lisa Goff (American Studies), and then launch an open source digital resource for students and faculty in and beyond UVA."},{"id":"kate-compton-creative-coding-workshop","title":"Workshop with Kate Compton: Making Things That Make Things: The Surprisingly Friendly World of Generative Programs","author":"laura-miller","date":null,"categories":null,"url":"kate-compton-creative-coding-workshop","layout":"events","content":"Making Things That Make Things: The Surprisingly Friendly World of Generative Programs Join Kate Compton in the Scholars’ Lab for a creative coding workshop on generative programs! Learn how to use Tracery – a super-simple tool and language for generating text, used for making twitterbots, artbots, games, and stories – and how Tracery fits into the world of generativity as a whole. We’ll also be learning about Generominos, generative ideation cards that help you design your interactive artwork, alt-control game, or generative art experiment. Bring your laptop! Everyone is welcome, no coding experience necessary! Please register to reserve your spot . Kate Compton is a generative artist, programmer, inventor, and researcher at the University of California, Santa Cruz, who is developing artificial intelligence to augment human creativity. She wrote the popular generative-text language Tracery and the bot-making language Bottery. See her work at galaxykate.com This workshop is part of the Puzzles, Bots, and Poetics Symposium, being held Friday Oct. 26 - Saturday Oct. 27, 2018, co-hosted by the Puzzle Poetry Group, the UVa Library’s Scholars’ Lab, and IHGC’s Humanities Informatics Lab with support from the Page-Barbour Committee."},{"id":"makerspace-open-house-2014-08-27","title":"Makerspace Open House","author":"jeremy-boggs","date":null,"categories":null,"url":"makerspace-open-house-2014-08-27","layout":"events","content":"New to 3D printing, microcontrollers, soft circuits? Stop by to check them out! There will be conversation, live demonstrations, and of course, refreshments. Hope to see you there!"},{"id":"makerspace-open-house-2014-09-10","title":"Makerspace Open House","author":"jeremy-boggs","date":null,"categories":null,"url":"makerspace-open-house-2014-09-10","layout":"events","content":"New to 3D printing, microcontrollers, soft circuits? Stop by to check them out! There will be conversation, live demonstrations, and of course, refreshments. Hope to see you there!"},{"id":"makerspace-open-house-2014-09-24","title":"Makerspace Open House","author":"jeremy-boggs","date":null,"categories":null,"url":"makerspace-open-house-2014-09-24","layout":"events","content":"New to 3D printing, microcontrollers, soft circuits? Stop by to check them out! There will be conversation, live demonstrations, and of course, refreshments. Hope to see you there!"},{"id":"makerspace-open-house-2014-10-08","title":"Makerspace Open House","author":"jeremy-boggs","date":null,"categories":null,"url":"makerspace-open-house-2014-10-08","layout":"events","content":"New to 3D printing, microcontrollers, soft circuits? Stop by to check them out! There will be conversation, live demonstrations, and of course, refreshments. Hope to see you there!"},{"id":"makerspace-open-house-2014-10-22","title":"Makerspace Open House","author":"jeremy-boggs","date":null,"categories":null,"url":"makerspace-open-house-2014-10-22","layout":"events","content":"New to 3D printing, microcontrollers, soft circuits? Stop by to check them out! There will be conversation, live demonstrations, and of course, refreshments. Hope to see you there!"},{"id":"makerspace-open-house","title":"Makerspace Open House","author":"laura-miller","date":null,"categories":null,"url":"makerspace-open-house","layout":"events","content":"Drop in anytime between 1:00 and 5:00 to explore the Scholars’ Lab Makerspace – a place for tinkering and experimenting with technologies like desktop fabrication, physical computing, and augmented reality. Staff will be present to demonstrate 3D printing, electronics, and tactile computing, and to showcase projects and work from the Scholars’ Lab, Robertson Media Center, and others. This is a hands-on, kid-friendly event."},{"id":"makerspace-workshop-advanced-3d-modeling-techniques","title":"Makerspace Workshop: Making Your First 3D Model","author":"jeremy-boggs","date":null,"categories":null,"url":"makerspace-workshop-advanced-3d-modeling-techniques","layout":"events","content":"This workshop introduces software, hardware, and techniques for creating a simple 3D model from an existing object. We’ll cover various ways of creating 3D models, demonstrate laser scanning with a Kinect and photogrammetry using a digital camera. We’ll spend most of the class capturing a model using photogrammetry, and if all goes well, you’ll leave the workshop with your own 3D model to embed on a web page, or edit to make a 3D print using our lab’s Makerbot. If you’d like to be a participant and not a spectator, bring a laptop and camera. (Cell phone cameras are fine!)"},{"id":"makerspace-workshop-advanced-3d-printing-techniques","title":"Makerspace Workshop: Advanced 3D Printing Techniques","author":"ammon-shepherd","date":null,"categories":null,"url":"makerspace-workshop-advanced-3d-printing-techniques","layout":"events","content":"** Get the most out of 3D printing. We’ll discuss advanced slicer settings and model manipulations to work around tricky printing situations, printer maintenance and troubleshooting, experimental filaments, and post-print finishing techniques. This workshop is intended for those with some experience with 3D printing or have joined us for our Introduction to 3D Printing.   Instructor: Shane Lin  and Julia Schrank If you can’t make this session but would like to learn more about 3D printing, please visit our the student consultants in our Makerspace or email scholarslab@virginia.edu to set up an individual appointment."},{"id":"makerspace-workshop-advanced-wearables-soft-circuits","title":"Makerspace Workshop: Advanced Wearables & Soft Circuits","author":"ammon-shepherd","date":null,"categories":null,"url":"makerspace-workshop-advanced-wearables-soft-circuits","layout":"events","content":"This workshop caters to those who have either started hacking wearables on their own or who had fun with our Introduction to Wearables &amp; Soft Circuits. Come with your own wearable project or be ready plan a new one in good company! The Makerspace’s extensive Arduino component library will be available for prototyping and Makerspace staff will help you take your project to the next level.  Instructor: Julia Schrank If you can’t make this session but would like to learn more about 3D printing, please visit our the student consultants in our Makerspace or email scholarslab@virginia.edu to set up an individual appointment."},{"id":"makerspace-workshop-easy-electronics-arduino-for-the-beginner-2","title":"Makerspace Workshop: Easy Electronics – Arduino for the Beginner","author":"laura-miller","date":null,"categories":null,"url":"makerspace-workshop-easy-electronics-arduino-for-the-beginner-2","layout":"events","content":"Come learn the basics of using the Arduino for fun or profit. No experience or equipment needed. Arduino kits and laptops provided to use in class, but you are welcome to bring your own. This workshop will go through the very basics of electricity, how to setup the Arduino, and building a first circuit; an LED nightlight. For the full schedule of the Scholars’ Lab  Fall 2018 Maker &amp; Code Workshop Series, see  here ."},{"id":"makerspace-workshop-easy-electronics-arduino-for-the-beginner-3","title":"Makerspace Workshop: Easy Electronics – Arduino for the Beginner","author":"laura-miller","date":null,"categories":null,"url":"makerspace-workshop-easy-electronics-arduino-for-the-beginner-3","layout":"events","content":"Come learn the basics of using the Arduino for fun or profit. No experience or equipment needed. Arduino kits and laptops provided to use in class, but you are welcome to bring your own. This workshop will go through the very basics of electricity, how to setup the Arduino, and building a first circuit; an LED nightlight. For the full schedule of the Scholars’ Lab  Fall 2018 Maker &amp; Code Workshop Series, see  here ."},{"id":"makerspace-workshop-easy-electronics-arduino-for-the-beginner","title":"Makerspace Workshop: Easy Electronics – Arduino for the Beginner","author":"laura-miller","date":null,"categories":null,"url":"makerspace-workshop-easy-electronics-arduino-for-the-beginner","layout":"events","content":"Come learn the basics of using the Arduino for fun or profit. No experience or equipment needed. Arduino kits and laptops provided to use in class, but you are welcome to bring your own. This workshop will go through the very basics of electricity, how to setup the Arduino, and building a first circuit; an LED nightlight. For the full schedule of the Scholars’ Lab  Fall 2018 Maker &amp; Code Workshop Series, see  here ."},{"id":"makerspace-workshop-fundamentals-of-digital-photography","title":"Makerspace Workshop: Fundamentals of Digital Photography","author":"laura-miller","date":null,"categories":null,"url":"makerspace-workshop-fundamentals-of-digital-photography","layout":"events","content":"Turn your camera’s mode dial off of Auto and learn the basics of digital photography. We’ll cover the basic components of exposure, depth of field, focal length and perspective, and color. This workshop will be most applicable to SLR, mirrorless, and advanced point-and-shoot cameras with manual operation modes, but many concepts will also be useful for basic point-and-shoot and cell phone cameras. Instructor: Shane Lin Scholars’ Lab workshops assume attendees have no previous experience. They will be hands-on with with expert assistance.  All are free to attend, and they are open to the UVa and larger Charlottesville community."},{"id":"makerspace-workshop-hack-your-pants-make-your-pockets-fit-your-phone-2","title":"Makerspace Workshop: Hack Your Pants – Make your Pockets Fit your Phone","author":"laura-miller","date":null,"categories":null,"url":"makerspace-workshop-hack-your-pants-make-your-pockets-fit-your-phone-2","layout":"events","content":"Are you tired of your phone poking out of your pocket? Did you get a new Phablet and now you have nowhere to stick it? This workshop will show you how to take an old tshirt and make your pockets bigger. No experience necessary, but you will need to bring your pants and an old tshirt. Warning: you will be cutting your pants pockets, and there is a possibility that the pocket will not be to your liking in the end. For the full schedule of the Scholars’ Lab  Fall 2018 Maker &amp; Code Workshop Series, see  here ."},{"id":"makerspace-workshop-hack-your-pants-make-your-pockets-fit-your-phone","title":"Makerspace Workshop: Hack Your Pants – Make your Pockets Fit your Phone","author":"laura-miller","date":null,"categories":null,"url":"makerspace-workshop-hack-your-pants-make-your-pockets-fit-your-phone","layout":"events","content":"Are you tired of your phone poking out of your pocket? Did you get a new Phablet and now you have nowhere to stick it? This workshop will show you how to take an old tshirt and make your pockets bigger. No experience necessary, but you will need to bring your pants and an old tshirt. Warning: you will be cutting your pants pockets, and there is a possibility that the pocket will not be to your liking in the end. For the full schedule of the Scholars’ Lab  Fall 2018 Maker &amp; Code Workshop Series, see  here ."},{"id":"makerspace-workshop-html-for-absolute-beginners-2","title":"Makerspace Workshop: HTML for Absolute Beginners","author":"laura-miller","date":null,"categories":null,"url":"makerspace-workshop-html-for-absolute-beginners-2","layout":"events","content":"Wondering how web sites work? Wanting to get started creating web content of your own, but have no idea how to do that? This is the class for you. We’ll cover everything from how URLs work to basic HTML coding skills to basic netiquette. This workshop is intended for absolute beginners with no knowledge of HTML. Our sessions assume attendees have no previous experience, and will be hands-on with step-by-step tutorials with expert assistance. All sessions will be taught in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community."},{"id":"makerspace-workshop-html-for-absolute-beginners","title":"Makerspace Workshop: HTML for Absolute Beginners","author":"ronda-grizzle","date":null,"categories":null,"url":"makerspace-workshop-html-for-absolute-beginners","layout":"events","content":"Wondering how web sites work? Wanting to get started creating web content of your own, but have no idea how to do that? This is the class for you. We’ll cover everything from how URLs work to basic HTML coding skills to basic netiquette. This workshop is intended for absolute beginners with no knowledge of HTML. Instructor: Ronda Grizzle"},{"id":"makerspace-workshop-intro-to-3d-modeling-printing-2","title":"Makerspace Workshop: Intro to 3D Modeling & Printing","author":"jeremy-boggs","date":null,"categories":null,"url":"makerspace-workshop-intro-to-3d-modeling-printing-2","layout":"events","content":"Do you think 3D modeling has to be expensive and time-consuming? Are you curious about 3D printing but unsure where to start? This workshop will introduce participants to the exciting world of desktop fabrication. We’ll provide a brief overview of current trends and tools for 3D modeling and printing. We’ll also go over the basics of model creation with photogrammetry, and discuss how 3D printing works, including a live demonstration with our Makerbot Replicator 2. While not necessary, participants can bring a camera (cell phone cameras welcome!) and a laptop to get better acquainted with the software we’ll introduce."},{"id":"makerspace-workshop-intro-to-3d-modeling-printing-3","title":"Makerspace Workshop: Intro to 3D Modeling & Printing","author":"laura-miller","date":null,"categories":null,"url":"makerspace-workshop-intro-to-3d-modeling-printing-3","layout":"events","content":"Do you think 3D modeling has to be expensive and time-consuming? Are you curious about 3D printing but unsure where to start? This workshop will introduce participants to the exciting world of desktop fabrication. We’ll provide a brief overview of current trends and tools for 3D modeling and printing. We’ll also go over the basics of model creation with photogrammetry, and discuss how 3D printing works, including a live demonstration with one of our Makerbots. While not necessary, participants can bring a camera (cell phone cameras welcome!) and a laptop to get better acquainted with the software we’ll introduce. If you can’t make this session but would like to learn more about 3D printing, please visit our the student consultants in our Makerspace or email scholarslab@virginia.edu to set up an individual appointment."},{"id":"makerspace-workshop-intro-to-3d-modeling-printing-4","title":"CANCELED: Makerspace Workshop: Intro to 3D Modeling & Printing","author":"laura-miller","date":null,"categories":null,"url":"makerspace-workshop-intro-to-3d-modeling-printing-4","layout":"events","content":"THIS WORKSHOP IS CANCELED TODAY DUE TO INCLEMENT WEATHER CONDITIONS. Do you think 3D modeling has to be expensive and time-consuming? Are you curious about 3D printing but unsure where to start? This workshop will introduce participants to the exciting world of desktop fabrication. We’ll provide a brief overview of current trends and tools for 3D modeling and printing. We’ll also go over the basics of model creation with photogrammetry, and discuss how 3D printing works, including a live demonstration with one of our Makerbots. While not necessary, participants can bring a camera (cell phone cameras welcome!) and a laptop to get better acquainted with the software we’ll introduce. If you can’t make this session but would like to learn more about 3D printing, please visit our the student consultants in our Makerspace or email scholarslab@virginia.edu to set up an individual appointment."},{"id":"makerspace-workshop-intro-to-3d-modeling-printing-5","title":"Makerspace Workshop: Intro to 3D Printing","author":"laura-miller","date":null,"categories":null,"url":"makerspace-workshop-intro-to-3d-modeling-printing-5","layout":"events","content":"Do you think 3D modeling has to be expensive and time-consuming? Are you curious about 3D printing but unsure where to start? This workshop will introduce participants to the exciting world of desktop fabrication. We’ll provide a brief overview of current trends and tools for 3D modeling and printing, followed by a hands-on demonstration of basic printer operation. Other topics will include common printing issues and a quick overview of model creation with photogrammetry. While not necessary, participants can bring a laptop to get better acquainted with the software we’ll introduce. Instructor: Shane Lin If you can’t make this session but would like to learn more about 3D printing, please visit our the student consultants in our Makerspace or email scholarslab@virginia.edu to set up an individual appointment."},{"id":"makerspace-workshop-intro-to-3d-modeling-printing","title":"Makerspace Workshop: Intro to 3D Modeling & Printing","author":"jeremy-boggs","date":null,"categories":null,"url":"makerspace-workshop-intro-to-3d-modeling-printing","layout":"events","content":"Do you think 3D modeling has to be expensive and time-consuming? Are you curious about 3D printing but unsure where to start? This workshop will introduce participants to the exciting world of desktop fabrication. We’ll provide a brief overview of current trends and tools for 3D modeling and printing. We?ll also go over the basics of model creation with photogrammetry, and discuss how 3D printing works, including a live demonstration with our Makerbot Replicator 2. While not necessary, participants can bring a camera (cell phone cameras welcome!) and a laptop to get better acquainted with the software we’ll introduce."},{"id":"makerspace-workshop-intro-to-3d-printing","title":"Makerspace Workshop: Intro to 3D Printing","author":"ammon-shepherd","date":null,"categories":null,"url":"makerspace-workshop-intro-to-3d-printing","layout":"events","content":"Do you think 3D modeling has to be expensive and time-consuming? Are you curious about 3D printing but unsure where to start? This workshop will introduce participants to the exciting world of desktop fabrication. We’ll provide a brief overview of current trends and tools for 3D modeling and printing, followed by a hands-on demonstration of basic printer operation. We will also discuss various types of 3D models and the best practices and equipment for bringing them to life. While not necessary, participants can bring a laptop to get better acquainted with the software we’ll introduce. Instructor: Julia Schrank If you can’t make this session but would like to learn more about 3D printing, please visit our the student consultants in our Makerspace or email scholarslab@virginia.edu to set up an individual appointment."},{"id":"makerspace-workshop-intro-to-wearables-and-soft-circuits-2","title":"Makerspace Workshop: Intro to Wearables and Soft Circuits","author":"laura-miller","date":null,"categories":null,"url":"makerspace-workshop-intro-to-wearables-and-soft-circuits-2","layout":"events","content":"Curious about wearable computing but afraid to ask? Have ideas about how to make your life simpler with hacks for your outerwear or accessories? This beginner workshop will introduce the basics of circuity and give an overview of current trends in wearable computing. Participants will make their own circuit using LED’s and conductive thread. Materials will be provided and no experience with sewing or electronics is necessary. [Note: This workshop was originally scheduled for Feb. 25, and had to be moved to accommodate a conflicting event.]"},{"id":"makerspace-workshop-intro-to-wearables-and-soft-circuits","title":"Makerspace Workshop: Intro to Wearables and Soft Circuits","author":"jeremy-boggs","date":null,"categories":null,"url":"makerspace-workshop-intro-to-wearables-and-soft-circuits","layout":"events","content":"Curious about wearable computing but afraid to ask? Have ideas about how to make your life simpler with hacks for your outerwear or accessories? This beginner workshop will introduce the basics of circuity and give an overview of current trends in wearable computing. Participants will make their own circuit using LED’s and conductive thread. Materials will be provided and no experience with sewing or electronics is necessary."},{"id":"makerspace-workshop-introduction-to-arduino-2","title":"Makerspace Workshop: Working with Arduinos I","author":"laura-miller","date":null,"categories":null,"url":"makerspace-workshop-introduction-to-arduino-2","layout":"events","content":"Do you want to build a device that interacts with social media? Heard about ways to embed switches or sensors in your physical items but don’t know where to find out more? Arduino is a tool for making microcomputers that can sense and control the physical world. This workshop will introduce participants to the basics of circuitry and programming through a series of hands-on exercises using our Arduino kits. No electronics experience required! Laptops will also be available for participants, but bring your own if you want to save and edit your projects after the workshop. If you can’t make this session but would like to learn more about Arduino boards and shields, please visit the student consultants in our Makerspace or email scholarslab@virginia.edu to set up an individual appointment."},{"id":"makerspace-workshop-introduction-to-arduino","title":"Makerspace Workshop: Introduction to Arduino","author":"jeremy-boggs","date":null,"categories":null,"url":"makerspace-workshop-introduction-to-arduino","layout":"events","content":"Do you want to build a device that interacts with social media? Heard about ways to embed switches or sensors in your physical items but don’t know where to find out more? Arduino is a tool for making microcomputers that can sense and control the physical world. This workshop will introduce participants to the basics of circuitry and programming through a series of hands-on exercises using our Arduino kits. No electronics experience required! Laptops will also be available for participants, but bring your own if you want to save and edit your projects after the workshop."},{"id":"makerspace-workshop-introduction-to-neatline","title":"Makerspace Workshop: Introduction to Neatline","author":"ronda-grizzle","date":null,"categories":null,"url":"makerspace-workshop-introduction-to-neatline","layout":"events","content":"Using Neatline, anyone can create beautiful, interactive maps, timelines, and narrative sequences from collections of archives and artifacts, telling scholarly stories in a whole new way. Join us for this hands-on introduction to Neatline. See http://neatline.org/ for more information. Instructor: Ronda Grizzle"},{"id":"makerspace-workshop-introduction-to-omeka","title":"Makerspace Workshop: Introduction to Omeka","author":"ronda-grizzle","date":null,"categories":null,"url":"makerspace-workshop-introduction-to-omeka","layout":"events","content":"Omeka was created to enable scholars, archives, libraries, museums, and independent researchers to build online exhibits of their work without having to know HTML or CSS. If you have a collection of digital resources that you want to exhibit on the web, Omeka could be a great tool to have in your toolkit. Instructor: Ronda Grizzle"},{"id":"makerspace-workshop-introduction-to-photogammetry","title":"Makerspace Workshop: Introduction to Photogrammetry","author":"ammon-shepherd","date":null,"categories":null,"url":"makerspace-workshop-introduction-to-photogammetry","layout":"events","content":"In this interactive workshop, participants will learn the basics of photogrammetry by making their own 3D model out of regular 2D photographs. We will discuss practical applications of photogrammetry, as well as the different strategies and tools available to accomplish personal research goals. While not necessary, participants can bring a digital camera or smartphone and laptop to get better acquainted with the photogrammetry process and software. Instructor: Jennifer Grayburn If you can’t make this session but would like to learn more about 3D printing, please visit our the student consultants in our Makerspace or email scholarslab@virginia.edu to set up an individual appointment."},{"id":"makerspace-workshop-introduction-to-sewing","title":"Makerspace Workshop: Introduction to Sewing","author":"laura-miller","date":null,"categories":null,"url":"makerspace-workshop-introduction-to-sewing","layout":"events","content":"Are you new to sewing? Would you like to learn the basics of operating a sewing machine? This workshop is for you. We will cover general sewing vocabulary, winding a bobbin and threading the machine, basic stitching, and basic understanding of needle and fabric type."},{"id":"makerspace-workshop-introduction-to-wearables-soft-circuits","title":"Makerspace Workshop: Introduction to Wearables & Soft Circuits","author":"ammon-shepherd","date":null,"categories":null,"url":"makerspace-workshop-introduction-to-wearables-soft-circuits","layout":"events","content":"Have you ever been frustrated by the limits of your clothing and accessories? Do you wish that your t-shirts’ slogans could light up or that your cycling jacket had turn signals on the back? This beginner workshop will introduce the basics of sewing soft circuits and explore current trends in wearable computing.  To hit the ground running, you will leave the workshop with your own pair of texting gloves, either made by hacking a favorite pair or using the samples we provide. All materials will be provided (unless you want to use your own gloves) and no previous experience with sewing or electronics is necessary. Instructor: Julia Schrank If you can’t make this session but would like to learn more about 3D printing, please visit our the student consultants in our Makerspace or email scholarslab@virginia.edu to set up an individual appointment."},{"id":"makerspace-workshop-programming-electronics-on-the-raspberry-pi-2","title":"Makerspace Workshop: Programming Electronics on the Raspberry Pi","author":"laura-miller","date":null,"categories":null,"url":"makerspace-workshop-programming-electronics-on-the-raspberry-pi-2","layout":"events","content":"In this workshop we’ll learn the basics of using your Raspberry Pi to control electronic circuits. No experience with electronics or Raspberry Pi required for this workshop. Due to limitations of hardware, this workshop only has  four  open spots.  Reserve your seat. For the full schedule of the Scholars’ Lab  Fall 2018 Maker &amp; Code Workshop Series, see  here ."},{"id":"makerspace-workshop-programming-electronics-on-the-raspberry-pi","title":"Makerspace Workshop: Programming Electronics on the Raspberry Pi","author":"laura-miller","date":null,"categories":null,"url":"makerspace-workshop-programming-electronics-on-the-raspberry-pi","layout":"events","content":"In this workshop we’ll learn the basics of using your Raspberry Pi to control electronic circuits. No experience with electronics or Raspberry Pi required for this workshop. Due to limitations of hardware, this workshop only has  four  open spots.  Reserve your seat. For the full schedule of the Scholars’ Lab  Fall 2018 Maker &amp; Code Workshop Series, see  here ."},{"id":"makerspace-workshop-working-with-arduino-ii","title":"Makerspace Workshop: Working with Arduino II","author":"ammon-shepherd","date":null,"categories":null,"url":"makerspace-workshop-working-with-arduino-ii","layout":"events","content":"Did you miss October 28 workshop on working with Arduino, or were you hoping to experiment more with the technology? Join the second of two workshops on Wednesday, November 4 in the Scholars’ Lab Makerspace to gain an introduction to the basics of physical computing, components of the Arduino software, and examples of projects that used the software. Like in the first workshop, we will also begin getting our hands dirty with the technology, and those who have more advanced physical computing skills are free to work ahead, more independently on prototyping projects.  Again, no previous experience in electronics or coding is necessary for participating in this workshop! Instructor: Margaret Furr If you can’t make this session but would like to learn more about Arduino boards and shields, please visit the student consultants in our Makerspace or email scholarslab@virginia.edu to set up an individual appointment."},{"id":"makerspace-workshop-working-with-arduinos-i","title":"Makerspace Workshop: Working with Arduino I","author":"ammon-shepherd","date":null,"categories":null,"url":"makerspace-workshop-working-with-arduinos-i","layout":"events","content":"Are you interested in learning about creative ways to sense and control parts of the physical world around you, through applications of electronics and coding? If so, join the first of two workshops on Wednesday, October 28 in the Scholars’ Lab Makerspace! At this workshop, you will have the opportunity to gain an introduction to the basics of physical computing, components of the Arduino software, and examples of projects that used the software. We will also start getting our hands dirty with the technology, so that you can develop your own ideas, and for those who have more advanced physical computing skills, you will be free to work ahead and more independently. No previous experience in electronics or coding is necessary for participating in this workshop. The only requirement is that you are curious about hacking the physical world with the technology! Instructor: Margaret Furr If you can’t make this session but would like to learn more about Arduino boards and shields, please visit the student consultants in our Makerspace or email scholarslab@virginia.edu to set up an individual appointment."},{"id":"makerspace-workshop-working-with-arduinos-iii","title":"Makerspace Workshop: Working with Arduinos III","author":"laura-miller","date":null,"categories":null,"url":"makerspace-workshop-working-with-arduinos-iii","layout":"events","content":"New to microcontrollers? Or used an Arduino before and want more time to play in a supportive environment? Come on by!  Arduino is a tool for making microcomputers that can sense and control the physical world. This workshop will introduce participants to the basics of physical computing and programming through a series of hands-on exercises using our Arduino kits. This workshop builds on the Working with Arduino I and II workshops, but they’re not required to attend this one. If you can’t make this session but would like to learn more about Arduino boards and shields, please visit the student consultants in our Makerspace or email scholarslab@virginia.edu to set up an individual appointment."},{"id":"makerspace-workshop-working-with-arduinos","title":"Makerspace Workshop: Working with Arduinos II","author":"laura-miller","date":null,"categories":null,"url":"makerspace-workshop-working-with-arduinos","layout":"events","content":"New to microcontrollers? Or used an Arduino before and want more time to play in a supportive environment? Come on by!  Arduino is a tool for making microcomputers that can sense and control the physical world. This workshop will introduce participants to the basics of physical computing and programming through a series of hands-on exercises using our Arduino kits. This workshop builds on the Working with Arduino I and II workshops, but they’re not required to attend this one. If you can’t make this session but would like to learn more about Arduino boards and shields, please visit the student consultants in our Makerspace or email scholarslab@virginia.edu to set up an individual appointment."},{"id":"mapathon-for-puerto-rico-disaster-relief","title":"Mapathon for Puerto Rico Disaster Relief","author":"laura-miller","date":null,"categories":null,"url":"mapathon-for-puerto-rico-disaster-relief","layout":"events","content":"Join the the Scholars’ Lab and the UVA Library to assist relief efforts in Puerto Rico. Many aid groups are reporting difficulty providing support to those in need because digital maps of the island are incomplete. Help us improve those maps using  Open Street Maps . You will need a laptop, but no experience is required. Feel free to stop in anytime between 1:00 and 5:00, and stay for as long as you are able. We will have coffee and cookies for those participating. http://www.pbs.org/newshour/rundown/volunteers-helping-puerto-rico-home-map-anyone-can-edit/"},{"id":"marlene-daut-on-haiti-and-the-digital-world","title":"Marlene Daut on Haiti and the Digital World","author":"laura-miller","date":null,"categories":null,"url":"marlene-daut-on-haiti-and-the-digital-world","layout":"events","content":"Haiti and the Digital World : Archiving Black Sovereignty Together in Life and Death In the spirit of Papa Legba (a Haitian lwa who acts as an arbiter between the human and non-human worlds), this paper examines the challenges and opportunities presented when using a digital humanities approach to narrate the creation of two independent “black” states in the early nineteenth century, a critical, but often forgotten, part of the story of the making of the modern world-system. Abdul JanMohamed and David Lloyd have written about “Archival work, as a form of counter-memory” that is “essential to the critical articulation of minority discourse.” However, because archives, like other kinds of texts, reflect the worldview of their creators, the archivist working to articulate “minority discourse” must be careful not to reproduce patterns of domination or cultural exploitation. For Haiti, this means that we must work against the idea that the abundant historical resources now made readily (and often freely) available by various digitization projects, represent a “new frontier” for research, an idea which encourages the notion that the country is “open for business” on a variety of levels. Instead, by using the metaphor of the crossroads, I demonstrate how a multi-modal approach—involving, content, context, collaboration, and access—can allow for alternative ways of (humanely) archiving black sovereignty with respect for both the living and the dead. Dr. Marlene Daut is an Associate Professor of African Diaspora Studies, Carter G. Woodson Institute for African-American and African Studies, American Studies at the University of Virginia. This talk, co-sponsored by the Carter G. Woodson Institute and the Scholars’ Lab, is free and open to the public."},{"id":"metagaming-talk-workshop","title":"Metagaming Talk with Stephanie Boluk and Patrick LeMieux","author":"laura-miller","date":null,"categories":null,"url":"metagaming-talk-workshop","layout":"events","content":"Metagames to Moneygames: Skin in the Game In 1987, a pyramid scheme called the “Plane Game” funnelled hundreds of thousands of dollars from the pockets of “passengers,” landing at least six of the game’s “pilots” in jail. In 2018, more ubiquitous moneygames are played with smaller stakes across far wider fields. From the Valve Corporation’s Flatland to grey market gambling with Counter-Strike gun skins, this talk will move from from the Steam Workshop to the Steam Marketplace to series of third-party websites that confuse and conflate gaming and gambling. Although strict distinctions are made between gambling and gaming in both US law as well as 20th century philosophies of games and play, these terms’ etymological roots are tightly wound. In a post-2008 age of precarity, the wage has once again become a wager. In 2012, Alex Galloway proclaimed “we are all goldfarmers,” but gun skins and skin gambling represent an even more complex and complete financialization in that players have moved from one mode in which labour time is exchanged for a clear wage (even if it’s grinding in World of Warcraft ) to one in which labour time itself becomes a wager. Ultimately skins are not simply texture files that wrap around the polygonal geometry of virtual weapons. Instead, they are objects of affinity and status, digital cash and casino chips, and a gun skins’ procedurally generated pattern, determined by a 9-digit floating point number selected upon unboxing, is more cryptocurrency than art asset. In this talk we follow the money, the skin, the flow, and the flight of new “plane games” as metagames become moneygames. There will also be a workshop led by the authors from 2:00 pm -4:00 pm in Alderman, room 421 Stephanie Boluk is an associate professor in the English Department and Cinema and Digital Media Department at the University of California, Davis. She is the co-author of Metagaming: Playing, Competing, Spectating, Cheating, Trading, Making, and Breaking Videogames ( https://manifold.umn.edu/project/metagaming ) with Patrick LeMieux and co-editor of The Electronic Literature Collection Vol. 3 ( http://collection.eliterature.org/3/ ) with Leonardo Flores, Jacob Garbe, and Anastasia Salter. For more information visit http://stephanieboluk.com . Patrick LeMieux is an artist, game designer, and assistant professor in the Cinema and Digital Media Department at the University of California, Davis where he co-directs the Alt Ctrl Lab. Recent projects include Metagaming, a co-authored book with Stephanie Boluk ( https://manifold.umn.edu/project/metagaming ) and Platform Games, a solo exhibition at Babycastles ( http://babycastles.com/Platform-Games ). For more information visit http://patrick-lemieux.com ."},{"id":"michelle-moravec-on-how-to-make-it-as-a-woman-into-wikipedia","title":"Michelle Moravec on How to Make it as a Woman in(to) Wikipedia","author":"laura-miller","date":null,"categories":null,"url":"michelle-moravec-on-how-to-make-it-as-a-woman-into-wikipedia","layout":"events","content":"Michelle Moravec is an associate professor of history at Rosemont College in Philadelphia, where she also teaches women’s and gender studies. Her method of Writing In Public is indebted to the practices of the feminist activists she studies.  She has published extensively about feminist art and social movements in the United States.  Her current project, The Politics of Women’s Culture, uses a combination of digital and traditional approaches to produce an intellectual history of the concept of women’s culture."},{"id":"minimal-computing-workshop-2019-02-07","title":"Minimal Computing Workshop","author":"brandon-walsh","date":null,"categories":null,"url":"minimal-computing-workshop-2019-02-07","layout":"events","content":"How to use basic computational techniques, as opposed to graphic user interfaces, in order to deploy and sustain web projects. In this workshop we will specifically look at Jekyll—a static site generator—and its humanistic uses. Alex Gil is the Digital Scholarship Librarian at Columbia University Libraries and Affiliate Faculty of the Department of English and Comparative Literature at Columbia University.  This event is co-sponsored by the UVA Library, DH@UVA, and the Scholars’ Lab."},{"id":"miriam-suzannes-practical-guide-to-losing-control","title":"Miriam Suzanne's Practical Guide to Losing Control","author":"laura-miller","date":null,"categories":null,"url":"miriam-suzannes-practical-guide-to-losing-control","layout":"events","content":"A Practical Guide to Losing Control What do we do about the power dynamics between creator and consumer, artist and audience, client and creative, developer and user, collaborators, even author and medium and object? As creators, it’s tempting to grab and consolidate our power, but sometimes the medium or audience has a different opinion. Maybe we’ll look at my experiences of chaos as a professional web developer and mixed-media artist — what’s worked and what hasn’t. Maybe we’ll talk about differences in media, or how they mix. Maybe we’ll talk about the performative turn, or some other academic jargon. Maybe we’ll get way off topic. These things are hard to predict. Miriam Eric Suzanne is an author, performer, musician, designer, and web developer — working with OddBird, Teacup Gorilla, and CSS Tricks . Her work deals with design thinking in digital humanites, as well as performance studies and critical code studies.  Questioning the politics of media and technology as an insider, her work also address practical concerns within media archeology broadly. She’s previously been artist in residence at the Media Archaeology Lab at the University of Colorado Boulder and lives in Denver, Colorado. She’s the author of Riding SideSaddle * and The Post-Obsolete Book, co-author of Jump Start Sass and 10 Myths on the Proper Application of Beauty Products, and creator of the Susy and True open-source design toolkits. Miriam has also been published under the names “Eric A Meyer” and “Eric M Suzanne” — but you should call her Miriam, or Mia, or maybe Princess DieHard, or something like that."},{"id":"more-advanced-websites-using-jekyll","title":"Workshop: Using Jekyll to Create Websites","author":"laura-miller","date":null,"categories":null,"url":"more-advanced-websites-using-jekyll","layout":"events","content":"Take your website building to the next level. This workshop requires some experience with HTML, CSS and Javascript. We will be using Jekyll to build a static website, with the benefit of Jekyll’s templates, tags, and other features. Bring your own laptop or we can provide one. We’ll work through a project together to learn the skills that you can apply later on your own."},{"id":"moving-people-linking-lives-an-interdisciplinary-symposium","title":"Moving People, Linking Lives: An Interdisciplinary Symposium","author":"laura-miller","date":null,"categories":null,"url":"moving-people-linking-lives-an-interdisciplinary-symposium","layout":"events","content":"Friday, March 20 events will take place in the Kaleidoscope Room in Newcomb Hall.\nSaturday, March 21 events will take place in Alderman 421 except for an evening reception in the Colonnade Club. Presentations and workshops will open dialogue across different fields, periods, and methods, from textual interpretation to digital research. Invited participants include specialists on narrative theory and life writing, prosopography or comparative studies of life narratives in groups, and the diverse field of digital humanities or computer-assisted research on cultural materials, from ancient texts to Colonial archives, from printed books to social media. Organized and hosted by Alison Booth, Jenny Strauss Clay, and Amy Odgen and sponsored by the Page Barbour Committee, the departments of English, French, and Art, the Institute for Humanities and Global Cultures, the Scholars’ Lab, the Institute for Advanced Technology in the Humanities, and other entities at UVa, all events are free and open to the public. A full listing of sessions can be found on the symposium website, and you can follow @livesdh on twitter for updates."},{"id":"nimble-tents-xpmethod-tornapart","title":"DH@UVA Speaker Alex Gil - Nimble Tents: xpmethod, #tornapart, and Other Tensile Approaches to the Fourth Estate","author":"Brandon-Walsh","date":null,"categories":null,"url":"nimble-tents-xpmethod-tornapart","layout":"events","content":"Many years after the idea of a digital humanities galvanized different genealogies of humanistic practice around the world, most institutions in North America have by now each attracted various forms of related talent to their libraries, departments and centers to help build capacity at the institutional level. What happens when that talent begins to collaborate across institutions at a massive scale? Or intra-institutionally guided by their own collaborative light outside established and unflinching reward mechanisms? In this presentation, Dr. Alex Gil will argue for a form of rapid organizing for change in non-hierarchical formats that can effectively draw from our collective talent pool in the digital humanities and adjacent formations. Using several specific case studies, including the most recent Torn Apart/Separados effort, the idea of a nimble tent, a mobilized humanities, will emerge as a possible bridge between the long-game of scholarship, and the short-game of political action in the now. Alex Gil is the Digital Scholarship Librarian at Columbia University Libraries and Affiliate Faculty of the Department of English and Comparative Literature at Columbia University. This event is co-sponsored by the UVA Library, DH@UVA, and the Scholars’ Lab."},{"id":"non-traditional-dissertations-panel-workshop","title":"Diss-entangling from the Monograph: A Non-traditional Dissertations Panel","author":"laura-miller","date":null,"categories":null,"url":"non-traditional-dissertations-panel-workshop","layout":"events","content":"As part of Grad Days 2018, join us for a conversation about dis-entangling from the monograph dissertation. Our panelists will share their own experiences in departing from the traditional printed format — through hip-hop, comics, code and other forms of doctoral research — and will answer your questions during our Q + A! Coffee and snacks will be provided. A.D. Carson is a performance artist and professor in Hip-Hop and the Global South at UVa. His award-winning 2017 dissertation from Clemson University, “Owning My Masters: The Rhetorics of Rhymes &amp; Revolutions,” is a digital archive that features a 34-track rap album. Nick Sousanis is a professor in Humanities &amp; Liberal Studies at San Francisco State University, where he is starting an interdisciplinary Comics Studies program. His doctoral dissertation, Unflattening, written and drawn entirely in comic form, was published by Harvard University Press in 2015. Amanda Visconti is the Managing Director of the Scholars’ Lab at UVa. Her 2015 dissertation from the University of Maryland fully acknowledges digital methods as scholarship by treating them as the dissertation instead of addenda to traditional written chapters. Her participatory digital edition of James Joyce’s Ulysses, InfiniteUlysses.com, was cited in the New York Times."},{"id":"panel-discussion-digital-pedagogy-in-a-liberal-arts-context","title":"Panel Discussion: Digital Pedagogy in a Liberal Arts Context","author":"laura-miller","date":null,"categories":null,"url":"panel-discussion-digital-pedagogy-in-a-liberal-arts-context","layout":"events","content":"For the last few years, UVA graduate students have had the opportunity to visit Washington and Lee University to facilitate workshops and give lectures on digital humanities topics. On Wednesday, October 26th W&amp;L Faculty and UVA graduate students will speak about the collaboration, discussing lessons learned from digital humanities in a liberal arts context and possibilities for future collaborations. The panel of speakers will discuss a range of topics broached in these classroom visits, including collaborative digital annotation, introductory project management for digital work, and humanities software development for non-programmers. Throughout, the presenters will focus on the professional, pedagogical, and personal opportunities that can be gained by graduate students gaining experience teaching in a liberal arts environment. We will have ample time for conversation, as we hope the event will seed future collaborations among researchers at both institutions. Panelists: Paul A. Youngman is Professor of German, Head of the Department of German and Russian, and Chair of the Digital Humanities Working Group at Washington and Lee University. His research focuses on the German cultural reception of various technologies. He has also published widely on Nanoscience and technology.  To learn more about Professor Youngman and his research, please visit https://www.hastac.org/u/pyoungman Mackenzie Brooks is Assistant Professor and Digital Humanities Librarian at Washington and Lee University. Prior to her current position, she worked as Metadata Librarian at W&amp;L and at the Loyola University Chicago Health Sciences Library. As a member of the Digital Humanities Action Team, she advises faculty and students on best practices for metadata standards in digital humanities projects. Additionally, she teaches undergraduate courses on scholarly text encoding and digital humanities. Holly Pickett is Associate Professor of English at Washington and Lee specializing in Religion and Early Modern Drama. Brandon Walsh is Mellon Digital Humanities Fellow and Visiting Assistant Professor in English at the Washington and Lee University Library. He received his PhD from the University of Virginia. He works at the intersections of modern and contemporary literature and culture, sound studies, and digital humanities. Sarah Storti is a PhD candidate in English at the University of Virginia. She was a 2011-12 Praxis Fellow, 2010-11 and 2012-13 NINES Fellow, and is currently working on a dissertation about Letitia Elizabeth Landon’s poetics of representation. Kelli Shermeyer is a doctoral candidate in English who works on modern and contemporary literature with a focus on drama and theater studies. She’s interested in how the digital humanities can help to record, preserve, and better analyze performance events."},{"id":"paul-fyfe-on-past-and-presentism-how-computers-see-victorian-periodicals-2","title":"Paul Fyfe on Past and Presentism: How Computers See Victorian Periodicals","author":"laura-miller","date":null,"categories":null,"url":"paul-fyfe-on-past-and-presentism-how-computers-see-victorian-periodicals-2","layout":"events","content":"This talk shares experiments at NC State University to use computer vision and image analytics on a corpus of nineteenth-century illustrated periodicals. These experiments help push digital humanities to consider more multimodal content than text as well as provoke reflections about the historical functions of illustrated periodicals and the development of concepts of “visualization.” Ultimately, he argues for a dynamic of digital experiment and Victorian media as an example of the “strategic presentism” underway in literary studies. **Paul Fyfe **is Associate Professor of English at North Carolina State University. His research and teaching include British Victorian literature, nineteenth-century book and media history, scholarly communications, and a variety of digital humanities practices.  Currently, he is working on a book called The Age of Transmission, a long history of digital humanities based in nineteenth-century media cultures. This work is generously supported by a 2018-2019 ACLS Burkhardt Fellowship at the National Humanities Center. He also participates in a few digital research projects, such as Oceanic Exchanges which tracks information flow across international nineteenth-century newspaper networks; Illustrated Image Analytics which experiments with how computer vision can search and sort Victorian periodical illustrations; Speech Across Dialects of English ( SPADE ) with colleagues in linguistics; and Victoria’s Lost Pavilion which virtually reconstructs Queen Victoria’s garden pavilion as a three-dimensional model."},{"id":"practicing-dh-symposium-keynote-ethan-wattrall","title":"Speaker: Ethan Watrall on Openness in Digital Heritage","author":"laura-miller","date":null,"categories":null,"url":"practicing-dh-symposium-keynote-ethan-wattrall","layout":"events","content":"Practicing Openness in Digital Heritage Encouraged by exciting advances in digital technology, the issue of openness has swept into almost every corner of the scholarly world.  Research, publication, teaching, public engagement, and even the very fabric of scholarly ethics have all been touched by this discussion.  The domain of heritage is hardly immune from the debate.  Disciplines such as History, Archaeology, Heritage Management, Museum Studies, and Anthropology along with heritage institutions such as libraries, archives, and museums are all wrestling with how openness within a digital space impacts their core identity and professional practice.\nIf one thing has emerged from this discussion, it is that there is no universal set of practices that can be homogeneously applied to all fields, disciplines, and types of institutions that address the challenge of openness.  Each domain demands an approach that is clearly and thoughtfully tailored to its unique professional circumstances.\nIt is within this context that this talk will parse the idea of openness, exploring the issue both broadly and within the unique context of digital heritage.  In order to maximize its value to scholars, professional practitioners, and institutions, the talk will also suggest a series of thoughtful strategies that can be leveraged in order to better embrace a more open approach to work within digital heritage. An anthropological archaeologist who has worked in North America and the Near East, Ethan Watrall is Assistant Professor in the Department of Anthropology and Associate Director of MATRIX: The Center for Digital Humanities &amp; Social Sciences at Michigan State University. In addition, Ethan is Director of the Cultural Heritage Informatics Initiative and the Cultural Heritage Informatics Fieldschool at Michigan State University.  Currently, Ethan is Co-PI of the NEH funded ARCS: Archaeological Resource Cataloguing System project, Director of the NEH funded Institute for Digital Archaeological Method and Practice project, and Co-PI of the NEH funded Archive of Malian Photography project. Ethan’s interests primarily fall in the domain of publicly engaged digital heritage and archaeology. Ethan is co-editor of Archaeology 2.0: New Tools for Communication and Collaboration, an open access volume published by the UCLA Cotsen Institute of Archaeology Press."},{"id":"praxis-fellows-panel-luncheon","title":"Praxis Fellows Panel & Luncheon","author":"laura-miller","date":null,"categories":null,"url":"praxis-fellows-panel-luncheon","layout":"events","content":"As the semester winds down, we’d like to invite you to join us for our year-end Praxis Graduate Student Fellows panel. Lunch will follow each presentation. Please plan to come for the presentation and stay for the refreshments! DASH-Amerikan: Keeping Up with the Kardashian Media Ecologies In 2015, the season premiere of  Keeping Up with the Kardashians  was the most viewed Sunday cable program, out performing the series finale of the critically acclaimed television drama,  Mad Men . Kim and company have never received the critical adulation that Don Draper elicited. Yet the Kardashians’ sheer popularity demands further inspection by scholars who claim interest in the cultural productions that reflect and shape our current historical moment. The Kardashians remain particularly important for their ability to normalize a matriarchal family structure and transform the traditionally private sphere of the home into their center of business, all while maintaining heteronormative assumptions about the objectification of women. Furthermore, their use of Twitter, Instagram, mobile apps, online blogs, and even tabloid coverage work together to engage fans in an unending advertisement for their show. Whether commenting on news events, promoting awareness of the Armenian genocide, or producing extravagant television specials, the Kardashian family has harnessed the 24/7 media landscape in new and unprecedented ways. The 2016-2017 Praxis Cohort, which includes Jordan Buysse (English), Alicia Caticha (Art &amp; Architectural History), Alyssa Collins **(English), **Justin Greenlee (Art &amp; Architectural History), Sarah McEleney (Slavic Languages &amp; Literatures), and Joseph Thompson (History), examines the Kardashian media empire to reveal the discursive power of celebrities, fans, and their critics. To do so, they created a data set of every tweet written by a member of the Kardashian family, as well as every relevant  US Weekly  article, by using web scraping, text analysis, and topic modeling techniques. Using these digital humanities methods, they aim to interrogate constructions of race, class, and sexuality in contemporary popular culture and the pervasiveness of social media on our daily lives."},{"id":"praxis-graduate-student-fellows-panel-luncheon","title":"Praxis Graduate Student Fellows Panel & Luncheon","author":"laura-miller","date":null,"categories":null,"url":"praxis-graduate-student-fellows-panel-luncheon","layout":"events","content":"Please join us for an Ivanhoe Roundtable! Our fantastic 2014-15 Praxis Fellows will describe the collaboratively-built Ivanhoe game and what makes it compelling. They will explore issues of multiple audiences and pedagogical possibilities through two example games.  Lunch will follow the presentation."},{"id":"praxis-program-fellows-final-presentation-spring-2019-04-29","title":"Praxis Program Fellows Final Presentation - Spring 2019","author":"first-last","date":null,"categories":null,"url":"praxis-program-fellows-final-presentation-spring-2019-04-29","layout":"events","content":"Final presentation of the 2018-2019 Praxis Program fellows. The Fellows will present Unclosure: An Act for the Encouragement of Learning, a DH project resulting from their year-long collaboration. The event will run from 10:30am–12 noon and lunch will follow. Presenters will include: Catherine Addington, Spanish Zhiqiu Jiang, Urban and Environmental Planning, Constructed Environment Emily Mellen, Music Eleanore Neumann, Art and Architectural History Mathilda Shepard, Spanish Chris Whitehead, History Unclosure: An Act for the Encouragement of Learning explores the possibilities for digital research, pedagogy, and play in the newly expanded public domain . Using Robert Frost’s New Hampshire (1923) as raw material, Unclosure presents a series of tutorials that seek to empower researchers, instructors, and creators to critically engage public-domain materials."},{"id":"praxis-program-presentations","title":"Praxis Program Presentation & Lunch","author":"brandon-walsh","date":null,"categories":null,"url":"praxis-program-presentations","layout":"events","content":"The Praxis Program Fellows will present the result of their year-long collaboration working on an original digital humanities project. The event will run from 10:30-12:00 and lunch will follow. Presenters will include: Monica Blair, History Ankita Chakrabarti, English Victoria Clark, Music, Critical and Comparative Studies Tanner Greene, Music, Critical and Comparative Studies Christian Howard, English Spyros Simotas, French Project Description: The 2017-2018 Praxis cohort has moved the material history of UVA out of Special Collections and onto Grounds. Using a mobile Artifical Reality application, UVA Reveal explores otherwise hidden stories, histories, and questions surrounding objects and areas at UVA. In doing so, UVA Reveal prompts users to re-examine their everyday spaces and critically reflect on the structure, culture, mission, and history of the university."},{"id":"programming-on-the-raspberry-pi","title":"Makerspace Workshop: Programming Electronics with Python on the Raspberry Pi","author":"ammon-shepherd","date":null,"categories":null,"url":"programming-on-the-raspberry-pi","layout":"events","content":"In this workshop we’ll learn the basics of using your Raspberry Pi for basic Python programming to control LEDs. You should be familiar with using a Raspberry Pi before joining this workshop. Due to limitations of hardware, this workshop only has four open spots. To reserve a spot, send an email to ammon@virginia.edu."},{"id":"public-libraries-and-academic-libraries-digital-partners","title":"Public Libraries and Academic Libraries: Digital Partners?","author":"jeremy-boggs","date":null,"categories":null,"url":"public-libraries-and-academic-libraries-digital-partners","layout":"events","content":"The growth and development of technology, computers, software, and the Internet have changed the ways in which libraries function, operate, and are being used by the communities they serve. Public libraries have been playing a bit of catch-up in many ways related to the growth of digital services due to the fact that public libraries are also still serving users whose information needs include more traditional resources. Is there a common ground in mission and scope that public libraries and academic libraries serve together? In what ways are these services complementary and what are the ways in which each of these institutions can learn from and share with one another?"},{"id":"raspberry-pi-workshop-2019-02-04","title":"Raspberry Pi Workshop","author":"ammon-shepherd","date":null,"categories":null,"url":"raspberry-pi-workshop-2019-02-04","layout":"events","content":"In this workshop we’ll learn the basics of using your Raspberry Pi to control electronic circuits. No experience with electronics or Raspberry Pi required for this workshop. Due to limitations of hardware, this workshop only has four open spots. Reserve your seat. Register here: https://cal.lib.virginia.edu/event/5009950"},{"id":"raspberry-pi-workshop-2019-03-04","title":"Raspberry Pi Workshop","author":"ammon-shepherd","date":null,"categories":null,"url":"raspberry-pi-workshop-2019-03-04","layout":"events","content":"In this workshop we’ll learn the basics of using your Raspberry Pi to control electronic circuits. No experience with electronics or Raspberry Pi required for this workshop. Due to limitations of hardware, this workshop only has four open spots. Reserve your seat. Register here: https://cal.lib.virginia.edu/event/5010175"},{"id":"raspberry-pi-workshop-2019-04-01","title":"Raspberry Pi Workshop","author":"ammon-shepherd","date":null,"categories":null,"url":"raspberry-pi-workshop-2019-04-01","layout":"events","content":"In this workshop we’ll learn the basics of using your Raspberry Pi to control electronic circuits. No experience with electronics or Raspberry Pi required for this workshop. Due to limitations of hardware, this workshop only has four open spots. Reserve your seat. Register here: https://cal.lib.virginia.edu/event/5010185"},{"id":"reading-booksigning-miriam-suzanne","title":"Reading & Booksigning: Miriam Suzanne","author":"laura-miller","date":null,"categories":null,"url":"reading-booksigning-miriam-suzanne","layout":"events","content":"Join us at OpenGrounds on the Corner as Miriam reads from her mixed-media, open-source novel, Riding SideSaddle*.  This event is free and open to the public. Riding SideSaddle * is a fragmented memory written on 250 interchangeable index cards — following a cast of friends as they navigate fluid genders, relationships, and bodies that resist order, category, or completion. Based loosely on the life of Margaret Clap, and the myth of Salmacis and Hermaphroditus — SideSaddle* is an open-source text published by SpringGun Press, released online by OddBird, and adapted to the stage by Buntport Theater and Teacup Gorilla ."},{"id":"research-data-services-spring-workshops","title":"Research Data Services Spring Workshops","author":"laura-miller","date":null,"categories":null,"url":"research-data-services-spring-workshops","layout":"events","content":"UVA Library’s Research Data Services group offers workshops throughout the semester which may be of interest to digital scholars in the arts and humanities.   They focus on data analysis and statistics, computation and software, as well as on Library resources and methods. Anyone in the UVA community may attend these free events. In January and February, they are offering: Intro to R\nIntro to Python Web Scraping in R with rvest\nIntroduction to Unix\nWeb Scraping in Python\nSentiment Analysis in R Character Manipulation in R\nText Processing and Topic Modeling in Python\nTopic Modeling in R\nIntroduction to R Markdown For more information and to register, please see Research Data Services’ workshops page."},{"id":"scholars-lab-fall-symposium-kari-kraus-daniela-rosner","title":"Scholars' Lab Fall Symposium: Story Circuits: Creating & Recovering Embodied Memory","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-fall-symposium-kari-kraus-daniela-rosner","layout":"events","content":"Due to the hands-on nature of these workshops, seating is limited. Please register early to reserve your spot! 9:30-11:30 - **Strange Frequencies: Book History in the Age of Sensors Books–as librarians and archivists know–have always been the record keepers of readers’ interactions with them: a book read by candlelight is likely to retain telltale drops of wax on its pages, for example, while one transported in the rain will acquire foxing stains. In this workshop, we’ll explore how much more can we learn about a book once we’ve endowed it with tiny digital prosthetics that document ever so much more of its history. Properly preserved and maintained, a book outfitted with sensors could enhance book history as a field of study by magnifying our ability to tell a story about its past, such as the precise date, time, and place it was opened or read aloud or subjected to the mishaps of a careless reader who spilled coffee on it. At the same time, by registering detailed information about human lives, such sensing books raise potentially troubling issues around privacy and surveillance. Through discussion, scenario building, and prototyping, we’ll investigate four inter-related questions: 1.) What types of sensors are the most interesting and revelatory to embed in physical books?  2.) What can we infer about the past by analyzing the activity traces captured by these sensors? 3.) What are the most compelling ways to visualize and display sensor data collected from books? 4.) How we can ensure that anti-surveillance values are reflected in the design of such digitally augmented books? Participants will have the chance to experiment with a variety of sensors, including those detecting motion, temperature, sound, and humidity. We’ll also go beyond the usual repertoire of commercially available technologies by examining some non-digital sensors, as well as a few gadgets that have achieved notoriety as “ghost tech”, such as ultra-sensitive vibration and electrostatic sensors: the “strange frequencies” of the workshop title. Kari Kraus is an associate professor in the College of Information Studies and the Department of English at the University of Maryland. Her research and teaching interests focus on new media and the digital humanities; textual scholarship, print culture, and the history of the book; digital preservation; game studies; transmedia storytelling; and speculative design. 12:00 - 1:30: Lunch 1:30 - 3:30 - Making Core Memory The project centers on an electronic quilt that materializes the work of early core memory weavers (see description here: http://makingcorememory.com). During the workshops I hand out “patch kits” made of a board loom, conductive fabric, yarn, and beads (in place of the original wire and ferrite cores). After people weave their patches, they plug their patches into the electronic textile quilt to unlock our archive of first-hand accounts of core memory production. The quilt plays audio clips from our archive and tweets from our @lolweavers account. I developed the project with the hope of examining the forms of gendered craftwork and its valuation as technical work over time – exploring and to some extent challenging the prevailing purification of high status cognitive labor associated with male engineers from the ostensibly unthinking and unskilled practices of women’s hands. Daniela Rosner is an Assistant Professor in Human Centered Design &amp; Engineering (HCDE) at the University of Washington. Her research investigates the social, political, and material circumstances of technology development, with an emphasis on foregrounding marginalized histories of practice, from maintenance to needlecraft. She has worked in design research at Microsoft Research, Adobe Systems, Nokia Research and as an exhibit designer at several museums, including the Adler Planetarium and Astronomy Museum.  "},{"id":"scholars-lab-open-office-hours-2-2018-09-19","title":"Scholars’ Lab Open Office Hours","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-open-office-hours-2-2018-09-19","layout":"events","content":"Wednesdays throughout the Fall Semester Just drop in – no appointment needed! Come with your curiosity, your ideas for projects, your technical, design, and project scoping questions, or just the desire to meet others engaged in digital work.  If you can’t make our Wednesday hours, you are always welcome to come by our office in Alderman 415 or schedule an appointment at  scholarslab@virginia.edu ."},{"id":"scholars-lab-open-office-hours-2-2018-09-26","title":"Scholars’ Lab Open Office Hours","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-open-office-hours-2-2018-09-26","layout":"events","content":"Wednesdays throughout the Fall Semester Just drop in – no appointment needed! Come with your curiosity, your ideas for projects, your technical, design, and project scoping questions, or just the desire to meet others engaged in digital work.  If you can’t make our Wednesday hours, you are always welcome to come by our office in Alderman 415 or schedule an appointment at  scholarslab@virginia.edu ."},{"id":"scholars-lab-open-office-hours-2-2018-10-03","title":"Scholars’ Lab Open Office Hours","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-open-office-hours-2-2018-10-03","layout":"events","content":"Wednesdays throughout the Fall Semester Just drop in – no appointment needed! Come with your curiosity, your ideas for projects, your technical, design, and project scoping questions, or just the desire to meet others engaged in digital work.  If you can’t make our Wednesday hours, you are always welcome to come by our office in Alderman 415 or schedule an appointment at  scholarslab@virginia.edu ."},{"id":"scholars-lab-open-office-hours-2-2018-10-10","title":"Scholars’ Lab Open Office Hours","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-open-office-hours-2-2018-10-10","layout":"events","content":"Wednesdays throughout the Fall Semester Just drop in – no appointment needed! Come with your curiosity, your ideas for projects, your technical, design, and project scoping questions, or just the desire to meet others engaged in digital work.  If you can’t make our Wednesday hours, you are always welcome to come by our office in Alderman 415 or schedule an appointment at  scholarslab@virginia.edu ."},{"id":"scholars-lab-open-office-hours-2-2018-10-17","title":"Scholars’ Lab Open Office Hours","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-open-office-hours-2-2018-10-17","layout":"events","content":"Wednesdays throughout the Fall Semester Just drop in – no appointment needed! Come with your curiosity, your ideas for projects, your technical, design, and project scoping questions, or just the desire to meet others engaged in digital work.  If you can’t make our Wednesday hours, you are always welcome to come by our office in Alderman 415 or schedule an appointment at  scholarslab@virginia.edu ."},{"id":"scholars-lab-open-office-hours-2-2018-10-24","title":"Scholars’ Lab Open Office Hours","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-open-office-hours-2-2018-10-24","layout":"events","content":"Wednesdays throughout the Fall Semester Just drop in – no appointment needed! Come with your curiosity, your ideas for projects, your technical, design, and project scoping questions, or just the desire to meet others engaged in digital work.  If you can’t make our Wednesday hours, you are always welcome to come by our office in Alderman 415 or schedule an appointment at  scholarslab@virginia.edu ."},{"id":"scholars-lab-open-office-hours-2-2018-10-31","title":"Scholars’ Lab Open Office Hours","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-open-office-hours-2-2018-10-31","layout":"events","content":"Wednesdays throughout the Fall Semester Just drop in – no appointment needed! Come with your curiosity, your ideas for projects, your technical, design, and project scoping questions, or just the desire to meet others engaged in digital work.  If you can’t make our Wednesday hours, you are always welcome to come by our office in Alderman 415 or schedule an appointment at  scholarslab@virginia.edu ."},{"id":"scholars-lab-open-office-hours-2-2018-11-07","title":"Scholars’ Lab Open Office Hours","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-open-office-hours-2-2018-11-07","layout":"events","content":"Wednesdays throughout the Fall Semester Just drop in – no appointment needed! Come with your curiosity, your ideas for projects, your technical, design, and project scoping questions, or just the desire to meet others engaged in digital work.  If you can’t make our Wednesday hours, you are always welcome to come by our office in Alderman 415 or schedule an appointment at  scholarslab@virginia.edu ."},{"id":"scholars-lab-open-office-hours-2-2018-11-14","title":"Scholars’ Lab Open Office Hours","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-open-office-hours-2-2018-11-14","layout":"events","content":"Wednesdays throughout the Fall Semester Just drop in – no appointment needed! Come with your curiosity, your ideas for projects, your technical, design, and project scoping questions, or just the desire to meet others engaged in digital work.  If you can’t make our Wednesday hours, you are always welcome to come by our office in Alderman 415 or schedule an appointment at  scholarslab@virginia.edu ."},{"id":"scholars-lab-open-office-hours-2","title":"Scholars’ Lab Open Office Hours","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-open-office-hours-2","layout":"events","content":"Wednesdays throughout the Fall Semester Just drop in – no appointment needed! Come with your curiosity, your ideas for projects, your technical, design, and project scoping questions, or just the desire to meet others engaged in digital work.  If you can’t make our Wednesday hours, you are always welcome to come by our office in Alderman 415 or schedule an appointment at  scholarslab@virginia.edu ."},{"id":"scholars-lab-open-office-hours-2018-02-05","title":"Scholars' Lab Open Office Hours","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-open-office-hours-2018-02-05","layout":"events","content":"Mondays throughout the Spring Semester Just drop in - no appointment needed! Come with your curiosity, your ideas for projects, your technical, design, and project scoping questions, or just the desire to meet others engaged in digital work.  If you can’t make our Monday hours, you are always welcome to come by our office in Alderman 415 or schedule an appointment at scholarslab@virginia.edu ."},{"id":"scholars-lab-open-office-hours-2018-02-12","title":"Scholars' Lab Open Office Hours","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-open-office-hours-2018-02-12","layout":"events","content":"Mondays throughout the Spring Semester Just drop in - no appointment needed! Come with your curiosity, your ideas for projects, your technical, design, and project scoping questions, or just the desire to meet others engaged in digital work.  If you can’t make our Monday hours, you are always welcome to come by our office in Alderman 415 or schedule an appointment at scholarslab@virginia.edu ."},{"id":"scholars-lab-open-office-hours-2018-02-19","title":"Scholars' Lab Open Office Hours","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-open-office-hours-2018-02-19","layout":"events","content":"Mondays throughout the Spring Semester Just drop in - no appointment needed! Come with your curiosity, your ideas for projects, your technical, design, and project scoping questions, or just the desire to meet others engaged in digital work.  If you can’t make our Monday hours, you are always welcome to come by our office in Alderman 415 or schedule an appointment at scholarslab@virginia.edu ."},{"id":"scholars-lab-open-office-hours-2018-02-26","title":"Scholars' Lab Open Office Hours","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-open-office-hours-2018-02-26","layout":"events","content":"Mondays throughout the Spring Semester Just drop in - no appointment needed! Come with your curiosity, your ideas for projects, your technical, design, and project scoping questions, or just the desire to meet others engaged in digital work.  If you can’t make our Monday hours, you are always welcome to come by our office in Alderman 415 or schedule an appointment at scholarslab@virginia.edu ."},{"id":"scholars-lab-open-office-hours-2018-03-05","title":"Scholars' Lab Open Office Hours","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-open-office-hours-2018-03-05","layout":"events","content":"Mondays throughout the Spring Semester Just drop in - no appointment needed! Come with your curiosity, your ideas for projects, your technical, design, and project scoping questions, or just the desire to meet others engaged in digital work.  If you can’t make our Monday hours, you are always welcome to come by our office in Alderman 415 or schedule an appointment at scholarslab@virginia.edu ."},{"id":"scholars-lab-open-office-hours-2018-03-12","title":"Scholars' Lab Open Office Hours","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-open-office-hours-2018-03-12","layout":"events","content":"Mondays throughout the Spring Semester Just drop in - no appointment needed! Come with your curiosity, your ideas for projects, your technical, design, and project scoping questions, or just the desire to meet others engaged in digital work.  If you can’t make our Monday hours, you are always welcome to come by our office in Alderman 415 or schedule an appointment at scholarslab@virginia.edu ."},{"id":"scholars-lab-open-office-hours-2018-03-19","title":"Scholars' Lab Open Office Hours","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-open-office-hours-2018-03-19","layout":"events","content":"Mondays throughout the Spring Semester Just drop in - no appointment needed! Come with your curiosity, your ideas for projects, your technical, design, and project scoping questions, or just the desire to meet others engaged in digital work.  If you can’t make our Monday hours, you are always welcome to come by our office in Alderman 415 or schedule an appointment at scholarslab@virginia.edu ."},{"id":"scholars-lab-open-office-hours-2018-03-26","title":"Scholars' Lab Open Office Hours","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-open-office-hours-2018-03-26","layout":"events","content":"Mondays throughout the Spring Semester Just drop in - no appointment needed! Come with your curiosity, your ideas for projects, your technical, design, and project scoping questions, or just the desire to meet others engaged in digital work.  If you can’t make our Monday hours, you are always welcome to come by our office in Alderman 415 or schedule an appointment at scholarslab@virginia.edu ."},{"id":"scholars-lab-open-office-hours-2018-04-02","title":"Scholars' Lab Open Office Hours","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-open-office-hours-2018-04-02","layout":"events","content":"Mondays throughout the Spring Semester Just drop in - no appointment needed! Come with your curiosity, your ideas for projects, your technical, design, and project scoping questions, or just the desire to meet others engaged in digital work.  If you can’t make our Monday hours, you are always welcome to come by our office in Alderman 415 or schedule an appointment at scholarslab@virginia.edu ."},{"id":"scholars-lab-open-office-hours-2018-04-09","title":"Scholars' Lab Open Office Hours","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-open-office-hours-2018-04-09","layout":"events","content":"Mondays throughout the Spring Semester Just drop in - no appointment needed! Come with your curiosity, your ideas for projects, your technical, design, and project scoping questions, or just the desire to meet others engaged in digital work.  If you can’t make our Monday hours, you are always welcome to come by our office in Alderman 415 or schedule an appointment at scholarslab@virginia.edu ."},{"id":"scholars-lab-open-office-hours-2018-04-16","title":"Scholars' Lab Open Office Hours","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-open-office-hours-2018-04-16","layout":"events","content":"Mondays throughout the Spring Semester Just drop in - no appointment needed! Come with your curiosity, your ideas for projects, your technical, design, and project scoping questions, or just the desire to meet others engaged in digital work.  If you can’t make our Monday hours, you are always welcome to come by our office in Alderman 415 or schedule an appointment at scholarslab@virginia.edu ."},{"id":"scholars-lab-open-office-hours-2018-04-23","title":"Scholars' Lab Open Office Hours","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-open-office-hours-2018-04-23","layout":"events","content":"Mondays throughout the Spring Semester Just drop in - no appointment needed! Come with your curiosity, your ideas for projects, your technical, design, and project scoping questions, or just the desire to meet others engaged in digital work.  If you can’t make our Monday hours, you are always welcome to come by our office in Alderman 415 or schedule an appointment at scholarslab@virginia.edu ."},{"id":"scholars-lab-speaker-series-creating-the-american-yawp","title":"Scholars' Lab Speaker Series: Creating The American Yawp","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-speaker-series-creating-the-american-yawp","layout":"events","content":"Democratizing the Digital Humanities: The American Yawp as Case Study After a year-long collaboration, over 350 historians have produced a beta edition of The American Yawp, a free and online, collaboratively built, open American history textbook designed for college-level history courses. This talk will explore the creation and dissemination of this project, the landscape of open educational projects in the humanities, the methods used to harness the energy of hundreds of academics, and the potential for large-scale collaboration and open resources to make practical the democratic promise of the digital humanities. Ben Wright is an assistant professor of history at Abraham Baldwin Agricultural College. His manuscript, Antislavery and American Salvation, is under advance contract with LSU Press. His digital projects include The American Yawp and abolitionseminar.org, a NEH-sponsored educational tool on the antislavery movement, designed for K-12 educators and their students. He also serves as managing editor of Teaching United States History, a critical forum discussing pedagogy in college-level American history courses. Joseph Locke is an assistant professor history at the University of Houston-Victoria, where he teaches courses in American history and researches the historical interplay between religion and the American South. His first book, Making the Bible Belt: Prohibition and the Politicization of Southern Religion, is forthcoming from Oxford University Press."},{"id":"scholars-lab-speaker-series-nick-laiacona-on-juxta-editions","title":"Scholars' Lab Speaker Series: Nick Laiacona on Juxta Editions","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-speaker-series-nick-laiacona-on-juxta-editions","layout":"events","content":"Using Juxta Editions to Create Digital Scholarly Editions Juxta Editions is a professional editing suite for the creation of digital scholarly editions. Scholarly editors, historians, archivists, and academic librarians are routinely using the Internet to share primary source material with one another and the public. However, the set of technologies required to deliver a state-of-the-art scholarly edition remains difficult to master. Using Juxta, you can transcribe, annotate, collate, and publish to the Internet a work or collection of related writings. The session includes a demonstration of Juxta Editions. Nick Laiacona is the president of Performant Software Solutions LLC . Performant builds custom software and websites for digital humanities projects, including: Juxta, Collex, Typewright, BRANCH, TextLab, BigDIVA, Viral Texts, and Book Traces. Laiacona has acted as technical lead on digital projects funded by the National Endowment for the Humanities, the Andrew W. Mellon Foundation, and the National Institutes of Health."},{"id":"scholars-lab-speaker-series-sarah-bond-on-dh-social-justice","title":"Scholars' Lab Speaker Series: Sarah Bond on DH & Social Justice","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-speaker-series-sarah-bond-on-dh-social-justice","layout":"events","content":"Digital Humanities and Social Justice Although people have been caught up in the correct definition of the term “digital humanities,” we should perhaps be more concerned with the how of DH rather than the what. This talk focuses on how digital approaches—3D modeling, augmented reality, GIS, and textual analysis, to name just a few—have begun to reveal evidence for social inequality, misogyny, racism, and marginalization. This talk highlights just a few local and international DH projects working to these ends; from redlining maps to the statistical analysis of the gender pay gap at public universities. Clearly, it is not about who is and is not a digital humanist that is the real issue in 2017. All humanists—digital or otherwise—have the power to band together in order to bring about transparency and hasten social awareness. If democracy truly “dies in darkness,” then perhaps DH can contribute some flashlights to the cause. Sarah E. Bond is Assistant Professor of Classics &amp; Associated Scholar, Digital Scholarship &amp; Publishing Studio at the University of Iowa."},{"id":"scholars-lab-speaker-series-thorny-staples-on-managing-research-data","title":"Scholars' Lab Speaker Series: Thorny Staples on Managing Smithsonian Research Data","author":"laura-miller","date":null,"categories":null,"url":"scholars-lab-speaker-series-thorny-staples-on-managing-research-data","layout":"events","content":"Managing the Record of Research at the Smithsonian Can institutions effectively manage cross-team digital research data in real time? Can it preserve that data so that it can be seamlessly presented in conjunction with publications? To answer those questions, the Smithsonian Institution has built a first pilot system, called Sidora, designed to be used by Smithsonian researchers to capture and organize digital “evidence” as they create it in their research process, and use it directly in their analysis and dissemination activities. The goal is to actively support the research process as it unfolds, leaving behind a coherent expression of the digital content for a complete research project that can permanently stand alongside related publications. Sidora, a general information architecture and software environment based on Islandora and Fedora, is designed to manage research output as if it were part of a network of information. Staples will present the architecture and demo the software, using research data from a complete excavation of an archaeological site in Panama, and an international study of mammal populations. Thorny Staples is currently the Director of the Office of Research Information Services at the Smithsonian Institution. He has previously been Director of the Fedora Project; Director of Community Strategy and Alliances for DuraSpace; CIO of the National Museum of American Art at the Smithsonian Institute; Director of Digital Library Research and Development at the University of Virginia; and Project Director at the Institute for Advanced Technology in the Humanities at the University of Virginia."},{"id":"speaker-amanda-licastro","title":"Speaker: Amanda Licastro on Digital Publishing & the Future of Scholarly Communication","author":"laura-miller","date":null,"categories":null,"url":"speaker-amanda-licastro","layout":"events","content":"** The Multimodal Millennium: The Future of Digital Publishing ** This presentation will focus on the evolving landscape of digital publishing from the perspective of an editor and author of online, interactive, academic texts. The speaker will showcase digital scholarship across mediums in order to highlight innovations in interactive media (from The New York Times and Kairos), peer review (from Hybrid Pedagogy, JITP, and the Modern Language Association), and collaborative models of authorship (from JITP, Kairos, and student work).  Additionally, practical advice and useable examples of effective approaches to digital writing pedagogy will be introduced and discussed, particularly concerning sample tools used to create, evaluate, and interact with webtexts. Audience members are encouraged to bring and use their smart phones or tablet devices for an interactive element of the presentation. Amanda Licastro is an Assistant Professor of Digital Rhetoric at Stevenson University in Maryland. Amanda’s fields of research include digital humanities, composition and rhetoric, textual studies, and interactive technology and pedagogy.  Recent publications include a co-authored chapter on “ Collaboration ” in Digital Pedagogy in the Humanities: Concepts, Models, and Experiments, and an article in the 20th anniversary edition of Kairos, “ The Roots of an Academic Genealogy: Composing the Writing Studies Tree ” with Ben Miller and Jill Belli, and her dissertation research investigates mulitmodal writing practices in open, online course environments. Amanda is also on the Editorial Collective of The Journal of Interactive Technology and Pedagogy, and will be teaching a course at HILT this summer on “ Digital Pedagogy and Networked Learning .” You can follow Amanda on twitter @amandalicastro or check http://digitocentrism.commons.gc.cuny.edu/ ."},{"id":"spring-gis-workshop-series","title":"GIS Spring Workshops","author":"laura-miller","date":null,"categories":null,"url":"spring-gis-workshop-series","layout":"events","content":"All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All are free and open to the UVa and larger Charlottesville community.  No registration, just show up! Making Your First Map with ArcGIS\nWednesday, February 1 2:00 - 3:00 pm · Alderman, Rm 421 Here’s your chance to get started with geographic information systems software in a friendly, jargon-free environment.  This workshop introduces the skills you need to make your own maps.  Along the way you’ll get a taste of Earth’s most popular GIS software (ArcGIS) and a gentle introduction to cartography. You’ll leave with your own cartographic masterpieces and tips for learning more in your pursuit of mappiness at UVa. Georeferencing a Map: Putting Old Maps and Aerial Photos on Your Map\nWednesday, February 8 2:00 - 3:00 pm  · Alderman, Rm 421 Would you like to see historic map overlaid on modern aerial photography?  Do you need to extract features of a map for use in GIS?  Georeferencing is the first step.  We will show you how to take a scan of a paper map and align in it in ArcGIS. Getting Your Data on a Map\nWednesday, February 15 2:00 - 3:00 pm  · Alderman, Rm 421 Do you have a list of Lat/Lon coordinates or addresses you would like to see on a map?  We will show you how to do just that.  Through ArcGIS’s Add XY data tool and Geocoding (address matching), it is easy to take your tabular lists and generate points on a map. Points on Your Map: Street Addresses and More Spatial Things\nWednesday, February 22 2:00 - 3:00 pm  · Alderman, Rm 421 Do you have a list of street addresses crying out to be mapped?  Have a list of zip codes or census tracts you wish to associate with other data?  We’ll start with addresses and other things spatial and end with points on a map, ready for visualization and analysis. Taking Control of Your Spatial Data: Editing in ArcGIS\nWednesday, March 1 2:00 - 3:00 pm  · Alderman, Rm 421 Until we perfect that magic “extract all those lines from this paper map” button we’re stuck using editor tools to get that job done.  If you’re lucky, someone else has done the work to create your points, lines, and polygons but maybe they need your magic touch to make them better.  This session shows you how to create and modify vector features in ArcMap, the world’s most popular geographic information systems software.  We’ll explore tools to create new points, lines, and polygons and to edit existing datasets.  At version 10, ArcMap’s editor was revamped introducing new templates, but we’ll keep calm and carry on. Easy Demographics\nWednesday, March 15 2:00 - 3:00 pm  · Alderman, Rm 421 Need to make a quick demographic map or religious adherence?  This workshop will show you how easily navigate Social Explorer.  This powerful online application makes it easy to create maps with contemporary and historic census data and religious information. Historic Census Data\nWednesday, March 22 2:00 - 3:00 pm  · Alderman, Rm 421 Would you like to map the poverty in Philadelphia around the turn of the 20th Century?  How about a racial breakdown by state in the 1860s?  This workshop will focus on how to download historic census boundary and tabular data to make historic demographic maps. Introduction to ArcGIS Online\nWednesday, March 29 2:00 - 3:00 pm  · Alderman, Rm 421 With ArcGIS Online, you can use and create maps and scenes, access ready-to-use maps, layers and analytics, publish data as web layers, collaborate and share, access maps from any device, make maps with your Microsoft Excel data, customize the ArcGIS Online website, and view status reports. You can also use ArcGIS Online as a platform to build custom location-based apps."},{"id":"summer-dh-incubator-fellows-panel-lunch","title":"Summer DH Project Incubator Fellows Panel & Lunch","author":"laura-miller","date":null,"categories":null,"url":"summer-dh-incubator-fellows-panel-lunch","layout":"events","content":"Summer Digital Humanities Project Incubator Fellows Presentation &amp; Lunch This summer the Scholars’ Lab hosted three Digital Humanities Project Incubator Fellows, collaborators James Ascher &amp; Sarah Berkowitz, and Ryan Maguire. During the course of their fellowships, these graduate students worked with Scholars’ Lab faculty and staff to rapidly prototype their project ideas and position themselves for future work in digital humanities. The fellows will present their cutting edge work that draws together experimental approaches to physical computing, textual criticism, and minimal computing. More information can be found in the abstracts below. Lunch will follow. Textual editing is always collaborative and this project is no different.  Building on the 1759 posthumous editing of Samuel Butler’s notes into a printed book and over a century of reprints, Charels W. Daves produced an edition in 1970 that returned to the Butler’s intentions as demonstrated in the extant manuscripts and printed editions.  Yet, for its excellence, this edition recovers a historical moment by imagining that Butler wrote his notes with the intention to publish.  Sarah Berkowitz and I asked what other sorts of historical intentions we might recover and we settled on recreating, digitally, the reading practices available for the 1759 copy we digitized.  Centering our inquiry on a particular physical copy, but asking theoretical questions about literature, we developed a process that uses continuous integration.  As both a metaphor from computer science and a technique we applied to our textual work, I’ll talk about how we developed a small team where historical curiosity drove the technological decisions we made in producing a digital platform: how we settled on Markdown over TEI, Pandoc over Saxon and makefiles with Travis-CI over Jekyll. James P. Ascher is a doctoral candidate in the English Department.  His work considers bibliographical and literary composites in eighteenth-century publishing. Over the summer, James Ascher and I have been digitally recreating a specific copy of Samuel Butler’s Genuine Remains (1759). While digital tools and transcription practices can tell us a great deal about a text that might not be visible to the naked eye, my focus has been using digital technologies to reveal Butler’s conception of character, and to hypothesize how that might have been understood by an eighteenth-century reader. The Genuine Remains contains a collection of Theophrastan Characters, which present us with an opportunity to witness fictional characters that exist free from prose narrative. When character is decoupled from narrative what remains? And can digital text analysis of Butler reveal anything about the broader notions of what constitutes “character”? Mixing Stylo and text analysis with other close reading techniques, I work through Butler’s characters to discern the building blocks of eighteenth-century character. Sarah Berkowitz is a PhD candidate in the English Department. Her work examines mediocrity, masculinity and character in the eighteenth-century novel. This summer at the Scholar’s Lab, I have been reimagining what physical form digital music might take while working in the MakerSpace with Ammon Shepherd. With the advent of sound recording a century ago, music gained a new corporeality via the physical forms of vinyl records, cassette tapes, and compact discs. This added an unprecedented physical persistence to an ephemeral art form. MP3’s seem to have reversed this only recently gained permanence by making music files into mutable streams of data floating in the digital cloud. Rather than viewing this with dismay, I believe we now have more possibilities available than ever before. With new technologies, music can take on virtually any physical form that one can imagine. Physical computing expands the artistic possibilities available to music makers. As an artist-scholar, I believe this work can be at the forefront of a newly creative era in which the art of music is merged with the arts of sculpture and computational design. To this end, I am refurbishing old iPod’s with Rockbox Linux replacing the original firmware, and developing custom fabricated “Ghost in the MP3” players using 3-d printing software and Raspberry Pi computers. This work explores new creative possibilities for what music might become in the 21st century. A doctoral candidate in Music Composition and Computer Technologies, Ryan Maguire grew up in Wisconsin where he earned a B.A. in Physics and taught mathematics after graduation. He later moved to Boston and completed postgraduate degrees at the New England Conservatory of Music and Dartmouth College in Music Composition and Digital Musics, respectively. His work connects composition with improvisation, analog with digital, acoustic with electronic,  lo-fi with high tech, and art with science."},{"id":"the-humanities-arcade-playing-with-history-and-culture-in-the-digital-age","title":"The Humanities Arcade: Playing with History and Culture in the Digital Age","author":"ronda-grizzle","date":null,"categories":null,"url":"the-humanities-arcade-playing-with-history-and-culture-in-the-digital-age","layout":"events","content":"On the surface, the union of the humanities and video games might seem odd, the former focused on thoughtful reflection, context and contingencies, and the latter on reflex, immediacy and instantaneous feedback. In practice, however, this union is increasingly proving to be an enormously profound one, with games providing a platform for more experiential ways of engaging history, literature, philosophy, and even religion. But what does it mean to design games within the humanities? What are the affordances and drawbacks in meshing gaming and humanistic inquiry? Join William “Bro” Adams, Chairman of the National Endowment for the Humanities, and a panel of award-winning NEH-funded game producers for a candid conversation about the challenges and opportunities in exploring history and culture through interactive media. Alongside discussing how the process of working in the humanities differs from (and, occasionally, aligns with) STEM-related fields, this workshop will highlight the development and production of four NEH-funded games, each at different stages of progress, and give the audience an opportunity to demo them along with their producers: Mission US, a multi-chapter, centuries-spanning game focused on US history\nWalden, a game, an open-world game that allows audiences to play as Henry David Thoreau as he chases inspiration (and sustenance) during his first year at Walden Pond.\nLost &amp; Found, a digital card game set in North Africa in the 12th century that teaches medieval religious legal systems, beginning with Maimonides’ Mishneh Torah. Players take the role of villagers balancing personal needs with those of the community\nThe Pox Hunter, a strategy game focused on a smallpox outbreak in 19th century Philadelphia Speakers:\nOwen Gottlieb\nAssistant Professor of Interactive Games and Media at the Rochester Institute of Technology\nTracy Fullerton\nTracy Fullerton, M.F.A., is a game designer, educator and author Lisa Rosner\nDistinguished Professor of History at Stockton University Marc Ruppel\nSenior program officer at the National Endowment for the Humanities, Division of Public Programs, where he specializes in digital media and transmedia storytelling William Adams\nChairman of the National Endowment for the Humanities Leah Potter\nWriter and designer for Electric Funstuff – an educational game studio located in New York City"},{"id":"transcript-a-thon-the-julian-bond-papers","title":"Transcript-a-thon: The Julian Bond Papers","author":"laura-miller","date":null,"categories":null,"url":"transcript-a-thon-the-julian-bond-papers","layout":"events","content":"#TranscribeBond Join us in the Scholar’s Lab for this all-day event transcribing a varied sample of the papers and speeches of Julian Bond, in an effort to produce a digital edition of his work. Refreshments will be provided! The Albert and Shirley Small Special Collections Library and the Scholars’ Lab are collaborating with the Carter G. Woodson Institute of African-American and African Studies in a crowd-sourced project to make the papers of Civil Rights legend and UVA professor Julian Bond available online. This two-day event begins on Tuesday, August 14 at 4:00 PM, at the Woodson Institute in 110 Minor Hall, where the scope and goals of the edition will be announced, with a reception to follow at 5:30 PM. Those who would like to attend the reception and/or participate in the transcription process can RSVP  here . Those wishing to participate, but unable to join in person, can still contribute remotely by accessing the  project workspace  on FromThePage when it goes live in August and engaging with the hashtag #TranscribeBond. Read more at UVA Today"},{"id":"using-git-for-beginners","title":"Workshop: Basics of Version Control with Git","author":"laura-miller","date":null,"categories":null,"url":"using-git-for-beginners","layout":"events","content":"Do you have file names like: Paper.doc, Paper-revision1.doc, Paper-v2.doc, Paper-Final.doc, Paper-Final2.doc? Have you heard of version control, but not sure how to get it working for you? This workshop will give you an overview of how to use Git and GitHub to version your documents (word processing, coding document, or any type of files). This is a workshop for absolute beginners, so no experience is required. We will mostly be talking about version control from a theoretical standpoint, and won’t be installing or using git on your own computers."},{"id":"uva-library-gis-workshop-collecting-your-own-spatial-data","title":"UVa Library GIS Workshop: Collecting Your Own Spatial Data","author":"ronda-grizzle","date":null,"categories":null,"url":"uva-library-gis-workshop-collecting-your-own-spatial-data","layout":"events","content":"Research projects often rely on fieldwork to build new datasets.  In this workshop we’ll focus on tools for spatial data collection. First we’ll take a quick look behind the curtain to see how GPS really works and how to use that knowledge to our advantage.  Then we’ll evaluate free or low-cost options to gather locations and associated attributes using handheld GPS devices, smartphones, and apps.  This workshop will introduce you to a range of devices and methods for mobile spatial data collection. All sessions are one hour and assume attendees have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials with expert assistance. All sessions will be taught in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community."},{"id":"uva-library-gis-workshop-georeferencing","title":"UVa Library GIS Workshop: Georeferencing – Putting Old maps and Aerial Photos on Your Map","author":"ronda-grizzle","date":null,"categories":null,"url":"uva-library-gis-workshop-georeferencing","layout":"events","content":"Have an old map or an aerial photograph that you would like to use as a spatial layer?  This session will teach you techniques to properly place your data and make it usable in GIS software.  We will also demo similar techniques for Google Earth. All sessions are one hour and assume attendees have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials with expert assistance. All sessions will be taught in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community."},{"id":"uva-library-gis-workshop-getting-your-data-on-a-map","title":"UVa Library GIS Workshop: Getting Your Data on a Map","author":"ronda-grizzle","date":null,"categories":null,"url":"uva-library-gis-workshop-getting-your-data-on-a-map","layout":"events","content":"Do you have GPS points or a list of latitude and longitude you would like to show as points on a map?  This session will show you how to turn your data into map layers and how to connect them to make lines and polygons as well. All sessions are one hour and assume attendees have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials with expert assistance. All sessions will be taught in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community."},{"id":"uva-library-gis-workshop-making-your-first-map","title":"UVa Library GIS Workshop: Making Your First Map","author":"ronda-grizzle","date":null,"categories":null,"url":"uva-library-gis-workshop-making-your-first-map","layout":"events","content":"Getting started with new software can be intimidating. This workshop introduces the skills you need to work with spatial goodness. Along the way you’ll get a taste of Earth’s most popular geographic software and a gentle introduction to map making. You’ll leave with your own cartographic masterpiece and tips for learning more in your pursuit of mappiness at UVa. All sessions are one hour and assume attendees have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials with expert assistance. All sessions will be taught in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community."},{"id":"uva-library-gis-workshop-points-on-your-map-street-addresses-and-more-spatial-things","title":"UVa Library GIS Workshop: Points on Your Map - Street Addresses and More Spatial Things","author":"ronda-grizzle","date":null,"categories":null,"url":"uva-library-gis-workshop-points-on-your-map-street-addresses-and-more-spatial-things","layout":"events","content":"Do you have a list of street addresses crying out to be mapped?  Have a list of zip codes or census tracts you wish to associate with other data?  We’ll start with addresses and other things spatial and end with points on a map, ready for visualization and analysis. All sessions are one hour and assume attendees have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials with expert assistance. All sessions will be taught in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community."},{"id":"uva-library-gis-workshop-taking-control-of-your-spatial-data-editing-in-arcgis","title":"UVa Library GIS Workshop: Taking Control of Your Spatial Data - Editing in ArcGIS","author":"ronda-grizzle","date":null,"categories":null,"url":"uva-library-gis-workshop-taking-control-of-your-spatial-data-editing-in-arcgis","layout":"events","content":"Until we perfect that magic “extract all those lines from this paper map” button we’re stuck using editor tools to get that job done.  If you’re lucky, someone else has done the work to create your points, lines, and polygons but maybe they need your magic touch to make them better.  This session shows you how to create and modify vector features in ArcMap, the world’s most popular geographic information systems software.  We’ll explore tools to create new points, lines, and polygons and to edit existing datasets.  At version 10, ArcMap’s editor was revamped introducing new templates, but we’ll keep calm and carry on. All sessions are one hour and assume attendees have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials with expert assistance. All sessions will be taught in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community."},{"id":"uva-medieval-colloquium-kathleen-kennedy","title":"UVa Medieval Colloquium: Kathleen Kennedy","author":"laura-miller","date":null,"categories":null,"url":"uva-medieval-colloquium-kathleen-kennedy","layout":"events","content":"Old Media Studies: New Wine in Old Skins Eddie Izzard asks us if we have a flag, and generally speaking, medievalists must answer in the negative. In this talk, Kathleen E. Kennedy unfurls the banner of Old Media Studies and argues that the interdisciplinary, transtemporal future of Medieval Studies may lie right under our hands, in the very books which serve as our primary research resources. Come hear about medieval hackers, Anonymous, and the prayerbook owned by one of Skelton’s associates and a crony of Henry VIII. Kathleen Kennedy is an Associate Professor of English at Penn State-Brandywine. She specializes in medieval and Early Modern English literature and history, and teaches courses on topics ranging from composition to Shakespeare, comic books to western civilization.  This event is co-sponsored by U.Va.’s Medieval Colloquium and the Scholars’ Lab."},{"id":"visual-thinking-workshop-with-nick-sousanis","title":"Visual Thinking Workshop with Nick Sousanis","author":"laura-miller","date":null,"categories":null,"url":"visual-thinking-workshop-with-nick-sousanis","layout":"events","content":"Join Nick Sousanis, comics creator and professor, for an interactive workshop that explores how comics can enrich your own critical practice through visual thinking. Participants will do drawing and cartooning activities that can be adapted in their scholarship and teaching practices.  Learn how to make comics and explore your own creativity! (No prior drawing experience required.) Nick Sousanis is a professor in Humanities &amp; Liberal Studies at San Francisco State University, where he is starting an interdisciplinary Comics Studies program. His doctoral dissertation, Unflattening, written and drawn entirely in comic form, was published by Harvard University Press in 2015."},{"id":"vr-workshop-2","title":"VR Workshop: VR Painting, Modeling and Graffiti","author":"laura-miller","date":null,"categories":null,"url":"vr-workshop-2","layout":"events","content":"This workshop will allow users to try their hand at virtual modeling, painting and graffiti using titles such as Google Blocks, Tiltbrush, Kingspray Graffiti and MasterpieceVR. No experience necessary. Visit the UVA Library Event Calendar, to sign up for this workshop and to see the full schedule of Virtual Reality workshops offered this fall: bit.ly/uvavrworkshop."},{"id":"vr-workshop-3","title":"VR Workshop: VR Storyboarding, Animation & Puppetry","author":"laura-miller","date":null,"categories":null,"url":"vr-workshop-3","layout":"events","content":"This workshop will explore creating animated 2D and 3D videos using puppets and characters in virtual environments. Participants will explore creating simple scenes with software such as Tvori, Mindshow and Flipboard Studio. No experience necessary. Visit the UVA Library Event Calendar, to sign up for this workshop and to see the full schedule of Virtual Reality workshops offered this fall: bit.ly/uvavrworkshop."},{"id":"vr-workshop-4","title":"VR Workshop: Photogrammetry - Capturing Your World","author":"laura-miller","date":null,"categories":null,"url":"vr-workshop-4","layout":"events","content":"This workshop will be an overview of software and techniques used to convert a series of photos to a 3D model and dataset. No experience required. Visit the UVA Library Event Calendar, to sign up for this workshop and to see the full schedule of Virtual Reality workshops offered this fall: bit.ly/uvavrworkshop."},{"id":"vr-workshop-5","title":"VR Workshop: Augmented Reality Tools","author":"laura-miller","date":null,"categories":null,"url":"vr-workshop-5","layout":"events","content":"This workshop will explore augmented reality game creation using ARIS Field Day and Unity3D. No experience necessary. Visit the UVA Library Event Calendar, to sign up for this workshop and to see the full schedule of Virtual Reality workshops offered this fall: bit.ly/uvavrworkshop."},{"id":"vr-workshop-6","title":"VR Workshop: Visual Scripting with Unity3D: Playmaker","author":"laura-miller","date":null,"categories":null,"url":"vr-workshop-6","layout":"events","content":"This session will involve an overview of the Unity3D asset Playmaker which allows users to create interactive content without coding by using logic flow diagrams. Basic familiarity with Unity3D is a bonus but not required. Visit the UVA Library Event Calendar, to sign up for this workshop and to see the full schedule of Virtual Reality workshops offered this fall: bit.ly/uvavrworkshop."},{"id":"vr-workshop-7","title":"VR Workshop: Virtual Music Creation","author":"laura-miller","date":null,"categories":null,"url":"vr-workshop-7","layout":"events","content":"Try your hand at creating music in virtual reality using tools such as Exa: The Infinite Instrument, Soundstage and Lyra. No experience necessary. Visit the UVA Library Event Calendar, to sign up for this workshop and to see the full schedule of Virtual Reality workshops offered this fall: bit.ly/uvavrworkshop."},{"id":"vr-workshop-creating-virtual-museums","title":"VR Workshop: Creating Virtual Museums","author":"laura-miller","date":null,"categories":null,"url":"vr-workshop-creating-virtual-museums","layout":"events","content":"This workshop will explore creating virtual museums using the free game engine Unity3D and found assets from the web. Participants will be guided through the initial stages of museum creation using stills, videos, audio and 3D models and will culminate with exporting projects for the web and for VR. No experience necessary. Visit the UVA Library Event Calendar, to sign up for this workshop and to see the full schedule of Virtual Reality workshops offered this fall: bit.ly/uvavrworkshop."},{"id":"what-is-the-raspberry-pi-and-how-to-use-it","title":"Makerspace Workshop: What is the Raspberry Pi and How to Use It","author":"laura-miller","date":null,"categories":null,"url":"what-is-the-raspberry-pi-and-how-to-use-it","layout":"events","content":"No experience or Raspberry Pi necessary. This workshop will introduce the Raspberry Pi, single board computer, and walk you through the steps of setting up the Raspberry Pi. This workshop is limited to four individuals, due to limitations of monitors, keyboards, and mice. If you would like to attend this workshop, please send an email to ammon@virginia.edu to register. In your email, let me know if you need a monitor, keyboard and mouse."},{"id":"work-in-progress-seminar-maria-skou-nicolaisen-on-current-trends-in-digital-literary-studies","title":"Work-in-Progress Seminar: Maria Skou Nicolaisen on Current Trends in Digital Literary Studies","author":"laura-miller","date":null,"categories":null,"url":"work-in-progress-seminar-maria-skou-nicolaisen-on-current-trends-in-digital-literary-studies","layout":"events","content":"Work-in-Progress Seminar: Maria Skou Nicolaisen Wednesday, November 30 Notions of Text Reflected by Digital Technology: A Preliminary Study of Current Trends in Digital Literary Studies noon - 1:30 pm · Clemons Library, Room 407    ( Note new location!) For this brownbag seminar, visiting Ph.D. scholar in Information Science from the University of Copenhagen, Maria Skou Nicolaisen will discuss her Dissertation-In-Progress. She is doing a comparative study of the conceptual and institutional frameworks guiding current digital literary projects at UVA’s English Department and the Stanford Literary Lab. Digital technology has introduced quantitative methods in the humanities, but the formation and alteration of the digital literary field to include large-scale textual analytics has tended to focus primarily on changing methodologies.  While the methodological impact of the digital on humanities disciplines is indeed an important research area, it seems to have left the equally important conceptual and theoretical changes underexamined in comparison. Maria will discuss how technological innovation challenges and transforms the notion of text, or more specifically, how digital remediation alters some basic assumptions about the nature and affordances of the textual object itself.  Working under the assumption that technological remediation equally uncovers and alters theoretical assumptions about the textual object, the project revolves around insights from current DH practitioners through interviews concerning their diverse literary projects. Please plan to join us on Wednesday.  Feel free to bring your lunch, and light refreshments will be served. Maria Skou Nicolaisen is a Ph.D. student at the University of Copenhagen, Denmark. She has chosen UVA to be both her host institution as well as a case study for her research mission. She is hosted by the Institute for Advanced Technology in the Humanities and has, during her time here, interviewed faculty from UVA’s English Department and the the Stanford Literary Lab."},{"id":"workshop-apples-ios-app-ecology-an-introduction","title":"Workshop: Apple's iOS App Ecology: An Introduction","author":"laura-miller","date":null,"categories":null,"url":"workshop-apples-ios-app-ecology-an-introduction","layout":"events","content":"Apple’s iOS App Ecology: An Introduction (for People Who Code or Want to Hire a Developer) Ross Harding will demonstrate the iOS development process he used to build PocketHinman, a bibliographical research tool developed (along with collaborator James Ascher ) as part of the IHGC Public Humanities Lab. This workshop and discussion should be useful if you already know how to code in Swift, think you could learn, but want a quick introduction to developing apps for iOS; or are interested in hiring a developer to help make an app and need to know how to set reasonable milestones for their work. Refreshments will be provided."},{"id":"workshop-copyright-ip","title":"Workshop: Copyright & Intellectual Property","author":"ronda-grizzle","date":null,"categories":null,"url":"workshop-copyright-ip","layout":"events","content":"Students, researchers, faculty, and university staff members need to understand copyright and intellectual property, and UVA Library’s Director of Information Policy Brandon Butler is the man who can enlighten you! Please join Brandon for a discussion and Q &amp;A about 21st century copyright law, intellectual property, and fair use. Instructor: Brandon Butler"},{"id":"workshop-creating-your-neatline-project","title":"Workshop: Creating Your Neatline Project","author":"ronda-grizzle","date":null,"categories":null,"url":"workshop-creating-your-neatline-project","layout":"events","content":"You have your data, and you’ve planned what you want to say about it, now how do you create a Neatline exhibit that tells that scholarly story? This session will introduce you to the Neatline interface with a hands-on tutorial. No prior knowledge of Neatline is assumed, and everyone is welcome. Instructor: Ronda Grizzle"},{"id":"workshop-easy-database-administration-with-python-django","title":"Workshop: Easy Database Administration with Python & Django","author":"ronda-grizzle","date":null,"categories":null,"url":"workshop-easy-database-administration-with-python-django","layout":"events","content":"What do you do when your data outgrows Excel? This happens on most projects, but moving to a full database can seem daunting. But the web framework Django can make this much easier. This workshop will talk about when you may need to transition out of Excel, what you gain by moving to a database, how to change the way you think about your data as you move to a Django site, and how to setup a simple Django site for managing your database and the data in it. To get the most out of this workshop, you’ll want to be comfortable working on the command line, and a passing familiarity with Python will be helpful also. Come by and talk to us beforehand if you have any questions about this. Instructor: Eric Rochester"},{"id":"workshop-intro-to-3d-printing","title":"Workshop: Introduction to 3D Printing","author":"ronda-grizzle","date":null,"categories":null,"url":"workshop-intro-to-3d-printing","layout":"events","content":"Are you curious about 3D printing but unsure where to start? Have you had a bit of experience with modeling, but you want to continue to explore? This workshop will introduce participants to the exciting world of desktop fabrication. We’ll provide a brief overview of current trends and tools for 3D modeling and printing, followed by a hands-on demonstration of basic printer operation. We will also discuss various types of 3D models and the best practices and equipment for bringing them to life. While not necessary, participants can bring a laptop to get better acquainted with the software we’ll introduce. Instructor: Shane Lin"},{"id":"workshop-intro-to-arduinos","title":"Workshop: Introduction to Arduinos","author":"ronda-grizzle","date":null,"categories":null,"url":"workshop-intro-to-arduinos","layout":"events","content":"Do you want to build a device that interacts with social media? Heard about ways to embed switches or sensors in your physical items but don’t know where to find out more? Arduino is a tool for making microcomputers that can sense and control the physical world. This workshop will introduce participants to the basics of circuitry and programming through a series of hands-on exercises using our Arduino kits. No electronics experience required! Laptops will also be available for participants, but bring your own if you want to save and edit your projects after the workshop. Instructor: Ammon Shepherd"},{"id":"workshop-intro-to-neatline-2","title":"Workshop: Introduction to Neatline","author":"ronda-grizzle","date":null,"categories":null,"url":"workshop-intro-to-neatline-2","layout":"events","content":"Join us for a hands-on introduction to Neatline, a set of plugins for Omeka developed by the Scholars’ Lab. With Neatline, anyone can create beautiful, complex maps and narrative sequences from collections of archives and artifacts, and connect maps and narratives with timelines that are more-than-usually sensitive to ambiguity and nuance. See http://neatline.org/ for more information. Instructor: Ronda Grizzle"},{"id":"workshop-intro-to-neatline","title":"Introduction to Neatline","author":"ronda-grizzle","date":null,"categories":null,"url":"workshop-intro-to-neatline","layout":"events","content":"Join us for a hands-on introduction to Neatline, a set of plugins for Omeka developed by the Scholars’ Lab. With Neatline, anyone can create beautiful, complex maps and narrative sequences from collections of archives and artifacts, and connect maps and narratives with timelines that are more-than-usually sensitive to ambiguity and nuance. See http://neatline.org/ for more information. Instructor: Ronda Grizzle"},{"id":"workshop-intro-to-omeka","title":"Introduction to Omeka","author":"ronda-grizzle","date":null,"categories":null,"url":"workshop-intro-to-omeka","layout":"events","content":"Omeka is a simple, free, web publishing system developed at the Roy Rosensweig Center for History and New Media at GMU. It was specifically built to enable scholars, archives, libraries, museums, and independent researchers to create online exhibits of their work without having to know HTML or CSS. If you have a collections of digital resources that you want to show in a scholarly way, Omeka could be a great tool to have in your toolkit. Instructor: Ronda Grizzle"},{"id":"workshop-introduction-to-3d-printing","title":"Workshop: Introduction to 3D Printing","author":"ronda-grizzle","date":null,"categories":null,"url":"workshop-introduction-to-3d-printing","layout":"events","content":"This workshop will introduce participants to the exciting world of desktop fabrication. We’ll provide a brief overview of current trends and tools for 3D modeling and printing, followed by a hands-on demonstration of basic printer operation. We will also discuss various types of 3D models and the best practices and equipment for bringing them to life. While not necessary, participants can bring a laptop to get better acquainted with the software we’ll introduce. Instructor: Shane Lin"},{"id":"workshop-introduction-to-design-principles","title":"Workshop: Introduction to Design Principles","author":"ronda-grizzle","date":null,"categories":null,"url":"workshop-introduction-to-design-principles","layout":"events","content":"Details coming soon! Instructor: Jeremy Boggs"},{"id":"workshop-introduction-to-docnow","title":"Workshop: Introduction to DocNow","author":"laura-miller","date":null,"categories":null,"url":"workshop-introduction-to-docnow","layout":"events","content":"In honor of Endangered Data Week, join us for an introduction to DocNow, a tool and community developed around supporting the ethical collection, use, and preservation of social media content.  Scholars’ Lab staff will be on hand to provide a brief introduction to the tool, a discussion of data collection, and a consideration of the complex, often fraught, stories that can be told through social media data. All skill levels are welcome and encouraged to attend!"},{"id":"workshop-introduction-to-neatline","title":"Workshop: Introduction to Neatline","author":"laura-miller","date":null,"categories":null,"url":"workshop-introduction-to-neatline","layout":"events","content":"Using Neatline, anyone can create beautiful, interactive maps, timelines, and narrative sequences from collections of archives and artifacts, telling scholarly stories in a whole new way. Join us for this hands-on introduction to Neatline. See http://neatline.org/ for more information. Sessions will be hands-on with step-by-step tutorials, and assume no previous experience. All sessions will be taught in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and open to the UVa and larger Charlottesville community."},{"id":"workshop-introduction-to-omeka","title":"Workshop: Introduction to Omeka","author":"laura-miller","date":null,"categories":null,"url":"workshop-introduction-to-omeka","layout":"events","content":"Omeka is a simple, free, web publishing system developed at the Roy Rosensweig Center for History and New Media at GMU. It was specifically built to enable scholars, archives, libraries, museums, and independent researchers to create online exhibits of their work without having to know HTML or CSS. If you have a collections of digital resources that you want to show in a scholarly way, Omeka could be a great tool to have in your toolkit. Our sessions assume attendees have no previous experience, and will be hands-on with step-by-step tutorials with expert assistance. All sessions will be taught in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community."},{"id":"workshop-meghan-ferriter-on-library-of-congress-lc-labs","title":"Workshop: Meghan Ferriter on Library of Congress' LC Labs","author":"laura-miller","date":null,"categories":null,"url":"workshop-meghan-ferriter-on-library-of-congress-lc-labs","layout":"events","content":"Join the Scholars’ Lab and Meghan Ferriter from the National Digital Initiatives division at the Library of Congress to explore LC Labs ( http://labs.loc.gov/ ). This new space is designed to empower exploration and discovery in the digital collections of the Library of Congress. In this workshop, you’ll tour labs.loc.gov and the Library of Congress’ crowdsourcing pilot Beyond Words; then you’ll dig into the crowdsourced &amp; public domain data generated by a generous public. Please bring your own device to explore labs.loc.gov and Beyond Words. Meghan Ferriter is Senior Innovation Specialist with the National Digital Initiatives division in the National and International Outreach directorate at the Library of Congress. The Library of Congress launched http://labs.loc.gov/ to provide a space for open experimentation with Library digital collections. There, they want to highlight the visualizations, research, and innovation that machine-readable collections and data make possible and to help others explore what’s possible."},{"id":"workshop-metagaming-with-stephanie-boluk-and-patrick-lemieux","title":"Workshop: 1001 Game Engines with Stephanie Boluk and Patrick LeMieux","author":"laura-miller","date":null,"categories":null,"url":"workshop-metagaming-with-stephanie-boluk-and-patrick-lemieux","layout":"events","content":"Bring your own laptop for a HANDS-ON workshop led by Stephanie Boluk and Patrick LeMieux, authors of  Metagaming: Playing, Competing, Spectating, Cheating, Trading, Making, and Breaking Videogames. How do you start making videogames? Do you begin by learning a suite of software from Photoshop to Maya to Unity to Steam? Or, echoing Melbourne’s local trashgame curators, Moshboy and BarSK, should we “make trashgames”? On http://itch.io hundreds of trashgames are uploaded every day by made on the fly, with no expertise, using tiny game engines built by and for individual makers. From Flick Game to Pling Pling, TinyChoice to Twine, and Bitsy to Bravitzlana, this hands-on workshop will introduce participants to a wide array of tools for making small, one-sitting games. Not quite 1001, in this a rapid prototyping and production pipeline demonstration will conclude by publishing at least three original videogames. This workshop follows a talk given by the authors earlier in the day. Stephanie Boluk is an associate professor in the English Department and Cinema and Digital Media Department at the University of California, Davis. She is the co-author of Metagaming: Playing, Competing, Spectating, Cheating, Trading, Making, and Breaking Videogames ( https://manifold.umn.edu/project/metagaming ) with Patrick LeMieux and co-editor of The Electronic Literature Collection Vol. 3 ( http://collection.eliterature.org/3/ ) with Leonardo Flores, Jacob Garbe, and Anastasia Salter. For more information visit http://stephanieboluk.com . Patrick LeMieux is an artist, game designer, and assistant professor in the Cinema and Digital Media Department at the University of California, Davis where he co-directs the Alt Ctrl Lab. Recent projects include Metagaming, a co-authored book with Stephanie Boluk ( https://manifold.umn.edu/project/metagaming ) and Platform Games, a solo exhibition at Babycastles ( http://babycastles.com/Platform-Games ). For more information visit http://patrick-lemieux.com ."},{"id":"workshop-planning-designing-your-neatline-project","title":"Workshop: Planning & Designing Your Neatline Project","author":"ronda-grizzle","date":null,"categories":null,"url":"workshop-planning-designing-your-neatline-project","layout":"events","content":"You’ve been doing research that spans both space and time, and you want to create an exhibit that explains your work visually, as well as textually. What’s a tool that can help you do that? How about Neatline? This workshop will guide you through the process of evaluating your data and figuring out how to illustrate your scholarly argument using map annotations and timelines. No previous experience with Neatline or other mapping tools is assumed or required. Open to everyone. Instructor: Ronda Grizzle"},{"id":"workshop-topic-modeling","title":"Digital Humanities Workshop: Topic Modeling","author":"laura-miller","date":null,"categories":null,"url":"workshop-topic-modeling","layout":"events","content":"We are pleased to present a Topic Modeling Workshop with David Mimno. David Mimno is Assistant Professor of Computer Science at Cornell University and chief maintainer for MALLET, a Machine Learning for Language Toolkit. He is one of the principal architects behind the widespread use of topic modeling in the digital humanities. In his visit, he will speak on both the critical and practical dimensions of topic modeling. Sponsored by the Data Science Institute, the Scholars’ Lab, IATH, and SHANTI. This event is free and open to the public but due to space considerations, please RSVP to shanti@virginia.edu if you plan to attend."},{"id":"workshop-using-the-hathi-trust-research-center","title":"DH Workshop: Introduction to the Hathi Trust Research Center","author":"laura-miller","date":null,"categories":null,"url":"workshop-using-the-hathi-trust-research-center","layout":"events","content":"Progressively greater access to digitized books and documents is creating opportunities for researchers to ask new data-driven questions about literary and linguistic change, history, and cultural trends. HathiTrust is a consortium of research libraries, and its digital library currently contains over 15 million items. The HathiTrust Research Center (HTRC) seeks to make this very large collection accessible for non-consumptive research via algorithmic text analysis. This hands-on session will introduce attendees to the HTRC’s tools and services, including how to execute algorithms against user-created sub-collections of text, how to use the HathiTrust+Bookworm tool for lexical trend discovery, and how to perform text analysis using HTRC-developed infrastructure and datasets.” The workshop, co-sponsored by the UVA Library’s departments of Arts &amp; Humanities Research, Research Data Services, and the Scholars’ Lab, is open to faculty, students and staff.  Lunch will be provided.  Please join us!"},{"id":"abby-holeman","title":"Abby Holeman","author":null,"date":null,"categories":null,"url":"abby-holeman","layout":"people","content":"Research Assistant in the UVa Department of Anthropology, former Scholars’ Lab Fellow. Her research interests include the intersection of cosmology and social organization at the site of Paquimé, to assess the nature of hierarchy and social differentiation in non-state societies, and exploring new ways to understand meaning in the archaeological record through symbolic and spatial analyses."},{"id":"abhishek-gupta","title":null,"author":null,"date":null,"categories":null,"url":"abhishek-gupta","layout":"people","content":"Abhishek Gupta is an undergraduate student in the School of Engineering and a 3D Technologies Student Assistant during the 2018-2019 academic year."},{"id":"adam-soroka","title":"Adam Soroka","author":null,"date":null,"categories":null,"url":"adam-soroka","layout":"people","content":"Adam is currently Senior Developer for UVa Library, and was formerly part of the Scholars’ Lab R&amp;D team."},{"id":"alex-gil","title":"Alex Gil","author":null,"date":null,"categories":null,"url":"alex-gil","layout":"people","content":"Textual critic, high theory acolyte, archive rat, hard-hat Caribbeanist, Rails aficionado, conference-trotter. My thesis focuses on the evolution of the play Et les chiens se taisaient by Aimé Césaire. Other projects include: an edition of Césaire, THATCampCaribe, a better Juxta, a class on #critcode, and Prism at the Praxis Program."},{"id":"alexandra-sanchez","title":"Alexandra Sanchez","author":null,"date":null,"categories":null,"url":"alexandra-sanchez","layout":"people","content":"Alex Sanchez is a rising senior at Berry College and a 2018 LAMI Fellow. She is an anthropology major with an interest in the anthropology of the body and expressive culture. As a part of the UVA Leadership Alliance Mellon Initiative- Digital Humanities, she is researching the phenomenon of cultural fetishism in the context of museums."},{"id":"alicia-caticha","title":"Alicia Caticha","author":null,"date":null,"categories":null,"url":"alicia-caticha","layout":"people","content":"Alicia is a doctoral candidate studying eighteenth- and nineteenth-century European art and material culture with Professor Sarah Betzer.  Her dissertation, “The Matter of Sculpture: Étienne-Maurice Falconet, Marble, Porcelain, and Sugar in Eighteenth-Century Paris,” poses the Academic sculptor as a nexus between Enlightenment aesthetic theory and the mass-production of sculpture during the burgeoning consumer culture of the 1760s.  Her research aims to explore the intersections between fine arts and decorative arts, sculpture’s role in a broadening global economy, and the relationship between marble and ephemeral forms of artistic production. Alicia hopes her foray into the Digital Humanities will give her the tools to study issues of ephemerality and temporality (melting sugar sculpture!)."},{"id":"alison-booth","title":"Alison Booth","author":null,"date":null,"categories":null,"url":"alison-booth","layout":"people","content":"Alison Booth is Professor of English and Academic Director of the Scholars’ Lab at the University of Virginia.  Her books include Homes and Haunts: Touring Writers’ Shrines and Countries (Oxford UP, 2016), How to Make It as a Woman: Collective Biographical History from Victoria to the Present _(2004), and _Greatness Engendered: George Eliot and Virginia Woolf (1992), as well as an edition of Wuthering Heights for Longman.  Her digital project, Collective Biographies of Women, is supported by ACLS, NEH, and UVA’s English Department, as well as the Scholars’ Lab and the Institute for Advanced Technology in the Humanities.  Collaborators include Rennie Mapp, Worthy Martin, Daniel Pitti, and Jeremy Boggs."},{"id":"alyssa-collins","title":"Alyssa Collins","author":null,"date":null,"categories":null,"url":"alyssa-collins","layout":"people","content":"Alyssa Collins is a PhD candidate in the English department at the University of Virginia. Her dissertation focuses on the intersections of race and technology as depicted in 20th century and contemporary African American literature, digital culture, and new media."},{"id":"amanda-visconti","title":"Amanda Visconti","author":null,"date":null,"categories":null,"url":"amanda-visconti","layout":"people","content":"Amanda Visconti is Managing Director of the Scholars’ Lab. She holds a Literature Ph.D. from the University of Maryland and an M.S. in Information from the University of Michigan, and has also worked as a professional web developer for over 10 years (with specialization in online knowledge-building communities, meaningful crowdsourcing websites, and reading/annotation interfaces). She serves as an elected member of the Association for Computing and the Humanities (ACH) executive council, as a member of the Modern Languages Association (MLA) Information Technology Committee, and as the founder/administrator of the Digital Humanities Slack . Her 2015 dissertation ( Dr.AmandaVisconti.com ) was the first humanities dissertation to fully acknowledge digital methods (code, design, user testing, blogging, no chapters) as scholarship by treating them as the dissertation instead of addenda to traditional written chapters. The focus of her dissertation was creating the participatory digital edition of James Joyce’s  Ulysses InfiniteUlysses.com, which attracted over 12,000 unique visitors in its first few weeks of open beta and was cited in  The New York Times in July 2016. Her greatest ambition is to be conductor of a dog train ."},{"id":"amelia-hughes","title":"Amelia Hughes","author":null,"date":null,"categories":null,"url":"amelia-hughes","layout":"people","content":"Amelia Hughes worked as a Cultural Heritage Informatics Intern with Will Rourk in Fall 2018."},{"id":"ammon-shepherd","title":"Ammon Shepherd","author":null,"date":null,"categories":null,"url":"ammon-shepherd","layout":"people","content":"Ammon is a Digital Humanities Developer and Makerspace Technologist at the Scholars’ Lab where he codes projects, builds tools to fix old projects, does some systems administration, 3D prints and plays with Arduinos and Raspberry Pis. Ammon has BA’s in History and German from ASU, an MA in History from GMU and is ABD PhD in History from GMU. His dissertation in rough draft is at http://nazitunnels.org/dissertation. Research interests include making in academia particularly in the humanities, 3D printing, Arduinos, Raspberry Pis, technology, aviation, religion. He is especially interested in using technology to do humanities research and presentation, often known as Digital Humanities, and creating a field of Physical Humanities that applies physical technology to the humanities."},{"id":"amy-boyd","title":"Amy Boyd","author":null,"date":null,"categories":null,"url":"amy-boyd","layout":"people","content":"Amy R. Boyd is a third-year Ph.D. student in English, specializing in nineteenth-century British fiction. She is interested in science and technology in and around literature, especially as technology and gender intersect."},{"id":"andrew-carl","title":"Andrew Carl","author":null,"date":null,"categories":null,"url":"andrew-carl","layout":"people","content":"Andrew Carl is a Makerspace Technologist."},{"id":"andrew-ferguson","title":"Andrew Ferguson","author":null,"date":null,"categories":null,"url":"andrew-ferguson","layout":"people","content":"Andrew Ferguson is a 2014–15 Praxis Fellow and a Ph.D candidate in English at the University of Virginia, working in the texts and media of the last hundred years. His dissertation, “The Game and the Glitch: Narrative Strategies and the Playerly Text,” uses modes of videogame play to explore alternate methods of textual engagement in novels. His biography of the science fiction writer R.A. Lafferty is forthcoming from the University of Illinois Modern Masters of Science Fiction series."},{"id":"ankita-chakrabarti","title":"Ankita Chakrabarti","author":null,"date":null,"categories":null,"url":"ankita-chakrabarti","layout":"people","content":"Ankita Chakrabarti is a 2017-2018 Praxis Program Fellow."},{"id":"annie-swafford","title":"Annie Swafford","author":null,"date":null,"categories":null,"url":"annie-swafford","layout":"people","content":"Annie was a 2012-2013 Scholars’ Lab Fellow, a 2011-12 Praxis Fellow and a PhD candidate in the Department of English."},{"id":"anthony-velazquez","title":"Anthony Velázquez","author":null,"date":null,"categories":null,"url":"anthony-velazquez","layout":"people","content":"Anthony is a rising senior in Linguistics and Communications at the University of Puerto Rico, Rio Piedras campus .  Among the myriad of subjects he is passionate about, Gender Studies is at the top of his list.  He, like many other 21-year-olds, is solely in the search of discovering who he wants to be for the remainder of his life."},{"id":"arin-bennett","title":"Arin Bennett","author":null,"date":null,"categories":null,"url":"arin-bennett","layout":"people","content":"Arin Bennett is a visualization specialist focused on the display, manipulation, and interaction of three-dimensional data using augmented reality (AR) and virtual reality (VR) technologies. Coming from a media-rich background in audio/video production and editing, he now focuses on making professional 3D capture and display technologies available to students, faculty and staff in order to facilitate new means of academic expression at the University."},{"id":"becca-peters","title":"Becca Peters","author":null,"date":null,"categories":null,"url":"becca-peters","layout":"people","content":"Becca does event planning and business operations for the team so her clever co-workers can keep on doing their amazingly cool jobs. She has more than 20 years of experience at UVA, having worked in administration, the Darden School, the Law School, and University Development before coming to the Scholars’ Lab in 2008.  She received her BA in History from Purdue University and is now working on her MSW at VCU.  Becca wears lots of hats (wife, mother of two, election official, Girl Scout leader), even though she looks terrible in hats, and loves movies, genealogy, storytelling, music, tattoos, cooking Irish food, and reading for pleasure (which doesn’t happen nearly enough these days)."},{"id":"ben-gorham","title":"Ben Gorham","author":null,"date":null,"categories":null,"url":"ben-gorham","layout":"people","content":"Ben Gorham is a 2017-2018 Digital Humanities Prototyping Fellow and a PhD student in Art and Architectural History."},{"id":"bess-sadler","title":"Bess Sadler","author":null,"date":null,"categories":null,"url":"bess-sadler","layout":"people","content":"Bess Sadler is currently Software Developer at Stanford University and was formerly Chief Architect for the Online Library Environment and Research and Development Librarian at University of Virginia Library"},{"id":"bethany-nowviskie","title":"Bethany Nowviskie","author":null,"date":null,"categories":null,"url":"bethany-nowviskie","layout":"people","content":"Bethany directed the Scholars’ Lab from 2007 to 2015, and is now Director of the Digital Library Federation at CLIR, the Council on Library and Information Resources. She remains affiliated with UVa as a Research Associate Professor of Digital Humanities in the English Department . Computing humanist/humane computationalist since 1996. Formerly director of the Scholars’ Lab and Department of Digital Research &amp; Scholarship at the University of Virginia Library, Special Advisor to the Provost at UVa, and a Distinguished Presidential Fellow at CLIR. @Nowviskie is also the immediate Past President of the Association for Computers and the Humanities and a past chair of MLA ’s Committee on Information Technology. Find out more on her website . Mother of two; tinkerer; not that kind of doctor."},{"id":"biyuan-zhao","title":"Biyuan Zhao","author":null,"date":null,"categories":null,"url":"biyuan-zhao","layout":"people","content":""},{"id":"brandon-phan","title":"Brandon Phan","author":null,"date":null,"categories":null,"url":"brandon-phan","layout":"people","content":"Brandon Phan is a Makerspace Technologist."},{"id":"brandon-walsh","title":"Brandon Walsh","author":null,"date":null,"categories":null,"url":"brandon-walsh","layout":"people","content":"Brandon Walsh is Head of Student Programs in the Scholars’ Lab in the University of Virginia Library. Prior to that, he was Visiting Assistant Professor of English and Mellon Digital Humanities Fellow in the Washington and Lee University Library. He received his PhD and MA from the Department of English at the University of Virginia, where he also held fellowships in the Scholars’ Lab and acted as Project Manager of NINES. His dissertation examined modern and contemporary literature and culture through the lenses of sound studies and digital humanities, and these days he works primarily at the intersections of digital pedagogy and digital humanities. Brandon serves on the editorial boards of the Programming Historian and The Journal of Interactive Technology and Pedagogy . He is a regular instructor at HILT, and he has work published or forthcoming with Programming Historian, Insights, the Digital Library Pedagogy Cookbook, Pedagogy, Digital Pedagogy in the Humanities, and Digital Scholarship in the Humanities, among others. More information can be found on his blog at walshbr.com ."},{"id":"bremen-donovan","title":"Bremen Donovan","author":null,"date":null,"categories":null,"url":"bremen-donovan","layout":"people","content":"Bremen is a filmmaker and a doctoral student in socio-cultural anthropology at the University of Virginia. Her research explores how personal experiences of legal processes and public representations of justice impact individuals, communities, and practices of policing. Her dissertation focuses on the experiences of West African immigrants living in European cities, and builds on a decade of work using collaborative filmmaking methods to address issues related to justice, security, and informal networks of control. Bremen has worked for international organizations including Namati: Innovations in Legal Empowerment, the Open Society Justice Initiative, Conciliation Resources, and has contributed to media outlets such as ABC News and the Guardian. She has facilitated creative workshops for young adults in different parts of the world, from New Urban Arts and Light House  in the U.S., to We Own TV in Sierra Leone, and Ciné Institute in Haiti. Bremen has an A.B. in Architectural Studies from Brown University."},{"id":"brooke-lestock","title":"Brooke Lestock","author":null,"date":null,"categories":null,"url":"brooke-lestock","layout":"people","content":"Brooke is a 2011-12 Praxis Fellow and MA candidate in the Department of English. She is currently working on a thesis which investigates Virginia Woolf’s moment of being as a biographical, historical, and narrative phenomenon in Woolf’s fiction and essays. Brooke is also a graduate research assistant in IATH, working on Alison Booth’s Collective Biographies of Women project."},{"id":"carin-yavorcik","title":"Carin Yavorcik","author":null,"date":null,"categories":null,"url":"carin-yavorcik","layout":"people","content":"Carin Yavorcik is an information studies graduate student at the University of Texas at Austin, where she is especially interested in archives and digital collections. She is currently working with the Scholars’ Lab on a TEI plugin for Omeka."},{"id":"carlos-paramo","title":"Carlos Paramo","author":null,"date":null,"categories":null,"url":"carlos-paramo","layout":"people","content":"Carlos Paramo is a rising senior majoring in Economics and Mathematics at the University of California Berkeley and a 2018 LAMI Scholar. His main fields of interest are public policy and policy impact evaluation, specially policies pertaining health and education. This summer at UVA, he is working with Dr. Gaurab Aryal in analyzing the competition dynamics of the pharmaceutical industry and the development process of new drugs."},{"id":"cassondra-hanna","title":"Cassondra Hanna","author":null,"date":null,"categories":null,"url":"cassondra-hanna","layout":"people","content":"Cassondra is a rising Sophomore at Fisk University in Nashville, Tennessee and a 2018 LAMI Scholar. At Fisk, she studies History &amp; Sociology with a minor in African-American Studies. Cassondra’s research interests follow the intersections of race and class, and their roles within the history of the African-American experience and disenfranchisement. This summer, with the Leadership Alliance Mellon Initiative at UVA, Cassondra will be researching the history of two historically black communities and the impact which was had on their development by 20th century Urban Renewal."},{"id":"catherine-addington","title":"Catherine Addington","author":null,"date":null,"categories":null,"url":"catherine-addington","layout":"people","content":"Catherine Addington is a 2017-2018 Makerspace Technologist and Praxis Fellow 2018-2019."},{"id":"cecilia-márquez","title":"Cecilia Márquez","author":null,"date":null,"categories":null,"url":"cecilia-márquez","layout":"people","content":""},{"id":"celeste-navas","title":"Celeste Navas","author":null,"date":null,"categories":null,"url":"celeste-navas","layout":"people","content":"Celeste Navas is a rising senior at University of California, Riverside where was also an archives processing intern at the Tomás Rivera Library. She is also a Chancellor’s Research fellow conducting research on the Guatemalan STD study (1946-1948), focusing on the conditions and policies of government based records during the Cold War period. Her summer research focuses on biographical narratives of Latina and/or queer women in the Collective Biographies of Women database."},{"id":"charity-revutin","title":"Charity Revutin","author":null,"date":null,"categories":null,"url":"charity-revutin","layout":"people","content":"Charity Revutin worked as a Cultural Heritage Informatics Intern with Will Rourk in Fall 2018."},{"id":"cho-jiang","title":"Cho Jiang","author":null,"date":null,"categories":null,"url":"cho-jiang","layout":"people","content":""},{"id":"chole-wells","title":"Chloe Wells","author":null,"date":null,"categories":null,"url":"chloe-wells","layout":"people","content":""},{"id":"chris-clapp","title":"Chris Clapp","author":null,"date":null,"categories":null,"url":"chris-clapp","layout":"people","content":"Chris Clapp is a Ph.D. candidate in the Department of Economics. The theme that unites his research is an analysis of the effects of public policies on individual behavior. He has previously published research that examines on-the-field success as a justification for the public subsidization of new sports stadia. His dissertation seeks to inform policymakers of the effects of congestion pricing policies on commuter behavior, residential location decisions, and ultimately congestion itself. He is also currently involved in a project that evaluates the impact of public interventions in the lives of local at-risk youth."},{"id":"chris-forster","title":"Chris Forster","author":null,"date":null,"categories":null,"url":"chris-forster","layout":"people","content":"Former NINES Fellow, Scholars’ Lab Fellow, and HASTAC Scholar. Currently Assistant Professor of English at Syracuse University. I completed my PhD at the University of Virginia in August, 2011."},{"id":"chris-gist","title":"Chris Gist","author":null,"date":null,"categories":null,"url":"chris-gist","layout":"people","content":"GIS Specialist with the University of Virginia Library where he builds spatial data collections, teaches GIS courses, and provides GIS user support . His research work includes various demographic, neighborhood indicator, and funding analysis projects in and around Richmond and various environmental science and humanities GIS projects. Chris is a certified GIS professional (GISP)."},{"id":"chris-peck","title":"Chris Peck","author":null,"date":null,"categories":null,"url":"chris-peck","layout":"people","content":"Chris Peck is a composer/performer whose work has been presented extensively around the US and internationally. His works involving collaboration with contemporary dance, audience participation, various mixtures of trained and “untrained” performers, and site specificity have been performed at the Venice Biennale, Performa, ImPulsTanz, Improvised and Otherwise, The Whitney Museum, and The Kitchen, and reviewed in publications such as The New York Times and Signal to Noise. He is a third-year PhD student in Composition and Computer Technologies in the McIntire Department of Music and a Praxis Fellow at the Scholars’ Lab."},{"id":"chris-whitehead","title":"Chris Whitehead","author":null,"date":null,"categories":null,"url":"chris-whitehead","layout":"people","content":""},{"id":"christian-howard","title":"Christian Howard","author":null,"date":null,"categories":null,"url":"christian-howard","layout":"people","content":""},{"id":"ciara-horne","title":"Ciara Horne","author":null,"date":null,"categories":null,"url":"ciara-horne","layout":"people","content":"Ciara Horne works as a Cultural Heritage Informatics Intern with Will Rourk in Spring 2019."},{"id":"claire-maiers","title":"Claire Maiers","author":null,"date":null,"categories":null,"url":"claire-maiers","layout":"people","content":""},{"id":"coke-matthews","title":"Coke Matthews","author":null,"date":null,"categories":null,"url":"coke-matthews","layout":"people","content":"Coke Matthews is a Makerspace Technologist in the Scholars’ Lab."},{"id":"dana-stefanelli","title":"Dana Stefanelli","author":null,"date":null,"categories":null,"url":"dana-stefanelli","layout":"people","content":""},{"id":"dana-wheeles","title":"Dana Wheeles","author":null,"date":null,"categories":null,"url":"dana-wheeles","layout":"people","content":""},{"id":"dave-richardson","title":"Dave Richardson","author":null,"date":null,"categories":null,"url":"dave-richardson","layout":"people","content":"Dave is a former GIS assistant in the Scholars’ Lab and a graduate of the Environmental Sciences program."},{"id":"david-flaherty","title":"David Flaherty","author":null,"date":null,"categories":null,"url":"david-flaherty","layout":"people","content":""},{"id":"david-mcclure","title":"David McClure","author":null,"date":null,"categories":null,"url":"david-mcclure","layout":"people","content":"Formerly Web Applications Developer on the Scholars’ Lab R&amp;D team, David graduated from Yale University with a degree in the Humanities in 2009 and worked as an independent web developer in San Francisco, New York, and Madison, Wisconsin before joining the lab in 2011. David was the lead developer on Neatline and works on research projects that use software as a tool to advance traditional lines of inquiry in literary theory and aesthetics."},{"id":"drew-macqueen","title":"Drew Macqueen","author":null,"date":null,"categories":null,"url":"drew-macqueen","layout":"people","content":"Drew MacQueen is a GIS Specialist in the Scholars’ Lab where he focuses on spatial data collection and analysis, and web-based GIS and visualization; occasionally dipping a toe into the waters of scripting and spatial database design."},{"id":"duy-nguyen","title":"Duy Nguyen","author":null,"date":null,"categories":null,"url":"duy-nguyen","layout":"people","content":""},{"id":"ed-triplett","title":"Ed Triplett","author":null,"date":null,"categories":null,"url":"ed-triplett","layout":"people","content":"Ed was a 2011-12 Scholars’ Lab Fellow and Praxis Fellow, and is a PhD candidate in the McIntire Department of Art."},{"id":"eleanore-neumann","title":"Eleanore Neumann","author":null,"date":null,"categories":null,"url":"eleanore-neumann","layout":"people","content":""},{"id":"elena-maltos","title":"Elena Maltos","author":null,"date":null,"categories":null,"url":"elena-maltos","layout":"people","content":"I am Elena Maltos and I will be entering my junior year at Heritage University in the state of Washington. I am a History major with a minor in Criminal Justice. My research this summer as part of the Leadership Alliance Mellon Initiative will focus on mass incarceration and felon disenfranchisement in the United States. My research will explore exactly how these practices have impacted voter turnout and representation of minorities in state and federal elections."},{"id":"elizabeth-bollwerk","title":"Elizabeth Bollwerk","author":null,"date":null,"categories":null,"url":"elizabeth-bollwerk","layout":"people","content":"Elizabeth will analyze the geospatial patterns of pipe use in early Native American settlements. She will pair her strong archaeological background with a suite of GIS technologies."},{"id":"elizabeth-fox","title":"Elizabeth Fox","author":null,"date":null,"categories":null,"url":"elizabeth-fox","layout":"people","content":""},{"id":"elizabeth-mitchell","title":"Elizabeth Mitchell","author":null,"date":null,"categories":null,"url":"elizabeth-mitchell","layout":"people","content":"As Community Advocate, Elizabeth Mitchell (PhD Candidate in Architectural History) advocates for the users of our DH projects, working on documentation, design, development, and project management approaches to champion user needs. She previously worked as Community Project Manager and as a research assistant on the NEH Neatline project."},{"id":"emily-de-leon","title":"Emily De Leon","author":null,"date":null,"categories":null,"url":"emily-de-leon","layout":"people","content":"Hello! My name is Emily de León. I am an uprising senior at DePaul University in Chicago, double majoring in Latin American and Latino Studies and Spanish Studies. I am currently a LAMI Scholar, a McNair Scholar, in hopes in pursuing a PhD in Latin American Cultures. The research I am working on this summer at UVA is about redefining what the Assimilation Theory is within the Latin American communities in the United States."},{"id":"emily-felber","title":"Emily Felber","author":null,"date":null,"categories":null,"url":"emily-felber","layout":"people","content":"Emily Felber works as a Cultural Heritage Informatics Intern with Will Rourk in Spring 2019."},{"id":"emily-mellen","title":"Emily Mellen","author":null,"date":null,"categories":null,"url":"emily-mellen","layout":"people","content":"Emily Mellen is a Public History Intern (2019) as well as a Praxis Fellow (2018-2018)."},{"id":"emily-senefeld","title":"Emily Senefeld","author":null,"date":null,"categories":null,"url":"emily-senefeld","layout":"people","content":"Emily Senefeld is a Ph.D. candidate in the Corcoran Department of History, where she serves as Director of the Project for Technology in History Education. She received her B.A. from Sewanee: The University of the South in 2005 and her M.A. in History from the University of Virginia in 2010. Her dissertation research focuses on the history of the Highlander Folk School from the 1930s to 1960s, with an emphasis on how the staff used folk and labor music, labor dramas, and documentary filmmaking in their labor and civil rights organizing."},{"id":"eric-johnson","title":"Eric Johnson","author":null,"date":null,"categories":null,"url":"eric-johnson","layout":"people","content":"Eric Johnson is the former Head of Outreach &amp; Public Services at the Scholars’ Lab in the University of Virginia Library. He holds an MA in US History (George Mason University) and an MS in Library and Information Studies (Florida State) and has research interests in information sharing among creative people; citizen history; user-generated content in libraries, archives, and museums (LAMs); museum and library history; hospitality theory as applied to LAMs; public service librarianship; and communities of practice."},{"id":"eric-rochester","title":"Eric Rochester","author":null,"date":null,"categories":null,"url":"eric-rochester","layout":"people","content":"My interests include text processing, text mining, and natural language processing, as well as web-development and general programming. Studied medieval English literature and linguistics at UGA . Dissertated on lexicography. Now I program in Haskell and write when I’m not building cool stuff for the Scholars’ Lab. Also, husband and parent. Do you notice that sleep isn’t on that list?"},{"id":"erielle-jones","title":"Erielle Jones","author":null,"date":null,"categories":null,"url":"erielle-jones","layout":"people","content":"Erielle Jones is a 2018 LAMI scholar. She is a rising senior at the University of Missouri: Columbia. As a double major in Sociology and History, she is interested in body politics and identity formation. This summer, she will be working with Professor Tony Lin on an analysis of the amelioration of white debt in multicultural settings."},{"id":"erik-deluca","title":"Erik DeLuca","author":null,"date":null,"categories":null,"url":"erik-deluca","layout":"people","content":"Erik DeLuca makes music that moves from being influenced by 90’s rock and the New York School of composers, to listening in quiet places. His dissertation, “Fieldworks: a Path to Composing” entwines the boundaries of acoustic ecology, audio documentary, anthropology, and electroacoustic music composition. In 2013 “Winter”—a piece for orchestra, voice, and recordings of silence—premiered in Denali National Park by the Fairbanks Summer Arts Festival Orchestra and “Community Listening in Isle Royale National Park”, a multi-media sonic ethnography, was featured on a panel at the Society for Ethnomusicology Conference. Erik is a PhD candidate in the Music Department at UVa."},{"id":"ethan-gruber","title":"Ethan Gruber","author":null,"date":null,"categories":null,"url":"ethan-gruber","layout":"people","content":"Classical archaeology student, Web services developer at American Numismatic Society ( @ANSCoins ), 3d modeler, former web applications developer in the Scholars’ Lab."},{"id":"ethan-reed","title":"Ethan Reed","author":null,"date":null,"categories":null,"url":"ethan-reed","layout":"people","content":""},{"id":"fitz-green","title":"Fitz Green","author":null,"date":null,"categories":null,"url":"fitz-green","layout":"people","content":"Former Scholars’ Lab desk consultant and current Director of Educational Ministries at the Center for Christian Study in Charlottesville. Fitz is completing his Ph.D. at U.Va., studying the history of the Early Church. He likes talking about how early church leaders read and interpreted their Bible."},{"id":"francesca-tripodi","title":"Francesca Tripodi","author":null,"date":null,"categories":null,"url":"francesca-tripodi","layout":"people","content":""},{"id":"gabriel-hankins","title":"Gabriel Hankins","author":null,"date":null,"categories":null,"url":"gabriel-hankins","layout":"people","content":"Gabriel Hankins is a PhD candidate in English Language and Literature, specializing in transnational modernism and new approaches to modeling distributed textual networks. Gabriel’s dissertation project investigates the idea of world government in fiction from 1919-1945, from Wells’s “Modern Utopia” and Huxley’s “Brave New World” to little-known “League of Nations” novels."},{"id":"gabriela-corona","title":"Gabriela Corona","author":null,"date":null,"categories":null,"url":"gabriela-corona","layout":"people","content":""},{"id":"gabriele-trinidad-perez","title":"Gabriela Trinidad Pérez","author":null,"date":null,"categories":null,"url":"gabriele-trinidad-perez","layout":"people","content":"Gabriela Trinidad Pérez is a rising Junior at University of Puerto Rico, Río Piedras. She is a Sociology student with emphasis on Culture and Gender. She’s also part of her university’s Honors Program, where she’s currently working on an undergraduate thesis on Sociology and Popular Culture. During the Summer, with the LAMI Program, she’ll be researching the representation of Latino Women in US Popular Culture, and the backlash that arises with this subject."},{"id":"gillet-rosenblith","title":"Gillet Rosenblith","author":null,"date":null,"categories":null,"url":"gillet-rosenblith","layout":"people","content":"Gillet is a PhD candidate in the history department with an interest in race and culture in the 20th century United States. Her dissertation examines the meaning of the home in the context of late twentieth century public housing. As someone who studies change over time, she very much looks forward to questioning the linear and causal understanding of time that undergirds history as a discipline with her fellow cohort members. She is excited to learn from her cohort and see what this exploration of time brings!"},{"id":"gillian-price","title":"Gillian Price","author":null,"date":null,"categories":null,"url":"gillian-price","layout":"people","content":"Gillian Price received a B.A. in Spanish from Carleton College in Northfield, Minnesota.  While studying at Carleton, she spent one semester studying abroad in Madrid, Spain.  Her senior year she worked as a Teaching Assistant for Spanish 101 and 102.  She is currently pursuing her PhD in Spanish at UVA and is teaching.  Her academic interests include 20th and 21st century Peninsular Literature and film. Gillian is a Student Assistant manning the SLab service desk and a PhD Candidate in the Department of Spanish, Italian, and Portuguese."},{"id":"grant-kim","title":null,"author":null,"date":null,"categories":null,"url":"grant-kim","layout":"people","content":"Grant Kim is a Makerspace Technologist."},{"id":"gwen-nally","title":"Gwen Nally","author":null,"date":null,"categories":null,"url":"gwen-nally","layout":"people","content":""},{"id":"jake-gianni","title":"Jake Gianni","author":null,"date":null,"categories":null,"url":"jake-gianni","layout":"people","content":"Jake Gianni works as a Cultural Heritage Informatics Intern with Will Rourk in Spring 2019."},{"id":"james-ambuske","title":"James Ambuske","author":null,"date":null,"categories":null,"url":"james-ambuske","layout":"people","content":"Jim Ambuske is a Ph.D. candidate in the Corcoran Department of History studying the Era of the America Revolution. His dissertation, tentatively entitled, “‘The Loss it Sustain’d by the Immense Drain of Men’: The Imperial Politics of Scottish Emigration to Revolutionary America, 1763-1803,” explores the ways in which massive emigration from Scotland to North America in this period forced Scots living at home and in the American colonies to reevaluate the benefits Scots and Scotland derived from participating in the British empire. He has served as a teaching assistant for courses in Ancient Greek and Roman, Middle Eastern, French Revolutionary, and Early American political, intellectual, and military history. He has been the instructor for two courses, a Fourth-Year Seminar on the American Revolution, and the First-Year Survey of American History, c. 1600 - 1865. In Fall 2014, he will teach another Fourth-Year Seminar, “America and Scotland in an Age of War and Revolution, 1754 - 1815.” A grateful recipient of a number of grants and fellowships, Jim served as Research Assistant for the NEH and ACLS-sponsored MapScholar project in Spring 2014, headed by Professors S. Max Edelson and Bill Ferster. In addition to joining the Scholars’ Lab in 2014-2015 as a DH Grad Fellow, he has the honor to hold a Doris G. Quinn Foundation Dissertation Completion Fellowship for the year as well. When not in the archive, classroom, or computer lab, Jim enjoys spending time with family and running Charlottesville’s numerous hills."},{"id":"james-p-ascher","title":"James P. Ascher","author":null,"date":null,"categories":null,"url":"james-p-ascher","layout":"people","content":"Doctoral candidate in the English Department who studies 18th century anglophone literature and bibliography, James puts media studies, textual criticism, ephemera, and paperwork history under the big-tent of bibliography. He’s interested in resistance to the Enlightenment, epistolary communities, and the history of features in books, as well as the light they shed on clandestine publishing and the history of obscenity. Previously, he was Assistant Professor in the Libraries and in English at the University of Colorado Boulder where he cataloged rare books, taught book history, and directed the ScriptaLab colloquium. His published work includes bibliographical methods, issues in diplomatic transcription, processes for collection surveys, and methods training and recruiting librarians. He also serves as a lab instructor teaching descriptive bibliography at the Rare Book School at the University of Virginia. He organized the 54th Annual Rare Books and Manuscripts Section Preconference, served as the Vice-President for Publications of the American Printing History Association, and served as Treasurer for the Fellowship of American Bibliophilic Societies. He thinks there is a spiritual connection between literate programming, lexicography, bibliography, and time. He is trying to find it while he is a Praxis Fellow. He doesn’t like writing bios."},{"id":"jared-benton","title":"Jared Benton","author":null,"date":null,"categories":null,"url":"jared-benton","layout":"people","content":"Jared Benton is a Ph.D. candidate in the McIntire Department of Art &amp; Architecture’s Classical Archaeology Program. He plans to use GIS mapping and analysis techniques to examine specialization in Pompeii’s bakeries as a case study of changes in the Roman economy in the first century CE."},{"id":"jason-kirby","title":"Jason Kirby","author":null,"date":null,"categories":null,"url":"jason-kirby","layout":"people","content":"Jason Kirby is a Ph.D. student in the Critical &amp; Comparative program of the McIntire Department of Music at UVa. His research interests include genre in popular music, American “roots” music, sound and music in cinema, and the relationship of popular music to regional identity."},{"id":"jean-bauer","title":"Jean Bauer","author":null,"date":null,"categories":null,"url":"jean-bauer","layout":"people","content":"In brief: I am an Early American historian, a database designer, and a photographer. I’m also sleep-deprived, but that probably isn’t related . . . Current Digital Humanities Librarian at Brown University, former Presidential Fellow in the Graduate School of Arts and Sciences, a former Digital Humanities Fellow in the University of Virginia Library’s Digital Scholars’ Lab, and a former a NINES Graduate Fellow."},{"id":"jelon-alexander","title":"Je'lon Alexander","author":null,"date":null,"categories":null,"url":"jelon-alexander","layout":"people","content":"Je’lon Alexander is a graduating senior at Morehouse College. His career interest focuses on 20th Century African American history, which include the Civil Rights Movement, Black Power Movement, student activism, and legal and political activism. Mr. Alexander is part of the UNCF Mellon-Mays Undergraduate Fellowship Program conducting his independent research entitled “The Rebellion of the Students: The Connection between the Atlanta Student Movement and the Atlanta Project.”"},{"id":"jennifer-foy","title":"Jennifer Foy","author":null,"date":null,"categories":null,"url":"jennifer-foy","layout":"people","content":"Jennifer Foy is a PhD candidate in English at the University of Virginia. Her research focuses on the axes of stigma and sympathy in eighteenth century literature and culture, and her dissertation is titled ‘Mapping Sympathy: Sensibility, Stigma, and Space in the Long Eighteenth Century.”"},{"id":"jennifer-grayburn","title":"Jennifer Grayburn","author":null,"date":null,"categories":null,"url":"jennifer-grayburn","layout":"people","content":"Jennifer is a 2015-2016 Makerspace Consultant and a 2014-2015 Praxis Fellow. She is a Ph.D. Candidate in the History of Art and Architecture and her research focuses on medieval architecture in Norse territories of NW Europe."},{"id":"jeremy-boggs","title":"Jeremy Boggs","author":null,"date":null,"categories":null,"url":"jeremy-boggs","layout":"people","content":"As Head of Research &amp; Development, I focus on front-end development, user interface, user experience, and aesthetics for Scholars’ Lab projects, as well as leading the Scholars’ Lab R&amp;D team. In addition to helping faculty and students on their research projects, I keep office hours and do research and teaching for our Makerspace . I’m currently researching methods for doing distant reading of comic books using computer vision. I’ve been a practicing digital humanist for a little over a decade now. I have a Master’s degree in History, and have taught courses in history, graphic design, new media, and American studies. I’m also an instructor at HILT and Accessible Future . I formerly served as the Communication Officer for the Association for Computers and the Humanities . Ari Beth’s and Ella’s father. Chaotic Good.\n  *[HILT]: Humanities Intensive Learning and Teaching"},{"id":"jim-cocola","title":"Jim Cocola","author":null,"date":null,"categories":null,"url":"jim-cocola","layout":"people","content":"In his study of modernist and post-modernist American poetry, Jim used Google Earth and other vernacular mapping tools to create a “mappemunde” of the way poets such as Charles Olson depicted place and how their own home locations influenced their work. Where is he now? Dr. Cocola is Assistant Professor of Literature, Film, &amp; Media at Worcester Polytechnic Institute."},{"id":"joe-gilbert","title":"Joe Gilbert","author":null,"date":null,"categories":null,"url":"joe-gilbert","layout":"people","content":"Formerly the Scholars’ Lab’s User Experience Web Developer and U.Va. Library and Scholars’ Lab desk consultant. Joe’s research interests include the scholarly implications of user interfaces, the ethical claims of American poetic discourse, and the cultural role of poetry in mid-20th-century America."},{"id":"joel-rhone","title":"Joel Rhone","author":null,"date":null,"categories":null,"url":"joel-rhone","layout":"people","content":"An English major at Howard University, Joel studies literature after WWII. His focus is on how militaristic narratives and ideology construct civilian reality.  Joel’s favorite books and movies include  A Tree Grows in Brooklyn, Go Tell It On the Mountain, It’s Kind of a Funny Story, and The Adjustment Bureau . He also loves plays, ice cream, and board games."},{"id":"jordan-buysse","title":"Jordan Buysse","author":null,"date":null,"categories":null,"url":"jordan-buysse","layout":"people","content":"Jordan is a PhD student in the English department working towards a dissertation on automation and literary composition. He is interested in text analysis and various AI-approaches to text, and has worked in the Makerspace tinkering with 3D printers and Arduinos."},{"id":"joris-gjata","title":"Joris Gjata","author":null,"date":null,"categories":null,"url":"joris-gjata","layout":"people","content":"With Albanian nationality, a major in International Relations and minor in International Economics from the Middle East Technical University (METU) in Turkey, an MSc in International Political Economy from London School of Economics (LSE) in the UK, an academic Turkish husband, two domestic shorthair US cats and two origin-not-known parakeets, I am on the path towards completing my PhD in Sociology at UVA with a dissertation on the emergence of new forms of regulation. Similar to my background, my interests are also diverse and multifaceted: innovation, organizations, the social construction of the economic, and globalization. However, the search for sustainable modes of productive order and chaos underlies all these research interests. I believe in interdisciplinary work and multicultural conversations. I love appreciating art, good food, new cultures, and learning foreign languages (I have good knowledge of English, Turkish, Italian, French, and basic knowledge of Greek, German, Spanish and Russian). But above all, I think there is nothing more relaxing than walking and observing the living nature (especially flowers and birds)."},{"id":"joseph-thompson","title":"Joseph Thompson","author":null,"date":null,"categories":null,"url":"joseph-thompson","layout":"people","content":"Joseph Thompson is a doctoral candidate in the Corcoran Department of History. His dissertation, “Sounding Southern: Music, Militarism, and the Making of the Sunbelt,” uses music to examine the cultural impact of the military-industrial complex since the 1950s. This analysis draws on methods of cultural history, political history, and sound studies to consider the shifting meanings of race, region, and citizenship over the last half of the twentieth century."},{"id":"joy-ikekhua","title":"Joy Ikekhua","author":null,"date":null,"categories":null,"url":"joy-ikekhua","layout":"people","content":""},{"id":"julia-haines","title":"Julia Haines","author":null,"date":null,"categories":null,"url":"julia-haines","layout":"people","content":"Julia Haines is a 2017-2018 Graduate DH Fellow."},{"id":"julia-schrank","title":"Julia Schrank","author":null,"date":null,"categories":null,"url":"julia-schrank","layout":"people","content":""},{"id":"justin-greenlee","title":"Justin Greenlee","author":null,"date":null,"categories":null,"url":"justin-greenlee","layout":"people","content":""},{"id":"kaitlin-mitchell","title":"Kaitlin Mitchell","author":null,"date":null,"categories":null,"url":"kaitlin-mitchell","layout":"people","content":"Kaitlin Mitchell is a rising junior at Northern Arizona University in Flagstaff, Arizona. She is a Sociology major, minor in English and Law, Rights and Justice, her interests also lie in poc queer studies, young adolescents, and women. During her time at the University of Virginia/Leadership Alliance, she worked on the Race and Faith in Communities and Churches Project, focusing on US Christianity and how it intersects with multiethnic communities and churches."},{"id":"katelynn-hester","title":"Katelynn Hester","author":null,"date":null,"categories":null,"url":"katelynn-hester","layout":"people","content":"Katelynn Hester is a rising junior at Southeastern Oklahoma State University in Durant, Oklahoma and a 2018 LAMI Scholar. She is a History major with a double minor in English and Spanish and is a member of her university’s Honors Program. Her research this summer will document the cultural shifts that took place within the Choctaw Nation due to the presence of Christian missionaries during the 18th and 19th centuries. "},{"id":"katherine-donnally","title":"Katherine Donnally","author":null,"date":null,"categories":null,"url":"katherine-donnally","layout":"people","content":"Katherine did user experience, information architecture, and design for the Scholars’ Lab, for Neatline and other research projects. Previously, Katherine was a 2017 student intern in the lab, working as a Social Justice Developer, and in that role worked on redesigning Take Back the Archive on both the front and back end with an aesthetic focus on public usership at a time of heightened political awareness. Additionally, she is using the project as a case study for research on ethical design choices that respect the lives behind social justice causes without sacrificing visual interest."},{"id":"katina-rogers","title":"Katina Rogers","author":null,"date":null,"categories":null,"url":"katina-rogers","layout":"people","content":"Katina was formerly the Scholarly Communication Institute’s Senior Research Specialist. She worked to share outcomes from conversations SCI convened on emerging models for authoring and publication and on graduate education reform. She also surveyed humanities-trained respondents who self-identified as working in alternative academic careers, and played a role in the expansion of the Praxis Program to a multi-institutional and international effort."},{"id":"kayla-pinson","title":"Kayla Pinson","author":null,"date":null,"categories":null,"url":"kayla-pinson","layout":"people","content":"Kayla Pinson is a student at Virginia State University studying History with minors in Philosophy and Mass Communications. Her research interest include African Historiography and Historical Memory. She is also a powerful Spoken Word Artist who hopes to combine her research interest and creativity to promote historical healing through poetry. In her spare time, she enjoys traveling and thought provoking conversations with strangers."},{"id":"kelli-shermeyer","title":"Kelli Shermeyer","author":null,"date":null,"categories":null,"url":"kelli-shermeyer","layout":"people","content":""},{"id":"kelly-johnston","title":"Kelly Johnston","author":null,"date":null,"categories":null,"url":"kelly-johnston","layout":"people","content":"As a former Geographic Information Systems Specialist for the Scholars’ Lab, Kelly worked with faculty, staff, and students to visualize, analyze, create, and manage geographic data. He earned a master’s degree in Geographic Information Science from Indiana University Purdue University Indianapolis. His research interests include applied geography and cartography as an art form."},{"id":"kerwin-holmes-jr","title":"Kerwin Holmes, Jr.","author":null,"date":null,"categories":null,"url":"kerwin-holmes-jr","layout":"people","content":"Kerwin Holmes, Jr. is a graduating senior at Morehouse College .  His career interest focuses on historical theology and the relationship between religious thought and social evolution through time.  Mr. Holmes currently conducts research for the UNCF/Mellon-Mays Undergraduate Fellowship Program, and his work is entitled “The Gospel of Slavery: A Study of Antebellum Southern American Christian Thought on African Slavery following the Second Great Awakening.”"},{"id":"laura-miller","title":"Laura Miller","author":null,"date":null,"categories":null,"url":"laura-miller","layout":"people","content":"The Scholars’ Lab supports a diverse community of digital scholars and scholarly makers across the University and beyond. As Head of Public Services, my main goal is to foster and grow that community. I convene our speaker events and workshops, designed to connect researchers - at all levels - with expertise, training, and intellectual discourse. I also manage our Makerspace and serve as the primary point of contact between the Scholars’ Lab and broader library services. I have a background in English and information science, was the librarian on the Fall 2015 voyage of Semester at Sea, and have an ongoing interest in digital publishing and open scholarly communication."},{"id":"lauren-reynolds","title":"Lauren Reynolds","author":null,"date":null,"categories":null,"url":"lauren-reynolds","layout":"people","content":"Lauren Reynolds was a Makerspace Technologist in the lab."},{"id":"lee-bidgood","title":"Lee Bidgood","author":null,"date":null,"categories":null,"url":"lee-bidgood","layout":"people","content":"Bluegrass music in the Czech Republic – there’s a lot of it, and there’s a lot of history and variety to the music-making. In addition to participant-observation and other person-level ethnographic work with folks who are part of the Czech bluegrass community, I broaden my work through web-based interactions. My online “Bluegrassová Mapa Ceské Republiky,” which is in construction with help from the UVA library and the Bluegrassova Asociace Ceske Republiky will help widen the perspective of my dissertation, indicate the extent of bluegrass activity in Czech Republic to outsiders, and enable new local community and inter-community connections."},{"id":"leif-frederickson","title":null,"author":null,"date":null,"categories":null,"url":"leif-frederickson","layout":"people","content":"Leif Frederickson was a Graduate Fellow in Digital Humanities and a PhD student in the history department."},{"id":"lilybeth-shields","title":"Lilybeth Shields","author":null,"date":null,"categories":null,"url":"lilybeth-shields","layout":"people","content":"Hello, I am a rising senior at the University of Puerto Rico-Humacao . I major in English with a minor in secondary ESL education. My research interests include linguistics, and education. Sometimes I write, sometimes I draw, sometimes I take pictures."},{"id":"lindsay-oconnor","title":"Lindsay O’Connor","author":null,"date":null,"categories":null,"url":"lindsay-oconnor","layout":"people","content":"I am a 2011-12 Praxis Fellow, a PhD candidate in the UVa Department of English, and a former AmeriCorps member, campus civic engagement coordinator, and criminal defense investigator. My dissertation uses theories of waste and excess to examine American literary responses to disaster from the 1927 Mississippi flood to the present."},{"id":"lydia-rodriguez","title":"Lydia Rodriguez","author":null,"date":null,"categories":null,"url":"lydia-rodriguez","layout":"people","content":""},{"id":"lydia-warren","title":"Lydia Warren","author":null,"date":null,"categories":null,"url":"lydia-warren","layout":"people","content":"Lydia Warren is a Critical and Comparative Studies PhD student interested in constructions and perceptions of race, gender, authenticity, and meaning in American vernacular music, as well as regional specificity, participatory discrepancy, and cultural tourism. After a decade of international touring and recording as a blues musician, Lydia began pursuing her love of music within academia, starting at Middlesex Community College. In 2011 she earned the school-wide honors of both a Follett scholarship and the annual music department award for “Most Outstanding Performance.”  Transferring to Smith College as an Ada Comstock scholar, she graduated in December 2013 with a B.A. in Music and received the Harriet Dey Barnum Prize for “Best All-Around Student of Music.” In 2013 Lydia interned at Smithsonian Folkways Records in Washington, D.C., sparking her interest in archival work and digital humanities. Lydia likes the out of doors, urban and rural exploration, and practicing yoga."},{"id":"madison-choi","title":"Madison Choi","author":null,"date":null,"categories":null,"url":"madison-choi","layout":"people","content":"Madison Choi will be entering her senior year at Chaminade University of Honolulu in the fall. She is double majoring in English and Communication and has an interest in pop culture, film, and television. Over the summer, Madison will be analyzing the Netflix original series  Stranger Things  as it revives distinct cultural elements from the 1980s."},{"id":"malikia-johnson","title":"Malikia Johnson","author":null,"date":null,"categories":null,"url":"malikia-johnson","layout":"people","content":"My name is Malikia Johnson and I am rising Junior at Howard University majoring in Sociology and Africana Studies. My research focuses on the representation of African- American women in K-12 textbooks and the effect this has on the cultural identity of young African- American girls ages 7 to 9."},{"id":"mandy-han","title":"Mandy Han","author":null,"date":null,"categories":null,"url":"mandy-han","layout":"people","content":"Undergraduate, School of Architecture"},{"id":"margaret-furr","title":null,"author":null,"date":null,"categories":null,"url":"margaret-furr","layout":"people","content":"Margaret Furr worked as a Makerspace Technologist."},{"id":"mariama-jalloh","title":"Mariama Jalloh","author":null,"date":null,"categories":null,"url":"mariama-jalloh","layout":"people","content":"Mariama Jalloh is a rising Senior at Spelman College and 2018 LAMI Fellow. She is a Political Science major on a Pre-Law track with a concentration on Gender and Politics. This summer in the University of Virginia’s Leadership Alliance Mellon program she will be conducting a comparison research the way psychological oppression affects Female Genital Mutilation victims in two environments."},{"id":"mark-nevin","title":"Mark Nevin","author":null,"date":null,"categories":null,"url":"mark-nevin","layout":"people","content":""},{"id":"marysia-serafin","title":"Marysia Serafin","author":null,"date":null,"categories":null,"url":"marysia-serafin","layout":"people","content":"Marysia Serafin is a Makerspace Technologist."},{"id":"mathilda-shepard","title":"Mathilda Shepard","author":null,"date":null,"categories":null,"url":"mathilda-shepard","layout":"people","content":""},{"id":"matt-ford","title":"Matt Ford","author":null,"date":null,"categories":null,"url":"matt-ford","layout":"people","content":"Matt Ford is a senior anthropology major at Vassar College in New York. Following a yearlong domestic exchange at Morehouse College in Atlanta, Matt’s summer research in the University of Virginia’s Leadership Alliance Mellon Initiative program examines the evolution of non-binary gender performance in Black fashion and popular music through Grace Jones, Prince, and self-portraiture. Matt is further interested in ethnomusicology, Black queer liberation theology, and free knowledge-sharing digital platforms."},{"id":"matt-mitchell","title":"Matt Mitchell","author":null,"date":null,"categories":null,"url":"matt-mitchell","layout":"people","content":"Matt Mitchell, a former SLab software developer, is currently a developer at hotelicopter."},{"id":"matt-munson","title":"Matt Munson","author":null,"date":null,"categories":null,"url":"matt-munson","layout":"people","content":"A former SLab Fellow, Matt is currently currently the Wissenschaftlicher Mitarbeiter at the Göttingen Center for Digital Humanities, DARIAH Arbeitspaket 2, Forschung und Lehre."},{"id":"matt-west","title":"Matt West","author":null,"date":null,"categories":null,"url":"matt-west","layout":"people","content":""},{"id":"matthew-gibson","title":"Matthew Gibson","author":null,"date":null,"categories":null,"url":"matthew-gibson","layout":"people","content":"Matthew Gibson is a rising Junior at Xavier University in Cincinnati, Ohio. His career interest is in Middle Eastern history and analyzing how revolutions in the Middle East begin. In his time at the University of Virginia/Leadership Alliance, he worked on a project titled “Seven Pillar of Destruction: Lawrence’s contribution to Counterinsurgency in the Middle East”. "},{"id":"meghan-page","title":"Meghan Page","author":null,"date":null,"categories":null,"url":"meghan-page","layout":"people","content":"Meghan Page worked as a Cultural Heritage Informatics Intern with Will Rourk in Fall 2018."},{"id":"mia-hickman","title":"Mia Hickman","author":null,"date":null,"categories":null,"url":"mia-hickman","layout":"people","content":"Mia Hickman is a rising sophomore at Morgan State University in Baltimore, MD and a 2018 LAMI Scholar. She is a honors student majoring in Sociology with a minor in Spanish. Her research interests include African religion in Latin America and the anthropological/sociological impact of film."},{"id":"monica-blair","title":"Monica Blair","author":null,"date":null,"categories":null,"url":"monica-blair","layout":"people","content":""},{"id":"nic-dalton","title":"Nic Dalton","author":null,"date":null,"categories":null,"url":"nic-dalton","layout":"people","content":"Nic Dalton is a Makerspace Technologist."},{"id":"nora-benedict","title":"Nora Benedict","author":null,"date":null,"categories":null,"url":"nora-benedict","layout":"people","content":"Nora’s research focuses on twentieth-century Latin American literature, descriptive bibliography, the intersection of digital and print culture, and the question of interpretation as influenced by textual materiality. Her dissertation, “The Fashioning of Jorge Luis Borges: Magazines, Books, and Print Culture in Argentina, 1930-1951,” focuses on the marked presence of books (and book production) in this Argentine writer’s life by examining the physical features of his works, which she reads through the lens of analytical bibliography and material studies. During her fellowship year, she will be working with the team of experts in the Scholars’ Lab to create a digital project that allows her to think deeply about the spatial (and temporal) evolution of the process of publication (with a variety of different mapping programs such as Neatline) in Jorge Luis Borges’s Argentina (1930-1951)."},{"id":"oguljan-reyimbaeva","title":"Oguljan Reyimbaeva","author":null,"date":null,"categories":null,"url":"oguljan-reyimbaeva","layout":"people","content":"Graduate student of Russian literature from Turkmenistan"},{"id":"olivia-morgan","title":"Olivia Morgan","author":null,"date":null,"categories":null,"url":"olivia-morgan","layout":"people","content":""},{"id":"pierre-dairon","title":"Pierre Dairon","author":null,"date":null,"categories":null,"url":"pierre-dairon","layout":"people","content":"Evangeline is a sign which, since Longfellow’s poem in 1847, has taken on a life of its own and is now displayed throughout multiple landscapes, supports and discourses. My project aims to find, follow and map these signs to better understand the network of representations that Evangeline inspired."},{"id":"purdom-lindblad","title":"Purdom Lindblad","author":null,"date":null,"categories":null,"url":"purdom-lindblad","layout":"people","content":"The Scholars’ Lab fosters a vibrant graduate community, rich with alumni, current fellows, student assistants, and fellow travelers. As the Head of Graduate Programs, I coordinate our two fellowship programs, the Graduate Fellowship in Digital Humanities and our innovative Praxis Program . In addition, I work with the international Praxis Network, which showcases new models of humanities methods training. I have a background in Religious Studies, American Studies, and Information Science. Currently, I am exploring Ivanhoe and our new MakerSpace, with a special focus on wearable computing."},{"id":"rachel-devorah-trapp","title":"Rachel Devorah Trapp","author":null,"date":null,"categories":null,"url":"rachel-devorah-trapp","layout":"people","content":"Rachel Devorah Trapp is a composer and digital art archivist. Her context-specific works for performance and installation crystallize in sound the habits of being: the daily patterns of ineffable exchange that bind our individual lives together. Pieces by Trapp have been performed by artists such as orkest de ereprijs, loadbang, Meehan/Perkins Duo, and Fred Frith at places such as the National Opera Center (NY), Pioneer Works (NY), the OPENSIGNAL Festival at Brown University (RI), Røst AiR (Norway), the International SuperCollider Symposium (CO), the Music for People and Thingamajigs Festival (CA), and Art in Odd Places (NC). Her projects have been supported by grants and residencies from New Music USA and Studio for Electro-Instrumental Music (STEIM). Trapp has contributed to the digital art archives of New Museum Digital Archive, New York Public Library Performing Arts Center, and Mills College Center for Computer Music. The 2015-2016 academic year is her second year of doctoral studies at the University of Virginia where she is also a Jefferson Fellow. She has previously studied music and other forms of information at Mills College, San José State University, and the City University of New York. racheldevorahtrapp.com"},{"id":"randi-lewis","title":"Randi Lewis","author":null,"date":null,"categories":null,"url":"randi-lewis","layout":"people","content":""},{"id":"ronda-grizzle","title":"Ronda Grizzle","author":null,"date":null,"categories":null,"url":"ronda-grizzle","layout":"people","content":"Ronda is the project management and training specialist for the SLab; the one who wrangles projects and helps SLab staff to find time and space to do their work; bringer of order from chaos; herder of metaphorical cats; a librarian by both training and inclination; fascinated by organizational and personal development; personal coach; comic book, tabletop game, and movie junkie."},{"id":"ryan-johnson","title":"Ryan Johnson","author":null,"date":null,"categories":null,"url":"ryan-johnson","layout":"people","content":"PhD Candidate, Department of Spanish, Italian, &amp; Portuguese"},{"id":"ryan-maguire","title":"Ryan Maguire","author":null,"date":null,"categories":null,"url":"ryan-maguire","layout":"people","content":"Ryan Maguire is a 2017-2018 DH Prototyping Fellow."},{"id":"ryan-russell","title":"Ryan Russell","author":null,"date":null,"categories":null,"url":"ryan-russell","layout":"people","content":"Ryan Russell is a rising senior at Morehouse College. There, he conducts research as a Mellon Mays Fellow, where his research interests focus on race and law. At the University of Virginia, as part of the Leadership Alliance Mellon Initiative, Ryan will be investigating the relevance of black political ideologies today, utilizing black political thought from the 1960s and 1970s as his framework."},{"id":"sara-castro","title":"Sara Castro","author":null,"date":null,"categories":null,"url":"sara-castro","layout":"people","content":"Sara Castro is from the University of Puerto Rico. She is double majoring in Foreign Languages (French and German) and Sociology, and is also a Mellon Mays fellow (2017-2019 cohort). During this summer, she will be conducting research on the relationship between educational language policy, language attitudes, and social/cultural identity in a classroom."},{"id":"sara-henary","title":"Sara Henary","author":null,"date":null,"categories":null,"url":"sara-henary","layout":"people","content":"Lecturer and Fellow at the University of Virginia. A graduate of Rhodes College, she received her Ph.D. in Politics from the University of Virginia."},{"id":"sarah-berkowitz","title":"Sarah Berkowitz","author":null,"date":null,"categories":null,"url":"sarah-berkowitz","layout":"people","content":""},{"id":"sarah-mceleney","title":"Sarah McEleney","author":null,"date":null,"categories":null,"url":"sarah-mceleney","layout":"people","content":"Sarah is a Ph.D. student in the department of Slavic Languages and Literature at UVa.  Her dissertation focuses on the cultural history of Crimea in the Soviet Union, and her main research interests concern the literature and media of the late USSR.  She has worked as a Graduate Teaching Assistant in the Slavic department, teaching 1st and 2nd year Russian at UVa.  She has also received an M.A. in Linguistics, and spent time teaching English as a second language.  In addition to Russian, she has also studied the Polish, French, and Ukrainian languages.  She is interested in the digital humanities in relation to the Slavic studies and is looking forward to learning new ways that digital technologies can be used to better understand literature and history."},{"id":"sarah-storti","title":"Sarah Storti","author":null,"date":null,"categories":null,"url":"sarah-storti","layout":"people","content":"Sarah is a 2011-12 Praxis Fellow, former NINES Fellow, and a PhD candidate in the Department of English."},{"id":"scott-bailey","title":"Scott Bailey","author":null,"date":null,"categories":null,"url":"scott-bailey","layout":"people","content":"I am a digital humanities developer in the Scholars’ Lab and a Ph.D. candidate in philosophical theology, writing a dissertation examining vulnerability at the intersection of theological anthropology and neuroscience. After years working heavily in theological ontology and metaphysics, I have a burgeoning interest in Javascript application frameworks, the role of the humanities in public discourse, and the pedagogical use of digital technologies."},{"id":"scott-spencer","title":"Scott Spencer","author":null,"date":null,"categories":null,"url":"scott-spencer","layout":"people","content":""},{"id":"sean-tennant","title":"Sean Tennant","author":null,"date":null,"categories":null,"url":"sean-tennant","layout":"people","content":""},{"id":"shane-lin","title":"Shane Lin","author":null,"date":null,"categories":null,"url":"shane-lin","layout":"people","content":"Shane works on the history of computing and the impact of digital technology on culture and politics. His dissertation, “Kingdom of Code: Cryptography and the New Privacy” tracks the development of civilian encryption technology and the emergence of cryptography as an academic field of study, the debates over crypto regulation, and the concomitant construction of a new, far more expansive notion of privacy from 1975 to 2000.  As part of his fellowship, Shane will build software tools to analyze a large collection of Usenet posts relating to cryptography in order to uncover the flows of influence between users and across subject domains."},{"id":"spyros-simotas","title":"Spyros Simotas","author":null,"date":null,"categories":null,"url":"spyros-simotas","layout":"people","content":"Spyros Simotas is a 2017-2018 Praxis Program Fellow."},{"id":"stephanie-kingsley","title":"Stephanie Kingsley","author":null,"date":null,"categories":null,"url":"stephanie-kingsley","layout":"people","content":""},{"id":"steven-lewis","title":"Steven Lewis","author":null,"date":null,"categories":null,"url":"steven-lewis","layout":"people","content":"I grew up in Atlanta, Georgia and earned a BA in Jazz Studies from Florida State University. I am currently in my second year of the PhD in the Musicology program at UVA. My research interests include late 20th century jazz neoclassicism, early jazz, and 19th century African-American secular music. I developed an interest in the digital humanities during the first year of my PhD and am very excited to broaden my knowledge as a 2014-2015 Praxis Fellow. In my spare time, I enjoy practicing my instruments, researching my family history, and cooking."},{"id":"swati-chawla","title":"Swati Chawla","author":null,"date":null,"categories":null,"url":"swati-chawla","layout":"people","content":"Swati Chawla is a 2014-15 Praxis Fellow and a Ph.D candidate in history at the University of Virginia. She has always been curious about how and why people make their homes in new places, and is currently pursuing this question through her research on Tibetan exile in India. She was trained in literary studies at the University of Delhi, where she taught as an assistant professor before coming to UVa."},{"id":"tamika-richeson","title":"Tamika Richeson","author":null,"date":null,"categories":null,"url":"tamika-richeson","layout":"people","content":""},{"id":"tanner-greene","title":"Tanner Greene","author":null,"date":null,"categories":null,"url":"tanner-greene","layout":"people","content":"Tanner Greene is a 2017-2018 Praxis Program fellow."},{"id":"tanya-clement","title":"Tanya Clement","author":null,"date":null,"categories":null,"url":"tanya-clement","layout":"people","content":"Tanya Clement is an Asst Professor at the School of Information at UT-Austin, with research interests in scholarly information infrastructure issues, particularly digitization, data curation, and scholarly publication in the humanities. She is advising students Zane Schwarzlose and Carin Yavorcik on their project developing a TEI plugin for Omeka with the Scholars’ Lab."},{"id":"tina-cheng","title":"Tina Cheng","author":null,"date":null,"categories":null,"url":"tina-cheng","layout":"people","content":""},{"id":"tom-finger","title":"Tom Finger","author":null,"date":null,"categories":null,"url":"tom-finger","layout":"people","content":"Tom is a PhD candidate in the Corcoran Department of History.  Soon after enrolling at UVa, he developed a plan for his dissertation that looked at the ways in which technologies, ecosystems, and human social groups interact over large scale economic systems. As a case study of these relationships, his dissertation highlights the growth of the North Atlantic grain trade between the United States and Great Britain during the nineteenth century.  He has found that energy flows regulated by technologies - in his case the grain trade – can go a long way towards explaining how social groups and political movements are formed.  To this end, he explores the relationship between classical liberal economic theory, the growth of railroads, and widespread agrarian unrest in the Midwestern United States following the Civil War."},{"id":"valeria-arce","title":"Valeria Arce","author":null,"date":null,"categories":null,"url":"valeria-arce","layout":"people","content":"Valeria Arce is a sophomore majoring in political science at the University of Puerto Rico, Rio Piedras.  This summer at UVA, she is working with Dr. Tony Tian-Ren Lin on developing a guide for researchers to use while studying Latin American communities using the human ecology model from the Thriving Cities Project."},{"id":"veronica-ikeshoji-orlati","title":"Veronica Ikeshoji-Orlati","author":null,"date":null,"categories":null,"url":"veronica-ikeshoji-orlati","layout":"people","content":""},{"id":"victoria-clark","title":"Victoria Clark","author":null,"date":null,"categories":null,"url":"victoria-clark","layout":"people","content":"Victoria Clark is a 2017-2018 Praxis Program Fellow."},{"id":"victoria-juarez","title":"Victoria Juarez","author":null,"date":null,"categories":null,"url":"victoria-juarez","layout":"people","content":"Victoria Juarez is a rising senior at Santa Clara University. She is a History and Spanish double major with a concentrated interest in early American history. At University of Virginia/Leadership Alliance she is working on a project in conjunction with the Law Library in digitizing the Scottish Session Papers. She will be evaluating the role that women played in these early historical court cases and what the implications were for transatlantic relationships."},{"id":"wade-goodrich","title":"Wade Goodrich","author":null,"date":null,"categories":null,"url":"wade-goodrich","layout":"people","content":"Wade Goodrich works as a Cultural Heritage Informatics Intern with Will Rourk in Spring 2019."},{"id":"wayne-graham","title":"Wayne Graham","author":null,"date":null,"categories":null,"url":"wayne-graham","layout":"people","content":"Wayne Graham was head of the Scholars’ Lab Research and Development team. He holds an MA in history from the College of William and Mary and his BA in history from the Virginia Military Institute . Before joining the Scholars’ Lab in 2009, he worked at the Colonial Williamsburg Foundation’s Department of Historical Research, then as a systems administrator and programmer at William and Mary’s Earl Gregg Swem Library . Over the course of those years, he worked as a historical researcher, TEI specialist, web developer, systems architect, dabbling in computer graphics, high-performance computing, and emerging technologies coordination. Wayne is an advocate of open source software, having worked on many projects over the course of his career. Some of the more notable projects include MyCroft, VuFind, SolrMarc, Blacklight . More recently, he has contributed the many repositories in Github, as well as the numerous Scholars’ Lab projects there. Having mentored many Digital Humanities and Praxis Fellows over the last several years in software development, he has also taught at the Humanities Intensive Learning and Teaching Institute, as well as writing a couple books on developing applications using the Facebook APIs . As a “full-stack” developer, Wayne’s technical expertise is in web application languages, systems design, and techncial training for humanities-based research questions. His research interests include public humanities, augmented and virtual reality, photogrammetry, and scholarly interface design."},{"id":"wendy-hsu","title":"Wendy Hsu","author":null,"date":null,"categories":null,"url":"wendy-hsu","layout":"people","content":"Wendy Hsu is Mellon Digital Scholarship Postdoctoral Fellow in the Center of Digital Learning &amp; Research at Occidental College. She received her PhD in the Critical and Comparative Studies in Music program in the McIntire Department of Music at the University of Virginia. Her research interests lie at the intersection of popular music performance and the transnational contacts between Asia and America, focusing on issues related to race/ethnicity, gender/sexuality, and migration."},{"id":"wendy-robertson","title":"Wendy Robertson","author":null,"date":null,"categories":null,"url":"wendy-robertson","layout":"people","content":"Wendy Robertson is a graduate student in the Department of Environmental Studies at the University of Virginia."},{"id":"will-rourk","title":"Will Rourk","author":null,"date":null,"categories":null,"url":"will-rourk","layout":"people","content":"Will Rourk scans historic buildings, monuments, and objects and renders the scans into 3D data. He thinks about metadata for virtual objects a lot. Ask him about Tibet and Irish tin whistles."},{"id":"zachary-stone","title":"Zachary Stone","author":null,"date":null,"categories":null,"url":"zachary-stone","layout":"people","content":""},{"id":"zane-schwarzlose","title":"Zane Schwarzlose","author":null,"date":null,"categories":null,"url":"zane-schwarzlose","layout":"people","content":"Zane Schwarzlose is an information studies graduate student at the University of Texas at Austin. Zane’s coursework is focused on usability and information architecture. When he’s not in school, he designs websites at a local IT company. Zane is working with Carin Yavorcik on developing an enhanced version of the TEI plugin for Omeka."},{"id":"zijia-zeng","title":null,"author":null,"date":null,"categories":null,"url":"zijia-zeng","layout":"people","content":"Zijia is a 3D technologies student specialist working on a UVA Parents Fund grant with the Scholars’ Lab."},{"id":"zoe-leblanc","title":"Zoe LeBlanc","author":null,"date":null,"categories":null,"url":"zoe-leblanc","layout":"people","content":"Zoe LeBlanc is a Digital Humanities Developer in the Scholars’ Lab."},{"id":"2008-09-08-digital-therapy-luncheon","title":"\"Digital Therapy\" luncheon","author":"Admin","date":"2008-09-08 13:36:19 -0400","categories":["Announcements"],"url":"digital-therapy-luncheon","layout":"post","content":"Please join us in the Scholars’ Lab at noon on Tuesday, September 9th, as we introduce our new Graduate Fellows in Digital Humanities as part of our first “Digital Therapy” Faculty and Grad Luncheon of the semester. With projects in social networking, geospatial analysis, and cultural mapping, these three doctoral candidates – Jean Bauer of the History Department, Pierre Dairon of the French Department, and Abigail Holeman of the Anthropology Department – are applying exciting new methods to the study of early American history, French literature, and Mesoamerican cosmology. Each will speak briefly about his or her research, and a free lunch will be served."},{"id":"2008-09-08-hello-world-2","title":"Hello, world!","author":"bethany-nowviskie","date":"2008-09-08 13:28:54 -0400","categories":["Announcements"],"url":"hello-world-2","layout":"post","content":"I’m here to cut the ribbon on the Scholars’ Lab blog. The Scholars’ Lab was established two short years ago at UVA Library as a site for innovation in the humanities and social sciences.  The idea was to combine the resources and expertise of the Library’s successful Electronic Text (Etext) and Geospatial and Statistical Data (GeoStat) centers with that of UVA’s Research Computing Support Group in a physical space that promotes collaboration and experimentation.  Now we’re extending the conversations that happen in our offices and in the SLab to a wider forum. The past two years have seen amazing work by our Graduate Fellows in Digital Humanities and by scholars from a variety of disciplines and fields who work in collaboration with our on-site experts.  Over the coming months, we’ll be inviting our Fellows, grad student consultants, Scholars’ Lab faculty and staff, visiting scholars, and UVA collaborators to share this blog and make it their own.  The substantive (and CC-licensed ) posts you’ll find here will be vetted to ensure that they represent sound scholarship and are squarely on-topic.  We’ll also use this space to share announcements about SLab events and links to interesting and evocative uses of technology in the humanities and social sciences. We invite you to comment and engage with us here!"},{"id":"2008-09-09-how-to-measure-text","title":"How to Measure Text?","author":"chris-forster","date":"2008-09-08 20:03:13 -0400","categories":["Digital Humanities","Visualization and Data Mining"],"url":"how-to-measure-text","layout":"post","content":"…the words we join have been joined before, and continue to be joined daily. So writing is largely quotation, quotation newly energized, as a cyclotron augments the energies of common particles circulating.\n- Hugh Kenner, The Pound Era &lt;/blockquote&gt; This month marks the beginning of the complicated process of starting up the Large Hadron Collider, the world’s largest particle accelerator (Kenner would haved called it a “cyclotron”), buried beneath the Franco-Swiss border. Near the top of the LHC’s agenda is having a peek into the fabric of space-time to see about the Higgs-Boson, the theorized source of mass. But to do so they’ll need data–lots of data. According to CERN, the event summary data extracted from the collider’s sensors will produce around 10 terabytes daily. That is something like, to use the cliché, the equivalent of a Library of Congress’s worth of data every day (the raw data is much much greater). The physics involved is obviously too complicated for a mere humanities major to discuss in any intelligent way. The interesting thing is the disparity between the sheer amount of data with which the LHC deals, as compared with the scale of the (textual) data of the humanities. How can the LHC, in a single day, focussed on a highly specific set of questions, produce as much information as the literary output of humans represented by the Library of Congress? Why, in short, is the textual data of the humanities so much smaller than the data produced by the LHC? It is, of course, in some ways a silly, completely naive question. But the differences, in size alone, of these two datasets are nevertheless instructive and worthy of consideration. We might oversimplify the matter, and say that the LHC’s data, collected from its sensors and culled by its arrays of servers, is fundamentally information-poor data. The challenge faced by the LHC project is sorting through the complexities of the data to find the relevant information that will allow physicists to answer the questions they have. Language, by contrast, is information rich–so rich that our challenge is not how to separate the wheat from the chaff, but how to deal with the sheer flood of information compressed in text. It is this fact that explains the disparity in size between the LHC’s data and the textual record of the humanities. The textual data of the humanities comes “preorganized” by language. While our digital texts encode only strings, language fills texts with syntactic and semantic information of which our systems of markup are completely oblivious. Martin Wattenberg at IBM’s Watson Research Center puts it well in his interview with Wired when he describes language’s ability to compress information: Language is one of the best data-compression mechanisms we have. The information contained in literature or email, encodes our identity as human beings. The entire literary canon may be smaller than what comes out of particle accelerators or models of the human brain, but the meaning coded into words can’t be measured in bytes. It’s deeply compressed. Twelve words from Voltaire can hold a lifetime of experience. What happens if we take this understanding of language seriously? How would it change the way we deal with textual data? Right now we have plenty of digital texts available, but in order to get the information out of the textual data we have to read it. Right now, only by reading do we attend to the specifically linguistic nature of textual data. Existing text analysis technologies and techniques remain largely quantitative, relying on machine learning techniques to classify texts that are represented by vectors of frequency counts. Key sources of linguistic information, however, like syntax, remain fundamentally unexploited. We are still, in effect, discarding some of the most basic sources of textual information–such as the order in which the words occur (seriously). One avenue, though admittedly crude, is to use a technique like part-of-speech tagging to supplement raw text with part-of-speech tags which provide a fuller, more information-rich digital representation of the linguistic data. By analysing such part-of-speech tags, taking them in pairs, or looking at where in a sentence they occur, we get some sense of how a writer uses language. We step, in short, over the threshold from a purely quantitative view of language use (e.g. how many times does “of” occur per thousand words? what are the most frequently occurring terms?), to a mode of analysis that is able to extract the sort of information that we, humans, are able to when we read. Such techniques are admittedly crude; but they begin to recapture the fundamentally linguistic nature of textual data which is too easily discarded in representations of natural languages. To truly capitalize on the information contained in textual data requires finding more ways to digitally attend to the specifically linguistic nature of textual data. We are trying to read the finely wrought braille of language through the burlap sack that current digital tools offer. With the combination of natural language processing tools (such as POS taggers, parsers, etc) and ever-more sophisticated machine learning techniques, we may be able to get closer. Humanities data is not, necessarily, smaller–it is just more compressed."},{"id":"2008-09-12-all-good-press-is-local","title":"all good press is local","author":"Admin","date":"2008-09-12 08:24:46 -0400","categories":["Announcements"],"url":"all-good-press-is-local","layout":"post","content":"Today’s edition of UVA Today covers our Grad Fellows program and our first luncheon of the semester. Read all about it!"},{"id":"2008-09-16-normality-for-or-against","title":"Normality: For or Against?","author":"jean-bauer","date":"2008-09-16 12:07:59 -0400","categories":["Digital Humanities"],"url":"normality-for-or-against","layout":"post","content":"I’m a historian who is currently designing and/or building four databases.  As I work through the complexities of each project, I’m struck by two thoughts. First: I’m overworked. Second: I like the way relational algebra makes me think. Good database design involves breaking a data set into the smallest viable components and then linking those components back together to facilitate complex analysis.  This process, known as normalization, helps keep the data set free of duplicates and protects the data from being unintentionally deleted or unevenly updated. As I research merchants in the eighteenth century and how they connected people and empires with far-flung locations and transfered goods and ideas across oceans, I find it helpful to break those multivalent connections into discrete units.  Who wrote to whom?  Who worked for whom?  Who became a diplomat or consul for the United States?  Who recommended him for that position?  And so on.  Each question has become a relationship in my design for the Early American Foreign Service Database (EAFSD), and by linking all this (and more) information together, the EAFSD will track how the U.S. Foreign Service developed over fifty years.  But there is a catch. When the database is done, I plan on publishing it online so that other researchers can have access to its data.  However, I cannot deny that the EAFSD was designed to answer questions specific to my dissertation.  Other researchers looking at information gathered from the papers of diplomats, consuls, and merchants will (hopefully) want to ask other questions which my database may or not be able to answer.  For example, I only focus on merchants who had a clear connection to the U.S. government ( i.e., received positions in the Foreign Service), which means that a large segment of the merchant community will not appear in the database. Along with the completed database I plan on releasing the source code (both for the database itself and the web application that permits the data migrations and the basic query structure) under an open source license, hopefully making it easier for other scholars to create their own relational databases to track social networks and institutional development.  Once those databases are published similar issues will arise. When a scholar decides to use a relational database in her research, she is making a decision about methodology – not theory.  A relational database does not dictate what scholars will find in a given data set, but rather shapes their search in ways that need to remain in the forefront of all our minds, even if the methodological discussions get relegated to footnotes or appendices.  If an astronomer has to state the specifications of the telescope along with the data received, a digital humanist should be clear about the choices she made (and why) in designing a database to facilitate her analysis and the analytical limits of the final design. I became a historian because I see the world as a complex and contingent place that doesn’t respond well to being forced into a constraining model.   While having the EAFSD is a necessary condition of my dissertation it is not a sufficient one. There are real world ambiguities and unpredictable turns in my subject matter which should not be modeled in a relational data structure.  High on this list are the many mistakes made by early American diplomats: John Adams picking a fight with the French Foreign Minister in the middle of the Revolutionary War (subject of my Master’s Thesis), James Monroe being recalled by a furious George Washington after denouncing (accurate) rumors regarding a new treaty with Great Britain, Thomas Jefferson breaking the Law of Nations to help Lafayette write the Rights of Man and Citizen, the list goes on and on.  On the other hand, while the database also fails to capture the sheer brilliance of Benjamin Franklin it does hint at John Quincy Adams’ compulsive attention to detail.  None of these stories or personalities map into the database, but they are all crucial to understanding how the newly United States interacted with the larger Atlantic World. Designing the EAFSD has sharpened my historical analysis but narrative prose blurs the edges back into the delightfully abnormal lives of the people I seek to understand."},{"id":"2008-09-17-feeds-coins-and-maps-oh-my","title":"Feeds, Coins, and Maps (oh, my)","author":"Admin","date":"2008-09-17 09:36:47 -0400","categories":["Announcements","Digital Humanities"],"url":"feeds-coins-and-maps-oh-my","layout":"post","content":"Staff from across the Library are offering learning opportunities through the Scholars’ Lab this week! First, Keith Weimer and Chris Ruotolo will give a workshop on using syndication to stay on top of news sources and scholarly journals.  Then, Chris Gist and Kelly Johnston will host the first meeting of an ongoing faculty/grad discussion group on geospatial technology for the humanities.  Finally, Ethan Gruber will present an innovative interface he has created to the UVA Art Museum ’s collection of Greek and Roman coins. Check our calendar for dates and times!"},{"id":"2008-10-09-bamboo-grows-quickly","title":"Bamboo Grows Quickly","author":"jean-bauer","date":"2008-10-09 08:55:18 -0400","categories":["Digital Humanities"],"url":"bamboo-grows-quickly","layout":"post","content":"In July I attended the fourth Bamboo Planning Workshop, held at Princeton University. For those of you unfamiliar with Project Bamboo (as distinct from the feeding of pandas), Bamboo is a series of workshops on the future of digital humanities designed by UC Berkeley and my alma mater, the University of Chicago.  The workshops are bringing together humanities scholars, content providers, administrators, and central IT personnel from universities to design an organization that will serve the needs of the digital humanities community. Typically, only high ranking faculty and administrators get to go, but after juggling the summer schedules of a small staff, my boss at Documents Compass, Holly Shulman, was kind enough to take me with her. In the first general session it quickly dawned on me that I was close to the only non-conference-staff graduate student in the room.  So, as they were passing around the cordless mic, I took a deep breath and raised my hand.  I thanked everyone for all the help they had already given the graduate students at their respective institutions and offered a small plea for continued assistance to those of us who were trying to start careers in a new field. After the session broke up and throughout the rest of the conference, I was amazed by the number of people who came up to me and asked for the “graduate student perspective.”  It was extremely encouraging to meet so many talented and driven professors, researchers, and computer scientists, who felt so protective of the next generation.  I have certainly found that to be true at UVa, and I spent a good deal of my time at Bamboo bragging about the Scholars’ Lab and how it meets many of the needs expressed by the other participants. No one knows what Bamboo will become just yet.  The Princeton conference was the end of Phase I, but there are four more phases still to come.  Whatever happens, simply getting all those people together in the same room paid for itself many times over.  I just soaked it all in. But, perhaps the best part of the conference was overhearing people talk as they walked out of the hotel. “Thanks for the tip on mashups, that is just what my project needs.”\n“You know, I don’t even know the name of my central IT contact.  I should look him or her up.”\n“Here’s my email.  Let me know when your project goes into beta, I’d love to check it out.”\n“I only talk with professors when their email is down.  I wonder what they’re working on.  Maybe my team could help.” UVa has developed its own Bamboo community in preparation for Phase II, beginning next week in San Francisco.  Watch this space to see what happens."},{"id":"2008-10-09-biblical-statistics","title":"Biblical Statistics","author":"matt-munson","date":"2008-10-09 07:39:38 -0400","categories":["Digital Humanities","Visualization and Data Mining"],"url":"biblical-statistics","layout":"post","content":"The first topic that I chose for my dissertation in UVA’s Department of Religious Studies was the “School of Saint Paul.”  I hoped to show the existence of a group of followers who surrounded Paul and engaged with him in the interpretation of the Old Testament.  In order to do this, I decided to investigate how Paul used scripture in his epistles and how the followers of Paul used the same scripture in their writings.  I anticipated finding certain portions of the Old Testament that either were used exclusively in the Pauline and post-Pauline literature or were used differently in the Pauline and post-Pauline literature than in the rest of the New Testament. But I had a problem.   The fund of Pauline and post-Pauline quotations and allusions to the Old Testament numbered more than 1000 cases.  How could I represent such a large set of data in a way that made them easily comprehensible?  A friend of mine suggested that I needed to represent the data graphically.  And a colleague here at the Scholars’ Lab, where I work as a graduate consultant, advised SPSS as the best way to accomplish such graphical representation. I already had a table that I had made in Microsoft Word of every usage of the Old Testament in both the Pauline and post-Pauline literature.  I needed to get this data into SPSS with as little headache as possible.  So, I converted the data into an Excel file, saved this in a format that SPSS could read, and then imported it into SPSS.  At that point, I had accomplished the hard part.  All that was left to do was to analyze and graphically represent this data.  And here is one example of what I produced: Unfortunately, this analysis of the data made it clear that the evidence was insufficient for my dissertation!  I found no significant chunks of the Old Testament that were used exclusively in the Pauline and post-Pauline literature.  And I discovered that trying to set the Pauline and post-Pauline use of scripture against that of the rest of the New Testament was speculative, at best.  I ended up having to change my dissertation topic.  But it was this statistical analysis and work in information visualization that made it clear to me that the evidence was insufficient.  Without it, it is possible that I would still be chasing the wild goose that was my previous topic."},{"id":"2008-10-21-art-in-the-slab","title":"Art in the SLab","author":"Admin","date":"2008-10-21 08:00:04 -0400","categories":["Announcements"],"url":"art-in-the-slab","layout":"post","content":"A bright, sunny, open space like the Scholars’ Lab begs to be filled  not only with students and faculty collaborating on digital projects, but also with art!  We’re pleased to follow last semester’s successful showing of the watercolors of E. F. Chilton with this semester’s photography exhibit by our own Jean Bauer . Jean is a Ph.D. candidate in the History department at UVA and a 2008-2009 Fellow in Digital Humanities at the Scholars’ Lab.  Her exhibit, entitled “Animal, Vegetable, Mineral: Slices of the (Mostly) Natural World”, is on display in the Lab right now."},{"id":"2008-11-04-google-scholar-neglected-corridors-of-the-interwebs","title":"Google Scholar:  Neglected Corridors of the Interwebs","author":"jason-kirby","date":"2008-11-04 07:20:12 -0500","categories":null,"url":"google-scholar-neglected-corridors-of-the-interwebs","layout":"post","content":"Welcome to my first post here on the Scholars’ Lab blog.  My name is Jason Kirby and I’m a third-year Ph.D. student in the Music department at UVa.  I’m in the “Critical and Comparative Studies” track of my program, which means I look at musical sound and musicians through a cultural studies lens.  I’m planning a dissertation on intersections between country and rock music over the past thirty years, and when considering the wide spectrum of academic musicology, I’m squarely a pop music studies guy.  I’ve written about artists ranging from Lucinda Williams to Throbbing Gristle—artists about whom there’s a fair amount of popular-press ink spilled, but not necessarily much scholarly writing (yet).  This brings me to the subject of today’s post. Google Scholar:  I enjoy it, and not for reasons which are necessarily immediately apparent.  As anyone who’s used it with the serious intention of finding real scholarship knows, as a search engine it can be a bit scattershot.  Google Scholar “Bob Dylan,” for instance, and in the top results one gets the mistaken impression that his hit song “Like A Rolling Stone” is a book.  But on the other hand, click through a few more pages of results and you’ll find a an excellent recent article, written by Albin Zak and published in the Journal of the American Musicological Society, which discusses musical, intertextual dialogue between Dylan and Jimi Hendrix.  Zak writes about song structure, and even works in Mikhail Bakhtin—what more could a pop musicologist want?  Plus, if you’re on Grounds, click the link and you’ve got the PDF saved on your desktop; all in less time than it would take to find the same article in AMS’s database (and certainly less time than it takes to page through a hard copy in the stacks). If the transition of scholarship online is like the Internet version of a large research library, then Google Scholar for me resembles something like the shelves of a used book store that’s cluttered and random, but stuffed with useful finds (sort of like the Strand in New York City).  Or perhaps a better analogy would be a used music shop where they have the CD cases on display, but the discs themselves are kept behind the counter.  In other words, especially if you’re not associated with a large research institution, you may well not be able to access the PDF file Scholar points you toward—not without subscribing to an online journal or purchasing the article.  This problem underscores a larger issue; as the Internet expands and ostensibly increases public access to knowledge, this free access to information isn’t always “free”.  There’s always a market interest in there somewhere, and Google Scholar reveals this more obviously than other digital research tools. Speaking of freedom and openness, one of the recurrent critiques of Google Scholar is that it does not allow users to see how search results are determined.  Though Google recently updated its Scholar search algorithm, as information standards bloggers have noted, we still know frustratingly little about the algorithm itself.  To be fair, other search engines, scholarly and not, have the same problem—it just becomes more high-profile in Google’s case. At this juncture, it’s unclear how the recent Google Books settlement (see Dan Cohen’s analysis ) or changes in PDF access through Google will impact the issue. Regardless, whatever tweaks Google has made to its search algorithm, it clearly still needs some work!  As librarians such as Péter Jascó have noted, the number of “hits” one gets through Scholar are often ridiculously overinflated (witness 11,100 for “Bob Dylan”), sorting options for those results are poor, and the search engine often has difficulty distinguishing an author’s name from the rest of an article’s text. [1] Given disappointments such as these, it’s not surprising that some librarians are concerned about the changes Google Scholar may bring to the world of academic inquiry.  The current generation of undergraduates is the first to grow up with the Internet a standard part of everyday life, and I think the anxiety some folks feel is that in a world of online-everything, what if speed and ease of use will eventually trump depth and breadth of inquiry?  Certainly, teachers and librarians can and will continue to orient students to the fact that there’s a whole world of research resources out there beyond a one-stop web search.  But it surely doesn’t help matters when Google publishes web ads appealing to the uber-procrastinator:  “Need six authoritative, relevant sources?  Before sunrise? Google Scholar .” So, seeing as I’m a teaching assistant here at the University, would I encourage my students to Google Scholar their entire way through their next paper?  Given what I’ve covered here, no. However, as you may have noticed, this a blog post about why I like Scholar.  So what keeps drawing me back?  I think it all returns to the fact that it’s an outstanding starting place for research inquiries.  What’s more, I find this particularly true for popular music studies inquiries.  Take, for instance, a paper I’m working on this semester about the “Bakersfield Sound” and genre boundaries in 1960s country music.  A preliminary search in RILM of the term “Bakersfield Sound” only got me a 2006 obituary of Buck Owens.  Similarly, a JSTOR search of the same term pulled only five results, a 1970s article on “urban cowboys” the only somewhat relevant one.  But when I Google Scholar’d “Bakersfield Sound,” mixed among some random debris I found an excellent 2005 article on identity and place in California country music by GH Lewis, an article whose bibliography in turn led me to other great sources on the topic. My point here?  Google Scholar, just like other search methods, seems to be better suited for some disciplines than others.  Popular music studies, my field, is noted for its hybridity, its tendency to skillfully poach from the methodologies of other disciplines while simultaneously rejecting some of their strictures.  It’s also known as a relatively recent field, with its basic antecedents in 1960s popular-press rock criticism. It’s still an up-and-comer as fields go, still working toward full scholarly respectability.  As such, is it really that surprising that pop music research queries (like those I mentioned above) might fall through the cracks of more traditional scholarly search engines?  From the vantage point of my particular niche, Google Scholar’s biggest advantage is the “wide net” it casts, giving less established fields a better shot.  More traditional discipline-bound search engines might learn something from this, or risk irrelevancy. Jascó, Péter. “Savvy Searching: Google Scholar Revisited.” Online Information Review Vol. 32, No. 1 (2008), pp. 102-114."},{"id":"2008-11-06-iterative-cosmologies","title":"Iterative Cosmologies...","author":"abby-holeman","date":"2008-11-06 09:52:07 -0500","categories":["Digital Humanities","Geospatial and Temporal"],"url":"iterative-cosmologies","layout":"post","content":"“During the Zuni Molawia ceremonial of 1915, when the house-tops were crowded, the roof of one of the houses enlarged that season caved in. The accident occurred, people began to say, because turquoise had not been deposited under the floor of the new chamber.” Elsie Clews Parsons Pueblo Indian Religion Vol. 1, 1939, p.105 The quote above, read some time ago, was one of the first things I read that spoke to the deeper meaning of many of the “ritual deposits” found by archaeologists. Specifically, how these deposits were connected to built space. I have since encountered innumerable studies from Anthropology, Archaeology, Architecture, Religious Studies, etc., that show how built space and the associated material are microcosms of a larger worldview. These studies demonstrate how space becomes place within a certain cultural logic. For example, in Mesoamerica among the ancient Maya, the quadripartite division of the world organized the gods themselves, ritual calendar (indivisible from the agricultural calendar), the layout of cities, the organization of hierarchy (as seen in four founding lineages noted in the Chilam Balam), the agricultural fields themselves (squares fields with each corner having an altar), the everyday house, the altar within the house, and even individual caches placed in the ground. This is what I like to call an iterative cosmology. This kind of layering can be seen across the globe. The interesting aspect of this link between worldview and space comes at the local level.  It demands interpretation that takes local cultural logic seriously. For example, the above quote suggests that the building fell because of improper offerings, not necessarily only from a lack of structural integrity, or put another way, improper offerings led to a lack of structural integrity and thus the building fell. What on earth does any of this have to do with digital technologies in the humanities? Well, for me the answer came when I tried to ask similar questions about cosmology in a society without surviving myths or writing. All of the studies I have seen come either from areas that do have existing origin myths and writing, or from ethnographic accounts in which the people themselves can explain. The area of northern Mexico (far outside of what is considered Mesoamerica) in which I am interested does not have either extant myths or writing. Yet given the iterative or multi-level, and spatial aspects of many (if not all) Native American belief systems, I believe these systems will leave material patterns behind. That is where GIS comes in. I turned to GIS as I floundered to come up with a methodology that would allow me to identify spatial patterns. I had an archaeological site in mind and access to enough data about this site that would allow me to answer my questions, but I had little idea about how to get at the spatial aspect of the patterns I had hoped to investigate. GIS technology allows me to take the data I have digitized and not only display it graphically, but conduct a spatial analysis in order to identify spatial structure. Hopefully that spatial structure will help me understand the over-arching cosmological principles that were active in northern Mexico during the 12th and 13th centuries. Thus, I am in the process of georeferencing (recently finished) original excavation maps, and digitizing all of the structures within this site (it’s an intra-site analysis), I will then plot artifact frequencies across the site along with certain cosmologically important architectural features (hearths and posts). All of this data comes from eight published volumes as well as unpublished field notes. None of it was in digital form before I started. From this I can pull actual coordinates of these things and run spatial statistics to ask a series of questions: do certain kind of artifacts group in certain areas of the site? Do the artifact assemblages in rooms that have offerings under the posts look different than assemblages in rooms without these kinds of offerings? If so how are the assemblages different? Where are the elaborated hearths in relation to posts with offerings? Are there discrete boundaries between areas with these elaborated hearths? Do the elaborated hearths co-occur with certain kinds of artifacts? And so on. So here I am currently wrangling with large amounts of data to be sure it is internally consistent and get it into ArcMap as the first step. In the back of my mind is how I will disseminate this information. I have heard various ideas, but am currently not sure about some of the best ways to disseminate GIS data….but more on that later!"},{"id":"2008-11-13-place-space-maps-and-more-on-gis-day","title":"Place, Space, Maps, and More on GIS Day","author":"Admin","date":"2008-11-13 07:09:14 -0500","categories":["Announcements","Geospatial and Temporal"],"url":"place-space-maps-and-more-on-gis-day","layout":"post","content":"Join us next Wednesday, November 19th, as we celebrate all things International GIS Day.  Anyone whose work is grounded in issues of space and place will find something of interest in these cross-disciplinary offerings, centering in cartography and geospatial technologies. Of special note is a public lecture by David Rumsey, who has worked for a decade to offer open access to his remarkable private map collection through a variety of innovative tools and interfaces. Most recently, he has made historical maps available as layers in Google Earth and on an island in Second Life.  Mr. Rumsey will speak on “Giving Maps a Second Life with Digital Technologies” at 4 o’clock in the Harrison-Small auditorium.  This event is co-sponsored by the Center for Emerging Research, Scholarship, and Arts at UVA (CERSA) and the Scholars’ Lab, and a reception will follow the talk. Schedule of Events: Charlottesville Area GIS Users Lunch\nwith a talk by Dr. John Scrivani, Virginia Dept. of Forestry\n12:00 - 1:30 in the Scholars’ Lab GIS Day Cake Cutting\n1:30 in the Scholars’ Lab GIS Day Open House\nwith 10 different GIS user-groups and projects exhibiting!\n1:30 - 2:45 in the Scholars’ Lab Tour of “On the Map: The Seymour I. Schwartz Collection of North American Maps 1500-1800”\n3:00 - 3:30 in the Small Special Collections Library Speaker: David Rumsey\n“Giving Maps a Second Life with Digital Technologies”\n4:00 - 5:30 in the Harrison-Small Auditorium Reception\n5:30 - 6:30 in the Small Special Collections Library Image from the David Rumsey Map Collection"},{"id":"2008-11-19-map-vocabularies","title":"Map \"Vocabularies\"","author":"wendy-robertson","date":"2008-11-19 06:09:46 -0500","categories":["Geospatial and Temporal","Visualization and Data Mining"],"url":"map-vocabularies","layout":"post","content":"For the past year, I have been working on the Scholars’ Lab Geospatial Data Portal, the lab’s effort to make our GIS data sets readily available to UVA students, faculty, and staff via the world wide web by using a suite of open source, open standards-based applications. A particular aspect of this project that I have enjoyed exploring is the way in which we display our visual information. Stop to think about the last paper map you used. Minor roads were probably displayed with a line of a certain color and thickness, highways with another. Green spaces were colored differently from open water and buildings etcetera. Cartographers have long toiled to develop visual representations of our environment and make them identifiable for the greater use. People naturally associate certain colors on a map with identifiable features in their environment (eg: the association of green on a map to forests, parks, and open areas). Much like a book, these symbols and representations must create a language which is understandable to the audience; else the information contained on the map will go unutilized. What I have done for the Geospatial Data Portal is to expand our symbolic vocabulary. I create styles; XML based documents which allow us to display visual information through symbols that our patrons will understand and identify with specific attributes. An example: I can map the waterlines for a given city with a solid pink line with a width of 2 pixels. While it is true that the information is mapped and is useful to an extent, I think there is a way to display the same information while making it more visually recognizable as city waterlines and ultimately making the information more useable to our patrons. Instead of a solid pink line of a single width, we can display the information as blue lines with differing widths dependant upon the size of the pipe (ex: a main line feeder pipe with a diameter of 15ft is represented as a blue line with a pixel width of 8, whereas a small pipeline with a diameter of 2ft is represented with a blue line with a 1 pixel width. So what has this accomplished? People tend to associate size on a map with importance in the real world, so by exaggerating the size difference of the pipe by weighting pixel width we can draw our users’ attention to the important locations on the map. And by using blue, we identify our information of interest as a water feature because most people associate blue on a map with water features in their environment. Now our patrons are able to go from displaying simple lines on a page to creating a map which displays intuitively symbolized information using only their internet browser. I believe this project has the potential to greatly expand the user-base for our GIS data sets and allow for new forms of scholarship because it makes the process of displaying information in an identifiable and comprehensible much more user friendly."},{"id":"2008-11-21-issues-with-translations-walt-whitman-and-jorge-luis-borges","title":"Issues with translations - Walt Whitman and Jorge Luis Borges","author":"gillian-price","date":"2008-11-21 10:38:42 -0500","categories":["Digital Humanities"],"url":"issues-with-translations-walt-whitman-and-jorge-luis-borges","layout":"post","content":"A couple of summers ago, I was desperate for a job so I caved. This was not, in fact, the first time. I remember typing Leaves of Grass for a whole nickel a page before I knew how to type properly, painstakingly pecking out what was then incomprehensible text. During my summer and winter break of 2006, then a proficient typist who had learned a bit of Spanish, I got a pay increase and went to work for my dad, Ken Price, encoding Álvaro Armando Vasseur’s 1912 translation of Whitman’s poetry for the Walt Whitman Archive. While working on the project, I did not reflect much upon the problems of translations, both within themselves and the issues and concerns in their representation on archives like the Whitman Archive. Frankly, I was too preoccupied about my quickly-approaching study abroad trip to Madrid. My mind was also engaged with memorizing xml code so I wouldn’t have to look up everything, and that was sufficiently taxing to keep me from having much time to ponderthe implications of the project I was working on. Ever since I developed an interest in Spanish, translation has been very interesting to me. As many suggest, including one of my personal favorites, Jacques Derrida, translation is not possible, but it is inevitable. That is, one cannot ever hope to achieve a perfect translation of any given text, but people still try to do it anyway. This issue is, coincidentally something we were considering the other week in my literary theory class and one example examined was the difference between the phrase “a dog’s life” in English and the Spanish approximation: “la vida de perro.” The phrase in English, while not entirely positive, also can’t be completely separated from our thoughts of the animal we pamper and keep as a pet, who often has a higher standard of living than many humans and doesn’t have to worry about how to make ends meet. “La vida de perro” in the Spanish-speaking world, on the other hand, is just the opposite and certainly is not a life you want to have. In Spain the word “perro” carries with it especially negative connotations because of the influence of Islamic culture in which dogs are seen as dirty creatures, so to call someone a dog, or to imply that she or he is one could be quite an insult. In his article “Transgenic Deformation: Literary Translation and the Digital Archive” which prefaces Vasseur’s translation on the archive, editor Matt Cohen grapples with several other concerns beyond the nitty gritty of translations including the big question of which texts are translated and why. Cohen cites Lawrence Venuti who asserts that “asymmetries, inequities, relations of domination and dependence exist in every act of translating.” The reasons we see certain texts translated and not others has everything to do with power relations that dictates which texts are deemed important enough to be translated, (and then represented on archives). Furthermore, the very process by which texts are represented on archives is, in a sense, a translation itself. Not only must one encode the text into XML, but the archivists must make creative decisions that translators make as well, such as how to display the text – whether to remain true to the author’s layout or to explore other options. Issues of translation like this that come up in working on the Whitman Archive are issues that I suspect will also arise in my collaboration with Jared Lowenstein on his Orbis Quartis project, which is an archive of some early and rare works by Jorge Luis Borges. I think the way Borges’ language and his constant “language slippage” will be a particularly interesting idea to consider when looking at translations of the often multiple versions of his texts."},{"id":"2009-01-05-digital-credibility-in-field-research","title":"“Digital Credibility” in Field Research","author":"wendy-hsu","date":"2009-01-05 09:37:05 -0500","categories":["Digital Humanities"],"url":"digital-credibility-in-field-research","layout":"post","content":"I’m an ethnographer/blogger. My dissertation research investigates the social and musical lives of American rock musicians of Asian descent. On the one hand, I follow the conventional methods of participant observation as I travel to ‘field sites’ such as nightclubs, bars, and coffee shops to witness live performances and hang out with musicians. On the other hand, I participate in the indie music scene by blogging (on yellowbuzz.org ) about my field research experiences. My online participation, however disembodied and virtual, is significant due to the centrality of user-produced or independent media in the indie rock music scenes. For the most part, these research methods take on two distinct lives. Sometimes they intersect and yield interesting results. +_=-__=-=-=- Ethnographic work on performing arts can sometimes be logistically challenging in our intensely mediated worlds. Typically I carry a number of recording devices including a digital SLR camera, a mini-DV recorder, a handheld digital audio recorder, a laptop computer, and a notebook. This list can be extended or shortened depending on the nature of activities (interviews vs. live performances). Sometimes it is contingent upon whether I expect to make music during my visits. Early this fall, I took a series of field research trips to New York City. On one of these trips, I doubled (well, actually tripled) my identity: field researcher, musician, and scholar. I was invited to perform and speak with students at Wheaton College in Norton, Massachusetts. I took the chance to double-dip this visit by scheduling some interviews and making plans to attend shows in New York. So I had a four-bag system: a backpack (my laptop, notebook, show flyers, The Village Voice, other paper products), a carry-on suitcase (audio-visual recording devices and clothes), an electric guitar case, and a guitar pedalboard (assorted guitar effect pedals). After the mini-residency at Wheaton College, I took the Amtrak to New York City. Long story short, my case of guitar effect pedals (worth $1500!) got stolen on the train a few stops north of New York Penn Station. I frantically filed a report with the Amtrak Police. No recovery prevailed. Bummed out as I was, I dragged myself to a midtown bar for an interview with Johnnie Wang of the band A Black China . After I told Johnnie about my misfortunes, he offered to buy me a beer. That was the beginning of our friendship. We bonded over being musicians first, then being Americans of Taiwanese/Asian heritage. My meeting with Johnnie invigorated me and reminded me of the purpose of my dissertation research. I went to a show the following night in New Jersey and had an interview meeting with Joe Kim of Kite Operations right before my flight back to Charlottesville, with one bag short. +_=-__=-=-=- It took me a while to figure out the educational values and perhaps the theoretical fruitfulness of this experience. This experience can be seen in light of a few issues: methodological approaches to technology, empathy (and relationship) with informants, and researcher’s ‘field identity.’  So, does technology enhance or hinder field research? Frankly, I didn’t end up using most of my recording devices on this trip. During interviews and other exchanges, my informants and I chatted away while I took mental notes. My field-note-taking took place only after the meetings ended. But oddly, (the loss of) technology brought me closer to my informants. The story of losing my guitar gear generated a sense of empathy from my informants. I share with them an intimate engagement with music-making technology. They too often travel with gear for both music-making and recording purposes and some have encountered experiences, personally or vicariously, with gear problems. In many ways, it’s not strange at all that I carry so much gear with me. The physical and social attachment to technology is a central part of being and moving around in this media-blasted world. In this case, technological gear adorns me as a tech-media savvy researcher and blogger. This kind of ‘digital credibility’ has helped me earn not only access to, but also empathy and respect from my field informants. Excess technological devices can weigh down users. But this is not only an academic concern specific to field research methods, as it is a more pervasive issue in the digital age. My responsibility is to figure out the best logistical and theoretical approaches to both online and offline interactions in my field research. I’m still working on it."},{"id":"2009-01-05-teaching-with-artstor","title":"Teaching with ARTStor","author":"fitz-green","date":"2009-01-05 09:36:48 -0500","categories":null,"url":"teaching-with-artstor","layout":"post","content":"I am a teaching assistant for a course on the early history of Christianity. When the professor for the course asked me to lecture for him on early church art and architecture, I was excited. I had recently come upon the new ARTStor online database, and couldn’t wait to find digital images of the churches I wanted to cover in my lecture. But then he said, “I’ll go over to the slide library with you sometime next week and introduce you to the folks there, and they’ll help you pull slides.” Now I had a conflict: Do I do it the old-fashioned way, my professor’s way? Or do I take advantage of what the latest technology has to offer? I was truly conflicted over this, so rather than making a decision at once I decided to prepare using both, and then decide. More work, yes, but this way I’d get to try both out and see the advantages of both (though, in reality, this was just my way of delaying making a decision). So, I started gathering slides of churches on ARTStor. Santa Sabina and Santa Maria Maggiore in Rome. San Vitale in Ravenna. The great thing about ARTStor is that you can save your groups of images indefinitely. So, if I were to go back and give this lecture again next semester, everything would already be prepared. Also, there are more images to draw from on ARTStor. There are groups of images that a university can purchase rights to, much like a journal subscription, so you have lots more options this way. No more having to use the old red-tinted picture that some art professor took with his point-and-click 35mm while on vacation 30 years ago (I saw my share of these as a student- I could usually tell how old the slide was because the professor’s car always ended up in one of the pictures). In many cases, the ARTStor images were of better quality as a result too. But here was the winning point in favor of ARTStor for me. You can save images for a lecture to a folder accessible by the students, so that after the lecture, or while studying for exams, students can go back and review the same images. This is a major bonus. I recall taking art history classes in which the only time you saw a given picture was during the lecture, and then you had to be able to identify it again if it was displayed during the exam. The students who took notes feverishly on every detail of a picture but didn’t look up at the images to study them always did poorly on exams, because when a picture was displayed again during the test they had no idea what it was! This ability to recall the images is not just an advantage for exam taking, but for learning in general. After I gave this lecture, I had one student approach me after class interested in a mysterious unidentifiable woman that appears in the arch mosaics of Santa Maria Maggiore. She thought she had seen a similar depiction in another class, and wondered if the two were related. With ARTStor, I could point her back to the exact image so that she could do more research into the possible connection. More interaction with the material for the students means more of a chance to get them excited about what they are learning, and this is reason enough for me to use ARTStor. All of that said, what did I end up using for the lecture? I decided that while I was working for this professor, I wanted to do things his way. I’ll have plenty of opportunity in the future to do things my own way. And, to be honest, the process of going to the slide library, pulling the physical slides, viewing them on light tables with a magnifying glass, dropping them in the carousel with the red dots pointing the right direction, this was all fun. There is something about the physicality of using the slides, like the physicality of a book, that I can understand not wanting to give up. But, for me, the advantages of ARTStor are so numerous that I can’t see not taking advantage of it."},{"id":"2009-01-16-social-media-and-the-inauguration","title":"Social Media and the Inauguration","author":"bess-sadler","date":"2009-01-16 08:46:21 -0500","categories":["Announcements","Digital Humanities","Geospatial and Temporal","Visualization and Data Mining"],"url":"social-media-and-the-inauguration","layout":"post","content":"Join us in the Scholars’ Lab Monday morning through Wednesday night next week, as we project the social media landscape surrounding next week’s historic presidential inauguration. We’ll be showing real-time Twitter and Flickr feeds that record people’s responses to the event and their efforts at citizen-journalism. We’ve also created a home-grown geospatial visualization so that you can follow the worldwide conversation! Visit the Lab for a little social interaction of your own, or access the site (which includes more information and related links) online."},{"id":"2009-02-04-on-asian-american-digital-identity-politics","title":"On “Asian American” Digital Identity Politics","author":"wendy-hsu","date":"2009-02-04 10:04:12 -0500","categories":["Digital Humanities"],"url":"on-asian-american-digital-identity-politics","layout":"post","content":"Everyday, I receive Google Alerts about any websites, blogs, or news feeds containing the keywords “Asian / American / music” in whatever order and combination that Google search engine finds. Most of the Alerts, unsurprisingly, point to stories related to U.S. politics. Interestingly, around the time of the 2008 Presidential Election, my InBox experienced a minor Google Alert “explosion” with news stories and criticisms listing all the color-based social groups, connecting Obama’s racial politics to the now dominant American ideology of multiculturalism. To my disappointment, none of these news stories included anything substantial information with regards to the Asian American (if there is such a thing) perspective on the Obama and Biden duo. Is “Asian American” coming to stand in for a keyword, tag (in the speak of blogosphere), or a hip buzzword in our current media environment as digitally informed and constructed? Is there “real content” beyond the textual reference of “Asian” and “American”? If so, how do we assess this content considering the methods of information retrieval, i.e. Google Alerts, and the context of presentation, i.e. hypertextual state of Internet media? Today, my Google Alerts linked me to a couple of exciting pages of content-worthy materials related to Asian American arts and culture. One of these is a New Yorker article titled “By the Skin of Our Teeth” about “The Shipment”, the new play by Young Jean Lee. The reviewer Hilton Als comments on the Lee’s “irreverent take on racial politics.” Commenting on her 2005 play “Songs of the Dragons Flying to Heaven”, featuring the self-violence of an Asian American female character, Lee declares her attitude toward the state of identity politics in the U.S: “For this project, I decided the worst thing I could possibly do was to make an Asian-American identity-politics show, because it can be a very formulaic, very clichéd genre, and very assimilated into white American culture. It’s almost become part of the dominant white power structure to have identity-politics plays about how screwed-over minorities are. It’s such a familiar, soothing pattern. . . . It’s become the status quo.” When I read the passage, I thought to myself, “now, here’s a kernel of wisdom” worth pursuing. What does she mean by “identity-politics show”? What consists of this ‘cliché genre’ of formulaic and assimilationist plays? A good content analyst would seek information about the playwright and this play. Before I jumped into my usual mode of performing a search on Google or Wikipedia search on Young Jean Lee, I slowed down and pondered about the path of information that allowed me to arrive at this intellectually compressed bit of information. The New Yorker tags this article with the following keywords: “The Shipment”; Young Jean Lee; Korean-Americans; Douglas Scott Streater; Race Relations; Asian-Americans; “Pullman, WA.” Google search engines must have picked up this article because of the tag “Asian-Americans.” But search engines are not able to make a qualitative distinction between this article [or other substantive articles] from the sources that simply use “Asian American” as a stand-in for cultural multiplicity and diversity. Unfortunately, Asian America still exists, in the digital environment, mostly under a pile of diversity-bound laundry lists at best, or pornography and ads for mail-order brides or other forms of race-related sex industry, at worst. The risk of being pigeonholed, tokenized, or even sexualized is no news to individuals of Asian descent in the United States. Playwright Young Jean Lee asserts provocative and vehement critiques for the discursive objectification of Asianness in her 2005 play which opens with a monologue by a woman with the name of “Korean-American”: “Have you ever noticed how most Asian-Americans are slightly brain-damaged from having grown up with Asian parents? It’s like being raised by monkeys—these retarded monkeys who can barely speak English and are too evil to understand anything besides conformity and status. . . . Asian people from Asia are even more brain-damaged, but in a different way, because they are the original monkey. . . . I am so mad about all of the racist things against me in this country, which is America. Like the fact that the reason why so many white men date Asian women is that they can get better-looking Asian women than they can get white women because we . . . have lower self-esteem. It’s like going with an inferior brand so that you can afford more luxury features.” This is intellectually dense, emotionally heavy stuff. But the fact that it’s available in a point-and-click fashion is astounding. Google Alerts prevent information from fossilization. Without Google Alerts, I would find this article somewhere down the line when I do archival search, plowing through databases for historical artifacts. The newness and immediacy of this information would be lost. Also, it would take many more steps to link this article to other articles related to the subject of “Asian / American / music” published today. The other noteworthy piece Google Alerts linked me to is an interview of jazz pianist Vijay Iyer by RVAjazz blog entitled “Intellect Meets Creativity .” Iyer speaks reflexively about his role as an Indian American musician in the Afro-centric tradition of jazz music: “I’m just fortunate to be able to interact with the music from my perspective, and to reconsider what resonances there might be with my own experience, or with anyone’s. The point is to honor that legacy and not commodify it, but also to learn from it. I think that America was invited to reconsider a lot of this in light of the ascent and success of Obama. Those are symptoms of a larger development in our culture - it’s about who we are and where we are and what time it is!” The juxtaposition between the New Yorker article on Young Jean Lee’s play and Vijay Iyer’s interview is intellectually curious. Iyer’s perspective on race in America is less dystopic than Lee’s. In fact, his alliance with African American culture and struggle speaks to a larger discourse about race in terms of minoritarian politics, quite contrary to the uncritical multiculturalist orientation. Iyer’s interview could tap into the historical and contemporary moments of Afro-Asian connections formed in anti-racist solidarity. My research aims to track these moments deliberately and shamelessly, making links and disconnects among them as they occur in real time. Information as such, categorized and recategorized based on similar or dissimilar terms, is generated and circulated at high volume daily on the Internet. Digital technologies allow discourse to flow in disparate, rhizomatic directions. The hypertextual state of Internet media is overwhelming to sort through, but this quality allows information to seep into unexpected cracks and generate surprising juxtapositions. Similar to keywords and tags, identity categories, also reproduce themselves in a semi-irrational, hypertextual fashion in our time. These contradictory patterns as discovered in the digital environment may best represent the schizophrenic style of identity proliferation that would mark our post-identity-politics (or post-Race) age."},{"id":"2009-02-04-peer-review-for-visual-aids","title":"Peer Review for Visual Aids?","author":"wendy-robertson","date":"2009-02-04 10:01:20 -0500","categories":["Visualization and Data Mining"],"url":"peer-review-for-visual-aids","layout":"post","content":"How frustrating is this: You sit down to take in some form of scholarly work (be it a book, an article, or a talk) and you find yourself increasingly confused with a bombardment of information from graphs and figures and maps which don’t make sense because they either have too much or too little information contained within them or the information is poorly labeled (if at all).  Or even worse, you are the person writing the book/article or giving the talk and instead of fielding questions on your scholarly processes, you are repeatedly explaining to the audience what your visual aids actually represent. A picture may be worth a thousand words, but if it is not a language your audience speaks, where have your efforts gotten you?\nTypically, when I read a scholarly article, my first read-through goes as follows: I read the abstract, I look at each one of the figures/maps/tables/graphs and their annotations, and I read the conclusion.  Its not until the second read-through that I examine the bulk of the text.  I think that words sometimes have the unfortunate tendency to obfuscate the true findings of research and, truth be told, I like to find out if I draw the same conclusions from the provided data as the author(s) do.  My process stumbles when I encounter articles with figures/graphs/maps etc. which have either a glut or a dearth of information contained in them, making non-intuitive to the uninitiated reader.  Some highlights:  A map of a state containing rivers, waterbodies, and watershed boundaries (the focus of this particular article) AND all of the major roads and highways (NOT the focus of the article).  All in gray-scale.  Add in the point locations and names of the state’s twelve most populous cities and cram it into a box three inches tall by five inches wide.  The focus of the article was on modeling and delineating the major and minor watersheds of the area in order to develop a best management practice for cooperating water districts.  Needless to say, that point was lost in the shuffle.  Another example which is all too common: a graph depicting change over time of 10 or more constituents using various dotted, dashed, and solid lines of variable thickness.  With that amount of information crammed into a single visual aid, the results are simply lost in the shuffle. We have writing clinics and public speaking critique sessions, why don’t we have a peer evaluation system for visual aids?  I think that many people (myself included) fall into a habit of having our material critiqued solely by our close working group.  While this is certainly a necessary step in the writing process–the people most familiar with our work are the ones most likely to pick up on the esoteric flaws–many scholars neglect to obtain peer review from individuals tangential to or completely outside of their small fields.  I would say that one of our main objectives as scholars is to use our work to excite interest from members of the scholarly community inside and outside of our focused area.   In my opinion, an important step towards this goal is to make our visual aids more accessible to the curious non-expert. I would like to see our scholarly community develop this type of peer-review network where we can utilize the human resources around us to improve our intellectual contribution to all of our respective fields.  We could have minds from a variety of fields of study working collaboratively to improve the accessibility (and therefore the use) of our collective body of knowledge.   I think the concept has amazing potential."},{"id":"2009-02-11-research-applications-for-3d-models-in-art-history","title":"Research Applications for 3D Models in Art History","author":"ethan-gruber","date":"2009-02-11 12:51:23 -0500","categories":["Digital Humanities","Visualization and Data Mining"],"url":"research-applications-for-3d-models-in-art-history","layout":"post","content":"These days, it is difficult to find a television documentary detailing an archaeological site that does not feature a representation in the form of a 3D model.  Computer models make good teaching tools.  A class of students may not have the opportunity to travel to Rome to view the Colosseum first-hand, and even if they did, they would have great difficulty visualizing what the mostly-ruined structure looked like 1,900 years ago.  A model based on the most recent archaeological research, however, can help fill in the gaps left by time and the elements. One of the more important aspects of a computer model is that it is dynamic.  Using software, a model can be adjusted to reflect newer theories of the site’s architectural reconstruction.  This is certainly a stark contrast to artists’ sketches and paintings, which, over time, tend to become outdated.  Importantly, like other visualization methods used in the humanities (such as GIS), 3D models can help scholars get a fuller picture of a site and formulate research questions that never would have been considered otherwise.  This is the case in my most recent research. Having never truly given up on the video game design aspirations of my high school days (I specifically remember my father turning the breaker off to the upstairs when I was up until 4 AM designing a Quake map), I have found a niche within my field of academic interest—Roman archaeology and architectural history.  While many of my Pompeianist classmates take a more traditional approach to graduate research projects, I chose to develop a 3D model of the House of the Faun, one of the largest and most famous houses in the city.  The model was constructed as accurately as possible based on the archaeological plan, a number of artists’ reconstructions, and photographs of the house (many gathered from Flickr ). The intent of the model was to test art historians’ philosophical assertions about Roman atrium houses.  With accurate lighting simulation (i. e., calibrating a simulated sunlight to the latitude and longitude of the house and to any point in time back to antiquity), high resolution images of the model rendered by Mentalray software gave me a glimpse of what the House of the Faun looked like at noon on January 1st, 100 B.C., which is something no artist can replicate. Coincidentally, lighting simulation may have an impact on how we consider the artwork within the house.  For example, when many art historians point to the colors of a mosaic as being proof of its Greek influence, can that assertion bear the burden of the fact that the mosaic was rarely in sunlight? Many of us have seen Roman floor mosaics hanging on the walls of American and European museums, but they have been removed from their original context.  Even in Pompeii, one of the best-preserved sites of the ancient world, the roofs collapsed long ago, making it difficult to visualize the natural lighting scenario within the House of the Faun and other structures within the city.  3D models allow us to put artworks back in their original context and consider how the ancients viewed them, which is quite different from how we view them now.  In this case, the computer model is more than just a teaching tool; it is a scholarly research tool."},{"id":"2009-02-22-coins-site","title":"Library Innovation Grant Yields Dividends for Numismatists","author":"Admin","date":"2009-02-22 12:25:46 -0500","categories":["Announcements","Digital Humanities","Research and Development"],"url":"coins-site","layout":"post","content":"A recent post by Ethan Gruber, a UVA Library staff member who has lately joined the Scholars’ Lab team, detailed his experiments with 3-dimensional modeling to re-contextualize Roman mosaics – right down to the interplay of light and shadow in ancient villas.  Now Ethan’s work on creating a scholarly interface for the study of Greek and Roman coins has been profiled in UVA Today .  This project came about through an internal UVA Library Innovation Grant and was undertaken in consultation with Art History professor John Dobbins, a 1994 IATH Fellow, whose Pompeii Forum project provided an early example for the utility of digital tools for archaeological inquiry.  The rare coins were scanned by Andrew Curley of the Library’s Scholarly Resources Digitization Services. Photo credit: Dan Addison.  Read the full UVA Today press release, or jump straight to the coins collection ."},{"id":"2009-03-11-50","title":"Mapping Regional Language Use","author":"wendy-robertson","date":"2009-03-11 06:15:36 -0400","categories":["Geospatial and Temporal","Visualization and Data Mining"],"url":"50","layout":"post","content":"So for the thousandth (or so it seems) time I’ve gotten into this discussion with my friends from the East Coast and Midwest (I’m from Texas) about the correct way to refer to a sweet carbonated beverage, and I have finally got to thinking about ways to map locally spoken slang and jargon using GIS.  Starting a database of ‘events’ where a person uses unique language in reference to a common-place item or occurrence (I have a friend from Wisconsin who calls the drinking fountain a “bubbler”) would be an insightful way to examine how jargon or slang starts and spreads geographically. So I decided to indulge my curiosity and create a small database consisting of the answers to two quick survey questions; What do you call sweet carbonated beverages?, and what state do you identify yourself as being “from”?.  I solicited friends and colleagues for the answers to these questions and ended up with about 150 useable responses (if you were one of the people who responded with “beer”, I thank you for the interest in the survey, but your answer was not included).  I chose to ask this question (please bear in mind that linguistics is not a focus of my studies) because regardless what you refer to it as, most people have had experience with a coke/soda/soda-pop/pop, which isn’t true for all objects of regional jargon (example: before moving to the East Coast I had never seen nor heard of scrapple) and I wanted to document the geographical extent and overlap of a single object rather than attempt to compare multiple similar objects with this first foray. Approximately 94% of respondents identified that they referred to sweet carbonated beverages as either “coke”, “soda”, “pop”, or “soda-pop”, so I chose to focus the mapping of this data on those four responses.  I took the responses I received and calculated a ‘count’ by state of each type of response; for example, I received a total of 4 responses from people who identified as being from the state of Missouri.  Three of the respondents refer to sweet carbonated beverages as “coke”, and one refers to it as “soda”.  I took these counts and normalized them to the total number of responses received from the state and used that percentage to map the responses by state broken into ~25%, ~50%, ~75%, and 100%.  For each response (coke, soda, pop, soda-pop) I chose a single color to represent responses on the map, and varied the transparency of the color to represent the percentage of the response (25% response = 75% transparency, 50% response = 50% transparency, 75% response = 25% transparency, and 100% response = 0% transparency).  I mapped all four responses separately first (figure 1). I chose to vary transparency as opposed to saturation of color (eg: monochromatic choropleth) because I wanted to be able to overlap the response maps to visualize the confluence of the regional terms yet keep the original colors of each response (figure 2). The map above shows the overlap of “coke” responses with “soda” responses, which are displayed by the variation in colors from bright red where 100% responses were “coke” to bright blue where 100% responses were “soda” and various shades of purple and pink in between where there was a mix of responses in that state.  This kind of map can be created using a map with a double ended scale, but that type of visualization is limited to displaying the spectrum between two absolute responses, which would mean that I could only display the confluence of two responses rather than all four (figure 3). One interesting thing I noticed when looking at the results of this survey is that I need to meet more people from the Pacific Northwest section of the country.  The other interesting result I noticed which is more pertinent to the questions asked in this study is the confluence of the regional jargon that occurs in the region that includes Kentucky, Indiana, Ohio, and Illinois.  This area represents the confluence of the “soda” and “pop” responses and is also the region with responses of “soda-pop”, a hybridization of “soda” and “pop”. This exercise seems to make the argument that assembling databases of ideas such as regional jargon and using tools like GIS to display that information is a thought provoking and possibly worthwhile endeavor.  (I’d like to thank all of my friends and colleagues who participated in this survey that allowed me to assemble and produce this study for digestion by the blogosphere. Thanks, you guys!)"},{"id":"2009-03-11-how-digital-humanities-can-improve-my-dissertation-part-1","title":"Mining and Mapping Apocalyptic Texts, Part 1","author":"matt-munson","date":"2009-03-11 06:10:20 -0400","categories":["Digital Humanities","Visualization and Data Mining"],"url":"how-digital-humanities-can-improve-my-dissertation-part-1","layout":"post","content":"I have used computer technology to help my work in biblical interpretation for a while. I learned to do complex digital word searches with the Bibleworks software package early in my graduate career. When I started working at the Scholars’ Lab in the summer of 2006, I was introduced to digital humanities. I found these technologies fascinating. But how, I asked, could they help me interpret ancient religious texts in their original languages? I recently posed this question to some of my colleagues in the Scholars’ Lab and was pleasantly surprised by the answers. In this two-part blog, I will consider these answers in relation to my dissertation, which focuses on several passages in the Apostle Paul that speak of the final fate of humankind. Some of these passages suggest that all people will, in the end, be made right with God. Other passages suggest that some people will be permanently alienated from God. I wish to discover the central kernel of Paul’s thinking about the fate of humankind (called soteriology) that would make both of these statements true. In this first entry, I will focus on how I plan to use text-mining to enhance my ability to compare dozens of Greek, Hebrew, and Latin texts with each other more quickly and more thoroughly than I could manually. The second part will focus on how geographic information systems (GIS) will help me to place Paul’s writings in spatial relationships with other writings of the same time. Text-mining involves parsing a digital text, inserting the words along with their linguistic features into a database, searching for patterns within the database, and, finally, evaluating the results. In my case, I will use text-mining methodologies to extract linguistic data from the Pauline texts as well as other early Jewish texts that speak of the fate of humankind. This process will be fairly straightforward for the Pauline texts. There are many versions of the Greek text of Paul that have linguistic data attached to the words. One need simply extract this data from the text and insert it into a database. I will use the text of the Bibleworks 6 package as my source for Paul. For other texts, this process will not be as easy. For instance, the Thesaurus Lingua Graecae has a huge collection of Greek texts for the period that interests me. But they have no linguistic data attached to the words. To attach linguistic data to these words, I will need to write a script, probably in PERL, to query the open source parsing engine from the Perseus Project at Tufts University ( www.perseus.tufts.edu ). I will then insert the results from these queries into the database for that text. The next step will be to design queries that will find appropriate relationships among the texts. Good methodology requires that I test my queries against a set of texts for which I know the results. I will test my queries on several Greek apocalyptic texts which I have already read carefully, noting the sections that relate closely to Paul. Once I have designed a set of useful queries, I will apply these queries to the databases I created earlier. The application of these queries should point to numerous texts that I will then manually analyze to determine their meaning and how they relate to the Pauline passages under investigation. If my investigation were purely manual, I would begin by reading the texts in English in order to find promising passages that I would then examine more closely in their original language, whether Greek, Hebrew, or Latin. This digital method, though, will do this first analysis using the original languages. That means that this computerized comparison of the texts in their original languages will find verbal and grammatical similarities that may be obscured or destroyed in translation. In the end, I expect text-mining to return data that would be only partially accessible by manual means. In my next post, I will consider how another area of digital humanities, geographic information systems, can help me to explore how the Pauline and apocalyptic texts are related spatially, instead of linguistically, to each other."},{"id":"2009-03-11-rome-reborn","title":"Rome Reborn","author":"fitz-green","date":"2009-03-11 06:14:37 -0400","categories":["Digital Humanities","Geospatial and Temporal"],"url":"rome-reborn","layout":"post","content":"My wife and I frequently engage in a strange kind of “culture war.” She thinks ancient Rome is the more interesting civilization, and I’m partial to ancient Greece. In these debates, I always tell her that I prefer philosophers to politicians. Still, I was excited when I first encountered Rome Reborn, a joint project between UVA’s Institute for Advanced Technology in the Humanities, a few other schools, and Google (who allows access to the project through Google Earth ). The goal of Rome Reborn is to create a 3D digital model of ancient Rome in the year 320. There are plans to extend the project over time, so that you will be able to track the development and growth of the city over time. The buildings have all been reconstructed by computer modeling, and mapped onto Rome’s actual terrain. What a cool project. I should say, before continuing, that if you want to check out Rome Reborn for yourself, you might have some trouble getting to it. First, you need to download Google Earth . Then, you need to turn on the “Ancient Rome 3D” layer, which listed under the “Gallery” layers. Next, get to Rome, zoom into the ancient city and click on a yellow building, which brings a popup window to add the ancient terrain, landmarks, and buildings. Then, you are finally ready to enjoy the model. (But be warned, if you don’t have a good computer with a fast processor and a hefty bit of RAM, you’ll only send yourself into conniptions rather than enjoy the grandeur of this ancient civilization.) My first impression, in wandering through the reconstructed forum on Google Earth, was of how chock-a-block the buildings are. You realize how many of the buildings are right on top of one another. You do get this feeling in person, walking around the ruins, but the 3D model captures the hustle and bustle of a true big city that is not conveyed adequately by pictures alone. This project will help scholars puzzle over details of the architecture itself, but having it available to such a wide audience on Google will also help those just learning about Rome. It has the potential to spark students’ interest in learning—for me, this is well worth the effort."},{"id":"2009-03-24-ada-lovelace-day","title":"Ada Lovelace Day","author":"bethany-nowviskie","date":"2009-03-24 14:31:27 -0400","categories":["Digital Humanities"],"url":"ada-lovelace-day","layout":"post","content":"Today has been declared – quite spontaneously, and to the cheers of a great many people – Ada Lovelace Day, a day on which to honor women working in technology by writing blog posts about their often-unsung achievements, and about ways in which they inspire and challenge us. I want to mention two women with whom I have worked closely in my career as a digital humanist.  The first is book artist and media theorist Johanna Drucker, with whom I collaborated on the design of interactive tools for humanities scholarship.  But forget the digital.  I want to thank Johanna for teaching me letterpress printing – from the minute and retrograde obsession of setting type to the athletic cranking of a Vandercook press – and all the way down to those gentle and girly concerns of a printer’s devil in pink: the best tactics for keeping one’s silken tresses out of the rollers, thus avoiding an unladylike scalping, and the preferred soap for scrubbing toxic lead from beneath one’s decidedly unmanicured nails.  It’s true that we made some crazy things online, and thought about some things – but Johanna and I also got inky, and I don’t want a post on women in technology to assume that it’s all bits and bytes, when there are other bites to care about, as well. And then there’s Bess Sadler, with whom it’s my great pleasure to work at the University of Virginia Library .  Ada Lovelace Day comes one day after a little milestone for Bess, the release into its native habitat of a piece of software that just might make library research a bit more joyful.  Bess has all the things it takes for a woman to succeed in technology: vision, energy, a highly specific strategy for shrugging off the crap, and a deep understanding of the little tweaks it takes to make a system (any system) sing. By the way, Bess and Johanna crossed paths here, in the creation of an archive that puts one brand of bite in bytes. Update : Since writing this, I’ve discovered that several other people have honored Bess today!  See posts by Amanda, Dorothea, and Randy ."},{"id":"2009-03-26-day-of-digital-humanities-2009","title":"Day of Digital Humanities 2009","author":"bethany-nowviskie","date":"2009-03-26 12:52:45 -0400","categories":["Announcements","Digital Humanities"],"url":"day-of-digital-humanities-2009","layout":"post","content":"Ever wonder how folks in the Scholars’ Lab spend their day?  Bethany Nowviskie, Director of Digital Research &amp; Scholarship at the UVA Library and Joseph Gilbert, Head of the Scholars’ Lab, recently participated in the “ Day in the Life of the Digital Humanities ” project initiated by our friends at the University of Alberta.  The “Day of DH” project encouraged scholars, administrators, students, and others who self-identify as “digital humanists” to blog about their day on March 18, 2009.  You can read about Bethany’s day and Joseph’s day, as well as the experiences of a host of other participants ."},{"id":"2009-03-26-scholarly-publishing-today-and-tomorrow","title":"Scholarly Publishing Today and Tomorrow","author":"ronda-grizzle","date":"2009-03-26 08:56:34 -0400","categories":["Podcasts"],"url":"scholarly-publishing-today-and-tomorrow","layout":"post","content":"Linda Bree from Cambridge University Press talks about “Scholarly Publishing Today and Tomorrow” [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public.2014484138.02014484145.2014989915/enclosure.mp3”]"},{"id":"2009-03-30-a-kindle-for-every-student","title":"A Kindle for Every Student?","author":"fitz-green","date":"2009-03-30 13:09:24 -0400","categories":["Digital Humanities"],"url":"a-kindle-for-every-student","layout":"post","content":"The blogosphere has been abuzz with diverse opinions on the release of Amazon’s new Kindle 2 . So far, most of the news has surrounded the controversial text-to-speech function and whether or not it violates copyright law (more on this here and here ). Regardless of its legality, the speech sounds mechanical, and I don’t see this posing a threat to genuine audio books read with intonation by real people. But my interest is not in this primarily, but in reading via ebook itself. I’ll admit, when it comes to ebooks, I’m still in the undecided camp. On the one hand, I love technology, and can’t resist the latest gadget. On the other hand, I consider myself a “book person.” And the book as physical object matters to me. I want to be able to pick it up, smell it, leaf through the pages. I’m guessing there’s not much to be said for ebook smell. Where Kindle does seem to have gotten it right is in the screen. I can’t read books or journal articles on my computer screen, because it’s just not like reading a book. There’s too much glare, it puts too much strain on the eyes, and it’s too distracting. Kindle has solved the glare and eyestrain problems with “electronic ink,” a new technology designed to make letters look more like they do on the written page. From what I can tell, this is a vast improvement over the first generation of ebooks. But what of the distraction factor? Christine Rosen argues that the Kindle is too distracting to generate productive reading. She tried to read Nicholas Nickleby on a Kindle, but got lured away into Wikipedia searches on Dickens. Alan Jacobs, however, argues almost the opposite . He writes that it is too hard to navigate pages to get to the internet—and that this is a good thing. It keeps you reading, because it’s too hard to leave the book you’re on. If I’m going to shell out the money for an ebook reader, though, I’d want it to do as many things as possible. Ultimately, the discipline required to stave off distraction is not inherent to the print book, but to the act of reading. It is something that is learned by readers, to varying degrees, when they learn to read (on this point, see a recent post, “In Defense of Readers” ). I can read a novel in a crowded coffee shop, on a busy beach, or just about anywhere without getting distracted. I can’t say the same of my computer. But an ebook needs to be able to do neat techie things that a computer can do in order to be worthwhile. After a point, it’s up to the user to learn how to read well on it. What would I want an ebook reader to do?  Here’s my wish list: I’d like to be able to read a book without interruption. But I’d also want to be able to read journal articles. Could I go to the UVA library website, get an article as .pdf from JSTOR, and read it on a Kindle? This is a copyright nightmare. But it sure would be nice (I did say that this was a wish list). For all of the ancient language work I do, I’d also love to have a Greek dictionary, or maybe even a bible program with ability to switch languages. I have this on my PDA, but a PDA screen is too small and too difficult to read from for any length of time.  The same goes for an ipod/iphone. This sort of basic reference work requires huge volumes of books that can’t be lugged around everywhere, but would always be at hand with an ebook reader. This would be of tremendous value for students. Even though many have praised the fact that the Kindle and its biggest competitor from Sony so far have limited the number of tasks that they can perform, this will ultimately restrict their usage to a niche market: avid readers with money to spend on gadgets. Unfortunately, this is a rather small market. Right now, ebook readers are not made to read all books. They are best suited for reading novels, especially quick page-turners. This leaves out all sorts of books, that are read in all sorts of different ways. Dictionaries aren’t read in the same way as Grisham thrillers, but they’re still books and they’re still “read” in their own way. The difference, I imagine, is that Amazon doesn’t have as much interest in books that are re-read or referenced, because they aren’t “consumed” in the same way and don’t create the need to go buy another book when one is finished. For me, the Kindle is still too proprietary, in terms of what can be read on it, and too limited, in terms of its non-reading functionality. Ebooks won’t replace books, unless they can do “e” things. I guess I’m OK with that. I like books."},{"id":"2009-03-30-electronic-text-analysis-and-the-wary-humanist","title":"Electronic Text Analysis and the Wary Humanist","author":"sara-henary","date":"2009-03-30 13:18:34 -0400","categories":["Digital Humanities","Visualization and Data Mining"],"url":"electronic-text-analysis-and-the-wary-humanist","layout":"post","content":"For a long list of complicated reasons, most practitioners of my discipline—political theory—tend to be suspicious of, if not altogether opposed to, the integration of computer technology into their research and teaching. While some scholars cite the superfluity of computer technology to the discipline (excepting, of course, Microsoft Word), others argue that the introduction of certain technologies might somehow actually endanger both thinking and learning (and who wouldn’t find the reduction of Plato to a series of PowerPoint slides, well, a tad reductive?). Nevertheless, working at the Scholars’ Lab has afforded me the opportunity to sample a range of digital scholarship tools/resources, some of which might appeal to that most skeptical of techno-skeptics, the political theorist. One such resource is TAPoR, the Text Analysis Portal for Research, a website that provides access to tools used in the analysis of electronic texts. Many classic texts are now available in electronic form, and “computer-assisted text analysis” (TAPoR website) enables the researcher to explore a text in ways that are difficult, if not impossible, using only conventional tools for text analysis such as the index or a concordance (though much electronic text analysis is modeled conceptually on both of these). This is generally accomplished by allowing the researcher to search a text for specific words or word patterns or to generate a listing of the most frequently used words. In the case of TAPoR’s word/word pattern search, the results are displayed in the context of the surrounding text—the words sought are in bold, while several additional words on either side give the researcher some sense of how the words are being used. One may employ this kind of analysis with many ends in view, though some common goals include: a) testing to see whether—and if so, how often—an author employs either specific language or a certain kind of language and b) exploring certain words or phrases in context in order to gauge the narrowness or expansiveness of the author’s meaning when such language is used. In order to explore the features and capacities of TAPoR, I brought to the portal an aspect of a particular research question that I had been thinking about for some time. To write my dissertation, I must provide at least a provisional answer to the following question: Does John Locke articulate a consistent view of the human person throughout his corpus? Although I was very familiar with the way he spoke about the “person” in one text, I was less familiar with his usage in other texts. After sketching a brief definition of the “person” based the first text, I proceeded to investigate whether Locke spoke of the “person” in similar terms in a second text. I pasted the URL of a webpage that contained the second text into TAPoR, which I then asked to search the document for the word “person.” I performed several additional searches, using other key words/phrases from my original definition as well as others that came to mind as I was searching. The results were illuminating. I discovered that although Locke tended to use the word “person” in a more ordinary, less philosophical sense in the second text, all of the basic features of the first text’s conception were nevertheless present. While this confirmed the intuition I had about the consistency of Locke’s view of the “person” across his texts (or at least two of them), the specific instances of personhood language that I isolated with the help of TAPoR will allow me to present a much more convincing defense of my position in the dissertation. Additionally, the fact that TAPoR allows the researcher to view all results of a word search simultaneously helped me to formulate more precisely what was going on in the text and to relate it to my more general argument–i.e. a looser, more familiar usage of “person” in certain contexts can co-exist with a unified, consistent account of personhood. TAPoR enabled me to “see” more than I otherwise would have in a text and could be a valuable resource for scholars in any field concerned with the close and careful reading of texts."},{"id":"2009-03-30-how-digital-humanities-can-help-my-dissertation-part-2","title":"Mining and Mapping Apocalyptic Texts, Part 2","author":"matt-munson","date":"2009-03-30 13:18:18 -0400","categories":["Digital Humanities","Geospatial and Temporal"],"url":"how-digital-humanities-can-help-my-dissertation-part-2","layout":"post","content":"As I explained in my last blog post, my dissertation will compare several statements about the final fate of humankind in Paul to similar statements in apocalyptic texts. In that post, I described how text-mining could help with the interpretation of the texts which stand at the center of my dissertation. In this post, I will discuss how geographic information systems (GIS) can help to visualize geographic relationships among texts. My ideas here, as in my first blog post, are the result of conversations with other staff members here at the Scholars’ Lab. The question that I pose and answer in this blog post is, What does geography have to do with the analysis of biblical texts? The short answer is, “Much, in every way.” But I can’t just assert that, I need to show it. Historical-critical study of the Bible has understood for the last two hundred years that the historical circumstances of any person or group profoundly affect the literature that that person or group produces. And scholars understand geographical location to be an integral part of any author’s historical circumstances. I was always dubious about the ability of GIS to help me with my research into the Apostle Paul. After all, scholars have no more evidence for Paul’s geographic location than he gives in his letters, and scholars have already thoroughly discussed this evidence. Another basic tenet of historical-criticism, however, is that we understand an author’s history better when we put it in relationship to the histories of other authors. This goes for geography as well. That means that I should put Paul in geographical relationship with the apocalyptic texts I will study. But this process will be more than simply plotting each work’s points of origin on a map. Since GIS is driven by databases, one can query the databases and display the results geographically. For instance, I may find that certain texts assert that the Messiah will descend with angels before the final judgment. If I have geographical data for these texts, I can tell GIS to show the place of origin of all texts that meet these criteria. I could then discover that all of these texts come from a certain area or that they all fall along a certain trade route. I might also discover that they have no apparent geographical similarity. And that is the beauty of GIS. I can follow leads quickly enough that pursuing a red herring no longer requires wasted hours or days. I can check out multiple leads in the time it would take to follow one lead manually. The ultimate question, however, is how this technology could help my research. One scenario will make its usefulness apparent. I will consider dozens of apocalyptic texts. If I find that a Paul shares some textual characteristic with only 2 of these texts, I would be hard pressed to show that these three sources by themselves demonstrate an historical pattern. But, if I could show that all three of these texts originated in approximately the same area at approximately the same time, I would show that the texts share more than just textual characteristics. This demonstration would relate the texts more closely to one another and thus strengthen my argument that the textual similarity represents a geographically specific historical pattern. Once such a pattern is recognized, I could interpret these three texts together to reach a fuller understanding of the textual characteristic that is partially represented in each text. And with GIS, one is not limited to analyzing one relationship at a time. One can also assign different symbols to texts depending on which characteristics they have. In this way, one can produce a graphical representation of textual features that may suggest relationships that otherwise would not have been clear. In the end, GIS technologies make it easier to analyze and visualize geographical relationships among texts. As a result, my interpretation of Paul would be based more firmly in Paul’s own historical circumstances."},{"id":"2009-03-31-mapping-then-and-now-are-we-ready-yet-for-academic-social-systems","title":"Mapping Then and Now: Are We Ready (yet) for Academic Social Systems?","author":"ronda-grizzle","date":"2009-03-31 08:51:32 -0400","categories":["Podcasts"],"url":"mapping-then-and-now-are-we-ready-yet-for-academic-social-systems","layout":"post","content":"Ian Johnson, Director of the Archaeological Computing Laboratory at the University of Sydney and TimeMap project leader discusses “Mapping Then and Now: Are We Ready (yet) for Academic Social Systems?” [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public.2014484138.02014484145.2019742791/enclosure.mp3”]"},{"id":"2009-04-03-hide-and-seek-blacklights-smart-search-functionality","title":"Hide and Seek: Blacklight's Smart Search Functionality","author":"fitz-green","date":"2009-04-03 12:26:29 -0400","categories":null,"url":"hide-and-seek-blacklights-smart-search-functionality","layout":"post","content":"Bethany wrote recently in praise of Bess Sadler’s work on Blacklight, and its recent release (as “VIRGObeta”). I’d like to offer my own (admittedly anecdotal, perhaps insignificant) praise. Yesterday I needed to go looking in the library for Jacob Neusner’s translation of the Mishnah into English. I typed into the search box on the UVA library homepage, “Neusner Mishnah.” Seems straightforward enough, right? When I had done the same on Google Books, the book I needed was the very first search result. (Of course, Google limited my viewing of the very page of text I needed to refer to, hence the need to consult the physical book at all!) But when I searched in the legacy UVA catalog system, I received 81 results, none of which were the book I needed. Admittedly, Jacob Neusner has written a lot about the Mishnah, and I was looking for a book he did not author, but translate. Now, I knew how to modify my criteria to get the results I wanted. But should I have to? Shouldn’t the search be smart enough to help me out?  Frustrated, but used to this experience, I next turned to Blacklight to try the same search. The book I needed popped up immediately, on the first page of results. Smart searching. How refreshing!"},{"id":"2009-04-03-illuminating-historical-architectur","title":"Illuminating Historical Architecture","author":"ethan-gruber","date":"2009-04-03 12:30:22 -0400","categories":["Digital Humanities","Geospatial and Temporal","Visualization and Data Mining"],"url":"illuminating-historical-architectur","layout":"post","content":"Following up on my introduction to using 3D models to recreate archaeological sites and perform meaningful academic analysis on simulated virtual environments, I will discuss in further detail my current project concerning the recreation of the House of the Drinking Contest in Seleucia Pieria, the port city of Roman Antioch. The house in its final phase dates to the third century A.D. and exhibited some of the most complete eastern Roman mosaics, all of which were removed from the site following the 1930’s excavations and placed in American museums (including Richmond’s very own Virginia Museum of Fine Arts ).  What better way to view the mosaics than to recreate the environment in which they existed?  Mosaics in museums are entirely out of their original context.  Many floor mosaics are now hanging on walls.  Even in occasions that museums create elaborate sets to mimick the rooms from which the artwork was taken, it is impossible to recreate the entire structure or accurately recreate the lighting and allow us to view the mosaics as the original owners of the House of the Drinking Contest would have. In my previous project of modeling the House of the Faun, one of the largest houses in Pompeii, I had a lot of information to work with.  I had many photographs and artists’ reconstructions to consider.  While the ceilings and roofs are gone, the walls are still more or less intact, and so are many of the wall paintings.  The House of the Drinking Contest is much more of a challenge since the walls collapsed and were removed long ago, leaving at most a half a meter of brick and rubble left.  There are clues, however, that let us accurately estimate the height of the walls, and hence a full reconstruction.  The plan indicates that columns were about 0.9 meters in diameter.  From our knowledge of classical orders and the overall dimensions of the house and rooms, we can assume the columns would not have been Corinthian or Ionic since both would have been too out of proportion with respect to the rest of the house.  The reason is that Corinthian and Ionic columns have 10:1 and 9:1 height-to-diameter ratios, respectively.  We can then safely assume an average-height Doric colonnade at a 5.5:1 ratio.  Other clues and experimentation with natural light simulation allow us to predict plausible window locations. (click for larger image) Lighting simulation and computer modeling enable us to take this a step further and create timelapse animations demonstrating how light shifted throughout the hours of the day or days of the year.  We then know when mosaics would have been exposed to direct sunlight or were in the shade.  I found it useful to create an animation of standing in the triclinium (dining room) of the house, looking west toward the courtyard, to see if the triclinium received direct sunlight at any point of the day.  So far I have found that it does on March 21st of A.D. 200, and probably throughout the spring and autumn.  In fact, the room’s mosaics are illuminated quite beautifully right around dinner time. [Link to video] . While there is still work to do in the modeling, texturing, and animation of this particular Roman house, the use of accurate modeling techniques and lighting simulation can have a profound impact on archaeology, particularly in cultures that are solar-oriented.  I attended the Computer Applications and Quantitative Methods in Archaeology conference last week in Williamsburg, and while there were many demonstrations of 3D models, none of the projects focused on incorporating temporal lighting and analyzing the outcome.  In nearly every case, temporal lighting is not even a consideration. I did get a chance to informally demonstrate some of my work on the House of the Faun and the House of the Drinking Contest to some other classical archaeologists who are also involved in virtual reconstruction, but this facet of computer modeling has yet to hit the mainstream digital archaeology field, it seems.  Perhaps I will have the opportunity to demonstrate it to a wider audience at CAA next year."},{"id":"2009-04-08-pandora-and-the-genes-of-music-genres","title":"Pandora and the \"genes\" of music genres","author":"jason-kirby","date":"2009-04-08 12:23:39 -0400","categories":["Digital Humanities","Visualization and Data Mining"],"url":"pandora-and-the-genes-of-music-genres","layout":"post","content":"Hello, it’s been a while since I blogged. You may remember me as the music Ph.D. student who was last heard from pondering the uses of Google Scholar. I’m on a new mission this semester, studying for my comprehensive exams.  One of the topics I am researching and preparing an essay on is about genre in popular music. The concept may seem initially so self-evident, you may wonder what there is to write about it, per se. Oh, but there’s lots. This is because the issue of genre always involves the issue of classification, which inherently provokes debate. Take, for instance, a star performer like Beck. His music often includes acoustic guitar, and he’s covered Mississippi John Hurt. So he must be a folkie. Oh wait, but he also apes Prince on some funky jams. So maybe he’s a pop star. But he also headlines a bunch of big rock festivals, and we find his music in the “Rock” section at the record store (wait, what’s a record store?). So I guess we’ll call him a rocker. My point being, popular music can be difficult to pin down using genre tags. You’ll find this evidenced in any number of press interviews with musicians who, when pressed by a journalist, pull out that time-worn chesnut that their sound is “unclassifiable”. Genre tags, be it pop, country, rock, hip-hop, salsa, what have you are almost like identifying pornography: I’ll know it when I see it. It’s often somewhat easier to identify what a genre isn’t than what it actually is . Fans and even so-called experts often have difficulty articulating why a particular song or artist fits in a given genre. Based on my readings for this exam topic thus far, I would argue this is because the act of classifying something is essentially making a statement about its meaning :  not just semantic/musical meaning, but also meaning that’s intensely cultural, and often political. That’s why I like musicologist Robert Walser’s definition of genre in popular music. Updating similar ideas that literary critic Tzvetan Todorov has explored, Walser argues that “Genres…come to function as horizons of expectations for readers (or listeners) and as models of composition for authors (or musicians).”[1] In other words, genres labels are modes of discourse wherein musicians, fans, and music industry workers collaborate to make meaning surrounding the music they love. And perhaps surrounding is the key word, there; one can debate about styles of music with a friend all night long, or a record store employee can create increasingly hyper-specialized bin cards for various sub-genres (Psychobilly, Krautrock, etc.). We circle around the sounds we hear through discourse about them, but ultimately, on a musical-sound level, how do we know that Beck belongs in the “rock” section? As Franco Fabbri has noted, problems with genre are “frontier” problems: “We meet with these whenever we attempt to indicate something which exists at the boundary of two or three zones of meaning.”[2] When considering genre tags, there seems to exist an inescapable gap between our need to accurately label a piece of music and the slippery semantic meanings of recorded sound, which may in any given moment resemble two, three, or even more genres. What then, friends, does this problem have to do with digital humanities? Well, as I research and read in preparation for this exam, I can’t help but frequently think of an Internet service called Pandora that’s richly illustrative of many of these genre issues. For those who may not know, Pandora, a free website based out of Oakland,  California since the early ‘00s, is a relatively new and different kind of streaming online radio station. Whereas the playlist at a streaming station such as UVa’s own WTJU is determined by in-the-studio DJs, and the “radio” playlist at a website such as last.fm is determined by a process called collaborative filtering (more on this in my next blog), Pandora is notable for its novel method of selecting songs for listeners. Here’s how it works: enter an artist or song into Pandora’s search engine, and the website will issue you a streaming series of songs by artists considered similar to the one you entered. And how is this similarity determined? Through the Music Genome Project, Pandora’s massive undertaking and claim to fame. The Music Genome Project, whose work is carried out by roughly fifty analysts at the company’s headquarters in Oakland, is an effort to deconstruct and categorize aspects of pop songs using over 400 different “musical attributes” that the company believes comprise the spectrum of recorded sound. It’s a rigorous close-listening endeavor, specifically intended to focus on aspects like timbre, tempo, harmonic movement, and instrumentation instead of aspects like album art and whether or not the artist has appeared on TRL. This scientific (or pseudo-scientific, depending upon one’s perspective) attention to sonic detail seems—to me at least—an attempt to get beyond the established languages of pop music genres by diving into the nitty-gritty which makes genres what they are. The company builds its credibility as an almost-biologic “genome” of musical characteristics through the depth and breadth of songs it has analyzed: over half a million and counting, according to Pandora’s website . Pandora says that each of these songs is listened to for 20 to 30 minutes by a trained analyst who tags the song with Pandora-authored characteristics—everything from “meandering melodic phrasing” to “chopped and screwed production”. The company claims that theirs is the most comprehensive effort to systematically categorize music—ever. From the perspective of an academic studying pop music genres, the Music Genome Project presents several fascinating issues. Until my next blog, I’ll briefly set aside an investigation of the company’s claim it can create a taxonomy of the “genes” of popular music. Instead, what I first find most interesting is the story of Pandora and the Music Genome Project’s origins. In a January 2006 interview with podcast program “Inside the Net”, Pandora co-founder Tim Westergren told hosts Leo Laporte and Amber MacArthur that he first originated the idea of a music genome while a struggling rock musician himself. He told them his band was “facing the challenge of trying to get known,” and in so doing brainstorming about what aspects of a rock song tend to attract the most commercial attention. Additionally, Westergren shared the intriguing information that he was a film composer at the same time, working for hire to complement a director’s visuals in a given scene. He told Laporte and MacArthur that “In that capacity, one of the things that I had to do was to try and figure out the music taste of a film director.” Westergren said that this challenge was part of what got him thinking about music in terms of distinct, differentiable attributes. It just so happens that one of my other exam topics this semester concerns film music soundtracks. Having read much of the academic literature on film music, what Westergren recounts here is fascinating—and doesn’t surprise me. The specific challenges a composer faces when working on a film—How do I balance my need to please the director with my need to express personal creativity?—is a central theme of the literature. Scholars even further back than Irwin Bazelon in 1975 have remarked that the collaboration is by nature difficult, “since the composer spends his entire life in music, working out specific musical relationships, while the director spends his time out of music, involved full-time with films—a visual medium—and only part-time with music, as it affects his film”.[3] Even if the director is a fan of music and has interesting ideas about how she wants it used, if she’s not a musician herself she may have difficulty communicating concepts to the composer which can be actualized musically. That challenge has interesting aspects as regards musical genre. On the one hand, bridging the communication gap between composer and director can often force each out of their comfort zone, resulting in new timbres, new melodies—innovation. Many pieces composed for film are quite short, maybe less than a minute in duration, so they often don’t have space in the film to unfold into full-blown genre exercises. This process of collaboration between sound and images in many ways results in the most “hybrid” music imaginable. From another perspective, however, film music cues are also remarkably genre-bound. Not necessarily bound by musical genres per se (classical, country, pop, rock, etc.), but bound by the generic conventions of film itself. Due to the standardized production practices of most movies, film as a medium tends to be more amenable to genre classification—and soundtracks can play a key role in that[4]. For instance, soaring strings sketching out an American traditional folk or “cowboy” song, and the audience suddenly knows we’re in a Western. A minor key piano melody and a sultry saxophone, and we know we’re watching a film noir. Given that these generic conventions of film music most certainly exist, it makes me wonder what sorts of films Westergren was scoring as part of his job. Perhaps offbeat indie-Sundance dramas which were aiming for a kind of transcendence of genre strictures? In any case, the fact that the Music Genome Project origin story involves the world of film music tells me that musical genre was on Westergren’s mind as he brainstormed—even if he sought to rebel against the concept. Additionally, the Project’s roots in soundtrack-for-hire work demonstrate that you can never take the music business out of the music: commercial forces and the presence of an paying audience (real or imagined) inflect in some way all decisions musicians and music industry workers make. As Westergren’s rock band tailoring their sound in an attempt to “get known” reminds us, market considerations are basically always in a dialectical relationship with “creativity” as a musical genre forms—and this includes classical and avant-garde genres which claim to be above that kind of stuff. Given the commercialized roots of the Music Genome Project, it’s a bit surprising to me that the music industry has fought Pandora as tooth-and-nail as they have. It’s common knowledge that traditional AM/FM commercial radio has been the music industry’s biggest promotional tool during the 20th century. But as traditional radio fades in influence and web radio such as Pandora ascends, the industry leaders have been notoriously less willing to jump onto the 21st-century Internet bandwagon of music promotion. Despite the fact that Pandora streams tracks instead of allowing users to illegally download—which means, in my opinion, that record company executives should be groveling at Pandora’s feet in thanks—the major-label music industry has seemed intent upon shutting them down. Or, to be more specific, the labels’ demands (through representative organization SoundExchange) for higher royalty payments created expenses Pandora was finding impossible to sustain. Following a federal board ruling mandating increased royalty rates for web radio, in August of last year the Washington Post quoted Westergren as saying Pandora was “approaching a pull-the-plug kind of decision”. On the brink of shutting down, Westergren appealed to the company’s subscribers to contact their local representatives on behalf of web radio—and the gambit seems to have worked. The ruling was reconsidered, and currently representatives from both web radio and the music industry are negotiating newer, more manageable royalty rates. Pandora seems to be approaching a delicate truce with the market forces which, to me, seem constitutive of its role as a source for promoting and discovering popular music. (In other words, what took the industry so long to accept current reality?) In the next installment of this blog, I’ll continue my exploration of Pandora, especially the logic behind its attempt to map “genes” of music and to approach music in a mode somehow “beyond” genre. [1] Walser, Robert. Running with the Devil: Power, Gender, and Madness in Heavy Metal Music . Middletown, CT: Wesleyan University Press, 1993. p. 29. [2] Fabbri, Franco and Iain Chambers. “What Kind of Music?”. Popular Music, Vol. 2, Theory and Method (1982), pp. 131-143. [3] Bazelon, Irwin. Knowing the Score: Notes on Film Music . New York: Van Nostrand Reinhold Company, 1975. [4] Holt, Fabian. Genre in Popular Music . Chicago: The University of Chicago Press, 2007. Pp. 4-5."},{"id":"2009-04-21-toward-the-historical-data-forge-what-happens-after-the-data-mining","title":"Toward the Historical Data Forge: What Happens After the Data-Mining?","author":"ronda-grizzle","date":"2009-04-21 08:50:18 -0400","categories":["Podcasts"],"url":"toward-the-historical-data-forge-what-happens-after-the-data-mining","layout":"post","content":"Bruce Robertson of the Mount Allison University Department of Classics and the Historical Event Markup and Linking Project talks about HEML: Historical Event Markup Language [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public.2014484138.02014484145.2053632731/enclosure.mp3”]"},{"id":"2009-05-04-mapping-out-the-geography-of-an-asian-american-music-blog","title":"Mapping the Digital Diaspora of a Dissertation Research Blog","author":"wendy-hsu","date":"2009-05-04 06:32:00 -0400","categories":["Digital Humanities","Geospatial and Temporal","Visualization and Data Mining"],"url":"mapping-out-the-geography-of-an-asian-american-music-blog","layout":"post","content":"At the onset of my field research in summer 2007, I launched a blog – YellowBuzz.org – with the intention to: 1) archive and organize my field notes in textual and audio-visual form; 2) convey my research purpose and progress to informant musicians and the public; 3) self-position as a “participant” in the scene. Since then, I have made over 160 posts, some directly linked and others tangentially related to my research findings about the activities and media of Asian American indie rock musicians. Over the past one and a half years, my field research blog has received attention from both print and online media.  Evidently, this blog has constructed a community consisting of musician- and music-enthusiast-visitors with an interest in Asian American and transpacific music-culture. This past January, I began tracking the blog traffic by using Google Analytics . This service monitors the physical location of site visitors and their interactions with the pages on the site. The geographical data are analyzed in terms of the number of visits per unit of geographical organization such as city, country/territory, sub continent region, and continent.  This information is also visualized in the form of an interactive map on which users can zoom in and out of specific locales and find site visit patterns specific to cities, countries, regions, or continents in the world. Over the last four months, I have been playing with the May Overlay function projecting geospatial patterns of the site traffic on my blog. These interactive moments have helped me imagine interesting questions such as: What is the geography of an electronic community based on the topic of “Asian American music,” the tagline of my blog? What does the geo-spatial terrain of this “digital diaspora” look like? Are there any striking patterns at each of the organizational level namely, the city, country, sub-continental region, and continent? What spatial boundaries are transcended and created in these visualizations? Or, fancifully, how does the digital geography of my blog reconfigure the more general social geography of “Asian America” online or offline? Today marks a 4-month anniversary of this thought experiment. I decided to take some screen shots of a few of the visualizations that I’ve found more meaningful in Google Analytics. This analysis uses data from a sample of 3,061 site visits collected from January 1 to April 30, 2009. I will highlight a few interesting findings below: 1) Here’s a map of blog visits in various U.S. cities. It appears that the visitors are concentrated in central Virginia (the home of yours truly), New York City, Boulder, Los Angeles, and San Francisco. Other than central Virginia and Boulder, these are areas of high concentration of Asian Americans and indie rock activities. I’m not quite sure how to explain the traffic flow from the Denver area (Boulder and Aurora, ranked sixth in this map) other than to link it to the thriving indie rock scene in Boulder and the physical location of an Asian/Japanese music blogger Shay of Sparkplugged . 2) According to this chart, 76% of the site visits have occurred within the boundaries of the United States. Next on the list are Canada, United Kingdom, and Australia, all English-speaking countries with close historical ties to American music. In the continent of Asia, countries such as Taiwan, South Korea, the Philippines, and Singapore have among the highest number of visitors to my site. I attribute this pattern to my blog posts about U.S.-based artists who have a large following in these particular countries. Specifically, Hsu-nami (of New Jersey) and Johnny Hi-Fi (SF-based) has strong ties to Taiwan; Kite Operations (New-York) to South Korea; Plus/Minus (New York) to the Philippines and Taiwan. 3) This last chart represents the sub-continental spread of the site visits. North America takes the lead (taking 80% of all visits). Northern Europe and Eastern Asia tie as second, followed by South-Eastern Asian and Western Europe. I’m not quite sure how to explain the high number of visits from Northern Europe other than to link it to the popularity of a Taiwanese metal band Chthonic in North Europe. Chthonic has a strong international presence, having worked with producers in Denmark and the U.S. including Rob Caggiano, the guitarist of Anthrax. In 2007, Chthonic toured with the OzzFest and established close ties with Taiwanese-American-led erhu rock group Hsu-nami. So what does this all mean? YellowBuzz, a blog on “Asian American music”, has constructed a global, transnational readership. Asian America in the online digital environment exists beyond the boundaries of the United States and the Asian continent. These observations of transnational crossings work against the geography of Orientalism: a now-classical theory within postcolonial studies that refers the representational control of the non-west by western-produced discourse.  The digital diaspora of YellowBuzz has tampered with the so-called east-west binary. Now if I were serious about pursuing the research on the transnationality of Internet music journalism, I would look for a correlation between blog content and traffic patterns. This would require systematic, post-to-post observations. I would also consider mapping information regarding Internet access and user demographic with the intention to find links between the blog statistics and general Internet sociality. I would also look for statistical and mapping methods more powerful than Google Analytics. But – to get back to my dissertation that asks: What paths do musicians and their music take as they establish routes crossing territories constructed by nation-states, corporations, international laws, etc? Unfortunately, these visualizations lack the analytical strength to provide an insight on the musicians’ perspective on the scene. They have offered a perspective on media, in particular in understanding the role of a music blog in constructing “Asian America.” In the coming months, I will be working on a digital humanities project with Joe Gilbert at UVa’s Scholars’ Lab pursuing questions related to the musicians’ side of the story. I hope to unravel the terrain of musicians’ sociality within the transnational scene of indie rock music by mapping out their tours, social networks on (SNS), and record distribution. Meanwhile, I’m experiencing a bout of euphoria loving the fact that I have reclaimed a free market analytical tool offered by Google for my academic(-y) ethnomusicological thought experiment."},{"id":"2009-07-24-institute-for-enabling-geospatial-scholarship","title":"Institute for Enabling Geospatial Scholarship","author":"bethany-nowviskie","date":"2009-07-24 07:17:22 -0400","categories":["Announcements","Digital Humanities","Geospatial and Temporal"],"url":"institute-for-enabling-geospatial-scholarship","layout":"post","content":"Through the generosity of the National Endowment for the Humanities, the Scholars’ Lab will host a three-track Institute for Enabling Geospatial Scholarship at the University of Virginia Library in November 2009 and May 2010. This Institute will bring scholars, cultural heritage professionals, and software developers together to support and develop geospatial projects and methods in the digital humanities. The NEH’s Institutes for Advanced Topics in the Digital Humanities program will support travel and lodging for 40 attendees as well as Institute faculty members. Dedicated funding is available for graduate students as well as faculty attendees. The Scholars’ Lab will provide $40,000 in funding for short-term scholar- and developer-in-residencies in humanities GIS to complement the Institute. The Scholars’ Lab also will develop and host an online information clearinghouse and fund visiting fellows in an effort to promote ongoing scholarly engagement, software development, and information sharing by Institute attendees around the theme of Enabling Geospatial Scholarship. See the Institute web site for more information – including application deadlines for each of our three “tracks,” on Stewardship, Software, and Scholarship."},{"id":"2009-09-10-digital-therapy-luncheon-september-2009","title":"Digital Therapy Luncheon September 2009","author":"ronda-grizzle","date":"2009-09-10 08:48:47 -0400","categories":["Podcasts"],"url":"digital-therapy-luncheon-september-2009","layout":"post","content":"Introducing our 2009/10 Digital Humanities Fellows and Scholarship Award Winners [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public.2014484138.02014484145.2466448832/enclosure.mp3”]"},{"id":"2009-11-05-creation-of-game-worlds","title":"Creation of Game Worlds","author":"ronda-grizzle","date":"2009-11-05 04:43:31 -0500","categories":["Podcasts"],"url":"creation-of-game-worlds","layout":"post","content":"Writer, game designer, and UVa alumnus Shane Liesegang talks about the “Disruptive Construction of Game Worlds” [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public.2014484138.02014484145.2733214823/enclosure.mp3”]"},{"id":"2009-11-05-olmsted-editing-to-mapping","title":"Olmsted: Editing to Mapping","author":"ronda-grizzle","date":"2009-11-05 04:00:17 -0500","categories":["Podcasts"],"url":"olmsted-editing-to-mapping","layout":"post","content":"Ethan Carr and Mandy Gagel of the Frederick Law Olmsted Papers discuss “The Papers of Frederick Law Olmsted: From Editing to Mapping?” [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public.2014484138.02014484145.2726749166/enclosure.mp3”]"},{"id":"2009-11-12-new-course-in-digital-humanities","title":"New Course in Digital Humanities!","author":"jean-bauer","date":"2009-11-12 06:58:37 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"new-course-in-digital-humanities","layout":"post","content":"Inspired by my fellowship at the Scholars’ Lab last year, I am teaching a course in the History Department this coming spring called, HIST 4501 “From Vellum to Very Large Databases: Historical Sources Past, Present, and Future.”   The course will examine how information about the past has been (and is being) preserved. Historians rely on primary sources to inform and defend their arguments about the past, but digital technology is altering the form and the content of available records and, in the process, raising fundamental questions about the nature of historical analysis.   I have designed the course to be “hands on,” so students will have the chance to examine illuminated manuscripts operate an early printing press geo-reference historical maps as they explore familiar and unfamiliar ways of recording information and reflect on how these formats affect the study of history. The course is for undergraduates and will meet on Wednesdays from 3:30-6:00pm.  For more information,  check out the course page at http://www.jeanbauer.com/vellum_to_vldb.html . “From Vellum to Very Large Databases” is a 4501 (Major Seminar), so students will sign up via a waitlist and then be added once they have received the instructor’s permission to enroll."},{"id":"2009-11-25-neogeography-from-tower-to-town-hall","title":"Neogeography: from Tower to Town Hall","author":"ronda-grizzle","date":"2009-11-25 04:55:40 -0500","categories":["Podcasts"],"url":"neogeography-from-tower-to-town-hall","layout":"post","content":"Andew Turner joined us in the SLab to discuss the neogeography movement, which has emerged from the rise of easy-to-use web-based maps and emphasizes community-led and colloquial uses of geospatial tools and techniques such as online maps, GPS, and location-aware phones, and its potential applicability to higher education. [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public.2014484138.02014484145.2841459673/enclosure.mp3”]"},{"id":"2009-12-15-dynamic-web-forms-for-the-creation-of-xml","title":"Dynamic web forms for the creation of XML","author":"ethan-gruber","date":"2009-12-15 06:20:25 -0500","categories":["Research and Development"],"url":"dynamic-web-forms-for-the-creation-of-xml","layout":"post","content":"Among my regular tasks in the Scholars’ Lab Research and Development department, I have been developing applications to enable users to easily edit XML metadata within web forms.  As those familiar with metadata creation workflows will know, methods for creating XML documents were prone to human error and required some level of technical knowledge.  With XForms, a W3C standard for creating dynamic web forms, the technical barriers for creating robust metadata can finally be removed. I began working with XForms (and the enterprise application, Orbeon, for rendering the forms and managing data interactions) in July as I improved Numishare, the open source application developed to deliver the University of Virginia Art Museum Numismatic Collection .  The UVA coin collection, described in Encoded Archival Description (EAD) is a visual site with no administrative back end.  In order to provide tools to entice users to try the new software, I had to develop a method that curators could use to quickly and easily manage artifactual information–standard creating, updating, and deleting commands.  I was able to accomplish this entirely with Orbeon and XForms.  The application is sophisticated enough to allow auto-suggest capabilities for controlled vocabulary (based on TermsComponent in Solr 1.4), as well as post directly to the Solr search index when the user saves or deletes a record. Most recently, I have been developing an XForms application that can generally be applied to the creation and management of EAD finding aid collections.  The project, titled EADitor, has been met with interest from the archival and library coding communities.  While there is much work left to do, the application has definite potential.  It is possible to interact with EAD data that resides in the institutional repository, simplifying the process by which the guides are edited and saved back in the repo. Other institutions have developed forms for other metadata standards common to libraries, so my colleague in the Scholars’ Lab, Adam Soroka, and I have started a listserv in order to facilitate better discussion and collaboration between units working on similar projects.  Hopefully, the library community will eventually use XForms applications that streamline the metadata creation process for all XML standards that are commonly used."},{"id":"2009-12-15-neatline","title":"Neatline","author":"bethany-nowviskie","date":"2009-12-15 11:08:14 -0500","categories":["Digital Humanities"],"url":"neatline","layout":"post","content":"I’ve called Neatline, the Digital Humanities Start-Up project Adam Soroka and I began developing in September, a “contribution to interpretive humanities scholarship in the visual vernacular.” Huh? This project will allow scholars (and other stewards of cultural heritage) to create Web-based geospatial and temporal visualizations that build on the rich EAD metadata libraries produce in describing their archival collections and making them more discoverable – but the crucial twist is that we didn’t want to think of our Neatline visualizations as products of the metadata. They’re not brain-dead algorithmic output or some kind of thoughtless expression of the archivist’s (nuanced, but necessarily broad) stance toward historical or literary documents of interest. (Yes, I’m asking for it; bring it on!) In other words, Neatline isn’t about the parsing of placenames and automated population of timelines with data. Rather, we’ve conceived this tool (really, as development proceeds, this approach, because Neatline is emerging as an arrangement of instruments and an attitude toward their use) as a kind of playspace for the scholarly interpretative act . In future posts, we’ll describe our development effort and I’ll delve a little into the conceptual background for Neatline in the Temporal Modelling project I undertook several years ago with Johanna Drucker . In the meantime, you can read about how we’re employing the Omeka plugin framework as a way to handle GIS services for scanned historical maps on the ScholarsLab.org project pages for Neatline-in-progress and our larger Omeka plugin work, or you can check out our dedicated project blog ."},{"id":"2009-12-16-large-files-and-omeka","title":"Large Files and Omeka","author":"wayne-graham","date":"2009-12-16 09:40:05 -0500","categories":["Research and Development"],"url":"large-files-and-omeka","layout":"post","content":"This issue came up for a friend of the Scholars’ Lab today on Twitter, but it’s hard to answer in 140 characters. It’s a question about allowing for larger file sizes in Omeka and there are a few ways to handle this.  (Because we want our new blog to be a combination of thoughtful essays on digital scholarship and quick answers to real-world technical problems, I thought I’d post here.) Since Omeka runs on PHP, this is actually a PHP configuration issue and not something you can currently tweak in Omeka. Basically, you just need to tell PHP to allow larger files sizes that are larger than the default. A very easy way to do this is to edit the .htaccess file that Omeka ships with along the following lines: php_value upload_max_filesize 20971520\nphp_value post_max_size 20971520 I’ll note here that doing things this way only affects your Omeka project. Another way to go about this is to add the above to the Apache configuration that defines from where Omeka should be served. For example: ``` &lt;VirtualHost *:80&gt; ServerName www.coolomeka.org\nDocumentRoot /var/www/omeka &lt;Directory “/var/www/omeka”&gt;\n    Options FollwSymLinks\n    AllowOverride All\n    Order allow,deny\n    Allow from all\n&lt;/Directory&gt; ErrorLog logs/omeka_error_log\nTransferLog logs/omeka_transfer_log\n\nphp_value upload_max_filesize 20M\nphp_value post_max_size 20M &lt;/VirtualHost&gt;\n``` Lastly, you can edit the php.ini file (usually in /etc/php.ini or /etc/php5/apache2/php.ini). Just do a search in the file and change the following settings: memory_limit = 32M\n  post_max_size = 20M\n  upload_max_size = 20M You typically don’t need to reload Apache (as long as you did not edit the Apache configuration file) to get these settings to work. For more info on this, check out these resources Description of core php.ini directives PHP Upload Configuration PHP Directives Related to (Large) File Upload"},{"id":"2010-01-07-calculating-county-to-county-distances-with-gis","title":"Calculating county-to-county distances with GIS","author":"kelly-johnston","date":"2010-01-07 10:40:08 -0500","categories":["Geospatial and Temporal"],"url":"calculating-county-to-county-distances-with-gis","layout":"post","content":"In the Scholars’ Lab we recently worked with a researcher whose study areas focused on several groups of US counties.  Of interest was the distance from every county within a group to every other county in that same group. We used geographic information systems (GIS) software to calculate these distances. GIS software creates, manages, analyzes, and visualizes geographically referenced data.  Environmental Systems Research Institute (ESRI) in Redlands, California, produces ArcGIS desktop, a GIS software suite.  The following examples use ArcGIS ArcMap software version 9.3.1 at the ArcInfo product level.  Instructions for accessing GIS software at the University of Virginia are here: http://guides.lib.virginia.edu/gis .  If you are not affiliated with UVA, contact your local IT support person or ESRI for information on accessing this GIS software. Calculating polygon centroids – When working with polygon features (like county boundaries) in GIS it is often necessary to locate the geographic center or centroid of each polygon as a point feature.  In ESRI’s ArcGIS desktop software the ‘Feature To Point’ tool creates polygon centroids. For more information see the ArcGIS online help for the ‘Feature To Point’ tool: http://webhelp.esri.com/arcgisdesktop/9.3/index.cfm?id=1798&amp;pid=1790&amp;topicname=Feature_To_Point_%28Data_Management%29 Creating a point distance table – Given a set of point features, GIS software can calculate the straight-line distance from each point in the set to every other point in the set.   The output distance table contains one row for each point-to-point combination along with the calculated distance.  In ESRI’s ArcGIS desktop software the ‘Point Distance’ tool creates a point distance table. For more information see the ArcGIS online help for the Point Distance tool: http://webhelp.esri.com/arcgisdesktop/9.3/index.cfm?id=1353&amp;pid=1347&amp;topicname=Point_Distance_%28Analysis%29 Example using the ‘Feature To Point’ and ‘Point Distance’ tools – Given a polygon dataset representing boundaries of a group of US counties, calculate the distance between each county in the group and every other county in the group. After ensuring the county polygon boundary file is projected using a distance-preserving projection, select the county polygons for the study area. Convert the selected county polygons to county polygon centroid points using the ‘Feature To Point’ tool. Generate the point distance table for all county centroid points created in step 2 using the ‘Point Distance’ tool.  Distance is expressed in the linear unit of the input dataset, which is meters in our example."},{"id":"2010-01-27-the-1907-massie-map-of-albemarle-co-is-now-in-the-portal","title":"The 1907 Massie map of Albemarle Co.","author":"dave-richardson","date":"2010-01-27 10:11:57 -0500","categories":["Geospatial and Temporal"],"url":"the-1907-massie-map-of-albemarle-co-is-now-in-the-portal","layout":"post","content":"![The 1907 Massie Map of Albemarle Co., VA](http://static.scholarslab.org/wp-content/uploads/2010/01/Massie1907_thumb500.jpg) The 1907 Massie Map of Albemarle Co., VA While going through our archives of scanned maps, we recently ran across a copy of Frank A. Massie’s 1907 “A new and historical map of Albemarle County, Virginia” [Special Collections, University of Virginia Library], commonly referred to as the Massie map, which contains a wealth of detailed historical information for the county in which the University of Virginia sits. After obtaining a more recent scan of the map from Special Collections and Andrew Curly (of the Library’s Digital Production Services), we georectified the digital map and added it to our geospatial data portal . Accessing the map through our portal ( here or here ) allows you to not only view the map at high resolution in your browser, but also to view the Massie map in Google Earth, as well as being able to overlay the Massie map over other base maps and other geospatial data layers (for example, by making a WMS call to our portal’s server from your desktop GIS; or by pulling the Massie map into your webpage-embedded dynamic map using Open Layers ). Among the many interesting historical features on the maps, are the locations of: The residences and named estates of prominent county landowners, including the oldest house in the county, plus various churches and schools, numerous dams, mills, and quarries, and local geologic and mineral resources; Where Revolutionary war prisoners where held (the Barracks that held Hessian troops and from which Barracks Road was named, as well as where Baron and Baroness de Reidesel resided) plus old militia rendezvous points; Routes and encampments of various military campaigns (notably those of British Lieutenant Colonel Banastre Tarleton’s 1781 raid on Charlottesville and Gov. Thomas Jefferson’s subsequent flight, General T. J. “Stonewall” Jackson’s Confederate Army’s march in 1862, General George A. Custer’s unsuccessful Union Army raid in 1864, and General Philip Sheridan’s Union Army’s march in 1865 along with locations of many of the mills and bridges Sheridan had destroyed); Existing and proposed rail lines of many now-nonexistent railroads; Locations where various murders and river drownings occurred. Additionally, the map contains tables showing city and county data from the 1900 census and a 1906 household survey (listing, among other things: total property values and taxes; numbers of livestock (1000’s of cattle, horses, hogs, and sheep, but only 24 goats!), wagons, watches, clocks, sewing machines, and pianos; number of manufacturers with their capital, number employees, and total wages; farm acreage; and enrollment figures for the white and colored public schools. There’s also an inset showing the major rail lines and railroad junctions for the state of Virginia."},{"id":"2010-02-10-lisa-rosner-the-anatomy-murders","title":"Lisa Rosner: the Anatomy Murders","author":"ronda-grizzle","date":"2010-02-10 07:30:06 -0500","categories":["Podcasts"],"url":"lisa-rosner-the-anatomy-murders","layout":"post","content":"Up the close\nAnd down the stair\nVisualizing the worlds\nOf Burke and Hare [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public.2014484138.02014484145.3381074823/enclosure.mp3”]"},{"id":"2010-02-15-more-on-pandora-genres","title":"More on Pandora:  genres, genomes, and musical taste...","author":"jason-kirby","date":"2010-02-15 04:39:52 -0500","categories":["Digital Humanities","Visualization and Data Mining"],"url":"more-on-pandora-genres","layout":"post","content":"Hello. In my last blog, I began my discussion of Pandora.com, the streaming audio website which offers a new kind of web radio to listeners. Enter a “seed” song into Pandora’s search engine, and the site will create a streaming “station” composed of songs that resemble your seed song. This process is powered by the Music Genome Project, a massive research endeavor which began in the early 2000s and is based out of the company’s Oakland, California headquarters. How is Pandora’s song-recommendation engine different than web radio platforms that came before it? Well, the majority of other online radio stations, such as last.fm, operate off a system called collaborative filtering. What is collaborative filtering? In layperson’s terms, collaborative filtering involves matching one user’s taste to another’s (or a series of other people). On a site like last.fm, over time a user amasses a playlist of songs they’ve expressed a preference for—a sort of musical taste profile. Last.fm’s search tools automatically identify other users with whom your tastes seem to overlap, and uses this information to power “radio” stations you can stream on the site. The process is pretty simple, and based on personal intuition and the data existing users have already entered into the system . Collaborative filtering powers aspects of many media websites, such as Amazon.com’s personal recommendation feature for shoppers. It does have some limitations for online radio listeners, however. As Pandora’s founder Tim Westergren pointed out in a 2006 interview with Leo Laporte and Amber MacArthur, collaborative filtering-powered online radio stations have a tendency to only recommend what is broadly popular in contemporary pop music. While independent-label music certainly has a strong presence on last.fm, a quick scan of various users’ profiles on the site may suggest that Westergren has a point. Even among “indie” users on last.fm, there’s a whole lot of Death Cab for Cutie and Modest Mouse ruling the playlists (nothing against either of these bands). Collaborative filtering doesn’t necessarily ensure that the site’s users will discover truly obscure stuff they hadn’t heard of before. And in keeping with my interest in genre boundaries vis-à-vis Internet radio, in interviews Westergren has attributed the problem to the mainstream music business’ interest in keeping consumers bracketed into genre-specific niches. In the aforementioned chat with Laporte and MacArthur, Westergren cited the “age-old problem in the music industry” wherein a tiny percentage of music released by a given label typically accounts for nearly all its sales—a problem codified by genre boundaries. Pandora, through its Music Genome Project, aims to circumvent this problem, by offering its users a new kind of recommendation engine. As I mentioned in my earlier post, the Music Genome is a systematic endeavor to deconstruct and analyze individual pop songs using over 400 “musical attributes” that the company has identified. These attributes include everything from tempo, to vocal timbre, to harmonic movement—even sound production aspects like echo and reverb. In other words, it is essentially a musicological approach in the strictest sense of the word. The focus is on sound itself, rather than a band’s cultural associations with other bands (as is the case in collaborative filtering). Indeed, Westergren bragged in the aforementioned interview that “when we recommend to you a piece of music, we don’t even know how popular it is.” Instead, what the Music Genome Project entails is the company’s roughly fifty analysts sitting down in the Oakland, CA headquarters and methodically tagging a given song using these 400+ attributes. Westergren has described the process in ways akin to the scientific method, noting that a percentage of songs the analysts deconstruct are reviewed twice for quality. The songs, categorized by attributes, are added to the Project’s over 500,000 songs (and counting) accumulating in the company’s database. Songs sharing a similar musical “DNA” are then automatically matched and linked by Pandora’s search engine when you enter in a “seed” song. Westergren has called the Genome “kind of like a musical taxonomy,” and I don’t think this language is accidental. As Fabian Holt has pointed out about musical genres, “Discourse on the temporal dimensions of categories is saturated with organicist metaphors, as in discussions of how genres are born, how they grow, mature, branch off, explode, and die .”1 Even though Pandora in fact aims to get around genre, it seems to me that this biologic language informs the company’s mission and direction. In any case, as a Pandora user, I have often benefited from the happy accidents occasioned by the way the Music Genome Project works. For instance, I entered in Pandora as a “seed” Bob Dylan’s song “Tonight I’ll Be Staying Here With You”, a lilting, mid-tempo country-rock stroll. The Genome built a streaming station for me that included folk-rocky chestnuts by relatively obscure ‘60s and ‘70s groups like UFO and Earth Opera. It’s likely that I would not have heard about these groups without Pandora, or at least that I would’ve heard about them years from now in another context. In this regard, it seems that Westergren does have something to boast about regarding his claim that Pandora’s search engine connects listeners with “invisible” music in a way that mainstream, genre-bound, multinational music corporations just can’t. On the other hand, there are several notable gaps in the logic and execution of Pandora and the Music Genome Project model. The first gap I feel compelled to point out is a very practical one. Returning to my example of the station based around “Tonight I’ll Be Staying Here With You,” Pandora is skilled in giving a user a lot of what they like. Enter in a twangy rock song like Dylan’s and you’ll get a station with loads of twangy rock songs. But there can be too much of a good thing; namely, I find homogeneity of songs’ tempo an issue on Pandora stations. “Tonight I’ll Be Staying Here With You” is a bit plodding, and I’ve found that over a few hours of playing this station, I mostly get one plodding song after the next. This can be useful in terms of finding hidden gems, but makes for monotonous, even frustrating listening over a span of a few hours. I do know that Pandora makes much of its “Thumbs Up/Thumbs Down” feature, which allows the user to indicate her or his preference for a given song. Pandora’s algorithms will adjust the playlist’s direction (ever so slightly) upon a “Thumbs Down” for a song you don’t care for. In a 2006 interview with the New York Times, Westergren describes this feature as a concession to human subjectivity (within an otherwise “objective” platform), and I agree. The “Thumbs Up/Thumbs Down” feature requires active listening and participation on the user’s part—generally a good thing, I’ll admit. But what if I want to just sit back with a cold beverage and let the music play? The Genome’s platform, as it currently works, seems unable to deliver the ebbs and flows in tempo and musical texture which I enjoy in a good mixtape or college radio show. These kind of practical gaps in Pandora’s service point me toward a larger theoretical problem worth discussing.  In its insistence upon musical sound as the key ingredient for making song recommendations, the Music Genome Project willingly suspends belief in some basic social facts about the way music works. Music is undeniably social, cultural, and political. It’s the soundtrack to our lives as we dance, eat dinner, exercise, commute to work, fall in love, and so on. Music blasts out of loudspeakers at political rallies. We argue with friends over drinks about the relative merit of this or that musical group. And music is always part of a commercial marketplace, even in this age of file-sharing. Given all this, I find Westergren’s claim that a band’s marketplace popularity is “completely irrelevant to what we do” a wee bit disingenuous, or at the very least requiring a willing suspension of disbelief regarding music’s social and marketplace role. The Music Genome Project’s near-exclusive focus on sound itself, coupled with its organicist rhetoric regarding “musical DNA”, seems to suggest the company believes it can map out music in its totality—that it can “crack the code” of music, so to speak. As an aspiring musicologist, this reminds me a bit of another massive scholarly endeavor which worked toward a similar goal of cataloging music: Alan Lomax’s Cantometrics project. Developed by Lomax in the late 1950s and into the ‘60s, Cantometrics was a project wherein Lomax and several co-researchers analyzed the performance styles of (mostly traditional “folk”) songs from hundreds of different cultures around the world, tagging them with a variety of traits. These performance traits, such as vocal timbre, were organized into a computerized system wherein elements of the different musics could be compared. Lomax made the bold claim that one could draw conclusions about the social structure of a given society based on some of these performance traits (societies noted for a certain style of singing were sexually repressive, for instance). Of course, this claim was quite controversial, and has been challenged by other scholars since as overly reductive and essentialist. Fortunately, the Music Genome Project doesn’t attempt to make the connection between music and social structures the way Cantometrics did; indeed, as I said, the Genome Project’s rhetoric seems to deny aspects of the social world, if anything. However, in the desire to systematically categorize and compare different aspects of music, one could say that the Genome Project and Cantometrics spring from a shared wellspring of human curiosity. One issue with this categorizing mission, though, is the problem of sample size. Lomax’s research was criticized for not casting a broad enough net in collecting these comparable performance traits. One could ask similar questions about the Music Genome Project’s scope. As I mentioned, the company’s website points out that its database currently features over 500,000 songs, and counting. This seems like a lot, but how useful is that number, when one considers the thousands and thousands of songs which are released commercially every year? And how can one ensure that multiple varieties, styles, and (yes, even) genres of music are adequately represented within those 500,000 songs? Additionally, Pandora shares potentially problematic assumptions with Cantometrics regarding humans’ ability to fully categorize and catalog the world, to reduce music to its essence. This descends from the Enlightenment idea that our natural world is fully knowable through empirical and objective observation. That bedrock assumption has been the basis for the natural sciences, and one can see its influence on a project like Alan Lomax’s. The problem is: while an empirical observation approach might work well for classifying different varieties of tree frogs, when one wades into the murky waters of human behavior, it’s a lot more difficult to claim objectivity. Indeed, for myself and for a growing number of musicologists and humanities scholars more broadly, it is basically impossible to claim objectivity in one’s understanding of the world. This is not to say that Pandora explicitly makes a claim of total objectivity, on their website or elsewhere. But as with Cantometrics, the fact that the company breaks songs down into discrete components and then makes comparisons and connections based on those components suggests that they believe music is knowable in some objective way. Pandora hasn’t made public a list of its over 400 “musical attributes”, but shares a handful of them on their website’s Frequently Asked Questions page. Some of the attributes they share make a lot of sense, and could even be called “objective”: major or minor key tonality, for instance. But consider an attribute like “headnodic beats”: in its FAQ entry, Pandora’s analysts admit they created the term themselves (it describes hip-hop beats which are strong, but not forceful enough to dance to). Given that probably almost no one outside of the Pandora offices uses this term, it can’t reasonably be called objective. This is not to say that an identification of some subjectivity within Pandora’s research model makes the whole enterprise come crashing down. Rather, I just wish to point out that while company prides itself on the cold objectivity of a computer algorithm choosing your music for you, human beings with subjective viewpoints created the components which power that algorithm. Related to this, both Pandora and Cantometrics raise questions regarding musical gatekeepers, tastemakers, and their authority. Indeed, as ethnomusicologist Steven Feld has mused in response to Lomax’s work, “What are the sources of authority, wisdom, and legitimacy about sounds and music? Who can know about sound? Is musical knowledge public, private, ritual, esoteric?”.2 Many researchers of pop music, pop culture, and genre agree that this issue of who is doing the classifying, categorizing, and ranking is a really important question. For instance, snobby clerks at your local independent record store may decide that Gillian Welch’s music belongs in the “folk” rather than “rock” section of the store. But where does the authority behind their judgment come from?   Their judgment is informed by their life experiences and backgrounds as (mostly) well-educated middle-class white males. These gatekeepers’ judgments are also informed by a deep knowledge of various musical genres: the ability to distinguish glam from punk from grunge, and so on. Since Pandora’s fifty music analysts perform a similar function, I find this aspect of their job paradoxical: though the company seems to pride itself on getting beyond musical genre, these analysts must be extremely well-versed in genre in order to do their jobs well. In a recent video post on Pandora’s blog, Westergren states that the purpose of the Music Genome Project is ultimately to connect musicians with audiences, in ways the traditional music business can’t.3 This is notably egalitarian rhetoric; it works off the assumption that consumers and musicians are empowered enough to seek each other out, and that they don’t need tastemakers dictating what music they should like. As I noted above, however, the computerized system that listeners use to connect with musicians is designed and maintained by a group of (relatively) elite tastemakers. And in Westergren’s public statements about these analysts’ qualifications, I read a certain degree of anxiety over what kind of authority is vested in that role of analyst. In his 2006 interview with Leo Laporte and Amber MacArthur, Westergren pointed out that while all their analysts are regularly-gigging musicians, in order to carry out the depth of analysis required for the Music Genome Project, one really “need[s] an academic background”. Thus, in addition to being a working musician, an analyst employed by Pandora also needs at least a four-year undergraduate degree in music theory. On a practical level, this makes a lot of sense to me. If you’re going to employ folks to analyze songs for you, wouldn’t you want them to have an understanding of musical principles on several different levels? On the other hand, on a theoretical level, Pandora’s insistence on both “street” and “book” smarts from its analysts demonstrates an unresolved subliminal conflict over whether “brains and corporate no-how” or “gut, ‘Id’ feelings” are what shape the music we listen to. Thus, in this way, Pandora and the Music Genome Project struggle with these issues of taste and knowledge hierarchies just like other public pop prognosticators, even as their seemingly objective research platform denies this social fact. This may read as though I am beating up on Pandora, but I hope the position I’m staking out is subtler than that. Rather, I have simply been attempting to point out some slight contradictions of logic within the Music Genome Project’s overall research platform. On a practical, user’s level, I enjoy the site. And to be fair to Pandora’s employees, on a certain level they seem to recognize the issues I am brining up here. For instance, in a recent post on Pandora’s official blog by one of its music analysts, Michael Zapruder likens evaluating songs to judging a baby beauty contest, and then points out, “ The idea that all music is equal and deserves equal rights is somehow fundamentally a democratic idea; as is the corresponding idea that the public, and not some small cadre of experts, is the best judge of musical quality. But the fact that some music not only attracts more listeners, but also seems to mean more to more people over a longer period of time, indicates that there is actually something fundamentally unequal about music as well .”4 In other words, perhaps this issue of taste isn’t an “either/or” problem, but rather a “both/and” one. And by its nature, it’s most likely a problem with no definitive answer. It seems that in his blog entry, Pandora employees like Zapruder are trying to find a practical, everyday way of working around and through this problem—and I can’t fault them for that. Certainly, the academic in me bristles when I see Pandora present something like “headnodic beats” as some kind of objective criteria for judging music. But on a practical level, it seems that these classifications, even if they’re vague (such as “vinyl ambience,” or what have you) are perhaps vague at least partly in the service of the listener’s experience—of trying to match users to interesting new music. It doesn’t seem that the point of the Genome is to categorize musical attributes simply for the sake of categorization. Rather, the point seems to be to put that information to use, making musical connections for the listener. So perhaps it’s a utilitarian reason why the Music Genome cuts certain logical corners on the “objective vs. subjective” question. Ultimately, Pandora’s service rests upon the assumption that sound itself is the only aspect which really matters when analyzing different forms of music. It assumes that sound automatically trumps the sociocultural boundaries of genre, taste, and marketplace. This isn’t true, of course: in the real world we live in, rhetoric surrounding genre and taste guide the musical choices we all make, from Walmart AC/DC lovers to bebop nerds. But the Music Genome Project’s fiction regarding the supremacy of sound is an important, if very one-sided, position to have out there in the world. In fact, it’s almost counter-cultural in a way, because journalists and advertisers often focus so much on image when considering contemporary pop music. Pandora’s vision is a kind of imagined musical utopia, making a particularly 21st-century-specific stand for the importance of musical sound—a stand made possible by the shared cultural resource of the Internet. Finally, closing with an idea my professor Fred Maus pointed out to me, when you’re confronted with enjoying a song you didn’t think you would like on Pandora (you enter in a Turbonegro song as your “seed” and are rewarded with a Poison song, for example), that tells us something important about genre boundaries. Your bemusement proves that musical genres exist. They’re cultural; they don’t hold up to objective scrutiny. And they’re based on something more than just musical sound; they’re built around assumptions that have to do with hierarchies of taste and class. Thus, paradoxically, we can learn quite a bit about the rules of genre from a website devoted to transcending those rules. 1  Holt, Fabian. Genre in Popular Music . Chicago: University of Chicago Press, 2007. Pg. 14. 2  Feld, Steven. “Sound Structure as Social Structure.” Ethnomusicology, Vol. 28. No. 3 (Sept. 1984), pp. 383-409. 3  http://blog.pandora.com/pandora/archives/2009/03/index.html March 15, 2009 entry. 4  http://blog.pandora.com/pandora/archives/2009/02/index.html      February 25, 2009 entry."},{"id":"2010-02-18-digital-therapy-cesaire-and-hawthorne","title":"Digital Therapy: Cesaire and Hawthorne","author":"ronda-grizzle","date":"2010-02-18 07:17:22 -0500","categories":["Podcasts"],"url":"digital-therapy-cesaire-and-hawthorne","layout":"post","content":"Graduate students Alex Gil and Ryan Cordell present their recent work on  digital editions of works by Nathaniel Hawthorne and Aimé Césaire. [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public.2014484138.02014484145.3390392681/enclosure.mp3”]"},{"id":"2010-03-25-mr-voronoi-meet-the-us-state-boundaries","title":"Mr. Voronoi, meet the US state boundaries","author":"kelly-johnston","date":"2010-03-25 10:57:34 -0400","categories":["Geospatial and Temporal","Visualization and Data Mining"],"url":"mr-voronoi-meet-the-us-state-boundaries","layout":"post","content":"In the Scholars’ Lab we are working with remarkably detailed datasets showing changes to US political boundaries over time.  We’ve all been fascinated with visualizations where the familiar outlines of the US states emerge from thousands of boundary changes to their underlying counties over the last few hundred years.  Did you know Virginia once spanned from the Atlantic Ocean to the Mississippi River? We’re developing a new web-based tool for visualizing these historic boundary changes and it’s nearly ready for prime time.  We’ll  announce the beta release here soon. So with the knowledge that US state boundaries have already been subject to drastic change over time, let’s have some fun with geographic information systems to visualize drastic mathematically-induced changes to those familiar US state boundaries. For our experiment, let’s keep all our current state capital cities right where they are since they are laden with the necessary infrastructure of government.  But we’ll move the state boundary lines Voronoi-style so anywhere you travel in each of our new states you’ll be closer to the state capital than any other state capital.  In other words, when you’re standing anywhere inside our newly outlined Virginia, you will always be closer to the Virginia state capital, Richmond, than any other state capital.  That seems very efficient.  Let’s have a look. Here’s that familiar grade-school wall map of the lower 48 US states and their capital cities.   Now let’s tweak the map with GIS software to reconfigure the states, Voronoi-style. Wow, what a difference Voronoi makes. Let’s measure just how much the states have changed in our new layout.   In absolute terms, Utah and New Mexico make the biggest land grabs while Texas and California lose the most real estate.  But as a percentage of their current area, Rhode Island is the big winner ballooning in size by over 240% while Massachusetts shrinks 60%. To visualize the state-by-state changes, Todd Burks from neighboring Clemons Library overlayed the two maps. Intrigued?  Read more about Voronoi and Thiessen polygon GIS techniques."},{"id":"2010-04-06-scholars-lab-newsletter","title":"Scholars' Lab Newsletter","author":"ronda-grizzle","date":"2010-04-06 11:28:19 -0400","categories":["Announcements"],"url":"scholars-lab-newsletter","layout":"post","content":"We’re pleased to announce that the inaugural issue of our monthly newsletter is now available for download. The newsletter will highlight projects currently in progress in the Scholars’ Lab, report on professional activities of the Scholars’ Lab staff, and list monthly events. If you have suggestions for projects that you’d like to see highlighted in future newsletters, please email me at rag9b@virginia.edu . Scholars’ Lab News for April 2010 (PDF version) Scholars’ Lab News April 2010 (text-only version)"},{"id":"2010-04-14-omeka-timeline-plugin","title":"Omeka Timeline Plugin","author":"wayne-graham","date":"2010-04-14 11:19:44 -0400","categories":["Research and Development"],"url":"omeka-timeline-plugin","layout":"post","content":"As part of our ongoing efforts on our Neatline grant, we needed to include a way of displaying temporal information and interacting with other data stored in Omeka. Just about the time we were starting to write this code, CHNM announced their Plugin Rush which pays an honorarium to give folks some incentive to pitch in and develop a plugin or two. Since we were going to develop the plugin anyway, we’re donating this back to the Omeka project, but we thought this might be a good opportunity to talk a little more about the development cycle for Omeka plugins, and hopefully inspire others to get involved. The specifications for the Timeline plugin are wonderfully documented and explained on the Omeka wiki. This actually illustrates a great practice that is all too often ignored…explicitly stating what some software should do. Taking the time to think through the “what” a piece of software should do will save  you time in the long run as it forces you to think about how everything fits together, alleviating ambiguity and allowing you to focus on the task at hand. For this specification, there were two requirements: The plugin should create a helper function for creating a SIMILE  Timeline widget from an array of items.  The helper function should allow you to  specify which metadata elements the time data should come from, as well  as the element that specifies the caption (by default, the Dublin Core  Title element). The timeline should allow for time intervals (start and end dates) and points in  time (singe date). While there are two requirements, it’s a good idea to break this us a little more into individual (atomic) tasks. First, we need to take a look at the SIMILE Timeline Widget documentation . Taking a quick look at their Getting Started guide, this seems pretty straight forward Include the Timeline javascript Create a div container in the HTML view (e.g. &lt;div id=\"timeline\"&gt;&lt;/div&gt; Format the items from Omeka as Timeline Events Add the Timeline.create() call to the Omeka HTML view The specification states that this needs to be a helper function. If you’re not familiar with this term, a helper function is a block of code that does some of the computation for another piece of code. In many frameworks, helper functions aren’t actual objects, but pieces of procedural code that can be accessed from across the application that help add functionality that isn’t exactly proper to place in a model or controller; essentially these are statically accessible functions (in coding terms) that can be called from properly instantiated objects (in this case a view). Now that we have good idea of what the code should do, let’s turn to some actual code. Omeka uses the Zend Framework to keep from doing a lot of repetitive programming, so most of the syntax of what we need to do is driven by how Zend handles PHP. On top of the Zend Framework, Omeka implements its own plugin infrastructure, so there are a few things we need to take in to account in our design. The Zend Framework is a model-view-controller (MVC) framework designed to organize code for maintainability and DRY-ness ( Don’t Repeat Yourself ). One the the hallmarks of most MVC applications is its physical separation of files and functionalities. In the case of Omeka plugins, the hierarchy is generally split into model, view, controller, and tests directory. For the Timeline plugin, since we are developing a helper function, we use a slightly modified directory structure: Timeline\n|__helpers\n|__tests\n|__views We also need some mechanism to tell Omeka about a plugin. This metadata is currently provided in a file called plugin.ini. This file is pretty straight forward, but  let’s go over it briefly: ``` [info]\nname=”Timeline”\nauthor=”Scholars’ Lab”\ndescription=”SIMILE Timeline for Omeka”\nlink=”http://omeka.org/codex/Plugins/Timeline”\nomeka_minimum_version=”1.0”\nomeka_tested_up_to=”1.2”\nversion=”1.1”\ntags=”Timeline, simile, chronology, time, temporal” ``` This file is what the Omeka admin interface uses to display information about your plugin. Two things that may not be initially obvious are the omeka_minimimum_version and omeka_tested_up_to lines. One fact you’ll learn about software development, especially with API development, is as projects mature, the needs of the API grow along with them. You want to be able to mitigate potential issues should the plugin API change by explicitly setting the minimum revision number that your plugin is tested against (you can get older revisions from the SVN tags repo at https://omeka.org/svn/tags/ ). Note: you can run multiple versions of Omeka on your machine for testing by checking out separate versions of the software in you web tree. For instance, you can have localhost/omeka1.0, localhost/omeka1.1, localhost/omeka_trunk . Setting this up is beyond the scope of this post (be sure to set up separate databases), but if you have questions, leave a comment. The next thing we need to do is tell Omeka what to do with our plugin. The top-level plugin.php file contains instructions ( hooks ) to tell the Admin interface what to do when a user installs or uninstalls the plugin. This is where we let Omeka know that items tagged with “Timeline” should use the Timeline Plugin, to set up some logging to help us debug when something goes wrong, and some default routes to help make “pretty” URLs. Now, with all the preliminary setup taken care of, now we can start developing the helper function. First, let’s examine the Controller which tells Omeka what to do when an action is requested by the framework. This is actually a fairly straight forward: ``` <?php\nclass Timeline_TimelinesController extends Omeka_Controller_Action\n\n{\n\tprivate $logger;\n\n\tpublic function init()\n\t{\n\t\t$this->_modelClass = 'Item';\n\t\t$writer = new Zend_Log_Writer_Stream(LOGS_DIR . DIRECTORY_SEPARATOR . \"timeline.log\");\n\t\t$this->logger = new Zend_Log($writer);\n\t}\n\n\tpublic function showAction()\n\t{\n\t\t$this->view->item = $this->findById();\n\t}\n\n}\n```\n\nLet's go over briefly what's going on here. There are some semantics in the way in which these Controller objects are named which are inherited from the way in which Zend handles Controllers. The **Timeline_TimelinesController** follows the convention of the \"package\" (the plugin) name, underscore, plural controller name (to handle multiple controllers), and finally \"Controller\" (which explicitly tells a programmer what function the Object performs). Because this is a Framework, we also want to be able to inherit a lot of behaviors without needing to code them ourselves, which is handled by the \"extends Omeka_Controller_Action\" (this is the base [CRUD](http://en.wikipedia.org/wiki/Create,_read,_update_and_delete) class for Omeka which overrides and extends the [Zend_Controller_Action](http://framework.zend.com/manual/en/zend.controller.action.html) object). The \"important\" part of the Controller code is really the showAction function which sets a variable named \"item\" in the view which contains a reference to the ID of an Omeka object. The rest just sets up a logger to keep track of what's going on.\n\n\n> **Note**: If you run in to problems with this plugin, it is most likely related to logging. For more on this, see the documentation on [Retrieving Error Messages](http://omeka.org/codex/Retrieving_error_messages).\n\n\nNow we can get into the guts of the actual helper function. When you boil down the code, most of this is JavaScript with some strategically placed PHP. What we did is create a helper function named \"createTimeline\" which actually does the work for us. This takes two required items, a div reference to associate the Timeline on your page, and an array of Omeka Items with which to populate the Timeline.\n\n```\nfunction createTimeline($div, $items = array(), $captionElementSet = \"Dublin Core\", $captionElement =  \"Title\", $dateElementSet = \"Dublin Core\", $dateElement =  \"Date\" ) {\n\t\techo js(\"prototype\");\n\t\tglobal $mets;\n\t\t$mets = array($captionElementSet, $captionElement, $dateElementSet, $dateElement);\n\t\t?> &lt;!--  we have to load the script in this funny way because we need to get the tag into the head of the doc\n\t\tbecause of the the funky way Simile Timeline loads its sub-scripts  --&gt;\n\t&lt;script type=\"text/javascript\"&gt;\n\t\tscripttag = document.createElement(\"script\");\n\t\tscripttag.src = \"http://static.simile.mit.edu/timeline/api-2.3.0/timeline-api.js?bundle=false\";\n\t\tscripttag.type = \"text/javascript\";\n\t\t$$(\"head\")[0].insert(scripttag);\n\n\t\tif (typeof(Omeka) == \"undefined\") {\n\t\t\tOmeka = new Object();\n\t\t}\n\n\t\tif (!Omeka.Timeline) {\n\t\t\tOmeka.Timeline = new Object();\n\t\t}\n\n\t&lt;/script&gt;\n\n\t&lt;script type=\"text/javascript\" defer=\"defer\"&gt;\n\t\tOmeka.Timeline.timelinediv = $(\"&lt;?php echo $div;?&gt;\");\n\n\t\tOmeka.Timeline.events = [\n\t\t&lt;?php\n\t\t\tfunction event_to_json($item) {\n\t\t\t\tglobal $mets;\n\t\t\t\treturn \"{ 'title' : '\" . getMet($item, $mets[0], $mets[1]) . \"',\n\t\t\t\t'start' : '\" . getMet($item, $mets[2], $mets[3]) . \"',\n\t\t\t\t'description' : '\" . getMet($item, \"Dublin Core\", \"Description\") . \"',\n\t\t\t\t'durationEvent':false }\";\n\t\t\t}\n\t\t\techo implode(',',array_map('event_to_json', $items));\n\t\t\t?&gt;\n\t\t\t];\n\n\t&lt;/script&gt;\n\t&lt;?php\n     echo js(\"createTimeline\");\n\t?&gt;\n\t&lt;script type=\"text/javascript\"&gt;\n\t\tEvent.observe(window, 'load', onLoad);\n\t\tEvent.observe(document.body, 'resize', onResize);\n\t&lt;/script&gt;\n\n\t&lt;?php } ``` There’s a lot going on here, and there is a mix of PHP in the JavaScript. The first thing is making sure the prototype.js library is included, then declaring a variable named “mets” in the global scope (to make it available to other variable scopes). After we’ve declared $mets, get in to the JavaScript to include on the page and introducing a new JavaScript Namespace (Omeka.Timeline) which allows you to extend this code in other views. The second script block actually formats Omeka items that you’ve called as Timeline Events in the JSON format calling a helper method we also include in the code: ```\nfunction getMet($item, $elementSet, $element) {\n\t $tmp = $item-&gt;getElementTextsByElementNameAndSetName($element, $elementSet);\n\t return addslashes( $tmp[0]-&gt;text ) ;\n} ``` This function returns the metadata for an Omeka item, which is then used the createTimeline’s sub-method of event_to_json to properly construct an event for Timeline. After all the JSON strings are created, we “glue” all the array elements with a comma with the implode function. As you can see, not a lot of code actually needs to be written to add functionality to Omeka. With a little research, and some pointers on syntax, extending Omeka can be done quite quickly and doesn’t require a degree in computer science. If you’re interested in getting started on a plugin, I highly recommend the Omeka dev list ; the community is growing and questions are answered quickly (usually by folks on the Omeka development team) and is a great way to learn about the technical issues surrounding developing software using the Omeka platform. Resources Omeka Plugin API Omeka Developer List Timeline Source Code Timeline Documentation Zend Framework Zend Framework Documentation"},{"id":"2010-04-20-automating-omeka-deployment-with-capistrano","title":"Automating Omeka Deployment with Capistrano","author":"wayne-graham","date":"2010-04-20 15:53:25 -0400","categories":["Research and Development"],"url":"automating-omeka-deployment-with-capistrano","layout":"post","content":"If you’ve done much web development, you’ll know that deploying applications can be a real pain. Typically you get some code (like Omeka), FTP it to your server, run the install, then go grab some plugins and themes and FTP them to your server. If you’re a bit more sophisticated, you may have put this in to an source code management (SCM) system like git, mercurial, or subversion, which then changes your workflow to editing on your local machine, committing the changes to your SCM, logging on to the command line interface for your server, running an update on the code, praying nothing breaks; if it does, you then try to roll back to a working version (you remembered to run svn info on the code before updating so you know what number to go back to). Even if everything goes swimmingly, that’s a lot of steps and way more applications than I like to fool with, and since it’s essentially doing the same thing over and over again, wouldn’t it be nice to automate this process? Enter Capistrano …If you’ve not used this before, essentially this automates the deployment of web applications to your server environment.  It’s written in Ruby, but allows you to deploy ANY type of web application (we use it for Cocoon, Rails, Java, and PHP applications in the Scholars’ Lab). If you’ve got a larger shop, you may also take a look at a web interface called Webistrano which allows non-programmer types to deploy software through a web interface. To show off the power of this software, I thought I’d write up how we use capistrano to deploy Omeka in various environments. The setup can be a little complex, but there are some good tutorials for getting started (see Setting up a Rails Server and Deploying with Capistrano on Fedora from Scratch and the Capistrano Getting Started ). The following code snips assume you have successfully installed capistrano and use Subversion as your SCM (if you need SVN hosting, you can start a new project on Google Code ; you can also use Github if you declare the git scm in the code). The first step in getting your Omeka project automated for capistrano is ensuring both the capistrano and railsless-deploy gems are installed (if you’re not a ruby-ist, gem is a package manager for Ruby applications and libraries): sudo gem install capistrano railsless-deploy Capistrano installs a new command on your system called “capify” which sets up the boilerplate for capistrano. Just execute the capify script in your project directory: cd /path/to/project/trunk\ncapify . This will create two files, Capfile in your root directory and a config/deploy.rb. You’ll need to edit the Capfile ever so slightly to add the requirement for the railsless-deploy gem. It should read as follows: ```\nload ‘deploy’ if respond_to?(:namespace) # cap2 differentiator\n# Dir[‘vendor/plugins/ /recipes/ .rb’].each { |plugin| load(plugin) } require ‘rubygems’\nrequire ‘railsless-deploy’\nload    ‘config/deploy’\n``` Now we just need to do some setup in the config/deploy.rb file to tell capistrano about Omeka. This is where you need to know a little about how your server is set up and you may need to slightly change your server set up in order to use capistrano. The way capistrano works is that it creates a releases directory on your path that holds “deployments” of you project. The latest version of the project is then symlinked the project directory into the releases. This allows you to very quickly undo a deployment that goes awry. As an example, we deploy projects to /usr/local/projects, so our omeka project would get deployed to /usr/local/projects/omeka. Capistano will create a few directories in /usr/local/projects/omeka: releases (timestamped directories of your application) shared (for log files, SCM cache, files you don’t want to be overwritten) current (symlink to latest directory in the releases directory) If you’re setting up Omeka, you will need to redirect the base Directory to the “current” symlink. Here’s the vhost entry we use for Omeka as an example (this is an Ubuntu server; you may need to change the log file path if you are deploying to another operating system). ```\n&lt;VirtualHost *:80&gt;\nServerName your.server.org\nDocumentRoot /usr/local/projects/omeka/current/\n&lt;Directory “/usr/local/projects/omeka/current”&gt;\nOptions FollowSymLinks\nAllowOverride All\nOrder allow,deny\nAllow from all\n&lt;/Directory&gt; ErrorLog /var/log/apache2/omeka_error.log\nTransferLog /var/log/apache2/omeka_access.log\n&lt;/VirtualHost&gt;\n``` Now, to edit the config/deploy.rb file to set things up for automated deployments. ```\n# You must always specify the application and repository for every recipe. The\n# repository must be the URL of the repository you want this recipe to\n# correspond to. The deploy_to path must be the path on each machine that will\n# form the root of the application path. set :application, ‘omeka’\nset :repository, ‘http://your.svn.path/repo/trunk’ set :deploy_to, “/usr/local/projects/#{application}”\nset :deploy_via, :remote_cache\nset :user, ‘deployer’\nset :runner, user\nset :run_method, :run default_run_options[:pty] = true ssh_options[:username] = user\nssh_options[:host_key] = ‘ssh-dss’\nssh_options[:paranoid] = false role :app, ‘www.coolomekaapp.org’\nrole :web, ‘www.coolomekaapp.org’\nrole :db, ‘www.coolomekaapp.org’ =============== # Custom Tasks\n# ===============\nnamespace :deploy do\ndesc ‘Make sure the archives directory has the proper permissions’\ntask :chmod_archive_dir, :except=&gt;{:no_release =&gt; true} do run “chmod g+rw #{current_path}/archives”\nend\ndesc ‘Sets up the intitial db.ini config’\ntask :upload_database_config, :except=&gt;{:no_release =&gt; true} do\ndb_config = «-INI\n[database]\nhost     = “your.db.host”\nusername = “db_user”\npassword = “db_password”\nname     = “db_name”\nprefix   = “omeka_”\n;port     = “”\nINI put db_config, “#{current_path}/db.ini” end desc ‘Move archives directory so it doesn’t get overwritten during deployments’\n task :move_archive_dir, :except=&gt;{:no_release =&gt; true} do\n run “mv #{current_path}/archives #{shared_path}”\n end desc ‘Just svn up the directory’\n task :svn_up, :except =&gt; {:no_release =&gt; true} do\n run “svn up #{current_path}”\n end desc ‘Link archives folder to shared directory’\n task :link_archives_dir, :except=&gt;{:no_release =&gt; true} do\n run “cd #{current_path} &amp;&amp; ln -snf #{shared_path}/archives archives”\n end end ====================== # Task Event Hooks\n# ====================== after ‘deploy:symlink’, ‘deploy:upload_database_config’, ‘deploy:link_archives_dir’\nafter ‘deploy:cold’, ‘deploy:chmod_archives_dir’, ‘deploy:move_archives_dir’\nafter ‘deploy’, ‘deploy:cleanup’\n``` Ok, there’s a lot going on here. I’ll briefly explain what’s going on, but there should be enough here for you to start hacking. But let’s see what tasks capistrano know about and you can call. If you are still in your project directory, just type cap -T to list all the capistrano tasks. Your output should look similar to this: ```\ncap deploy                        # Deploys your project.\ncap deploy:check                  # Test deployment dependencies.\ncap deploy:chmod_archive_dir      # Make sure the archives directory has the …\ncap deploy:cleanup                # Clean up old releases.\ncap deploy:cold                   # Deploys and starts a cold' application.\ncap deploy:migrate                # Run the migrate rake task.\ncap deploy:migrations             # Deploy and run pending migrations.\ncap deploy:pending                # Displays the commits since your last deploy.\ncap deploy:pending:diff           # Displays the diff’ since your last deploy.\ncap deploy:restart                # Restarts your application.\ncap deploy:rollback               # Rolls back to a previous version and rest…\ncap deploy:rollback:code          # Rolls back to the previously deployed ver…\ncap deploy:setup                  # Prepares one or more servers for deployment.\ncap deploy:start                  # Start the application servers.\ncap deploy:stop                   # Stop the application servers.\ncap deploy:symlink                # Updates the symlink to the most recently …\ncap deploy:update                 # Copies your project and updates the symlink.\ncap deploy:update_code            # Copies your project to the remote servers.\ncap deploy:upload                 # Copy files to the currently deployed vers…\ncap deploy:upload_database_config # Sets up the intitial db.ini config\ncap deploy:web:disable            # Present a maintenance page to visitors.\ncap deploy:web:enable             # Makes the application web-accessible again.\ncap invoke                        # Invoke a single command on the remote ser…\ncap log:tail_log                  # Tail the rails log file\ncap shell                         # Begin an interactive Capistrano session. Some tasks were not listed, either because they have no description,\nor because they are only used internally by other tasks. To see all\ntasks, type `cap -vT’. Extended help may be available for these tasks.\nType cap -e taskname' to view it. `` To use a specific capistrano task, you just type the command listed. But let’s get back to the actual script and go over that briefly. Part of the installation process of Omeka requires you to reset the permissions of the archives directory. This is handled by the chmod_archive_dir. However, because of the way that capistrano deploys applications, the archives folder would get overwritten in every deployment. To get around this, we move the archives directory to the shared folder, then create a symlink from the current directory to the shared/archives directory. There’s a task to upload_database_config that you can have in your cap script (we deploy out of private repos), but if you’re deploying out of a public repo, you may want to just put the db.ini file on the server in the shared directory and symlink it into the current_path. Lastly, there are times where you just need to do an “svn up” (or git pull) to update something small and not need to do a full deployment. This is where the cap deploy:svn_up helps out….guess what it does :) Capistrano also provides task even hooks to execute specific tasks after specific events. In this script, when you do a cold deployment (when you are setting things up for the first time), the script will change the permissions on the archives directory, then move the archives directory to the shared directory. When you do a normal deploy (after the directory has gotten a proper symlink), the script will upload the database config, then symlink the archives directory and run a cleanup (keep the last 5 versions you deployed). While there’s still a bit of up-front set up to do on your server, capistrano significantly speeds up your ability to to consistently deploy software, quickly roll-back if (that changes to “when”, if you write code long enough) problems occur, and reinforces a development process that involves SCM! Resources Capistrano Automated PHP Deployment with Capistrano Railsless-deploy Capistrano Tutorials"},{"id":"2010-05-07-gis-the-rare-tartan-plaid-point-dispersion-problem","title":"GIS: The (rare) Tartan-Plaid Point Dispersion Problem ","author":"dave-richardson","date":"2010-05-07 11:07:20 -0400","categories":["Geospatial and Temporal"],"url":"gis-the-rare-tartan-plaid-point-dispersion-problem","layout":"post","content":"Have you ever wondered what would happen to your map of points if while converting your coordinates from latitude/longitude in degrees, minutes, seconds (DMS) to decimal degrees (DD) you messed up the math?  Ever seen a weird tartan-like plaid pattern emerge on your map from points that were suppose to be uniformly spread out over the known extent?  Or wonder why coordinates are much more commonly stored as decimal degrees by computer GIS applications instead of the degrees-minutes-seconds most of us learn growing up?  If so, this blog entry from the Scholars’ Lab at the University  of Virginia Library is for you! First a little digression to explain latitude and longitude, and why computer GISs generally prefer decimal degrees when expressing lat/lon as a coordinate pair. Latitude and Longitude is a spherical coordinate system for describing location upon a sphere (or upon an object that’s approximately a sphere, like the Earth).  Just as there are 360 degrees in a circle, there are 360 degrees of longitude (numbered 180 W (-180) to 0 to 180 E (+180) on either side of the Greenwich Prime Meridian) and 360 degrees of latitude (numbered 90 S (-90) to 0 to 90 N (+90) from the south Pole to the Equator (the Prime Parrallel, so to speak) to the North Pole… and back again on the other side of the globe, to complete the circle).  Each degree can be subdivided into 60 minutes, and each minute subdivided into 60 seconds.  Further subdivision of seconds is expressed as fractions or decimals.  Thus you could express the geographic location of the Scholars’ Lab at UVA as being at 38º 02’ 12.3540” N, 78 º 30’ 19.7928” W (or +38º 2’ 12.3540”, -78 º 30’ 19.7928”). Computer GIS programs all want the northing and easting coordinate pair saved as just two numbers (one number for latitude, one number for longitude) instead of three different fields to contain the degrees, minutes, and seconds for latitude and another three fields for longitude.  This makes it much easier for the computer to plot location.  Many GIS programs also prefer the coordinates to be ordered longitude, latitude since that mimics X, Y coordinates.  Since there are 60 minutes in a degree and 60 seconds in a minute (or 3600 seconds in a degree [60 x 60]), you could write the location of the Scholars’ Lab as -78 – (30/60) – (19.7928/3600), 38 + (02/60) + (12.3540/3600) which is -78.505498°, 38.036765° in decimal degrees. Now back to the emergent tartan-plaid problem.  What would happen if instead of converting to decimal degrees, you simple wrote out the degrees-minutes-seconds numbers in the format DD.MMSSssss ?  The Scholars Lab location would become -78.30197928, 38.02123540.  But the computer GIS would still interpret this as decimal degrees, and would compress all points falling within a 1x1 degree box into just the first 6/10ths x 6/10ths of the box, with a gap without points filling the rest of the box.  Spread over a large region, this would result in a tartan plaid-like pattern emerging. So what is supposed to look like this: Would end up looking like this: And both the correct and incorrect versions together: Not that this is a common mistake people using GIS make by any stretch, but when someone has a question about why their points are all coming out misaligned with strange empty striping patterns, it can take a little while to deduce what’s going on if you’ve never seen the results of such a mistake before."},{"id":"2010-05-07-introducing-davila","title":"Introducing DAVILA","author":"jean-bauer","date":"2010-05-07 11:10:46 -0400","categories":["Digital Humanities","Grad Student Research","Visualization and Data Mining"],"url":"introducing-davila","layout":"post","content":"Jean Bauer, former Scholars’ Lab Graduate Fellow in Digital Humanities announces: “I have just released my first open source project.  HUZZAH!” DAVILA is a database schema visualization/annotation tool that  creates “humanist readable” technical diagrams.  It is written in Processing with the toxiclibs physics  library and released under GPLv3.  DAVILA takes in the database’s schema  and a pipe separated customization file and uses them to produce an  interactive, color-coded, annotated diagram similar in format to UML.   There are many applications that will create technical diagrams based on  database schema, but as a digital humanist I require more than they can  provide. Technical diagrams are wonderfully compact ways of conveying  information about extremely complex systems.  But they only work for  people who have been trained to read them.  If you design a database for  a historian, and then hand him or her a basic E-R or UML diagram, you  will end up explaining the diagram’s nomenclature before you can talk  about the database (and oftentimes you run out of time before getting  back to the research question underlying the database).  This removes  the major advantage of technical diagrams and can also create an  unnecessary divide between the technical and non-technical members of a  digital humanities development team. I have become fascinated by how documenting a project (either in  development or after release) can build community.  I’m not just talking  about user generated documentation (ala wikis), but rather the feeling  created by a diagram or README file that really takes the time to  explain how the software works and why it works the way it does.  There  is a generosity and even warmth that comes from thoughtful, helpful  documentation, just as inadequate documentation can make someone feel  stupid, slighted, or unwanted as a user/developer.  I will be writing on  this topic more in the months to come (perhaps leading up to an  article).  In the meantime, check out DAVILA and let me know what you  think. Project homepage: http://www.jeanbauer.com/davila.html"},{"id":"2010-05-07-julie-meloni-n-dimensional-archives","title":"Julie Meloni: N-dimensional Archives","author":"ronda-grizzle","date":"2010-05-07 12:03:11 -0400","categories":["Podcasts"],"url":"julie-meloni-n-dimensional-archives","layout":"post","content":"Julie Meloni, Jerome McGann, and Bethany Nowviskie discuss ways of reconsidering the multivalent cultural record in a digital age [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public.2014484138.02014484145.3871948813/enclosure.mp3”]"},{"id":"2010-05-11-why-ruby","title":"Why Ruby?","author":"wayne-graham","date":"2010-05-11 11:27:02 -0400","categories":["Research and Development"],"url":"why-ruby","layout":"post","content":"Stemming from a Twitter conversation last month, I thought it would be a good idea to describe – in more than the 140 character bursts that Twitter allows – why we at the Scholars’ Lab often promote Ruby, opposed to one of the other 4 or 5 languages we develop with. This isn’t an attempt to declare one language “the best,” but is meant to lay out some of the fundamental reasons why we use Ruby in the context of our digital humanities work and why we think it’s a nice language to suggest to folks just starting to program. Qualification People often think of the Scholars’ Lab as a Ruby (and Rails ) shop, which isn’t quite the case. We code in many different languages.  In the past several moths, we have written The Mind is a Metaphor in Ruby (with Rails). For Better For Verse uses Wordpress with TEI and JSP, and some more recent work on a William Faulkner audio archive employs Cocoon with Solr . In addition to those collaborations with UVa faculty, we’ve been writing plugins for Omeka (and dusting off our PHP skills) and have created a discovery interface for our GIS infrastructure in Ruby (with Sinatra ). If you analyze the technologies we currently deploy, it turns out we use Cocoon + Solr more than anything else, though we’re starting to move away from that particular stack as our approach for tool development. The Scholars’ Lab has a lot of experience with all types of languages, and depending on the circumstances, we choose different tools to accomplish any given task. However, after quite a bit of time helping people get started on a programming path, I’ve come to appreciate some of the features Ruby provides in getting new programmers up to speed. Learning Curve Every language has a learning curve. However, once you get the hang of some of the basics of computer languages (flow control, data structures, objects, etc.), the biggest differences come from syntax. All web languages make certain programming exercises easy, and once you buy in to the way in which that language handles programming constructs, moving between languages for experienced programmers becomes a simpler exercise in exploring syntax and built-in functionality. As one of my computer science professors posited, generally the first language you learn governs the way you code until something significantly better comes along. For a lot of folks getting in to programming for the first time, this usually means either taking a class or finding someone to show you the basics. If you’re in higher education, this has typically meant the tool of choice is PHP. However, having seen the look of bewilderment on the faces of enough graduate students and faculty members as I attempt to explain the difference between sprintf and printf (printf returns the length of the formatted String and sprintf returns the formatted String), I’ve come to believe that the syntax of a programming language (and it’s readability) is an exceptionally important part of a language, especially when teaching basics of software construction. Method Chaining Without getting into how the PHP and Ruby duck-type primitive data types and data structures, one big difference in syntax between the two is how one combines multiple method calls together. Ruby uses method chaining for objects whereas PHP uses “bolted-on” functions (think of these as order-of-operations from your high school Algebra class). Let’s look at this brief example of addressing and sorting an associative array in PHP and its equivalent in Ruby: $projects = array(\"solr\" =&gt; 4, \"php\" =&gt; 1, \"rails\" =&gt; 2, \"jsp\" =&gt; 3);\n$keys = array_keys($projects);\nsort($keys);\n$sorted = array_slice($keys, 0, 3); If you’re a little more advanced, you might refactor (rewrite) the code to look more like this (methods anonymously “bolted-on” to one another): $projects = array(\"solr\" =&gt; 4, \"php\" =&gt;  1, \"rails\" =&gt; 2, \"jsp\" =&gt; 3);\n$sorted = array_slice(sort(array_keys($projects)), 0, 3); Now, the same examples in Ruby syntax: projects = {\"solr\" =&gt; 4, \"php\" =&gt;  1, \"rails\" =&gt; 2, \"jsp\" =&gt; 3}\nsorted = projects.keys.sort.slice(0,3) Or, even more concisely: projects = {\"solr\" =&gt; 4, \"php\" =&gt;  1, \"rails\" =&gt; 2, \"jsp\" =&gt; 3}\nsorted = projects.keys.sort[0..3] I’ve found that Ruby’s method chaining syntax makes more sense to new programmers than the more mathematical “bolted-on” syntax. Blocks Ruby has a neat construct that you use all over the place to create anonymous functions (a technical term for creating specific functionality without defining a new function to define the action). Let’s take a function to sort an array of projects. First, in PHP: ``` function sort_projects_by_count($a, $b)\n{\n    if($a -&gt; counts == $b -&gt; counts)\n    {\n        return 0;\n    }\n    return($a -&gt; counts &gt; $b -&gt; counts) ? +1 : -1;\n} usort($projects, “sort_projects_by_count”); ``` And the same thing in Ruby: projects.sort do |a, b|\n    a.counts &lt;=&gt; b.counts\nend Ok, so this is a bit of an unfair comparison, but here is an analogous version of the Ruby code in PHP: usort($projects, create_function($a, $b, 'if($a-&gt;counts == $b-&gt;counts){\n    return 0;}return ($a-&gt;counts &gt; $b-&gt;counts ? +1 : -1)); No matter how you slice it, Ruby syntax just feels more human. Even if you don’t know exactly what’s going on, looking up one operator in the Ruby syntax as opposed to following the logic flow and determining what “? +1 : -1” means (it’s shorthand for an if-then statement) makes the act of reading code much easier. Monkeypatching If you’re not familiar with the term, “monkeypatching” is what programmers call changing or extending a base class (like an array or string object) to add functionality or change the way it works. Let’s say you really need to be able to test a string to see if it looks like an integer, you could create a monkeypatch along these lines: class String\n    def is_int?\n        self =~ /^[-+]?[0-9]*$/\n    end\nend This code snip extends the String class and uses a Regular Expression (regex) to test if a given String is an integer (number) by simply calling “is_int?” (notice the question mark at the end of the definition; this is used for methods that return a Boolean value). That’s a little advanced, but it does show off a very useful piece of functionality of the language that allows you to do a better job dealing with a duck-typed language . Frameworks Many people when talking about Ruby associate its use in web development with the Rails MVC framework . Just as PHP has Zend, CodeIgniter, CakePHP, symfony, etc., Ruby has Rails, Merb, Sinatra, Camping, and many more. Rails is the 900-pound gorilla of Ruby frameworks, and has a lot of nice features to get new applications off the ground quickly and some really great online guides to setting things up (I frequent RailsGuides ). Since we often suggest Rails to our collaborators I’ll focus on this framework, but there are several other frameworks out there to choose from. Think of Ruby (or PHP for that matter) as a pile of building materials: you can to build anything you want if you know how to put everything together. Rails, on the other hand, is like a prefab house where workers pour a foundation, set the house up, and then leave you to add the drywall, siding, windows, and roof. If you need a new component for your prefab house, a sales representative is standing by to help immediately ship you what you need. Generators To extend the previous metaphor, generators are like sales representatives that allow you to place orders for new aspects of your site.  To create all the erb templates (the default templeting language for Rails), controller methods, model, database migrations, routes, and tests for a new project with a single line, you might run something like the following: script/generate scaffold topic title:string description:text I moved away from using scaffolding pretty quickly, but it does provide a nice starting point for new programmers to build interactions with data models. Templates The default templating engine in Rails is erb which provides a convenient method for generating views of  your models. One of erb’s most important features is the use of “partials,” pieces of code that are used in multiple views by calling the render method. I often replicated this behavior in PHP by calling an include somewhere in a view. ActiveRecord The key to database interactions in Rails is ActiveRecord. As an SQL expert, I have to admit this part of the Rails framework drove me a bit batty at first, but then again I’ve been writing SQL for over 10 years (my colleagues often roll their eyes when I start writing it with relational algebra nomenclature), so allowing a framework to abstract this particular piece took some getting used to. If you’re new to programming, though, this means you don’t have to learn SQL but instead can use Ruby-style syntax to interact with your database without necessarily needing to care what your RDBMS back-end happens to be. Take this example of looking up a book review where you have both a “book” and “review” table. In PHP you would do something like this (this snip will only work with a MySQL connection but has some sub-selection stuff going on): ```\nfunction query($sql)\n{\n    global $conn;\n    return mysql_query($sql, $conn);\n} function recent_reviews($count)\n{\n    $query = query(sprintf(‘SELECT b.book_title, r.review_id, r.created_at, r.id, review_counter.review_total\n        FROM reviews r, books b,\n            (SELECT count(*) AS review_total FROM reviews) AS review_counter\n        WHERE r.book_id = b.book_id\n        ORDER BY created_at DESC\n        LIMIT %d’, $count); return $query } print_r(recent_reviews(5));\n``` Now, for the ActiveRecord equivalent: reviews = Review.find(:limit =&gt; 5, :order =&gt; \"created_at DESC\");\nputs reviews.inspect Because of the way in which ActiveRecord sets up its model associations, you’ll have access to the different name scopes to print out the same information, just in far less code. However, if you really want to, you can pass your SQL to get more granular control over the syntax: reviews = Review.find_by_sql(\"SELECT b.book_title, r.review_id, r.created_at, r.id, review_counter.review_total\n        FROM reviews r, books b,\n            (SELECT count(*) AS review_total FROM reviews) AS review_counter\n        WHERE r.book_id = b.book_id\n        ORDER BY created_at DESC\n        LIMIT ?\", count); One of the real beauties of the ActiveRecord methods is that as long as you’re using the generic ActiveRecord syntax, your data persistence layer can be pretty much any RDBMS and be changed with a couple lines in the configuration file. The trade-off however, is that you lose a few things and can make slightly more work for yourself than you might anticipate. One important caveat is that ActiveRecord doesn’t create foreign keys when you set up reference fields. This is actually by design as it’s using an object-oriented idiom (an object should validate the presence of another, without the underlying persistence layer enforcing any type of constraint), but I find myself adding these in to ensure that the RDBMS takes advantage of the pre-calculated indexes to improve overall performance. I should also mention that I think the ActiveRecord model has some real limitations. As you develop your models, you will most like be tweaking its fields, which in turn requires new migrations, and you may forget which fields are actually in your models. There are plugins that help with this, but you do need to take additional steps to have this information placed somewhere convenient (I use a pre-commit git hook that calls the annotate gem to dynamically annotate my model schemas). Security You’ll notice in the last examples I was doing some funny stuff in both the PHP and Ruby examples to protect against SQL injection attacks . If you’re using the ActiveRecord methods of addressing objects, Rails will take care of this for you. If you’re using PHP, you’ll either need to do this yourself (sprintf is commonly used) or rely on a framework to parametrize your statements (you don’t want someone deleting everything in your database). You also need to protect yourself from Cross-site Scripting Attacks (XSS) by escaping HTML from fields with dynamic content. erb has a helper function html_escape (h is the shorthand) which escapes this data. In Rails 3, this will change slightly and erb will automatically html_escape model output unless you explicitly tell it not to escape the field. One less thing to remember! Testing Testing is important, and I try to preach its virtues every chance I get. After finishing a project, the typical programmer won’t touch the code again until the application breaks. Good testing will save you (or the person that inherits the code) a lot of time discovering exactly what broke the application. Let’s face it: testing is a pain in PHP, and I rarely see it done well. What I’ve really enjoyed about Ruby development is the fact that no matter how you code there is an appropriate testing framework available (I use rspec ). There’s a strong emphasis on not only unit testing, but also integration and acceptance testing. There are also libraries that give you an idea of how well your code is tested, something I sorely missed from my Java coding endeavors. One other huge plus is that every gem on the rubygems.org site includes test-coverage metrics to give you an idea of how well the code you want to install is tested. PHP also has testing frameworks with PHPUnit and PHPSpec being rather popular. I won’t say too much about the PHP testing frameworks other than to say that there are analogous frameworks for writing and running tests in PHP and Ruby. However, I’ve noticed a slightly more concerted effort to think through the inclusion of the tools in Ruby and their integration into the coding workflow than I’ve experienced with PHP. With the latter language, I’ve often fell in to the trap of writing the code, getting it to where I want it, and then, really as an afterthought, writing basic unit tests to get rather skimpy code coverage. As a case in point, a mantra in Rails development is TATFT . BryanL on TATFT from Bryan Liles on Vimeo . Deployment Two typical objections raised when contemplating Ruby development are that “Ruby doesn’t scale” and that server setup is a real pain. These are valid concerns, but as with many open source projects with a large number of fanatical supporters, the Ruby community has steadily made improvements in these areas. Actually, for the vast majority of our readership, these issues can be filed away in the solved category. Scaling I’ve always found objections to scaling a bit troubling. Scaling is one of those over-used terms that means different things to different people, but most of the “Rails doesn’t scale” comes from Twitter’s experience with the framework. They found, as they scaled horizontally (adding more servers) to handle loads of 11,000+ requests per second, that a bottleneck existed at the data persistence level as Rails doesn’t, by default, provide a mechanism to to address multiple databases. Twitter has since moved parts of their code base to Scala but has retained the majority of their code in Rails and has developed some rather ingenious messaging capabilities to talk to the appropriate abstraction layers that one needs in very large enterprise applications. While Twitter shows that Rails is capable of scaling (with lots of work), quite honestly the likelihood of any of our applications needing this level of engineering is slim. I will say, however, that there are relatively simple methods of scaling with your infrastructure should you start running into performance issues. We have, for example, an application written in pure Ruby on Rails deployed as a Tomcat application (the details of which are completely outside the scope of this article, but the application gets all the benefits of an Enterprise class Java environment with the ease of Rails development). Server Support The Ruby language is included in most (if not all) modern Linux package systems and makes installation a snap. The other “major” piece of software you’ll need is RubyGems (a package manager for Ruby libraries), which is also generally available as a managed package. Note : There is a major change occurring with the development of Rails 3. Rails is moving from a system of system-wide gems to application-level gems with the introduction of GemBundler . This approach is a more stable method of deploying application requirements which not only allows you to ensure that application libraries are properly resolved, but also provide better granular control over which libraries are deployed in specific contexts. There was a time where deploying Rails applications was a real bear. Then along came Phusion Passenger (aka mod_rails). This allows you to run Rails (actually any Rack-based application) through Apache and Nginx without any other port management, service process monitoring, file cleanup, etc. As long as Apache is running, so is your Rails app! Community of Support The Rails community is pretty great in getting folks off the ground. As with any technology there are a fair number of curmudgeons, but leaders in the community as quick to remind people to be nice (see Yahuda Katz’s The Blind Men and the Elephant: A Story of Noobs ). There are several corporations backing Rails development ( EngineYard is a big one) and when the Google Summer of Code for Rails program wasn’t continued, the Rails community was able to raise $100,000 in three days to support a Ruby Summer of Code . There are a number of really good podcasts ( Ruby5 and The Ruby Show are good), vodcasts ( Railscasts ), tutorial sites ( ACSIIcasts ), blogs ( RailsDispatch, ThoughtBot, EngineYard ), open source projects (lots on github ), open source books ( Rails Tutorials ), and some really good reference books from The Pragmatic Programmers . There’s even some humor… Summary As much as it drives language purists crazy when I say it, there’s nothing that you can do in Ruby that you can’t replicate in PHP (and vice-versa). In my experience, getting people set up with a functioning web application is far easier with Ruby than it is with PHP (and less prone to spaghetti code ), and the deployment options make Ruby (plus a framework) a great starting point for new programmers to get their feet wet with web application development (check out Heroku ). If you’re an experienced developer, does it make sense to drop PHP and rewrite your code base? Absolutely not. However, at some point, you will be faced with the prospect of needing to migrate a legacy application where Ruby may make a lot of sense. As someone who has spent quite a bit of time de-tangling spaghetti code from Perl CGI and mixed HTML and PHP pages, I’m hoping that the people that will eventually be migrating my Ruby code will not need to perform the level of coding archaeology we’ve needed to perform. Like most things in life, choosing the correct tool for the job needs some careful consideration and planning. Ruby makes a lot of sense for getting applications off the ground quickly and reinforcing good practices like testing, code separation, and readability that I find important in forming new digital humanities programmers. Web development over the last decade has become exceptionally complex (AJAX, web services, web standards, multiple browsers, etc.) and the real hope is that by using Ruby and Rails as an approach, people will be inspired to continue down a development path to both enrich their own scholarship and impact the larger digital humanities community without becoming frustrated by syntax. This is a bit of an experiment which we and other digital humanities shops are undertaking, and in which we’re inviting everyone to participate.  No matter the language, we should all be engaged in teaching best practices in project design and management, in software development techniques, in the construction of usable and elegant interfaces, and in the application of these things to humanities scholarship, through which everyone wins! Other Resources Rails for PHP Developers 7 Reason I switched back to PHP after 2 years on Rails RubyGuides JRuby"},{"id":"2010-05-17-frontiers-in-spatial-humanities","title":"Frontiers in Spatial Humanities","author":"bethany-nowviskie","date":"2010-05-17 14:11:29 -0400","categories":["Announcements","Digital Humanities","Geospatial and Temporal"],"url":"frontiers-in-spatial-humanities","layout":"post","content":"[UPDATE: video for the “Frontiers” event is now available!] We’re crowd-sourcing the keynote to the final round of the Scholars’ Lab/NEH 2009-2010 Institute for Enabling Geospatial Scholarship .  With all of these fantastic attendees on hand – not to mention the Institute faculty – how could we let the opportunity slip by? Frontiers in Spatial Humanities: Lightning Presentations We are pleased to host 40 rapid-fire, 2-minute demos of boundary-pushing projects in spatial humanities.  The scholars presenting their work come from 27 different institutions, and were competitively selected to attend this prestigious program, funded by the National Endowment for the Humanities .  Some of our Institute faculty will also offer brief glimpses of their work as part of a whirlwind tour of emerging work in humanities GIS. While admission to the Institute itself is now closed, “Frontiers in Spatial Humanities” and the reception that follows are open to the public! I’d like to thank the NEH for its generous funding of our training program, and the University of Virginia Library for supporting the Scholars’ Lab – as well as the “Frontiers” reception, to which you’re all invited! Thursday, May 27th, 3:30-5:00pm\nHarrison-Small Auditorium For more information about the SLab and our NEH-funded Institute for Advanced Topics in the Digital Humanities, please visit: http://scholarslab.org/geospatial/"},{"id":"2010-06-01-frontiers-in-spatial-humanities-video","title":"Frontiers in Spatial Humanities (video)","author":"joe-gilbert","date":"2010-06-01 05:53:38 -0400","categories":["Announcements","Digital Humanities","Geospatial and Temporal","Podcasts"],"url":"frontiers-in-spatial-humanities-video","layout":"post","content":"A video stream of the final event of our NEH-funded Institute for Enabling Geospatial Scholarship (or #geoinst as it’s known on Twitter) is now available!  Thanks to all our wonderful participants for making these lightning talks, collectively entitled “Frontiers in Spatial Humanities,” so thought-provoking. The Scholars’ Lab/NEH Institute for Enabling Geospatial Scholarship was held at the University of Virginia Library May 25-27, 2010 and concluded with a set of two-minute, three-slide lightning talks by Institute attendees on their own spatial humanities projects and works-in-progress."},{"id":"2010-06-17-expanding-the-capabilities-of-omeka","title":"Expanding the Capabilities of Omeka","author":"ethan-gruber","date":"2010-06-17 10:00:59 -0400","categories":["Research and Development"],"url":"expanding-the-capabilities-of-omeka","layout":"post","content":"Because I have a keen interest in the description of cultural heritage artifacts and in doing interesting things with metadata, in recent months I have developed a handful of Omeka plugins to meet these interests.  My first foray into plugin development for the application was with the EAD Importer .  The EAD Importer, as the name suggests, extracts item-level metadata (along with a bit of collection-level metadata, like rights) from Encoded Archival Description finding aids and generates a CSV file which can be imported through the CSV Import plugin developed by the Omeka crew.  The plugin would be useful to archivists who would like to use Omeka to build online exhibits of their collections.  I took this framework a step further to create a plugin that is capable of importing any flat XML into Omeka by transforming that file into a CSV file. Most recently, I have turned my attention to expanding the descriptive abilities of Omeka into the realm of collections of artwork.  Omeka items are described with Dublin Core, which is capable of describing anything, though not particularly well.  I developed VraCoreElementSet, which incorporates VRA Core fields into the Edit Item form.  VRA Core is a much more semantically appropriate schema for describing art and artifacts.  Since it was conceived as an XML standard (not strictly a flat list of fields), some elements have hierarchical sub-componenets.  For example, a work may have several agents involved in its production, and each agent has a name as well as a role, culture, birth date, and, as the case may be, a death date.  The VraCoreElementSet plugin creates a table for agents so that a user may enter this data separately.  Then in the Edit Item form, the user may select VRA Core agents from a drop down menu restricted by the records in the agents table.  Items may also be exported to schema-compliant VRA Core XML.  There is still some work remaining on this plugin, but it is well on its way toward completion. Now that the Scholars’ Lab has contributed EAD Importer and VRA Core Element Set plugins, Omeka may attract new institutional users from the library, archive, and museum fields, who may have otherwise settled for proprietary applications to disseminate their digital collections."},{"id":"2010-06-30-wms-vs-tilecaching","title":"WMS vs. tilecaching","author":"adam-soroka","date":"2010-06-30 03:59:35 -0400","categories":["Geospatial and Temporal","Visualization and Data Mining"],"url":"wms-vs-tilecaching","layout":"post","content":"In our work on Neatline, we have made a deliberate choice to start by constraining ourselves to map-sources that are quickly and easily provided through WMS . This leaves out (for now) two popular sources of map imagery; Google Maps and Open Street Map . I’m going to explain why we made that choice, and why, when we do come to make these sources usable with Neatline, we will do so with great care and with an eye to scholarly method. All two-dimensional maps (as opposed to globes) are projected . That is, the curved three-dimensional surface of the Earth is transformed onto a flat two-dimensional surface. This can be done in an infinite variety of ways, many of which have been mathematically characterized and named by cartographers, for whom they are necessary tools. We must note, however, that no such transform can obtain a perfect representation of a section of the Earth. The mapmaker must choose which qualities to preserve and in what measures. Is it more important to provide an accurate depiction of relative areas or of relative lengths? Is the area around Greenland to be kept in the focus of accuracy, or that around New Zealand? Each map therefore carries with it from its creation certain choices like these, part of the arguments the map makes about the world by its very construction. We chose WMS on which to start building our tools because, amongst other reasons, it allows for the transmission of projection information as part of its operation. This fact allows us to produce imagery from historical maps (themselves in any number of projections) and maintain the original choices the mapmaker made. Google Maps and Open Street Map are not WMS sources. They can be described as tile caches, huge reservoirs of rendered imagery. As such, they offer their own choices about how the world is to be projected. (Google’s choice has become so closely associated with Google that it is known widely as “the Google projection”.) Now we come to an important technical distinction; WMS services are able (depending on the capabilities of the specific software in use) to reproject their contents. That is, in response to a specific request for imagery, they can produce the imagery in a projection different from the one in which it was stored. GeoServer, the software we are using for Neatline, has a library of thousands of projections to which users can add more as desired. This allows us to take imagery from a WMS source and lay it under a historical map layer while maintaining the original projection for that of the map as a whole. Tile caches, by and large, do not allow for this. (Google Maps offers its one projection, and Open Street Map offers two.) This means that in order to lay historical map imagery over a layer from one of these sources, we would have to reproject the foreground (historical imagery) overriding the choices of the mapmaker and introducing additional choices of our own about what facets of the geographies at stake are to be preserved and which abandoned. (Neogeographers will remark that georectifying a digital image introduces similar issues. This is true, but unavoidable for our purposes. We would like to avoid compounding the matter in a way that is subtle and hard to detect.) We are working out means by which we can provide the undeniable utility of popular tilecaching services in a way that is respectful of the historical context and story of map artifacts. Until we do, we will continue to concentrate on the more flexible and sophisticated apparatus provided by WMS."},{"id":"2010-07-23-on-xforms","title":"On XForms","author":"ethan-gruber","date":"2010-07-23 13:08:30 -0400","categories":["Research and Development"],"url":"on-xforms","layout":"post","content":"Several months ago, I wrote a post about my XForms development in the Scholars’ Lab as part of a research project. I’m currently working on two research projects that utilize the standard: EADitor (Encoded Archival Description management and dissemination framework) and Numishare (geared towards online delivery of numismatic collections, though other artifacts can be represented). Despite its promise, XForms has not quite swept up the library world yet (though it is most definitely generating some buzz). The W3C standard is a definition for creating dynamic webforms that handle complex, hierarchical XML data–the type of stuff libraries deal with daily. However, only in recent years have XForms processors matured to the point they are ready for mass-market consumption. There are numerous private firms developing XForms applications, including Wachovia, Cisco, and Pfizer. It is also used to some degree in the academic community. As far as I am aware, not many institutions are running it in production, though some are rapidly moving in that direction. The XForms4Lib listserv created in the fall has 80 members from across North American and European academia. Which brings me to my point. Matt Zumwalt, active code4lib member and Ruby on Rails/institutional repository developer, boldly declared XForms to be dead . I offer this critique: There are some inaccuracies in this post that I would like to address. First of all, HTML 5 forms do not supplant XForms as an option for collecting user inputted data. HTML5 is much simpler, and thus has broader appeal. XForms enables the creation of much, much more complex models, with far more sophisticated controls and validation. Moreover, if XForms was a dead language in January 2008, with the release of the HTML5 specification, and that IBM had dropped support, then why do you suppose the XForms 1.1 specification was released in October 2009, edited by a representative of IBM? No, XForms is very much alive. It has a small, but very active community, which is especially visible with the Orbeon development community. XForms is best used as a definition of dynamic forms that are processed server-side, not in the browser (which pushes a lot of processing demand onto the user, which isn’t good). There are some good, open source frameworks out there. Orbeon is the best, and has many users from both industry and academia, including Pfizer, Leap Frog, Wachovia, UCSB, Stanford, and the National Archives. In fact, Orbeon XForms applications form a large part of the enormous workflow of the NARA Electronic Records Archives project, which is a multi-year project contracted to Lockheed Martin and has a financial backing of close to a half a billion dollars (I have heard). XForms, dead? A lot of the design flaws you describe are in actuality implementation flaws. Development of a Rails-based framework seems to me like an enormous waste of time and money. You can adapt the MODS editor developed by Brown to such a task. It has already been proven that you can interact with metadata delivered through REST from a Fedora repository. And MODS is fairly simple as a a metadata standard. Care to take a stab at TEI or EAD? When you began your research in 2007, Orbeon was a fairly young application. But the standard and its delivery and processing applications have evolved since then. Only in the last two or three years has XForms grown into a viable solution. Moreover, since it is a W3C standard, you can pick your forms up and migrate them to a new framework fairly easily. Is your Rails application sustainable in the long term? Are today’s jQuery functions going to work in 2015’s browsers? These are things you need to consider when contemplating a web form standard.&lt;/blockquote&gt; Fedora is a Tomcat application. So is Solr. So is adore-djatoka, which UVA/Hydra utilizes for jpeg2000 delivery. And so is Orbeon. ActiveFedora and any Rails-based MODS editor seem to me like the third wheel in the repository relationship. But in all seriousness, the sustainability of a boutique Rails application that is heavily dependent on the javascript functions of 2010 should be a serious concern to repository developers. jQuery is all the rage today, but it could blow away in the wind five years from now. This is the very thing that the XForms working group set out to prevent when they introduced a standard approach to dynamic webforms."},{"id":"2010-08-18-code-reviews-and-the-digital-humanities","title":"Code Reviews and the Digital Humanities","author":"wayne-graham","date":"2010-08-18 04:47:57 -0400","categories":["Digital Humanities","Research and Development"],"url":"code-reviews-and-the-digital-humanities","layout":"post","content":"The following was a response I made in an email exchange with Tom Elliot of the Pleiades Project and Bethany Nowviskie. Our conversation was prompted by Tom’s inquiry on planning, budgeting for, and conducting a code review as part of a grant-funded project. What follows is a slightly modified (and expanded) version of that email conversation. Testing and code review is something that has been on my mind a lot lately as our shop has been shifting its focus from boutique, one-off projects, to building upon frameworks maintained by other organizations. As these code bases continue to grow, we need to ensure that subtle changes to the core functionality of the underlying systems do not propagate into bugs in our code. We also need a way to handle this situation quickly and efficiently when this does arise. This was especially reinforced by two recent projects our group undertook to migrate nearly decade-old software on to new servers. If you ask anyone in the office, they will most likely roll their eyes when I start beating the testing drum. These are great tools for not only generating pretty green and red bar charts, but also documenting the intention of the programmer in writing the code, and zeroing in on bugs where they occur without weeks of hunting. However, this is only one of the tools in the chest for writing solid code, sans bugs. In fact, there are a lot of sophisticated, freely available, automated tools that help programmers of all skill levels not only write more consistent code, but also zero in on potential performance issues and just plain smelly code (that they obviously wrote just to get running and fully intended to go back and fix later). Over the years, tools that measure code complexity (like PMD, PHPMD, and flog ), code dependency analyzers ( JDepend, PHPDepend, and rcov ), copy/paste detection (in PMD, flay, and phpcpd ), and enforcing coding standards (a la PHPCode Sniffer and rails_best_practices), along with not only unit and integration tests (in whatever style you choose), but a code coverage analysis reports that provides feedback on which lines were executed, go a long way in reducing the number of bugs in code. These tools are really pre-emptive step in writing stronger, more elegant, and ultimately more sustainable code, all before once gets to the point of performing a human code review. While I don’t need to be building software per-se, I have started experimenting with the Hudson continuous integration server as a dashboard to get a quick snapshot of how these different metric tests all play together in the code that our team writes. It is no longer good enough to simply have code functioning, we need the code to pass certain thresholds of quality and sustainability before we can release. Where we find issues in the code, like test coverage, high cyclomatic complexity, lots of copy-n-pasted code, or high volatility in dependency scans, we can sit down and perform a rather focused mini code review (resembling the pair-programming idiom) on that section of code to refactor a better solution or approach To this end, we’re currently working on a set of baseline testing and reporting tools for our projects. Currently, we have Ant scripts for our PHP and Java projects, and a gem bundle for Rails and Sinatra projects. While we take this approach in the Scholars’ Lab, we were wondering if there were others out there that had opinions or experiences to share about code review during development? If you do, leave a comment, write a post, or tweet at us (@scholarslab, @nowviskie, @wayne_graham) – and at @paregorios, who started the conversation in the first place. We’d love to hear about your best practices (and even horror stories) and philosophy on what constitutes good software and useful code reviewing, including whether you think current trends in open source development constitute a good-enough review for DH projects. Further Resources Java PMD JDedend junit Clover Javadoc Checkstyle PHP PHP Depend PHPMD phpcpd PHP Codesniffer phpDocumentor Ruby Flog Flay metric_fu rdoc Javascript QUnit JSCoverage Continuous Integration Hudson Phing CruiseControl Issue Tracking Google Code GitHub Project Kenai Jira Redmine"},{"id":"2010-09-09-omeka-solr-and-tei","title":"Omeka, Solr, and TEI","author":"ethan-gruber","date":"2010-09-09 07:50:16 -0400","categories":["Digital Humanities","Research and Development"],"url":"omeka-solr-and-tei","layout":"post","content":"One of the most vital tools that computers bestow upon the humanities scholar is the ability to rapidly sort and group data that are relevant to the scholar’s own research needs.  A digital collection of several thousand artifacts is useful, but it is even more useful if, for example, the user can filter the results for lithographs created or published by a certain person or corporate identity.  Omeka’s built-in search mechanism is fairly simple, and it may suffice for most collections, but it may also fall short of providing the kind of advanced querying abilities that scholars are growing accustomed to with other digital collections, such as Northwestern’s Winterton Collection or modern library catalogs such as the one released publicly here at the University of Virginia Library in July.  Apache Solr is an open-source Java-based search index that provides this functionality. Folks in the Scholars’ Lab and other U.Va. Library departments have been using Solr for a number of years.  I have used it for nearly a dozen different projects since 2007, when Bess Sadler (now with Stanford’s Digital Library Systems and Services group) introduced it to the department.  About two months ago, I began work on a Solr plugin for Omeka which would post public collection items to a Solr index.  The search results then would be rendered in the public theme.  A table in the Omeka database contains all of the elements that the user may select as facets, displayable fields, or sortable fields, and the user may check boxes in a form in the administrative panel to customize the Solr results.  Collections, item types, and tags may also be selected as facet, displayable, or sortable fields, and thumbnail images may be displayed in the search results.  The simple admin interface to the variety of Solr options outlined above can transform your Omeka collection into a great resource that visitors can manipulate to meet their own research interests. Yesterday, I released SolrSearch 0.9 .  In this most recent version of the plugin, text nodes from XML files attached to items are indexed for full text searching.  SolrSearch, then, is an important plugin to install in conjunction with TeiDisplay, a plugin the Scholars’ Lab developed for rendering Text Encoding Initiative (TEI) XML files.  Therefore, not only can a user read TEI transcriptions of textual works, but search the collection for words or phrases in these works as well.  SolrSearch will feature a hit highlighting option in a future version so that the user may see their search keywords in context. I know of at least one institution that is using SolrSearch (at least, in an experimental state) for their collection, so hopefully as more people begin to use it, a larger developer group can form around adapting Solr features to Omeka.  Solr is useful for controlled vocabulary services, and it would be great to maximize the application’s capabilities."},{"id":"2010-09-10-the-methodological-turn","title":"The Methodological Turn","author":"bethany-nowviskie","date":"2010-09-10 14:26:09 -0400","categories":["Announcements","Digital Humanities"],"url":"the-methodological-turn","layout":"post","content":"Exactly how does one acquire the “tools of the trade” in digital humanities research? Thursday, September 16th 4pm in the Scholars’ Lab Ray Siemens from the University of Victoria is Director of the Digital Humanities Summer Institute and President of the Society for Digital Humanities/Société pour l’étude des médias interactifs (SDH/SEMI). Ray will talk about training for digital humanities research including the types of courses offered at DHSI, the students and established scholars who participate, and how these groups work together to enhance methodological training. Julie Meloni, an INKE Fellow at the University of Victoria, will describe her vision for a self-paced but peer-guided independent learning system for mid-career scholars who wish to refocus their research in the digital humanities, but who have only traditional and analog training. Please join us in the SLab for light refreshments and conversation with our speakers!"},{"id":"2010-09-15-scholars-lab-fall-newsletter","title":"Scholars' Lab Fall Newsletter","author":"ronda-grizzle","date":"2010-09-15 11:50:01 -0400","categories":["Announcements"],"url":"scholars-lab-fall-newsletter","layout":"post","content":"The Scholars’ Lab Fall 2010 newsletter (pdf) is now available for download. Download In this edition, we introduce our four new Grad Fellows, as well as our 2010 Scholar in Residence and visiting scholars, and we’ve included the complete Fall 2010 schedule of events. Please be aware that the event schedule may change, so check the official events calendar for updated information. SLab Event Calendar"},{"id":"2010-09-16-weve-come-a-long-way-baby","title":"We've come a long way, baby.","author":"bethany-nowviskie","date":"2010-09-16 07:04:50 -0400","categories":null,"url":"weve-come-a-long-way-baby","layout":"post","content":"Thanks to Megan Brett, Research Database and Records Manager at the Montpelier Foundation, we are able share with you a piece of ephemera from UVa Library’s computing past: a pamphlet on “Computer Literature Search.” “Why use a computer search? Consider the time it takes to search manually through the many issues of printed indexes. The computer searches these indexes in seconds; the search is faster, more comprehensive, and often more precise, as there are more subject access points and greater flexibility in combining terms in a computer search.” The pamphlet continues with an offer to split evenly the costs of search with Library patrons – “based on computer connect-time and on the number and format of citations printed.”  Check out a PDF of the pamphlet, here (1mb).  It is coded “10-84.” Is this from 1984? Please comment if you can shed light on the date of the pamphlet, or want to share memories of early digital and computer-assisted scholarship at UVa.  We’d also be very happy – in the semester in which we’ve rolled out a new Virgo interface based on Project Blacklight (first prototyped here in the Scholars’ Lab !) – to see more ephemera from UVa Library’s long engagement with digital research."},{"id":"2010-09-24-synchronizing-development-databases","title":"Synchronizing Development Databases","author":"wayne-graham","date":"2010-09-24 10:27:06 -0400","categories":["Research and Development"],"url":"synchronizing-development-databases","layout":"post","content":"As a developer, I routinely work on multiple machines during the course of a project. One of the biggest pains is working on a database-driven project is that I often need to move the data on machine X to machine Y, make changes, then move the updated data from machine Y back to machine X. Back in the day (ok, so like last week), I would typically write a mysqldump/pgdump script that would dump the data to a tarball, then scp the data around as needed. If it were really important, I might take the time to set up rsync, or even a master/slave configuration for the data. What I found, however, is that I could “break” my development databases and I did this often, wasting time recovering from this foolishness. There had to be a better way. Turns out there is. If you’ve ever deployed anything to heroku, you’ll find they have a really neat way to allow you to synchronize your databases. From the command line, you can pull the database running on the server to your local database (and it actually doesn’t matter if you’re running sqlite, mysql, or postgresql locally, it just works) with: heroku db:pull mysql://user:pass@localhost/mydb\nheroku db:pull sqlite://path/to/my.db Need to push changes to the server? heroku db:push Behind the scenes, heroku is using the taps gem, so you can actually use this same technique for your local setups. The following will walk through a “typical” (e.g. the way I have my dev system set up) use case for integrating taps in to your workflow. I use a Mac, so if you’re on Linux or worse (Windows), you’ll need to slightly adjust some of these directions. The first thing you need to do is make sure that your gems are up-to-date. From a terminal, issue this command: sudo gem update --system Now, we need the taps gem: sudo gem install taps This will take a while as the library dependencies are calculated, and the documentation is generated, but you will some something along these lines: devbox:~ user$ gem install taps\nBuilding native extensions.  This could take a while...\nSuccessfully installed json_pure-1.4.6\nSuccessfully installed rack-1.2.1\nSuccessfully installed sinatra-1.0\nSuccessfully installed mime-types-1.16\nSuccessfully installed rest-client-1.4.2\nSuccessfully installed sequel-3.15.0\nSuccessfully installed sqlite3-ruby-1.3.1\nSuccessfully installed taps-0.3.12\n8 gems installed\nInstalling ri documentation for json_pure-1.4.6...\nInstalling ri documentation for rack-1.2.1...\nInstalling ri documentation for sinatra-1.0...\nInstalling ri documentation for mime-types-1.16...\nInstalling ri documentation for rest-client-1.4.2...\nInstalling ri documentation for sequel-3.15.0...\nInstalling ri documentation for sqlite3-ruby-1.3.1...\nInstalling ri documentation for taps-0.3.12...\nInstalling RDoc documentation for json_pure-1.4.6...\nInstalling RDoc documentation for rack-1.2.1...\nInstalling RDoc documentation for sinatra-1.0...\nInstalling RDoc documentation for mime-types-1.16...\nInstalling RDoc documentation for rest-client-1.4.2...\nInstalling RDoc documentation for sequel-3.15.0...\nInstalling RDoc documentation for sqlite3-ruby-1.3.1...\nInstalling RDoc documentation for taps-0.3.12... You will need to have this installed on each of the boxes you want to be able to push/pull to/from. Ok, assuming you’ve got the taps gem installed on all the computers you want to use, you need to fire up the taps server on each box that actually responds to the push/pull requests. This is a simple Sinatra application that runs and listens for push/pull requests. To fire this up, issue the command: taps server mysql://user@localhost:port/database tapsusername tapspassword Let’s unpack this a little. The taps server needs to know what database to connect to, and a secret user/password to use. Let’s say you’re running MAMP with the default mysql server and accounts running, and you want to be able to sync your Omeka database. Your connection string would look like this: taps server mysql://root@localhost:8889/omeka tapuser IeEf643 Now we can test that the server is running by pointing your browser at http://localhost:5000 . You should see something along these lines after using the username and password you set with the server: Now this doesn’t actually do anything, just ensures that you have the server up-and-running. Now to get the data loaded on another box… Assuming you’re on another computer now (and that you’re not blocking port 5000 on the host machine), you issue a pull command (assuming you’ve already created the omeka database in the MAMP phpMyAdmin): ``` taps pull mysql://root@localhost:3306/omeka http://tapuser:IeEf643@remoteip:5000 ``` Again, assuming you don’t have a firewall port blocking issues, you should see the tables getting propagated on your system: ```\nReceiving schema\nSchema:        100% |==========================================| Time: 00:00:22\nReceiving data\n25 tables, 228 records\nomeka_item_ty: 100% |==========================================| Time: 00:00:00\nomeka_tags:    100% |==========================================| Time: 00:00:00\nomeka_entity_: 100% |==========================================| Time: 00:00:00\nomeka_items:   100% |==========================================| Time: 00:00:00\nomeka_section: 100% |==========================================| Time: 00:00:00\nomeka_element: 100% |==========================================| Time: 00:00:00\nomeka_record_: 100% |==========================================| Time: 00:00:00\nomeka_tagging: 100% |==========================================| Time: 00:00:00\nomeka_users_a: 100% |==========================================| Time: 00:00:00\nomeka_element: 100% |==========================================| Time: 00:00:00\nomeka_element: 100% |==========================================| Time: 00:00:00\nomeka_entitie: 100% |==========================================| Time: 00:00:00\nomeka_data_ty: 100% |==========================================| Time: 00:00:00\nomeka_item_ty: 100% |==========================================| Time: 00:00:00\nomeka_options: 100% |==========================================| Time: 00:00:00\nomeka_entitie: 100% |==========================================| Time: 00:00:00\nomeka_mime_el: 100% |==========================================| Time: 00:00:00\nomeka_section: 100% |==========================================| Time: 00:00:00\nomeka_items_s: 100% |==========================================| Time: 00:00:00\nomeka_process: 100% |==========================================| Time: 00:00:00\nomeka_collect: 100% |==========================================| Time: 00:00:00\nomeka_exhibit: 100% |==========================================| Time: 00:00:00\nomeka_files:   100% |==========================================| Time: 00:00:00\nomeka_plugins: 100% |==========================================| Time: 00:00:00\nomeka_users:   100% |==========================================| Time: 00:00:00\nReceiving indexes\nResetting sequences ``` You should now have a functional copy of all your data from the server machine. Now all you have to do is make your changes, then push those changes back to the server. ``` $ taps push mysql://root@localhost:8889/omeka http://tapuser:IeEf643@localhost:5000\nSending schema\nSchema:        100% |==========================================| Time: 00:00:20\nSending data\n25 tables, 0 records\nomeka_item_ty: 100% |==========================================| Time: 00:00:00\nomeka_tags:    100% |==========================================| Time: 00:00:00\nomeka_section: 100% |==========================================| Time: 00:00:00\nomeka_entity_: 100% |==========================================| Time: 00:00:00\nomeka_items:   100% |==========================================| Time: 00:00:00\nomeka_element: 100% |==========================================| Time: 00:00:00\nomeka_tagging: 100% |==========================================| Time: 00:00:00\nomeka_users_a: 100% |==========================================| Time: 00:00:00\nomeka_record_: 100% |==========================================| Time: 00:00:00\nomeka_entitie: 100% |==========================================| Time: 00:00:00\nomeka_element: 100% |==========================================| Time: 00:00:00\nomeka_element: 100% |==========================================| Time: 00:00:00\nomeka_item_ty: 100% |==========================================| Time: 00:00:00\nomeka_options: 100% |==========================================| Time: 00:00:00\nomeka_data_ty: 100% |==========================================| Time: 00:00:00\nomeka_entitie: 100% |==========================================| Time: 00:00:00\nomeka_section: 100% |==========================================| Time: 00:00:00\nomeka_mime_el: 100% |==========================================| Time: 00:00:00\nomeka_process: 100% |==========================================| Time: 00:00:00\nomeka_items_s: 100% |==========================================| Time: 00:00:00\nomeka_collect: 100% |==========================================| Time: 00:00:00\nomeka_plugins: 100% |==========================================| Time: 00:00:00\nomeka_files:   100% |==========================================| Time: 00:00:00\nomeka_exhibit: 100% |==========================================| Time: 00:00:00\nomeka_users:   100% |==========================================| Time: 00:00:00\nSending indexes\nResetting sequences ``` So what can go wrong? This is a young project, so there are a few things you should know. As of the time of writing this (taps v 3.12.0), there are a few issues being worked on: Foreign Keys get lost in the schema transfer Tables without primary keys will be incredibly slow to transfer. This is due to it being inefficient having large offset values in queries. Multiple schemas are currently not supported I strongly suggest only using this in a development setting for non-Rails projects . Rails-based projects have a special object for table relations which help manage keys. If you’re doing heavy database development, use the tools your database provides (mysqldump/pgdump) to create snapshots of your data! Script it, crontab it, download it!"},{"id":"2010-10-15-open-access-week-events","title":"Open Access Week Events","author":"bethany-nowviskie","date":"2010-10-15 12:19:42 -0400","categories":["Announcements"],"url":"open-access-week-events","layout":"post","content":"You are cordially invited to an Open Access Week Luncheon Monday, October 18 at noon in the Scholars’ Lab The Scholars’ Lab is proud to celebrate Open Access Week with a conversation led by Associate Professor of Education Brian Pusser (chair of last year’s Faculty Senate Task Force on Scholarly Publications and Authors’ Rights), and UVa Associate General Counsel Madelyn Wessel .  We hope you’ll join us for a tasty lunch, some interesting and educational short films, and a lively discussion of issues surrounding authors’ rights and open access to scholarly work. Then, come back that very afternoon for… **Wine, Cheese, &amp; Legalese **\nwith Madelyn Wessel Monday, October 18 at 4:00 p.m. in the  Scholars’ Lab This is an event designed especially for graduate students and those who advise them: wine, cheese, and straight talk about copyright, fair use, and your dissertation.  Madelyn has a story or two that will curl your hair."},{"id":"2010-10-19-its-alive-introducing-the-early-american-foreign-service-database","title":"It's [A]live!: Introducing The Early American Foreign Service Database","author":"jean-bauer","date":"2010-10-19 11:42:33 -0400","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"its-alive-introducing-the-early-american-foreign-service-database","layout":"post","content":"It is with great pleasure, and no small amount of trepidation, that I announce the launch of the Early American Foreign Service Database (EAFSD to its friends).  While the EAFSD has been designed as an independent, secondary source publication, it also exists symbiotically with my dissertation “Revolution-Mongers: Launching the U.S. Foreign Service, 1775-1825.” I created the EAFSD to help me track the many diplomats, consuls, and special agents sent abroad by the various American governments during the first fifty-years of American state-building.  Currently the database contains basic information about overseas assignments and a few dives into data visualization (an interactive Google map and Moritz Stefaner’s Relation Browser). I have been a reluctant convert to the principles of Web 2.0, and I keenly feel the anxiety of releasing something before my perfectionist tendencies have been fully exhausted.  The pages of the EAFSD are therefore sprinkled with requests for feedback and my (hopefully humorous) under construction page, featuring Benjamin West’s unfinished masterpiece the “American Commissioners of the Preliminary Peace Agreement with Great Britain.” Over the next few months (and coming years) I will be adding more information to the database, allowing me to trace the social, professional, and correspondence networks from which American foreign service officers drew the information they needed to represent their new (and often disorganized) government.  I will also be enhancing the data visualizations to include hypertrees, time lines, and network graphs. This launch has been over two years in the making.  As I look back over that time, I am amazed at the generous support I have received from my colleagues at the University of Virginia and the Digital Humanities community writ large.  I wrote an extended acknowledgments page for the EAFSD, my humble attempt to recognize the help and encouragement that made this project possible. Launching the EAFSD also gives me a chance to test Project Quincy, the open-source software package I am developing for tracing historical networks through time and space.  The EAFSD is the flagship (read guinea pig) application for Project Quincy.  I hope my work will allow other scholars to explore the networks relevant to their own research. To that end the EAFSD is, and always will be, open access and open source."},{"id":"2010-10-22-old-school-hydro","title":"\"Old School Hydro\" in the Scholars' Lab","author":"Admin","date":"2010-10-22 10:39:05 -0400","categories":["Announcements","Geospatial and Temporal"],"url":"old-school-hydro","layout":"post","content":"Please join us on November 4th (or look for our podcast) to get your feet wet with Old School Hydro: Modern and Historic Surveying Aboard the NOAA Ship “Thomas Jefferson!” Thursday, November 4\n3:00 p.m.\nScholars’ Lab During the summer of 2010, U.Va. History professor and Scholars’ Lab GIS collaborator Max Edelson took a berth aboard the NOAA Ship Thomas Jefferson as it charted the waters off the western Keys of Florida. For a week, he learned about modern coastal surveying and hydrography first hand and interviewed the TJ’s officers and scientists about their experiences using sonar-based sensing to measure the extent of the Deepwater Horizon oil spill. To better get a grasp on the first rigorous colonial surveys of Florida created in the 1760s and 1770s, he enlisted some of the crew to recreate early modern methods by tracing the contours and measuring the depths of a harbor in Key West. When asked what they were up with their lead lines and sextants by puzzled crew mates, they replied, “We’re off to do some old-school hydro.” This talk describes the art and science of surveying and mapmaking in and around the Florida Keys across 250 years. All Scholars’ Lab events are free and open to all. No registration is required. We hope to see you in the Scholars’ Lab! And check out our full calendar of events for the Fall semester."},{"id":"2010-10-29-smarter-paper-maps","title":"Smarter Paper Maps","author":"kelly-johnston","date":"2010-10-29 09:27:50 -0400","categories":["Digital Humanities","Geospatial and Temporal","Visualization and Data Mining"],"url":"smarter-paper-maps","layout":"post","content":"It’s a quiz.  I’ll name the required skills, you name the profession.  Go. Identifying map projections and coordinate systems Interpreting map scale Understanding techniques of cartographic relief Interpolating latitude &amp; longitude Calculating geographic extent rectangles Too easy?  Well the profession I’m describing is not Geographic Information Systems guru or Cartographer or Neogeographer .   In fact, my list describes just a few of the skills you’ll need to be a first-rate Map Cataloger . Thanks to a thoughtful invitation from Jennifer Roper, I learned a bit about map cataloging alongside her UVA Library Cataloging and Metadata Services team at a workshop led by one of the very best, Paige Andrew of Penn State University.  Of course Mr. Andrew sports a world map necktie. The session made clear to me many of the skills needed to be a strong map cataloger are the same skills needed to be proficient in Geographic Information Systems.  That skills list above would serve as a good starting outline for a GIS 101 syllabus and all those skills are key to the map cataloging workflow. And Mr. Andrew’s traveling map collection was a treat to explore with examples of bleeding edge maps, segmented maps, pop-up maps, remote sensing maps, tourist maps, and maps with and without covers and neatlines.  Who knew map catalogers must be skilled in both map unfolding for preservation and map re-folding (harder!) for panel measurements. Today was eye-opening for me in thinking about using a geographic search to discover paper maps in a library catalog.  With catalogers now capturing the latitude and longitude of each corner of the map as part of their workflow, finding every paper map that intersects any point on the earth becomes possible.  It’s sometimes called searching by geographic extent.  Just point to any spot on a map of the world and find all the maps in the collection that cover that spot. Geographic search relies on the kind of extent rectangle metadata map catalogers now create every day.  We’ve implemented geographic search for digital datasets in our GIS Portal http://gis.lib.virginia.edu by creating just such extent rectangle metadata for every digital dataset. Seems map catalogers are setting the stage for that same kind of discovery for paper map collections.  Paper maps are getting smarter every day."},{"id":"2010-11-09-senior-developer-position","title":"Are you our new Senior Developer?","author":"wayne-graham","date":"2010-11-09 04:19:17 -0500","categories":["Announcements"],"url":"senior-developer-position","layout":"post","content":"Still fairly fresh on the scene, but drawing on a long history in digital humanities and spatial and data-driven work at the University of Virginia, the UVa Library’s Scholars’ Lab has developed great projects and hosted amazing events over the past three years. We now have an opportunity to add new collaborators to round out our team! (So stay tuned for future postings later this semester.) Today, we’re looking for a Senior Developer who can build, test, and debug code and who loves to get ‘under the hood’ to figure out how things work. Senior Developer, Scholars’ Lab As a senior web and software developer reporting to the Head of R&amp;D for the Scholars’ Lab, you will be responsible for enhancing, maintaining, and optimizing projects related to digital research and scholarship. Not only should you enjoy writing well organized, highly tested code, but you should enjoy working with a great group of teammates and scholarly stake holders to solve hard problems in both software engineering and the digital humanities. You will need to fit into a fast-paced, interdisciplinary environment where technology enables creative vision – and where you can take good advantage of the “20% time” that all Scholars’ Lab and Department of Digital Research &amp; Scholarship faculty and staff are granted to pursue professional development and their own (often collaborative) R&amp;D projects. Responsibilities: Write scalable, well-factored, well-tested application code Lead development projects Work with support teams to find and fix application bugs Work with cross-functional teams to develop design solutions Specialized Knowledge and Skills: In-depth knowledge of multiple programming languages including PHP, Perl, Ruby, and Java Full-time experience with PHP and Ruby frameworks (e.g. Zend and Rails) Proven ability to quickly learn new systems and environments Experience working on open source projects Efficient problem solving techniques Excellent verbal and written communication skills Knowledge of multiple RDBMSes (PostgreSQL, Oracle, MySQL) Javascript skills (AJAX, DHTML, jQuery and similar JS frameworks) Familiarity with version control systems (Subversion and Git) Experience Test-Driven development (PHPUnit, SimpleTest, RSpec, Shoulda, etc.) Linux experience a plus (RHEL, Fedora Core) If this sounds like you… we invite you to APPLY FOR THE JOB . Salary is commensurate with experience, and expected to range between approximately $58,000 and $106,000 per annum. We’re looking to fill this position quickly, so please don’t delay!"},{"id":"2010-11-23-humanities-design-ux-architect","title":"Are you our new Humanities Design (UX) Architect?","author":"bethany-nowviskie","date":"2010-11-23 09:08:33 -0500","categories":["Announcements"],"url":"humanities-design-ux-architect","layout":"post","content":"Building on a nearly twenty-year history in digital humanities and spatial and data-driven scholarship at the University of Virginia, the Scholars’ Lab has developed great projects and hosted amazing events at UVa Library over the past three years.  In addition to our current search for a full-time, permanent Senior Developer, we now have the opportunity to add some UX love to the work of our R&amp;D; division. We’re looking for a Humanities Design (UX) Architect who can create and guide exciting, professional user experiences, is passionate about the quality of his or her HTML and CSS, and wants to be part of a team that does great work in the digital humanities. Humanities Design (User Experience) Architect As the Humanities Design Architect of UVa Library’s department of Digital Research and Scholarship, you will be responsible for the design and implementation of effective and inspiring digital resources for teaching and scholarship. We are looking for someone who is highly technically skilled and a talented designer, and who has a deep background in humanities or social science scholarship. This general faculty position in the Scholars’ Lab is for a true “hybrid academic”  – someone who can communicate effectively with humanities and social science scholars, help guide the work of graduate students in our digital humanities fellowship program, and focus intently on the presentation and interaction layer for next-generation digital scholarship. We seek a person who can fit into a fast-paced, interdisciplinary environment where good design truly matters.  Because we collaborate as peers with UVa faculty and graduate students, we are looking for someone with a personal research agenda, who will take excellent advantage of the “20% time” all Scholars’ Lab faculty and staff are granted to pursue professional development and their own projects. This is a two-year, general faculty position, with possibility of renewal.  Salary will be commensurate with experience.  Benefits are excellent, and include generous funding for continuing education and travel, retirement plans, and 22 days of vacation per year. Primary Responsibilities: Conduct user research for creating user models Drive the interaction process, feature discussions, and functional requirements for Scholars’ Lab projects Create wireframes and prototypes Conduct informal usability tests Work closely with R&amp;D Team Specialized Knowledge and Skills: Passionate about interactive experience across a variety of media (web, mobile) with a strong desire for innovation Experience as a project manager or technical team leader on scholarly projects Experience with user-centered design patterns and methodologies Experience running user testing and conducting accessibility testing Comfort with complexity and ambiguity, and a passion for the challenges of the humanities and social sciences Advanced understanding of user interface client technologies such as Javascript, AJAX, HTML, CSS, etc. Experience creating standard user experience deliverables, including Site Maps Process flows Personas Use Cases Concept Models Wireframes Interactive prototypes Strong presentation and communication skills Expertise in current design tools and a sophisticated personal research agenda related to the user experience of digital humanities and social science projects. If this sounds like you… we invite you to APPLY FOR THE JOB! Consideration of applications will begin immediately and continue until the position is filled."},{"id":"2010-12-08-podcasts","title":"Podcasts!","author":"ronda-grizzle","date":"2010-12-08 09:41:41 -0500","categories":["Podcasts"],"url":"podcasts","layout":"post","content":"Greetings, Scholars’ Lab fans. We have three new podcasts to share since we last updated. Please enjoy! Introducing our 2010/2011 Scholars’ Lab Fellows - Tom Finger, Jared Benton, Chris Clapp, and Alex Gil. [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.5154837771/enclosure.mp3”] Ray Siemens (University of Victoria) and Julie Meloni (INKE Fellow at the University of Victoria) discuss opportunities and approaches for training humanities scholars in digital methods. [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.5154837767/enclosure.mp3”] Jo Guldi (Harvard Society of Fellows) discusses The City Made of Words : Text Mining the Spaces of Subaltern Agency in Britain, 1848-1919 . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.5154837763/enclosure.mp3”]"},{"id":"2010-12-14-mark-sample-haunts-place-play-and-trauma","title":"Mark Sample: Haunts: Place, Play, and Trauma","author":"ronda-grizzle","date":"2010-12-14 05:45:48 -0500","categories":["Podcasts"],"url":"mark-sample-haunts-place-play-and-trauma","layout":"post","content":"Prof. Mark Sample re-imagines locative media tools as platforms for renegotiating space and telling stories in literary, critical, and creative contexts. [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.5484615210/enclosure.mp3”]"},{"id":"2010-12-15-building-omeka-exhibits-with-fedora-repository-content","title":"Building Omeka Exhibits with Fedora Repository Content","author":"ethan-gruber","date":"2010-12-15 08:53:53 -0500","categories":["Digital Humanities","Research and Development"],"url":"building-omeka-exhibits-with-fedora-repository-content","layout":"post","content":"Our NEH-funded Neatline project has inspired the Scholars’ Lab to develop or enhance several new Omeka plugins recently. (See our full list .) One of these is FedoraConnector, which is designed to enable administrators to attach Fedora datastreams (a digital object – whether image, XML like TEI or EAD, or video) to Omeka items.  This is fundamentally different from attaching files to an item–the datastream is not duplicated and stored within Omeka’s archive.  Rather, a reference to the Fedora object (PID) is stored within a new table in the Omeka database that associates the item with the URL of the datastream that is accessed (and rendered) with Fedora’s REST API.  The plugin also supports importing Dublin Core and MODS metadata into the DC Element Set in Omeka.  The importers can be extended to map from any metadata standard into DC. The benefit to this architecture is that it enables dynamic rendering of the most current version of the Fedora object, and thus there is no issue about storing duplicate files in the Omeka disk space that can be deprecated by updates to the original Fedora object.  Additionally, FedoraConnector can take advantage of institutional-specific services that are developed for delivering content.  For example, thumbnail and medium-sized page images are rendered in real time by querying the University of Virginia Library’s JPEG2000 server and requesting deliverables at a specific dimension.  Disseminators, or handler functions for rendering Fedora content based on mime-type and/or datastream type, are extensible. [![TEI document from Fedora](http://static.scholarslab.org/wp-content/uploads/2010/12/segmental-tei-110x110.png)](http://www.scholarslab.org/digital-humanities/building-omeka-exhibits-with-fedora-repository-content/attachment/segmental-tei/) TEI document from Fedora Earlier this year, we released a beta version of a plugin for rendering TEI files into HTML within Omeka.  Called TeiDisplay, this plugin was enhanced by the insertion of several hooks that execute FedoraConnector functions (if FedoraConnector is installed) to render TEI XML datastreams on the fly directly from the repository.  TeiDisplay supports, as the documentation for the plugin indicates, selection of customized XSLT stylesheets and two display types: entire document and segmental view (with table of contents and by-section rendering).  Indeed, documents coming from Fedora can be rendered dynamically with the same set of options. But what about indexing the document?  This is why the Scholars’ Lab developed SolrSearch last summer to replace Omeka’s default mySQL search with the more advanced search options afforded by Solr, an open source search index.  SolrSearch supports facets, sorting, hit highlighting, and a handful of other options.  Originally designed to index the full text of Omeka files with a text/xml mime-type, SolrSearch was enhanced to index the full text of Fedora datastreams with a text/xml mime-type as well, enabling full text searching, faceted browsing, and hit highlighting of the aforementioned TEI files referenced from a repository. [![solr](http://static.scholarslab.org/wp-content/uploads/2010/12/solr-300x116.png)](http://www.scholarslab.org/digital-humanities/building-omeka-exhibits-with-fedora-repository-content/attachment/solr/) Solr search of TEI file in Omeka So in essense, the range of plugins the Scholars’ Lab has created for Omeka can enable creation of attractive and cutting-edge public user interfaces for collections of Fedora objects.  Coupled with our Neatline plugins, which are all about geospatial and temporal interpretation of archival collections, this work bridges a well-recognized gap between the  volume of digital content housed in sophisticated repositories and the curators, scholars, and end users who seek access to it and wish to interpret it in online exhibits."},{"id":"2011-01-06-mapping-an-asian-american-indie-rock-digital-diaspora","title":"Mapping an Asian American Indie Rock Digital Diaspora","author":"wendy-hsu","date":"2011-01-06 08:59:57 -0500","categories":["Digital Humanities","Geospatial and Temporal","Grad Student Research"],"url":"mapping-an-asian-american-indie-rock-digital-diaspora","layout":"post","content":"My dissertation project investigates the musical and social life of current independent rock  musicians of Asian descent. This research looks at the music,  interviews, and social interactions of these musicians. How do I do  this? Prior to working with UVa’s Scholars Lab,  my method of field research had been participant observation: attending  concerts, doing formal and informal interviews, interacting with the musicians’ friends and fans, listening to their recorded music, organizing local  performances on their tours…an immersion in these musicians’  multi-faceted musical life. As soon as I began my field research, I  discovered that the notion of “the field” has changed because of the  prevalent usage of digital social media among the musicians of my study.  The Internet, is no longer just a means of communication between me and  my informants. Digital social media make up an important site of social  interactions and creative expressions. Not only that, it is the key to  social networking and community building for these musicians. Thus the  “field” of my investigation came to include the digital social terrain  that I navigate within the scope of dissertation research. This post focuses on the map of one of the bands that I study: The Kominas .  The Kominas is a South Asian American punk band that spawned in Boston,  now based in Philadelphia. Recombining sounds from the Boston  ska-and-crust-punk scene with 1970s Bollywood movies and Bhangra music  from their parents’ dusty tape collection, The Kominas evokes a  radically transnational sonic landscape. [ Example “Par Desi” ] Since 2006,   the band has been vigorously creating a translocal social terrain via  face-to-face interactions through touring and online social networking.  The Kominas’ do-it-yourself network is comprised of Muslim-,  South-Asian-identified, and other taqwacore -inspired musicians, listeners, artists, filmmakers, and bloggers. In this post, I ask:  What does The Kominas’ “digital diaspora” look  like geographically and spatially? First, I will describe the digital  methods I used to map this community. Digital Methods – Web-scraping and Visualization To create such a map, I designed and executed out a two-phase method. Phase 1 is web-scraping,  the process of mining data from the Internet. This process entails  first, locating a source of useful geographic data, and then harvesting  this information programmatically. I was interested in two sets of data,  specifically: the physical location of the band’s performance tours;  and the self-reported (physical) location of the friends in an online  community. The first set of data, regarding performance locations, was  found on The Kominas’ official website. The information regarding friend  locations was found in its most complete form on the social networking  site Myspace . To extract and process these data sets, with the help of Joe Gilbert,  I wrote a program using Ruby to parse out the relevant information in  the source code of the profile pages of The Kominas’ Myspace friends.  The Kominas [as of April 2010] had close to 3,000 friends on Myspace.  These are all Myspace users who have requested to become friends with  The Kominas, or vice versa. Using Mechanize,  a Ruby gem, the program extracted all the geographically related text  from the Myspace profile pages of 2,867 friends. Using the Geokit, a ruby gem that implements the Google Geocoder, the program translated this information into a set of spatial coordinates, specifically, latitude and longitude. Phase 2 - geospatial visualization – is the process of turning the harvested data into a meaningful visualization. Using OpenLayers,  an open-source mapping program, I created a dynamic map containing all  the points of the physical locations of the band’s Myspace friends and  performance tours. To contextualize the reading of the physical points, I  added various map layers. For example, I added a Google street map  layer to label the visualization with the proper name of countries and  cities. The rest of my efforts were spent to refine the map, to make it  readable and meaningful. The Kominas’ Digital Diaspora Map: GO! To interact with the map, click on the above image. This screenshot  shows the global distribution of The Kominas’ Myspace friends. The  reddish pink clusters represent the friend density in the respective  locales. The size of the cluster is an approximate representation of the  number of friends in one location. A baselayer of the world’s regions – marked by various shades of  green in the background - helps contextualize the friend distribution  across continental boundaries. At a macro level, this map articulates a  radically transnational and inter-continental distribution of friends.  Areas of high friend density include: North America, Europe, and Asia.  The story of translocality becomes more complex as we zoom in on the map  to get more geographic detail. In my dissertation, combining maps,  music analysis, and interviews, I examine how the members of The Kominas  position themselves geographically and ethnically vis a vis their vastly transnational world. Questions and Concerns These maps tell a story, a particular kind of story that situates a  humanist study of a music-culture within a particular geographic  context. In the context of my dissertation, these maps add a spatial  texture to the understanding of the translocal social terrain of a  U.S.-based musicians of Asian descent. And the visualization process  helps me  to analyze the musicians’ questioning of their sense  of  ethnic and national belonging  and  to situate the ethnographic  details  of my 24-month field research  within  a global context. Here are some general questions and concerns that I’ve  encountered in creating and using these dynamic maps. To express density  using a clustering pattern, I used an algorithm that balances point  density and readability, so that the contrast between the smallest and  the largest clusters is adjusted. In this case, a single-point cluster  can be seen and the largest concentration of the friends of the  northeast of the United States doesn’t dominate the entire map. This  presents the question, am I interested in representing the mathematical  reality of this friend community? Or is there some part of the story  that I was more interested in telling? Which level of detail is most  useful? I’ve discovered that these maps do not provide any answers to my  research questions. They, in fact, present an interpreted reality that  generate further useful questions. A map is certainly not a dissertation  chapter; but it provides a spatial and geographical context for the  musical and social experiences of the musicians in my study. How I use these maps, of course, depends on the narrative that I want  to tell. At a very macro, global level, zoomed all the way out, these  maps can look very similar across bands: with large clusters in the  North American region, some clustering in Europe, and some but less in  other regions of the world. NOT SO INTERESTING… Of interest to me, in my dissertation, are the patterns of the band’s  transnational connections to musicians and fans in Asia. What is the  band’s friend distribution in Asia? Is it useful to compare the  Asia-based friend distribution across band? I have shown two screenshots  of two bands’ friend distribution in Asia. On the top is The Kominas.  On the bottom is Kite Operations, a New-York-based noise rock band. This comparison presents interesting results: These two maps show  that The Kominas, a South Asian American punk band has created a social  geography much more concentrated in South and Southeast Asia; whereas  Kite Operations, with 3/4 of the members being of Korean descent, has  stronger friend presence in East Asia, specifically in South Korea. The  difference in friend distribution shown by these images can provide a  sketch for illustrating a different “Asia” as created through the  cultural practice of “friending” on Myspace by American artists of Asian  descent. Combining Digital Methods with “Conventional Methods” These digital methods seem to have an orthogonal relation to more  conventional ethnographic methods. Until these new digital methods  become accepted in ethnomusicology and cultural anthropology, I must  find a way to integrate the new with the old. [Yes, I have  thought-experimented with a set of digitally engaged ethnographic methods .] Here are some ideas for this integration: Showing the map to the musician-informants: Asking them if  they are surprised by the results of my study. Asking them questions  about how they feel about these places in the world? Personal or musical  connections to these places? Toward a Geospatial Music Analysis: Many musicians that I  study are pre-occupied with geography. In their lyrics, they often  discuss being trapped or living in a limbo between two worlds. They talk  about their feelings regarding certain meaningful place and space in  their music. It’d be potentially fruitful to juxtapose the musical and  social geographies of a single band. Mapping genre/sonic differences: Here I suggest the  possibility of incorporating sonic qualities such as tempo, timbre,  volume, studio effects, and language/dialect into geospatial information  technology and system. Such a tool would be immensely powerful for the  study of the world’s music-cultures at the local and global level. For  example, the World Musical Map project by Ozan Aksoy based at the New Media Lab at the Graduate Center of CUNY explores the rupture between audio  boundaries and actual national borders. Another example is Lee Byron’s  visualization of the listening history on Last.FM . Here’s my attempt to start a digital (ethno)musicology. Are there any other takers? The Kominas’ Digital Diaspora Map: It’s Your Turn. GO! Tips: Double-click to zoom in on the map Upper-left: turn on/off various layers: Google Street/Satellite;  world’s regions; Muslim-majority countries; clusters (friend density);  friends (individual points);** **gigs. Scroll on the map by clicking + holding + moving the cursor"},{"id":"2011-01-24-2011-12-grad-fellows","title":"2011-2012 Graduate Fellows' Program","author":"ronda-grizzle","date":"2011-01-24 06:52:45 -0500","categories":["Announcements","Grad Student Research"],"url":"2011-12-grad-fellows","layout":"post","content":"The Scholars’ Lab is proud to host, for the fourth year, a prestigious UVa Library Graduate Fellowship program in Digital Humanities. This fellowship is designed to support advanced graduate students doing significant and innovative work in DH by making resources and expertise available in all areas of digital humanities research – from content creation and data management, to advice on intellectual property issues. Funding for the Fellowship program was established by the Jeffrey C. Walker Library Fund for Technology in the Humanities, the Matthew &amp; Nancy Walker Library Fund, and a challenge grant from the National Endowment for the Humanities. Among other projects, past Scholars’ Lab fellows have used Ruby on Rails and MySQL to create a database of the Early American foreign service, GIS tools to explore geospatial patterns of pipe development in Native American settlements, computational linguistic techniques to evaluate the authorship of a Victorian novel and to explore patterns in early Biblical apocalyptic literature, and have used web APIs for social networking sites to explore Asian-American indie rock culture. More information about our Fellows can be found on the community page at the Scholars’ Lab website. Application information and instructions for the fellowship program are available from the Scholars’ Lab website . Application packets are due no later than March 1, 2011."},{"id":"2011-01-31-putting-american-community-survey-data-to-work","title":"Putting American Community Survey Data to Work","author":"chris-gist","date":"2011-01-31 05:33:50 -0500","categories":["Geospatial and Temporal","Visualization and Data Mining"],"url":"putting-american-community-survey-data-to-work","layout":"post","content":"This past week, I read an article that claims the number of people “getting around” by bicycle is steadily growing. The article references the American Community Survey (ACS) and the League of American Bicyclists (LAB). Considering I am a certified instructor from the LAB, I wanted to check the data for myself (and map it). I originally went to Social Explorer (SE) to view and map the data. While SE allowed me to easily view data and make some quick thematic maps, it has some limitations. SE only allows you to map one attribute at a time and does not allow for custom attributes (like the rate change in bicycle riding between 2005 and 2009). Also SE only goes back to 2006 for the ACS.  My journey next took me to American FactFinder (AFF), the U.S. Census’s data portal. In AFF, I was able to get all the transportation survey data and decided to map rate changes in mass transit, bicycling and walking.  I had to manipulate the downloaded Excel file to make it “GIS ready.” There are some interesting things going on within this map.  First, why do a majority of the counties have no data?  The ACS only surveys communities with greater than 65K in population.  Therefore, a majority of U.S. counties are not included in the ACS.  Several counties have data for either 2005 or 2009 but rate change can only be calculated for counties with both.  Secondly, how can adjacent counties have such wild differences in rates of change for bicycle usage?  This question is much tougher to answer.  However, the most likely issue is inaccuracies in the ACS survey results stemming from small sample sizes and differences in the way questions are asked.  The LAB article does mention these issues in passing. Keeping in mind the classification bins are not uniform across these maps, I think there may be some interesting parallels between counties that have lost mass transit and gained walkers/bicyclists, etc. I made a few more maps showing the density of cyclists in 2005 and 2009 at a larger scale. If you would like more information about the U.S. Census, the ACS, Social Explorer, GIS or bicycling, please contact me at cgist[at]virginia.edu.  Larger version of the maps are also available."},{"id":"2011-02-03-spring-2011-newsletter","title":"Spring 2011 Newsletter","author":"ronda-grizzle","date":"2011-02-03 09:35:08 -0500","categories":["Announcements"],"url":"spring-2011-newsletter","layout":"post","content":"Our Spring ‘11 ( PDF ) newsletter is now available for download. Our newsiest newsletter yet includes articles about employment opportunities in the SLab, the applications deadline for our Graduate Fellows program, SLab staff adventures, SLab collaborations, and THATcamp Virginia 2010. Download your copy today! http://tinyurl.com/SLabSpring11"},{"id":"2011-02-04-web-applications-specialist","title":"Are you our new Web Applications Specialist?","author":"wayne-graham","date":"2011-02-04 10:51:07 -0500","categories":["Announcements"],"url":"web-applications-specialist","layout":"post","content":"Are you an enthusiastic Web developer with an interest in the humanities or cultural heritage? UVa Library seeks a Web Applications Specialist to help develop software in our internationally-recognized Scholars’ Lab . The ideal candidate is detail-oriented, eager to work collaboratively, and stays involved with the latest Web and digital humanities technologies. We’re seeking someone passionate about tackling technical problems in the digital humanities – preferably a person with both a technical and liberal arts background, prepared to build next-generation DH interfaces and tools. Our new Web Application Specialist will also be able to take advantage of the “20% time” that all Department of Digital Research &amp; Scholarship faculty and staff are granted to pursue professional development and their own (often collaborative) R&amp;D projects. This is a full-time, permanent position at UVa. Web Applications Specialist As a Web Applications Specialist reporting to the Head of R&amp;D for the Scholars’ Lab, you will be responsible for building, testing, and debugging code, developing documentation, and assisting at troubleshooting. You should possess an attention to detail and a high level of accountability and responsibility. We’re looking for someone who enjoys technical challenges, likes to figure out how things work, and stays involved in the latest Web and digital humanities technologies. You will need to be able to fit in to a creative and collaborative environment. Want to join us as we create great scholarly interfaces? 2-3 years of experience developing web applications with tech skills demonstrated via one or more of the following: your open source work your github repository your awesome blog your code samples from side projects or your production web site (handling real traffic) knowlege of SQL, git, svn, HTML, CSS, Javascript ability to work with technical and non-technical collaborators thanks to your great communications skills experience with software development (maybe even including Agile methodologies) Duties and Responsibilities: Build, test, and debug code Write test cases Estimate coding projects Provide consultation on collaborative projects Develop documentation Assist in the debugging and system troubleshooting for existing software written in a variety of languages and platform Qualifications: 1+ years full-time experience with web development (Rails and PHP preferred) 2+ years experience of standards compliant HTML, CSS, and Javascript Javascript skills (AJAX, JQuery or similar JS framework) Experience with Test Driven Development (Shoulda, RSpec, PHPUnit) Experience with relational database management systems (MySQL, Postgresql) Familiarity with version control systems Understanding of software life cycle Strong foundation in OO programming and practices Experience with Omeka a plus If this sounds like you… We encourage you to APPLY FOR THIS JOB . Salary is commensurate with experience, and expected to range between approximately $43,500 and $75,500 per annum. We’re looking to fill this position quickly, so please don’t delay! Consideration of applications will begin immediately and continue until the position is filled."},{"id":"2011-02-10-dh-dev-picks","title":"DH Dev Picks","author":"wayne-graham","date":"2011-02-10 11:30:07 -0500","categories":["Research and Development"],"url":"dh-dev-picks","layout":"post","content":"Part of mission here at the Scholars’ Lab is provide guidance for folks working on digital projects. As such, I do my best to keep up with trends in software development. For a while I’ve just been adding these to my delicious account to make it a bit easier to find references later. However, recent trends in the way Yahoo! is handling its properties (specifically with delicious), made the think a bit harder about this approach and I thought I might try something else. The idea here is to have a regular series of posts with links that I find interesting and I think are are of some utility to other DH developers. Awesome Fontstacks : If you’re a developer who does design, or a designer who codes, browser support for beautiful fonts opens a lot of doors for creating compelling presentations of your work. Using Git to manage a web site : Web workflows vary widely, but wouldn’t it be nice to use your SCM for deploying your web application? This post describes a method of using git to deploy web pages. Getting Comfortable With SSH : I use keys, aliases, and remote tunneling as part of my workflow, and this post provides a nice introduction to all of these. I would be remiss, however, if I didn’t point out something they highlight on the post: this method is pretty insecure. After you get your feet wet, be sure to check out a much more secure method over on Github My Github Resume : Use github? This site does a nice job creating a resume/cv of your code commits 31 CSS Code Snippets To Make You A Better Coder : If you’re like me, I can’t keep browser-specific differences in the implementation of the CSS spec straight in my head. These code snippets help. Add them to a program like Snippely, SnipMate,\nor your favorite IDE, you can throw them in your code as needed. 960 Grid on jQuery-Mobile : I’m a fan of 960 grid and jQuery. While these techniques for use on mobile devices has a way to go, this technique does show some promise. Maze Algorithms : Great set of posts on solving tough problems algorithmically. These are also nice if you’re not familiar with the pseudo-code commonly used to express discrete algorithmic logic. Very clear explanation of the problem and the approach used to solve the issues."},{"id":"2011-02-15-scholars-lab-and-chnm-partner-on-omeka-neatline","title":"Scholars' Lab and CHNM Partner on \"Omeka + Neatline\"","author":"Admin","date":"2011-02-15 08:58:57 -0500","categories":["Announcements","Digital Humanities","Geospatial and Temporal","Visualization and Data Mining"],"url":"scholars-lab-and-chnm-partner-on-omeka-neatline","layout":"post","content":"The Scholars’ Lab at the University of Virginia Library and the Center for History and New Media ( CHNM ) at George Mason University are pleased to announce a collaborative “Omeka + Neatline” initiative, supported by $665,248 in funding from the Library of Congress . The Omeka + Neatline project’s goal is to enable scholars, students, and library and museum professionals to create geospatial and temporal visualizations of archival collections using a Neatline toolset within CHNM’s popular, open source Omeka exhibition platform.  Neatline, a “contribution to interpretive humanities scholarship in the visual vernacular,” is a project of the UVa Library Scholars’ Lab, originally bolstered by a Start-Up Grant from the Office of Digital Humanities at the National Endowment for the Humanities. Omeka is an award-winning web-publishing platform for the display of cultural heritage and scholarly collections and exhibits, funded by the Institute of Museum and Library Services, Alfred P. Sloan Foundation, Andrew W. Mellon Foundation, and Samuel H. Kress Foundation. This two-year initiative will allow CHNM and the Scholars’ Lab to expand and regularize a partnership that developed informally between the two centers over the course of the past year.  Collaboration has already resulted in improvements to the core functionality of Omeka by CHNM and has led the Scholars’ Lab to produce a number of prototype plugins making Omeka a more attractive and viable option for scholarly partnerships with larger libraries and cultural heritage institutions. These include: improved data import (including EAD, a common archival standard); Solr-powered searching and browsing; and Fedora-based repository services.  Further development will improve existing plugins, add preservation workflows, and refine the Neatline toolset for integration and sophisticated editing and scholarly annotation of historical maps, GIS layers, and timelines. Enhancements to Omeka’s core APIs, improved documentation, regular “point” releases, and a new Exhibit Builder will strengthen Omeka’s already large and robust user and developer communities. Omeka + Neatline is one of six contract awards made by the Library of Congress in a program that aims both to improve the Library’s own content management and content delivery infrastructure and to contribute to collaborative knowledge sharing among broader communities concerned with the sustainability and accessibility of digital content. In July of 2010, the Library of Congress targeted approximately $3,000,000 toward Broad Agency Announcements covering three areas of research interest related to these goals. Technical proposals were openly solicited from expert, multi-disciplinary communities in both academic and commercial settings in three areas: Ingest for Digital Content, Data Modeling of Legislative Information, and Open Source Software for Digital Content Delivery. In addition to guiding software development work at the Scholars’ Lab and CHNM, project directors Tom Scheinfeldt and Bethany Nowviskie will use the Omeka + Neatline project as an opportunity to document and disseminate a model for open source, developer-level collaborations among library labs and digital humanities centers."},{"id":"2011-02-17-dh-dev-links-for-214","title":"DH Dev Links for 2/14","author":"wayne-graham","date":"2011-02-17 04:41:15 -0500","categories":["Research and Development"],"url":"dh-dev-links-for-214","layout":"post","content":"There seem to be quite a few links this week loosely grouped around interface design, with some other geeky goodness mixed in. Isotope jQuery Plugin : Seriously one of the coolest jQuery plugins I’ve seen in a while. When I get some time, I plan to spend some time with this plugin and Solr results for a couple of projects… Duke, Exeter, and Howe I Wish Computer Science was Taught : Good post on some of the shortcomings of the current state of computer science education. Queuing Theory Books Online : Eventually you’ll find a problem that could use some queuing. This is a great list of books that give a great theoretical handling of the subject. Here is the Biggest Mistake You Will Make on Amazon EC2 : With the popularity of cloud computing, there are some things you need to remember when deploying to platforms like EC2. Take away from the article…be careful rebooting server instances! What is the On-Boarding Process for new Employees at Twitter : While most DH centers have pretty small teams, I’m always interested in how bigger companies bring new developers in and cultivate their cultural beliefs. This is a nice piece on how Twitter brings new people in. How to Use Dropbox to Organize Your Startup’s DH Project’s Documents : Just replace “startup” in this article with “your project” and this has some good tips for keeping your project documents organized. Pull Request Diff Comments : Github now allows you to comment on individual lines on pull requests! Very useful for code reviews. Landing Page Best Practices : Remember Flash splash pages? This post has some great tips on designing landing pages to engage users in the content of your site. IE 9 Beta : Let’s face it, a lot of people don’t have access to other browsers for various reasons (can’t install software on work computer). Even though IE is painful, Microsoft swears IE 9 will be better… Better grids: Lessons learned from Design for Developers : Thoughtbot is putting on a series of training sessions to teach design principals to developers. Take away on this…3 and 4 column grid systems provide a solid foundation that introduces clarity to your web layouts. Documentation is Freaking Awesome : Enough said. Paul Irish on HTML 5 Boilerplate : Anyone who’s looked at any HTML I’ve done in the last 6 months will notice that it starts with HTML 5 Boilerplate (usually with some 960gs). This is a nice presentation by Paul on its development and use. Reuse your JavaScript as jQuery Plugins : Nice post on converting JS scripts in to jQuery plugins using a couple of different patterns. Definitely worth a look if you are writing any Javascript. How to use Dropbox as a git server : Can’t/Don’t want to use github as your SCM? This tutorial shows how you might use Dropbox to replicate your git repo (you’ll need to adjust the paths if you’re a Windows user). Announcing TileMill: A Modern Map Design Studio Powered by Open Source : The folks at DevelopmentSeed are doing some amazing work. If you have need to create some maps, TileMill looks like a nice piece of software to get you running without needing to install a large server infrastructure! Hacked Floppy Disk Plays Starwars Music : Just for fun"},{"id":"2011-02-23-head-of-outreach-consulting","title":"Are you our new Head of Outreach & Consulting?","author":"bethany-nowviskie","date":"2011-02-23 04:37:50 -0500","categories":["Announcements"],"url":"head-of-outreach-consulting","layout":"post","content":"We’re excited to announce a fantastic job opportunity: a faculty-level position at the University of Virginia Library with responsibility for running public services in our growing and well-respected Scholars’ Lab . Head of Outreach and Consulting Are you an excellent and enthusiastic communicator with a strong background in technological approaches to humanities and social science scholarship? UVa Library seeks a Head of Outreach and Consulting to coordinate public services in our internationally-recognized Scholars’ Lab. The ideal candidate is detail-oriented, eager to work collaboratively, and able to represent – to internal and external audiences – UVa Library’s involvement in the digital humanities. This supervisory position is responsible for day-to-day operations in the Scholars’ Lab (overseeing staff dedicated to geospatial, data-driven, and text-based research consultation) and plays a key role in our program for Graduate Fellows in the Digital Humanities .  This position reports to the Library’s Director of Digital Research &amp; Scholarship. The position is for a true “hybrid” or “alternative academic” — someone with a strong service ethic and sense of hospitality who can also collaborate as a true intellectual partner with faculty and graduate students, enabling next-generation DH in the Scholars’ Lab. The Head of Outreach and Consulting should also be able to take good advantage of the “20% time” that all Department of Digital Research &amp; Scholarship faculty and staff are granted to pursue professional development and their own (often collaborative) R&amp;D; projects.  This is a full-time, permanent general faculty position at UVa. Primary Responsibilities: Outreach and Scholars’ Lab Management: oversight of day-to-day operations and consulting services, coordination of intellectual programming (speaker series, workshops, etc.) and organization of Graduate Fellows program; Scholarly Projects Consultation: development of intake process, workplans, and MoUs for new scholarly collaborations, in consultation with Scholars’ Lab R&amp;D; Education and Professional Development: pursuit of own scholarly R&amp;D; agenda related to the humanities or social sciences and publication of results and/or presentation at appropriate conferences. Specialized Knowledge and Skills: Working knowledge of digital humanities technologies and directions.  Strong public service orientation and interest in guiding faculty projects from conceptualization to the formulation of workable project plans.  Excellent communication skills, including the ability to present complex technical information to a generalist audience and a clear understanding of the perspectives and needs of the professoriate. Previous experience in public service in an academic library setting and experience in scholarly research, writing, and web development preferred. Education: Masters Degree or PhD in humanities or social sciences. Experience: 3 years experience with project management and/or hands-on development of digital projects related to digital humanities or cultural heritage. Salary and Benefits: Salary commensurate with experience and competitive depending on qualifications. General faculty status. Excellent benefits, including 22 days of vacation; TIAA/CREF and other retirement plans along with generous funding for travel and professional development. If this sounds like you… we invite you to APPLY FOR THE JOB! Consideration of applications will begin immediately and continue until the position is filled."},{"id":"2011-02-28-speaker-series-jeremy-boggs","title":"Speaker Series: Jeremy Boggs","author":"ronda-grizzle","date":"2011-02-28 06:48:28 -0500","categories":["Announcements","Digital Humanities"],"url":"speaker-series-jeremy-boggs","layout":"post","content":"Jeremy Boggs, last semester’s scholar-in-residence at the Scholars’ Lab, will give a talk at 2pm on Thursday, March 3 . Boggs is Associate Director of Research at the Center for History &amp; New Media at George Mason University, as well as a Ph.D. candidate in GMU’s Department of History. During his tenure at CHNM, where he was a founding member of the THATCamp movement, Boggs has been deeply involved in developing conversations about the definition of Digital Humanities and how best to prepare scholars to work on digital projects. His Scholars’ Lab talk, entitled A Plea for Open Digital Humanities Work: or, A DH Grad Student Reflects on Years of ‘Study’, argues that defining a canonical set of skills for digital humanists is ultimately a fruitless endeavor. Instead, Boggs urges the development and promotion of digital humanities tools that are open, well-documented, and learnable, allowing scholars to create projects from these tools that carry these same characteristics. If the digital humanities community can promote a culture of open projects and toolsets, scholars will be able to explore and expand their skills by learning from collleagues’ work. Dr. Bethany Nowviskie will act as respondent for this talk. [![](http://static.scholarslab.org/wp-content/uploads/2011/02/JohnFawcett-boggs-300x300.jpg)](http://www.scholarslab.org/digital-humanities/speaker-series-jeremy-boggs/attachment/johnfawcett-boggs/) Image released under Creative Commons license by Flickr user John Fawcett"},{"id":"2011-03-04-workshop-data-visualization","title":"Workshop: Data Visualization","author":"ronda-grizzle","date":"2011-03-04 07:37:38 -0500","categories":["Announcements"],"url":"workshop-data-visualization","layout":"post","content":"Workshop: An Introduction to VIDI Wednesday, March 9 at 10:00 a.m. – Noon in Alderman Library, Room 317 Aaron Presnall and Dante Chinni of The Jefferson Institute Presented by the Scholars’ Lab, SHANTI &amp; The Department of Politics Got data, but not certain how it might be displayed digitally with its particular numeric, spatial and temporal attributes? This workshop promises an introduction to VIDI, a user friendly suite of Drupal modules for displaying, graphing and mapping data, with illustrations from the Patchwork Nation project."},{"id":"2011-03-10-welcoming-eric-rochester","title":"Welcoming Eric Rochester!","author":"bethany-nowviskie","date":"2011-03-10 06:04:41 -0500","categories":["Announcements"],"url":"welcoming-eric-rochester","layout":"post","content":"The Scholars’ Lab is very pleased to welcome Dr. Eric Rochester as the new Senior Developer on our R&amp;D team! Eric is an accomplished computational linguist and digital humanities scholar and developer. His past work includes appointments with the Oxford University Press and Georgia’s Linguistic Atlas projects, as well as consultancies and programming positions at a number of technology firms. Eric’s doctorate is in English from the University of Georgia, where he concentrated on medieval literature and wrote a dissertation entitled Schwa: A Dictionary Pronunciation Database System .  Schwa examines the production of lexicographical pronunciations, both from a technical and a theoretical perspective, and establishes a set of best practices, well-grounded in the history and present state of dictionary pronunciation. To test his research and theorizing, Eric implemented a lexicographical pronunciation system – and he’ll be bringing that brand of thoughtful, iterative scholarly software development to his new role in the SLab. You can find more information about Eric at his website and follow @erochest on Twitter. He starts on Monday, and we’re thrilled that he and his wife Jackie and daughter Melina are joining the Scholars’ Lab family!"},{"id":"2011-03-18-unsworth-talk","title":"Unsworth to speak at UVa","author":"bethany-nowviskie","date":"2011-03-18 14:17:25 -0400","categories":["Announcements"],"url":"unsworth-talk","layout":"post","content":"The UVa Digital Humanities Speaker Series presents: John Unsworth on “Idiosyncrasy at Scale: Data Curation in the Humanities” Friday, March 25\n3:00pm (reception follows)\nSouth Lawn Auditorium (NAU 101) This talk is co-sponsored by IATH, SHANTI, and the Scholars’ Lab Abstract: Unsworth will argue that, in the past, the humanities have been characterized by data sets that are significantly smaller than those in the sciences, but also significantly more idiosyncratic. Now we face a situation where humanities data, at least in textual form, and soon enough in other forms, is large-scale. Can it still be idiosyncratic? What are the trade-offs between the requirements of curation and the needs of the users for whom that data is curated? What models or initiatives are out there that could help us grapple with idiosyncrasy at scale? John Unsworth is dean of the Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign.  John was the founding director of IATH at UVa, and remains one of the most prominent figures in the field of digital humanities."},{"id":"2011-03-25-welcoming-jeremy-boggs","title":"Welcoming Jeremy Boggs!","author":"bethany-nowviskie","date":"2011-03-25 09:05:43 -0400","categories":["Announcements","Digital Humanities"],"url":"welcoming-jeremy-boggs","layout":"post","content":"We’re thrilled to announce that Jeremy Boggs will be joining the Scholars’ Lab and the staff of UVa Library’s Digital Research &amp; Scholarship department this June, in the role of Humanities Design Architect. Jeremy comes to us from the wonderful Center for History and New Media (CHNM) at George Mason University, where he serves as Associate Director of Research and as development manager for Omeka, an open-source web publishing system for cultural heritage collections.  Jeremy was also one of the originators of THATCamp, the popular technology and humanities “unconference.” He is completing his doctoral dissertation at Mason, “The Designing Historian,” and the UVa community has benefited from Jeremy’s presence this year as a visiting scholar in the Scholars’ Lab. In his new role, Jeremy will be advising faculty and grad students on design and user experience aspects of their digital projects in both the humanities and humanistic social sciences and contributing to ongoing work in Scholars’ Lab R&amp;D;, with special attention to “ Neatline,” a set of tools we’re developing with funding from the Library of Congress and in collaboration with CHNM. We look forward to welcoming Jeremy and his wife Jill to the Scholars’ Lab family this summer!"},{"id":"2011-03-28-welcoming-our-201112-graduate-fellows","title":"Welcoming our 2011/12 Graduate Fellows","author":"ronda-grizzle","date":"2011-03-28 09:33:26 -0400","categories":["Announcements","Grad Student Research"],"url":"welcoming-our-201112-graduate-fellows","layout":"post","content":"The Scholars’ Lab is pleased to welcome our 2011/12 cohort of Graduate Fellows in Digital Humanities : Gabriel Hankins from the Department of English, Randi Lewis from the Corcoran Department of History, and Edward Triplett from the McIntire Department of Art. Gabriel’s fellowship project will explore global modernisms by “modeling the network of textual relationships between global modernist writers and the social and institutional networks of the League of Nations project.” Randi will use GIS technology to create digital maps showing “the changing patterns of Salem trade and the global reach of Salem’s maritime economy” during the Early Republic. Edward will create a database of geo-referenced fortress-monasteries and churches created or occupied by military orders in medieval Spain to begin the process of exploring “how the military orders created a fortified border between Christianity and Islam while displaying varying degrees of cultural permeability across this same border.” Please plan to join us at the September 2011 Digital Therapy Luncheon where we’ll formally introduce our new Fellows, and they will present a short introduction to their projects."},{"id":"2011-04-01-scholars-lab-becomes-lab-for-digital-bs","title":"Scholars' Lab becomes Laboratory for Digital Byzantine Sigillography","author":"bethany-nowviskie","date":"2011-04-01 06:55:01 -0400","categories":["Announcements"],"url":"scholars-lab-becomes-lab-for-digital-bs","layout":"post","content":"FOR IMMEDIATE RELEASE: Through the generosity of an anonymous donor, the Scholars’ Lab at the University of Virginia Library is proud to announce a change in name and concentration.  Effective April 1st, the SLab (which has hitherto supported work in GIS, qualitatitive and quantitative analysis, and interpretive and textual scholarship in the humanities and social sciences) will sharpen its focus and be known as the Laboratory for Digital Byzantine Sigillography . “Among major research libraries,” said University Librarian Karin Wittenborg, “UVa has made the most longstanding investments in Digital BS.  Digital BS has enriched the landscape of the humanities and social sciences immeasurably,” she continued. “It’s just in the air in the former Scholars’ Lab.  You can almost smell it!” Department director Bethany Nowviskie agrees. “At UVa Library, we look for “shovel-ready” projects in the digital humanities and social sciences.  In terms of Digital BS, scholars here have been piling it higher and deeper for decades.” The name change comes with a significant shift to teaching and training, collaborations with UVa faculty, and admissions criteria for the lab’s Graduate Fellowships in Digital Humanities.  Current Graduate Fellow Alex Gil was reached for comment at his dusty carrel in the bowels of Alderman Library:  “As a doctoral candidate in English, you might expect me to be concerned about this new focus on digital Byzantine sigillography.  Far from it!  My dissertation is just full of digital BS!  I’m ecstatic, and now plan to take an extra four to five years to immerse myself in it.”  Former fellow and ethnomusicologist Wendy Hsu will defend her dissertation in the Lab later this month, before taking up a Mellon postdoctoral fellowship at Occidental College.  She looks forward to “spreading all this Virginia BS westward.” Outreach &amp; Training Specialist Ronda Grizzle was careful to specify that the Lab will retain its two internal units: Consultation Services and Research &amp; Development.  One representative of Consultation Services, Kelly Johnston, was mucking around in the field and unavailable for comment beyond the following text message, sent to Grizzle repeatedly: “Sphragistics FTW.”  Meanwhile, Wayne Graham, head of BS R&amp;D, looks forward to fresh projects: “We intend to give a whole new meaning to the word ‘vaporware.’”  New R&amp;D hires Eric Rochester (Senior Developer) and Jeremy Boggs (Design Architect) expressed excitement at the opportunity to “ride the wave of digital BS at UVa.” Some may see UVa Library’s new, exclusive concentration on digital Byzantine sigillography as a corrective to the broadening into meaninglessness of the phrase “digital humanities.”  Nowviskie disagrees: “We’re an open community of practice.  Everyone is welcome under the big tent of Digital BS.”"},{"id":"2011-04-14-project-launch-spatial-humanities","title":"project launch: \"Spatial Humanities!\"","author":"bethany-nowviskie","date":"2011-04-14 12:24:12 -0400","categories":["Announcements","Digital Humanities","Geospatial and Temporal","Visualization and Data Mining"],"url":"project-launch-spatial-humanities","layout":"post","content":"Over the past two years, with generous support from the National Endowment for the Humanities, the Scholars’ Lab at the University of Virginia Library has hosted an Institute for Enabling Geospatial Scholarship . Today we’re pleased to announce the launch of “Spatial Humanities,” a community-driven resource for place-based digital scholarship: http://spatial.scholarslab.org/ This site responds to needs identified in conversation with our 21 Institute faculty members and 56 participants (humanities scholars, software developers, and map &amp; GIS librarians).  It includes: an evolving, crowdsourced catalog of research resources, projects, and organizations; a set of framing essays on the spatial turn across the disciplines by Dr. Jo Guldi of the Harvard Society of Fellows; GIS-related feeds from Q&amp;A sites and other forms of social media; and a peer-reviewed, occasional publication for step-by-step tutorials in spatial tools and methods. Please help us keep this resource current by contributing to it! You can: use Zotero to freely upload research citations, projects, and links to groups; contribute your own tutorials and helpsheets in “Step By Step” format for peer review and formal publication; adopt the #geoinst hashtag on Twitter and Delicious; ask related questions and offer help on DH Answers or the GIS Stack Exchange; and post your commentary on the essays we’ve shared. Learn more about our NEH Institute: http://spatial.scholarslab.org/about/ and about how you can contribute to the “Spatial Humanities” site: http://spatial.scholarslab.org/contribute/ Many thanks to the NEH, the staff of the Scholars’ Lab, our Institute advisory board and faculty, and the scores of Institute participants and fellows who helped to define the project!"},{"id":"2011-04-19-announcement-bagitphp-library","title":"Announcement: BagItPHP Library","author":"eric-rochester","date":"2011-04-19 06:16:01 -0400","categories":["Announcements","Research and Development"],"url":"announcement-bagitphp-library","layout":"post","content":"The Scholars’ Lab is pleased to announce the initial release of a PHP library implementing BagIt 0.96 . BagIt is a specification from the Library of Congress for bundling and transmitting multiple files along with their meta-data. You can check out the project page at http://github.com/scholarslab/BagItPHP/ . Our work on BagItPHP stems from the open source “Omeka + Neatline” project, a collaboration of the Scholars’ Lab with the Roy Rosenzweig Center for History and New Media.  “Omeka + Neatline” is supported by the Library of Congress. Downloads You can download the library either as a ZIP or tarball, or you can clone the repo with git: [sourcecode language=”bash”]\ngit clone git://github.com/scholarslab/BagItPHP\n[/sourcecode] Use: Creating Bags To create a bag, simply instantiate a new BagIt object with the name of a directory that doesn’t exist, add files to it, and package it into a tarball with the name of the bag: [sourcecode language=”php”]\nrequire_once ‘lib/bagit.php’; $bag = new BagIt(‘./new-directory’); $bag-&gt;addFile(‘./exhibit/index.html’, ‘index.html’);\n$bag-&gt;addFile(‘./exhibit/imgs/1.png’, ‘imgs/1.png’);\n$bag-&gt;addFile(‘./exhibit/imgs/2.png’, ‘imgs/2.png’); $bag-&gt;package(‘./new-directory’);\n// The bag package will be created named ./new-directory.tgz.\n[/sourcecode] Use: Reading Bags To read a bag, simply open an existing back, validate it (optional), fetch remote resources, and iterate over the files, copying them or processing them in some other way. [sourcecode language=”php”]\nrequire_once ‘lib/bagit.php’; $bag = new BagIt(‘./existing-bag.zip’); $bag-&gt;validate();\nif (count($bag-&gt;getBagErrors()) == 0) {\n    $bag-&gt;fetch-&gt;download(); foreach ($bag-&gt;getBagContents() as $filename) {\n    copy($filename, 'final/destination/' . basename($filename));\n} } [/sourcecode] For more information about the methods that are available, please see the documentation. Let Us Hear from You If you’re using this library or have any feedback on it, we’d love to hear from you! We are relying on the GitHub issues tracker for code feedback, so you can file bugs or other issues there . If you have a more general question, feel free to post here."},{"id":"2011-04-19-jeremy-boggs-a-plea-for-open-digital-humanities-work","title":"Jeremy Boggs: A Plea for Open Digital Humanities Work","author":"ronda-grizzle","date":"2011-04-19 08:58:54 -0400","categories":["Podcasts"],"url":"jeremy-boggs-a-plea-for-open-digital-humanities-work","layout":"post","content":"Hello there, Scholars’ Lab fans! Visiting Scholar Jeremy Boggs spoke in the Scholars’ Lab on March 3, giving a talk entitled A Plea for Open Digital Humanities Work: or, A DH Grad Student Reflects on Years of ‘Study’ . Jeremy will join the faculty of the Scholars’ Lab full time in June, as our new Humanities Design Architect . We hope you’ll enjoy the podcast of his talk. [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.6793487437/enclosure.mp3”]"},{"id":"2011-04-25-unsworth-idiosyncrasy-at-scale","title":"John Unsworth, \"Idiosyncrasy at Scale\"","author":"Admin","date":"2011-04-25 06:38:33 -0400","categories":["Podcasts"],"url":"unsworth-idiosyncrasy-at-scale","layout":"post","content":"Idiosyncrasy at Scale: Data Curation in the Humanities On March 25th, John Unsworth, Dean of the Graduate School of Library and Information Science at the University of Illinois, Urbana-Champaign spoke as part of the UVa Digital Humanities Speaker Series.  This is a speaker series jointly sponsored by SHANTI, IATH, and the Scholars’ Lab at UVa Library. In this talk, Unsworth announced that Indiana University and the University of Illinois were soon to launch the HathiTrust Research Center – which will develop “cutting-edge software tools and cyberinfrastructure to enable advanced computational access to the growing digital record of human knowledge.” A formal press release for the project has since appeared. As usual, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.7592112318/enclosure.mp3”]"},{"id":"2011-05-02-welcoming-david-mcclure","title":"Welcoming David McClure!","author":"wayne-graham","date":"2011-05-02 07:42:01 -0400","categories":["Announcements"],"url":"welcoming-david-mcclure","layout":"post","content":"We’re delighted to welcome our new Web Applications Specialist, David McClure, to the Scholars’ Lab R&amp;D team. David graduated from Yale University with a degree in Humanities in 2009, and has been working as an independent web developer in San Francisco, New York, and Madison, Wisconsin for the last few years. David is the creator of Public Poetics, an elegant PHP interface for collaborative, web-based commentary on poetry.  He characterizes this project as a design experiment in addressing “a problem of content ‘over-accumulation’ that tends to plague many existing systems of textual annotation and commenting.”  Not only does David have a strong aesthetic sense and background in web application development, he also has experience in computer graphics, having worked with graphics libraries while developing software for the Cognitive Science department at Yale. David is also an avid outdoorsman.  His essay “ Mountain Magic on the Pyrenean High Route ” was published in Backpacking Light online. David McClure will start with us in late May, and we’re excited to bring him on as a collaborator!"},{"id":"2011-05-03-myron-gutmann-data-access","title":"Myron Gutmann on Data Access","author":"Admin","date":"2011-05-03 05:56:30 -0400","categories":["Announcements"],"url":"myron-gutmann-data-access","layout":"post","content":"The UVa Digital Humanities Speaker Series presents: Myron Gutmann on “Data Access for Research and Teaching in the Twenty-First Century” Friday, May 6\n4:00pm (reception follows)\nMonroe Hall, Room 120 This talk is co-sponsored by IATH, SHANTI, the Scholars’ Lab, and the College of Arts &amp; Sciences Quantitative Collaborative Lecture Series Abstract: The scientific community is facing new opportunities and new requirements in the ways that data are managed and made available for future research. The biggest change that we see is the dramatic increase in the volume of data produced by observations, experiments, and simulations, which has turned what was already a steady stream of data into a flood.  That rising tide of data is being shared by research networks that span the globe, calling for new infrastructure and new architectures that will allow researchers to make use of data from around the world and engage in new long-distance collaborations. These new collaborations now mostly involve researchers, but the availability of new forms of data and the creation of new mechanisms for sharing those data make it possible to expand access in a meaningful way to students and citizen scientists. At the same time, policy makers are moving forward rapidly to require that data from publicly-financed research projects be shared with other researchers, while they simultaneously concern themselves with protecting the privacy and confidentiality of human research subjects. This presentation will discuss these changes in the data preservation and sharing environment, especially as they relate to data for the social, behavioral and economic sciences, and suggest ways that all the potential stakeholders in the process – funding agencies, universities, data archives, libraries, researchers, teachers, and students can work together in the future to get the most out of our data investments. Myron Gutmann is Head of the National Science Foundation’s Social, Behavioral &amp; Economics Directorate and Professor in the Department of History at the University of Michigan."},{"id":"2011-05-13-welcoming-eric-johnson","title":"Welcoming Eric Johnson!","author":"bethany-nowviskie","date":"2011-05-13 10:18:42 -0400","categories":["Announcements"],"url":"welcoming-eric-johnson","layout":"post","content":"We’re proud to announce that Eric Johnson will join the faculty of UVa Library’s Digital Research and Scholarship department this July as Head of Outreach &amp; Consulting.  In his new role, Eric will direct day-to-day public services in the Library’s vibrant Scholars’ Lab and help to coordinate our intellectual programming as well as our transformative program for Graduate Fellows in the Digital Humanities, now entering its fifth year. Eric comes to us from The Thomas Jefferson Foundation (Monticello) where he currently serves as New Media Specialist, guiding Monticello’s connection to its user community and the development of organizational media policies and services.  While at the Foundation, Eric participated in a number of digital initiatives, including the Jefferson Encyclopedia and the Thomas Jefferson Libraries project, and coordinated more traditional library services for users such as reference and research assistance for Foundation staff and visiting fellows.  He holds degrees in history from the College of William &amp; Mary and George Mason University, as well as an MS in Library and Information Science from FSU, and has two decades of experience in public service in the library and cultural heritage sector.  Eric’s recent and forthcoming publications showcase his thoughtful approach to user engagement in digital scholarship, and his deep commitment to libraries as places of community and hospitality for scholars, students, staff, and the general public. We look forward to welcoming Eric and his wife, Sheryl, to the Scholars’ Lab family this summer!"},{"id":"2011-06-09-student-jobs","title":"grad student jobs in Scholars' Lab R&D","author":"bethany-nowviskie","date":"2011-06-09 11:52:40 -0400","categories":["Announcements","Grad Student Research"],"url":"student-jobs","layout":"post","content":"Are you a maker? A builder? A tinkerer at heart? The Scholars’ Lab in Alderman Library seeks energetic and imaginative UVa graduate students from all disciplines looking to hone their digital skills. Interested in cutting-edge technologies for the digital humanities and interpretive or humanistic social sciences? Want hands-on experience in project management and the design of innovative, beautiful, and useful tools for scholarship and cultural heritage?  We encourage you to apply for this apprenticeship opportunity with our Research and Development team ! Scholars’ Lab R&amp;D is engaged in a number of projects leveraging textual encoding techniques, advanced search services, geo-temporal visualization, relational database systems, and modern Web development frameworks.  The intent of these assistantship positions is to allow you to work as part of a team, learning design, coding, and project management skills that will be invaluable to your own research and make you highly marketable within the broad field of the digital humanities. Students will have the opportunity to partner on Scholars’ Lab collaborations with UVa faculty and to help test Omeka plug-ins and craft geo-temporal scholarly arguments using Neatline, a tool we’re building with funding from the Library of Congress and in partnership with the Rosenzweig Center for History and New Media .  We’ll also be undertaking a brand-new project with our R&amp;D assistants in Fall 2011: “Crowdsourcing Interpretation.” These are 10-hour-per-week positions with the opportunity to work longer hours in the summers and between semesters.  We hope to hire several  grad students, who may also be interested in becoming Scholars’ Lab Graduate Fellows in Digital Humanities . UVa graduate students may apply through CavLink .  Search for “Scholars’ Lab Research &amp; Development Assistant.”"},{"id":"2011-06-15-myron-gutmann-data","title":"Myron Gutmann, \"Data Access for Research and Teaching in the Twenty-First Century\"","author":"ronda-grizzle","date":"2011-06-15 08:35:44 -0400","categories":["Announcements","Podcasts"],"url":"myron-gutmann-data","layout":"post","content":"Data Access for Research and Teaching in the Twenty-First Century On May 6th, Myron Gutmann, Head of the NSF’s Social, Behavioral &amp; Economics Directorate and Professor in the Department of History at the University of Michigan, spoke as part of the UVa Digital Humanities Speaker Series.  Mr. Gutmann’s talk was jointly sponsored by the Scholars’ Lab at UVa Library, SHANTI, IATH, and the College of Arts &amp; Sciences Quantitative Collaborative. In his talk, Gutmann discusses changes in the twenty-first century data access and preservation environment, especially as relate to data for social, behavioral and economic sciences. He suggests ways that all potential stakeholders in the process — funding agencies, universities, data archives, libraries, researchers, teachers, and students — might work together to get the most out of our data investments. As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.8349921266/enclosure.mp3”]"},{"id":"2011-06-17-omeka-development-with-vagrant","title":"Omeka Development with Vagrant","author":"eric-rochester","date":"2011-06-17 12:09:12 -0400","categories":["Announcements","Research and Development"],"url":"omeka-development-with-vagrant","layout":"post","content":"For the latest information about this system, please see the README file. It contains this information, plus I’ll keep it up-to-date. One of the biggest annoyances in a developer’s life is managing the software required to work on several different projects. Usually, this doesn’t just mean having different systems or programming languages, but having different versions of them as well. It isn’t a chop-your-limb-off problem, but more a death-by-1000-paper-cuts. One solution is virtual machines . They promise so much: A separate machine that can get as trashy as needed, without actually messing up the sacred laptop or desktop. Unfortunately, they can also be difficult or time-consuming to set up properly, and when a developer has to do that maybe once a week or more for new projects, it adds up quickly. Enter Vagrant . This makes defining, setting up, maintaining, and using a VM easy. Using Vagrant, I’ve been working to get an out-of-the-box Omeka VM working for development. I haven’t quite succeeded, but it’s getting there. This involves a base box (not available yet) and a Chef Solo cookbook with instructions for setting up the VM. Here’s how to use it to create a VM environment to develop a theme, plugin, or site with Omeka. These instructions work on a Mac, but they assume only that you have access to a Terminal/console window, a text editor, and a web browser, so with the right changes, it should work on any system. Note : Some of the resources (like the base box) aren’t publicly available right now. We’re working on that. Watch this space for more information. Get Vagrant and VirtualBox Vagrant’s written in Ruby, so assuming you have Ruby and RubyGems installed, just do this: [sourcecode language=”bash”]\ngem install vagrant\n[/sourcecode] Getting VirtualBox is more complicated. Check the VirtualBox website for how to install it. Setting up the Working Directory Once the software is installed, you’ll need to set up a working directory and initialize it for Vagrant. You’ll also need to download the Chef cookbooks that you’ll use. [sourcecode language=”bash”]\nmkdir omeka-project\ncd omeka-project\ngit clone https://github.com/opscode/cookbooks.git\ngit clone git://github.com/scholarslab/cookbooks.git slab-cookbooks\nvagrant init omeka-project PATH-TO/base-centos32.box\n[/sourcecode] The last command created a file called “Vagrantfile”. (It also pointed to a file that won’t exist on your system. We’re working on a URL for hosting the base box. When it’s available, use that URL in place of PATH-TO.) Go ahead and open it up in your favorite text editor. Vagrantfile is just Ruby, nothing scary there. We need to add a few lines. At the bottom of the file, just before the “end,” insert these: [sourcecode language=”ruby”]\nconfig.vm.provision :chef_solo do |chef|\n  chef.cookbooks_path = [“cookbooks”, “slab-cookbooks”]\n  chef.add_recipe “omeka”\nend\nconfig.vm.forward_port(‘mysql’, 3306, 3333)\nconfig.vm.forward_port(‘apache2’, 80, 8080)\n[/sourcecode] The first four lines tell Vagrant to set up the system using Chef Solo, and they tell Chef to use the cookbooks we downloaded from GitHub and to use the “omeka” recipe. The last two lines tell Vagrant to set up port forwarding so we can access the web server and database from the host machine, without needing to log onto the VM. Set up Omeka Now we’re ready to set up Omeka. By default, the system assumes that your Omeka code is in a subdirectory of your working directory and that it is named “omeka.” (This — and many other things — are configurable, but that’s beyond the scope of this post.) These commands will download the latest version of Omeka (as of the time I’m writing this) and change permissions on the archive directory so the web server can write to it. [sourcecode language=”bash”]\ncurl -O http://omeka.org/files/omeka-1.3.2.zip\nunzip omeka-1.3.2.zip\nmv omeka-1.3.2 omeka\nchmod -R a+rwx omeka/archive/\n[/sourcecode] There used to be something here about setting up your “db.ini” file. The Omeka Chef recipe now takes care of that for you. Start the VM Everything’s in place. Now it’s time to start the VM. From the console, just enter this command: [sourcecode language=”bash”]\nvagrant up\n[/sourcecode] A lot of lines will scroll by. Many minutes will pass. Apache, PHP, and MySQL will be installed. When you get your prompt back, you should be ready to go. You probably missed it, but these lines were near the beginning of all that output: [sourcecode language=”text”]\n[default] – mysql: 3306 =&gt; 3333 (adapter 1)\n[default] – apache2: 80 =&gt; 8080 (adapter 1)\n[default] – ssh: 22 =&gt; 2222 (adapter 1)\n[/sourcecode] These tell how you can communicate with your newly minted VM. Since it’s using port forwarding, you can pretend like you’re talking to your host box, but using the ports listed above: [sourcecode language=”bash”]\nmysql -uomeka -pomeka –protocol=TCP –port=3333 omeka\nopen http://localhost:8080/\nvagrant ssh\n[/sourcecode] Finishing the Omeka Installation Now you need to finish setting up Omeka. Just point your browser to the Omeka instance (http://localhost:8080) running on the VM and fill in the installation information like you normally would. Nothing special here. Developing The Omeka code running the site is on your host machine, in the omeka/ directory that you created above. You can put the plugins and themes that you want to use into there, and you can edit them as you like. Closing Down When you’re done for the day and you want your resources back, you can just suspend the VM by calling this: [sourcecode language=”bash”]\nvagrant suspend\n[/sourcecode] When you’re done with the project and you want to destroy the VM, the database, and everything on it, give this command: [sourcecode language=”bash”]\nvagrant destroy\n[/sourcecode] Next Steps, or What Can I Do Since It’s Vaporware? For you, if you’re interested in this, you may want to become familiar with Vagrant by looking at their instructions for Getting Started . For me, I wrote this today in the spirit of releasing early and often, and there’s lots for me to do. Here’s a little of what I’m planning initially: Make the base box publicly available; Write a README for the cookbook and Chef metadata for the Omeka Recipe; More recipes for more systems; Add phpunit, phpmd, and other PHP systems to help improve code quality; and Add a Rakefile with tasks for running phpunit, dumping the database, and other things. Hopefully this will make all of our lives easier. Stay tuned."},{"id":"2011-06-22-untimely-coding-scholars-lab-weekly-roundup","title":"Untimely Coding: Scholars’ Lab Weekly Roundup","author":"david-mcclure","date":"2011-06-22 08:00:36 -0400","categories":["Research and Development"],"url":"untimely-coding-scholars-lab-weekly-roundup","layout":"post","content":"Although the air conditioning here in Alderman Library keeps the office a bit cooler than the summer heat that’s descended on Charlottesville, things are definitely heating up in the Scholars’ Lab! May, June, and July each mark the arrival of a new face in the lab.  In May, I was delighted to make the move to Virginia and join the team as a web developer. Jeremy Boggs (of Omeka and CHNM fame) started as our new Humanities Design Architect at the beginning of this month. In the second week of July, Eric Johnson will come on board as our new Head of Outreach and Consulting. Eric Rochester, our Senior Developer who started in March, is starting to seem like an old-timer… With all the fresh intellectual energy bouncing off the walls, we’re making good progress on a number of exciting projects: BagIt plugin for Omeka – What if you have a big collection of files in one location and you want to move them onto your Omeka site?  Depending on the size of the collection and the locations of the files, this could be anything from a chore to a nightmare.  The BagIt specification was designed to provide an easy and reliable way to package and transmit files from one location to another.  Using the BagitPHP library that Eric released last month, I’ve been putting the finishing touches on an Omeka plugin that makes it possible to import and export “bags” from your Omeka site. Timeline plugin for Omeka – With the arrival of Jeremy (Omeka’s Development Lead), we’ve been mapping out the next development cycle of the Omeka + Neatline project.  First up is the Timeline plugin, which may get a significant update in the form of a new JavaScript widget to power the core timeline functionality.  The current version of the plugin uses a timeline application that doesn’t play well with the larger Omeka code ecosystem.  Jeremy is looking into a number of alternatives that would clear up the conflicts and make it possible to create a more robust and feature-rich interface for adding content to a timeline. Continuous integration with Jenkins – As part of an ongoing effort to build testing and quality control into our development process, we’ve been exploring the possibility of hooking up our Omeka plugin repositories to a continuous integration server like Jenkins .  With Jenkins running in the background, each new piece of code that we submit to one of our projects triggers a “build,” which runs through the software and checks to see the new version of the code passes a series of tests.  If the tests pass, the code gets automatically deployed.  This makes it possible to iterate applications quickly and aggressively without having to worry about the possibility of broken code slipping out the door unnoticed. In addition to these regularly scheduled efforts, we’ve also been pursuing a number of smaller projects, experiments, and collaborations.  We were delighted to have the opportunity earlier this month to hear cultural historian Tim Sherratt talk about his adventures using linked open data and third-party API development to help rediscover the lives of thousands of non-white Australians who were affected by the “White Australia” policies of the 20th century.  The slides for his presentation, “Confessions of an Impatient Historian,” are available on Slideshare .  During his visit, Tim also pointed us in the direction of JSTOR’s “ Data For Research ” resource, which served as the basis for some interesting explorations into how JSTOR keeps track of information like word counts and citation relationships. Meanwhile, Eric has been working on setting up virtual machines with Vagrant (see his blog post from last week) and I’ve been building an experimental link aggregator on Django designed to serve as a testing platform for non-traditional approaches to organizing comments and other types of text-based discussions. Stay tuned for exciting code to come!"},{"id":"2011-06-27-andrew-hankinson-applications-of-the-music-encoding-initiative-in-optical-music-recognition","title":"Andrew Hankinson, \"Applications of the Music Encoding Initiative in Optical Music Recognition\"","author":"ronda-grizzle","date":"2011-06-27 06:52:11 -0400","categories":["Podcasts"],"url":"andrew-hankinson-applications-of-the-music-encoding-initiative-in-optical-music-recognition","layout":"post","content":"Applications of the Music Encoding Initiative in Optical Music Recognition On April 20th, the Scholars’ Lab welcomed visiting scholar Andrew Hankinson, a PhD candidate in the Distributed Digital Music Archives and Libraries Lab at the Schulich School of Music at McGill University. Mr. Hankinson’s talk introduced work at the Distributed Digital Music Archives and Libraries Lab on creating optical music recognition systems for recognizing printed music books, specifically focusing on the use of MEI to encode and preserve musical works. Hankinson also presented applications in notation data analysis, searching, and discovery of printed music works. As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.8395768334/enclosure.mp3”]"},{"id":"2011-06-27-scholars-lab-grad-fellows-digital-therapy-talks","title":"Scholars' Lab Grad Fellows, \"Digital Therapy\" Talks","author":"ronda-grizzle","date":"2011-06-27 06:53:05 -0400","categories":["Podcasts"],"url":"scholars-lab-grad-fellows-digital-therapy-talks","layout":"post","content":"Scholars’ Lab Grad Fellows Chris Clapp, Tom Finger and Alex Gil Our Grad Fellows in Digital Humanities are asked to make a final presentation the results of their work at springtime luncheon talks. This April, our 2010-2011 fellows (Chris Clapp, Department of Economics; Tom Finger, Corcoran Department of History; and Alex Gil, Department of English) spoke in the Scholars’ Lab. Chris Clapp Chris (our first Econ fellow) discussed his research on the effects of congestion pricing policies on  commuter behavior, residential location decisions, and ultimately  congestion itself. [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.7587118138/enclosure.mp3”] Tom Finger Tom discussed his dissertation research highlighting the growth of the North Atlantic grain trade between the United States and Great Britain during the nineteenth century, offering a case study of the ways in which technologies, ecosystems, and human social groups interact over large scale economic systems. [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.8388458488/enclosure.mp3”] Alex Gil Alex showed a series of mock-ups that exemplify his concept for “deep representation” of texts in a digital environment. His talk focused on showing where we are now in terms of digital scholarly editing, and where we can go from here. Alex used examples from his own editorial work on Aimé Césaire’s Et les chiens se taisaient . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.8395768094/enclosure.mp3”] As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU ."},{"id":"2011-07-05-fedora-connector-new-and-improved","title":"Fedora Connector - New and Improved","author":"david-mcclure","date":"2011-07-05 06:16:31 -0400","categories":["Announcements","Research and Development"],"url":"fedora-connector-new-and-improved","layout":"post","content":"This week we’re pleased to announce that we’ve finished up the first pass on a major redesign of our Fedora Connector plugin for Omeka.  Fedora Connector makes it possible to connect an Omeka site to a Fedora Commons repository.  After entering basic information about the location of the Fedora repository, you can create linkages between “datastreams” in Fedora and the native Omeka items on your site.  Once those associations are configured, you can import all of the metadata from Fedora into Omeka with a single click. You can grab the code on GitHub . We’ve completely redesigned the front-end administrative interfaces and made a lot of plumbing upgrades to the code that communicates with Fedora.  One of the challenges with this type of “glue” software – code that crosses back and forth between different systems and data standards – is that it’s always something of a losing game to try to cover all possible use cases.  For example, Fedora Connector needs to query the XML in a Fedora datastream for a particular set of nodes that can be used to populate the native Dublin Core fields for Omeka items.  That’s pretty straightforward if the datastream is already encoded in Dublin Core – but if it’s not, the plugin needs a “conversion table” of sorts so that it knows that node X in Dublin Core corresponds to node Y in whatever schema the target datastream is marked up in. Trying to code out all possible combinations of metadata standards would devolve into a big game of programming whack-a-mole.  Some permutations are obvious.  For example, the new version of Fedora Connector ships with code to convert from the commonly used MODS metadata format.  But the edge cases aren’t so obvious, and beyond a certain point it becomes a fool’s errand to try to guess to hard about software will be used in the wild. Luckily, this is just the sort of situation where open source really shines!  Eric and Wayne concocted a highly clever system that essentially creates a “sub-plugin” ecosystem inside of the Fedora Connector plugin that makes it possible to modularly snap in new chunks of code to handle specific metadata formats.  This way, if you need the plugin to communicate with an unsupported metadata standard on your Fedora server, you can just write out the XPath queries that map onto the Dublin Core fields, and, with a minimal amount of easy-on-the-eyes boilerplate, plug the new importer into Fedora Connector. Here’s how it works.  New importers just inherit from an abstract class called FedoraConnector_AbstractImporter, which handles all of the utility tasks of fetching content from Fedora and writing new values to the Omeka database.  All you have to do is define two functions.  The first, called canImport(), essentially just announces to the sub-plugin system that datastreams of metadata type X (whatever standard your code is supporting) can now be imported.  And the second, called getQueries(), takes the name of Dublin Core value and returns the queries needed to find the corresponding value(s) in the datastream content. First, though, we need to tell PHP where to find the abstract class: [sourcecode language=”php”]\nrequire_once FEDORA_CONNECTOR_PLUGIN_DIR . ‘/libraries/FedoraConnector/AbstractImporter.php’;\n[/sourcecode] Next, declare your new class and tell it to inherit from the abstract base class: [sourcecode language=”php”]\nrequire_once FEDORA_CONNECTOR_PLUGIN_DIR . ‘/libraries/FedoraConnector/AbstractImporter.php’; class NameOfFormat_Importer extends FedoraConnector_AbstractImporter\n{ }\n[/sourcecode] Make sure to name your class with the format [NameOfFormat]_Importer so that Fedora Connector can see it.  Now, add the declarations for the two functions: [sourcecode language=”php”]\nrequire_once FEDORA_CONNECTOR_PLUGIN_DIR . ‘/libraries/FedoraConnector/AbstractImporter.php’; class NameOfFormat_Importer extends FedoraConnector_AbstractImporter\n{ public function canImport($datastream)\n{\n\n}\n\npublic function getQueries($name)\n{\n\n} } [/sourcecode] In canImport(), you just need to return a true or false value depending on whether or not the passed in datastream matches the format that your importer is designed to handle.  This can be done by just checking for equality and returning the result of the evaluation: [sourcecode language=”php”]\nrequire_once FEDORA_CONNECTOR_PLUGIN_DIR . ‘/libraries/FedoraConnector/AbstractImporter.php’; class NameOfFormat_Importer extends FedoraConnector_AbstractImporter\n{ public function canImport($datastream)\n{\n\treturn ($datastream-&gt;metadata_stream == ‘NameOfFormat’);\n}\n\npublic function getQueries($name)\n{\n\n} } [/sourcecode] Now, all that’s left is to process the Dublin Core $name variable getting passed into the getQueries() function and return an array of XPath queries that will pluck the corresponding nodes out of a document marked up in the format that you need to accommodate.  Just run a switch-case on the $name and run through all the Dublin Core values: [sourcecode language=”php”]\nrequire_once FEDORA_CONNECTOR_PLUGIN_DIR . ‘/libraries/FedoraConnector/AbstractImporter.php’; class NameOfFormat_Importer extends FedoraConnector_AbstractImporter\n{ public function canImport($datastream)\n{\n\treturn ($datastream-&gt;metadata_stream == ‘NameOfFormat’);\n}\n\npublic function getQueries($name)\n{\n\n\tswitch ($name) {\n\n\t\tcase 'Title':\n\n\t\t\t$queries = array(\n\t\t\t'//*[local-name()=\"mods\"]/*[local-name()=\"titleInfo\"]'\n\t\t\t\t. '/*[local-name()=\"title\"]'\n\t\t\t);\n\n\t\tbreak;\n\n\t\tcase 'Creator':\n\n\t\t\t$queries = array(\n\t\t\t'//*[local-name()=\"mods\"]'\n\t\t\t\t. '/*[local-name()=\"name\"][*[local-name()=\"role\"] = \"creator\"]'\n\t\t\t);\n\n\t\tbreak;\n\n\t\t[...handle all DC values...]\n\n\t}\n\n\treturn $queries;\n\n} } [/sourcecode] Stick the file in the FedoraConnector/Importers directory and you’re good to go!  We’ve also implemented a nifty method of adding custom “renderers” to the plugin - code that controls the display of various data formats on the public-facing Omeka site.  Check out the README.md file on GitHub for details, and watch for more development on the plugin in the near future."},{"id":"2011-07-05-tim-sherratt-confessions-of-an-impatient-historian","title":"Tim Sherratt, Confessions of an Impatient Historian","author":"ronda-grizzle","date":"2011-07-05 06:06:14 -0400","categories":["Podcasts"],"url":"tim-sherratt-confessions-of-an-impatient-historian","layout":"post","content":"Confessions of an Impatient Historian On June 8th, the Scholars’ Lab welcomed visiting scholar Tim Sherratt, digital historian, web developer and cultural data hacker who’s been developing online resources relating to archives, museums and history since 1993. Tim is currently employed by the National Museum of Australia, as well as being an Adjunct Associate Professor in the Digital Design and Media Arts Research Cluster at the University of Canberra. He is one of the organizers of THATCamp Canberra and is a member of the interim committee of the new Australian Association for the Digital Humanities. Tim’s talk investigated what happens when you equip an impatient historian with some rudimentary coding skills and a borderline obsession. Reflecting on his experiences in and around a number of Australia’s national cultural institutions, he considered how digitally-enhanced impatience can change our relationship both with our sources and the institutions that hold them. Explore Tim’s digital tools and renegade APIs at WraggeLabs Emporium . As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.8576900213/enclosure.mp3”]"},{"id":"2011-07-21-introduction-to-linked-open-data-at-rare-books-school","title":"Introduction to Linked Open Data at Rare Book School","author":"eric-rochester","date":"2011-07-21 10:15:20 -0400","categories":["Research and Development"],"url":"introduction-to-linked-open-data-at-rare-books-school","layout":"post","content":"Yesterday, I was fortunate to be invited by Andrew Stauffer and Bethany Nowviskie to present at their Rare Book School course, Digitizing the Historical Record . I talked about Linked Open Data (LOD), and afterward, Dana Wheeles talked about the NINES project and how they use RDF and LOD. I tried to present a gentle, mostly non-technical introduction to LOD, with an example of it in action. Hopefully, this posting will be a 50,000 foot overview also. The Linked Open Data Universe Linking Open Data cloud diagram, by Richard Cyganiak and Anja Jentzsch. http://lod-cloud.net/ The first thing to know about LOD is that it’s everywhere. Look at the Linked Open Data cloud diagram above. All of these institutions are publishing data that anyone can use, and their data references others’ data also. Linked Data vs Open Data vs RDF Data First we need to unpack the term Linked Open Data : Linked is an approach to data. You need to provide context for your data; you need to point to other’s data. Open is a policy. Your data is out there for others to look at and use; you explicitly give others this permission. Data is a technology and a set of standards. Your data is available using an RDF data model (usually) so computers can easily process it. (See Christopher Gutteridge’s post for more about this distinction.) Five Stars Creating LOD can seem overwhelming. Where do you start? What do you have to do? It’s not an all or nothing proposition. You can take what you have, figure out how close you are to LOD, and work gradually toward making your information a full member of the LOD cloud. The LOD community talks about having four-star data or five-star data. Here are what the different stars denote: You’ve released the data using any format under an open license that allows others to view and use your data; You’ve released the data in a structured format so that some program can deal with it (e.g., Excel); You’ve released the data in a non-proprietary format, like CVS; You’ve used HTTP URIs (things you can type into your web browser’s location bar) to identify things in your data and made those URIs available on the web so others can point to your stuff; You explicitly link your data to others’ data to provide context. (This is all over the web. Michael Hausenblas’ explanation with examples is a good starting point.) Representing Knowledge A large part of this is about representing knowledge so computers can easily process it. Often LOD is encoded using Resource Description Framework (RDF) . This provides a way to model information using a series of statements. Each statement has three parts: a subject, a predicate, and an object. Subjects and predicates must be URIs. Objects can be URIs (linked data) or data literals. The predicates that you can use are grouped into vocabularies . Each vocabulary is used for a specific domain. We’re getting abstract, so let’s ground this discussion by looking at a specific vocabulary and set of statements. Friend of a Friend For describing people, there’s a vocabulary standard called Friend of a Friend (FOAF) . I’ve used that on my web site to provide information about me. (The file on my website is in RDF/XML, which can be frightening. I’ve converted it to Turtle, which we can walk through more easily.) I’ll show you parts of it line-by-line. (Ahem. Before we start, a disclaimer: I need to update my FOAF file. It doesn’t reflect best practices. The referencing URL isn’t quite the way it should be, and it uses deprecated FOAF predicates. That said, if you can ignore my dirty laundry, it still illustrates the points I want to make about the basic structure of RDF.) First, &lt;code&gt;@prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; .\n&lt;/code&gt; This just says that anywhere foaf: appears later, replace it with the URL http://xmlns.com/foaf/0.1/ . &lt;code&gt;[] a &lt;http://xmlns.com/foaf/0.1/Person&gt;;\n&lt;/code&gt; This is a statement. [] just means that it’s talking about the document itself, which in this case is a stand-in for me. The predicate here is a, which is a shortcut that’s used to tell what type of an object something is. In this case, it says that I’m a person, as FOAF defines it. And because the line ends in a semicolon, the rest of the statements are also about me. Or more specifically, about [] . &lt;code&gt;foaf:firstName \"Eric\";\nfoaf:surname \"Rochester\";\nfoaf:name \"Eric Rochester\";\nfoaf:nick \"Eric\";\n&lt;/code&gt; This set of statements still have the implied subject of me, and they use a series of predicates from FOAF. The object of each is a literal string, giving a value. Roughly this translates into four statements: Eric’s first name is “Eric.” Eric’s given name is “Rochester.” Eric’s full name is “Eric Rochester.” Eric’s nickname is “Eric.” The next statement is a little different: &lt;code&gt;foaf:workplaceHomepage &lt;http://www.scholarslab.org/&gt; .\n&lt;/code&gt; This final statement has a URI as the object. It represents this statement: Eric’s workplace’s home page is “ http://www.scholarslab.org/ ”. If this was a little overwhelming, thank you for sticking around this far. Now here’s what you need to know about modeling information using RDF: Everything is expressed as subject-predicate-object statements; and Predicates are grouped into vocabularies. The rest is just details. Linked Open Data and the Semantic Web During my presentation, someone pointed out that this all sounds a lot like the Semantic Web . Yes, it does. LOD is the semantic web without the focus on understanding and focusing more on what we can do. Understanding may come later—or not—but in the meantime we can still do some pretty cool things. So What? The benefit of all this is that it provides another layer for the internet. You can use this information to augment your own services (e.g., Google augments their search results with RDF data about product reviews) or build services on top of this information. If you’re curious for more or still aren’t convinced, visit the Open Bibliographic Data Guide . They make a business case and articulate some use cases for LOD for libraries and other institutions. For Example Discussing LOD can get pretty abstract and pretty meta. To keep things grounded, I spent a few hours and threw together a quick demonstration of what you can do with LOD. The Library of Congress’ Chronicling America project exposes data about the newspapers in its archives using RDF. It’s five-star data, too. For example, to tell the geographic location that the papers covered, it links to both GeoNames and DBpedia . The LoC doesn’t provide the coordinates of these cities, but because they express the places with a link, I can follow those and read the latitude and longitude from there. I wrote a Python script that uses RDFlib to read the data from the LoC and GeoNames and writes it out using KML . You can view this file using Google Maps or Google Earth. Here’s the results of one run of the script. (I randomly pick 100 newspapers from the LoC, so the results of each run is different.) View Larger Map You can find the source for this example on both Github and BitBucket: https://github.com/erochest/loc-chronicling-map https://bitbucket.org/erochest/loc-chronicling-map/overview Resources Throughout this post, I’ve tried to link to some resources. Here are a few more (not all of these will be appropriate to a novice): The Wikipedia page on linked data . The Open Bibliographic Data Guide, which provides rationales for LOD. A portal to LOD resources and tools . A portal maintained by the W3C . The LOD cloud . The four rules of LOD . The five rules . Linked vs Open vs Data . A book on publishing LOD on the internet ."},{"id":"2011-08-04-this-week-in-open-source","title":"This week in Open Source","author":"wayne-graham","date":"2011-08-04 02:30:00 -0400","categories":["Research and Development"],"url":"this-week-in-open-source","layout":"post","content":"After reading a post on one of my favorite blogs, Giant Robots Smashing Into Other Giant Robots, I was inspired to start this series chronicling highlights in our Open Source development efforts. It was a busy week for the Scholars’ Lab R&amp;D team, with updates to the FedoraConnector, NeatlineMaps, and Timeline plugins for Omeka, as well as updates to our Vagrant cookbooks (and an Omeka dev example ), and BagItPHP library. FedoraConnector David McClure ( davidmcclure ) greatly improved the workflow for updating metadata from FedoraCommons objects in to an Omeka instance 04b8ee8. Users can now set not only system-wide defaults for polling updates to metadata records from a Fedora server, but also on a per-field basis within individual records, giving users greater control over their metadata pulls. NeatlineMaps David McClure ( davidmcclure ) also has begun work on a major refactor of the NeatlineMaps plugin on the refactor branch of the project. There is now much smarter support for dynamically determining a map’s bounding box ( c4090fe ). Just a note if you are checking out this branch: this commit ( 032adf3 ) adds the map in the item view, but is default code and displays the map incorrectly due to a bug in the projection. Next up is a major revamp of the administration interface, which will provide a facade for GeoServer inside of Omeka, making it easy to manage collections of maps and assign them to servers and namespaces. Timeline Jeremy Boggs ( clioweb ) has re-implemented the Timeline plugin to use the jQuery Timeglider plugin . This update ( b078df8 ) replaces the plugin’s use of the SIMILE Timeline and begins to add support for a custom JSON output ( 51eb12a ) for Omeka records for use in Timeglider timelines. Vagrant Eric Rochester ( erochest ) has been working on standardizing our development, testing, and deployment environments using virtual servers with Vagrant . He has set up a repository of cookbooks which automate the setup of development environments for various projects. For a cool example, check out the dev environment for our project with Louis Nelson, chronicling the architectural development of Falmouth Jamaica . BagitPHP We spent some time this week moving our Omeka plugins (and other libraries) to a new Jenkins server. In getting the PEAR/Pecl packages properly set up, Wayne Graham ( waynegraham ) updated the ant script for the various testing and code reports that we run on our software  ( 465fa89 ). Jenkins To better automate our testing environment, David McClure ( davidmcclure ) worked out the final issues with automating the testing of our plugins with multiple versions of Omeka. We are currently targeting the current stable tag of Omeka, and the master branch . We are working on turning everything green, but the major work of integrating the plugins in to the test environment is complete. It took a bit of tinkering to get the plugins to build correctly when they’re not sitting inside of an Omeka installation tree (as they would in a standard configuration). Watch this space for a more detailed post about how to replicate this set up."},{"id":"2011-08-08-life-liberty-and-the-pursuit-of-mappiness","title":"Life, Liberty, and the Pursuit of Mappiness","author":"kelly-johnston","date":"2011-08-08 05:35:18 -0400","categories":["Digital Humanities","Geospatial and Temporal","Visualization and Data Mining"],"url":"life-liberty-and-the-pursuit-of-mappiness","layout":"post","content":"Mr. Jefferson ended his best-known sentence with “ Life, Liberty and the pursuit of Happiness .”   The only thing missing was maps. In the Scholars’ Lab, we’re all about the spatial goodness.   Inspired by Kansas State University’s Seven Deadly Sins maps, we set about converting the qualities of life, liberty, and the pursuit of happiness into quantities we could visualize through maps.  Brainstorming commenced on how best to measure the unmeasurable.   Mappiness ensued. We calculated a score for Life, for Liberty, and for the Pursuit of Happiness for each county in the continental U.S. and mapped how each county’s score deviated from the mean.  So our maps highlight extremes, both high and low. Life mapped combined male and female Life Expectancy At Birth data for 1999. Liberty mapped US Census datasets for 2000 to measure the institutionalized population as a percent of the total population. Pursuit of Happiness mapped the ratio of arts, entertainment, and recreation establishments to the total population from the 2002 US Economic Census. Transforming datasets from a spreadsheet to a map takes advantage of our  human ability to consume mass quantities of information visually.  Rows of numbers stashed away in academic journals and US  Census tables come alive when mapped to show comparisons with their neighbors both near and far.  Patterns and clusters appear.  New questions are asked. New answers come.  New maps emerge. And here’s good news:  If you find our measures of Mr. Jefferson’s famous phrase lacking, software tools to combine and manipulate datasets that may share only common geographic markers can now make cartographers  of us all . Happy mapping!"},{"id":"2011-08-10-web-development-template-rails-3-1-html-5-boilerplate-960-gs","title":"Web Development Template: Rails 3.1, HTML 5 Boilerplate, 960.gs","author":"eric-rochester","date":"2011-08-10 04:54:16 -0400","categories":["Research and Development"],"url":"web-development-template-rails-3-1-html-5-boilerplate-960-gs","layout":"post","content":"One of the best things about web development is that there are so many tools around to make the job easier. One of the worst things about web development is that there are so many tools around, and they just don’t play well together. Often, they’re not fighting publicly. Instead, they’re digging at each other passive-aggressively and making everyone’s life slightly unpleasant. I’ve taken three of these tools and put them together in a directory and tried to smooth things over: Ruby on Rails 3.1, a framework for developing web applications in Ruby; Some useful Ruby Gems; HTML5 Boilerplate, a template for creating web pages using the latest technologies and best practices; and 960 Grid System, a set of CSS rules for laying out content on web pages in columns. Putting these together isn’t difficult, and it’s not really a lot of work. But now you don’t have to do any of that. You can just clone the template from GitHub and start building things: https://github.com/scholarslab/rails31-template For more details on how to use this, see the README file ."},{"id":"2011-08-15-last-week-in-open-source","title":"Last week in Open Source","author":"wayne-graham","date":"2011-08-15 05:42:51 -0400","categories":["Research and Development"],"url":"last-week-in-open-source","layout":"post","content":"Another busy week in the Scholars’ Lab R&amp;D offices. If you have anything to contribute, remember, pull requests are welcome! Rails 3.1 Template Eric Rochester ( erochest ) spent some time building a template for Rails 3.1 projects which include HTML 5 Boilerplate and 960gs for erb template layouts, along with rspec-rails, annotate, faker, webrat, spork, and factory_girl_rails gems to help speed spinning up a new rails project. He wrote a nice post on using this technique, then we refactored it a bit, using the rails application templating system, as well modifying the inclusion of styles and javascripts as we read more of the sprockets 2.0 source to see how it actually handles combining CSS and Javascript. The entire group has been working with Rails a bit more the past couple of weeks (we have lots of experience in the group with Python and PHP), and there were some humorous responses when I asked their thoughts on rails and the new asset pipeline: “I like it a lot more than I used too…” - Jeremy Boggs “The asset pipeline is easy enough to override…” - Eric Rochester “Meh…” - David McClure Octopress Eric Rochester reworked his blog in Octopress, a Jekyll framework for blogging ( 0b331a9 ). No more database overhead, just straight up HTML goodness! If you feel Wordpress is overkill for your needs, and love markdown (or other minimalist markups), you should definitely check this technique out. Chef Eric Rochester found a bug in the Chef when using httpd on CentOS 6 ( pull request ). This update changes the default pid file for the httpd daemon and updates the Apache configuration to appropriately assign the pid as expected. There is really nothing worse than a stale pid file that stalls a daemon. NeatlineMaps David McClure is getting closer to finishing a major refactor of the Neatline Maps plugin for Omeka, which makes is possible to display .tiff files hosted in GeoServer with a “slippy” map courtesy of OpenLayers . The current maps branch of the plugin has quite a number of UX improvements that ease the building of composite maps from of a number of files and associate them with Omeka items. The Mind is a Metaphor Wayne Graham has started the work of migrating a Rails 2 application to Rails 3 using upgrade plugins and rake tasks. Not only is this application upgrading frameworks, but also is being upgraded to run on Ruby 1.9. As support for Ruby 1.8.7 will reach its EOL, with normal maintenance until June 2012 and security fixes until June 2013. After spending some time grinding on this, enough has changed that I’m going to try a new tack and merge models and controllers into a blank project because of the difference in how rails sets itself up depending on if 1.9 or 1.8 is used to initialize the project. Git Flow We are attempting to standardize our git workflow to make a bit more sense and be “safer.” After reading the great piece by Vincent Driessen (“ A successful Git branching model ” pdf ), we’ve begun utilizing gitflow .  Essentially it  breaks down to a master branch (the release), a develop branch (your next version), release versions (archive of past releases). You then create local “feature” branches that you merge in to the develop branch, merging up the tree as needed. I personally am digging the fact that to merge my local branches I no longer have to type: [sourcecode language=”bash”]\ngit co develop\ngit merge –no-ff mybranch\ngit branch -d mybranch\ngit push origin develop\n[/sourcecode] There are only two lines now: [sourcecode language=”bash”]\ngit flow feature finish mybranch\ngit push origin develop\n[/sourcecode]"},{"id":"2011-08-17-charlottesvilles-street-car-system-in-gis","title":"Charlottesville's Street Car System in GIS","author":"chris-gist","date":"2011-08-17 05:27:38 -0400","categories":["Digital Humanities","Geospatial and Temporal"],"url":"charlottesvilles-street-car-system-in-gis","layout":"post","content":"Did you know that Charlottesville once had streetcars?  Since moving to town, I’ve heard tales of the once-thriving transportation system that connected Fry’s Spring, UVa and downtown.  It wasn’t until an inquiry came in from a student looking for GIS data for the system that I investigated it. [![](http://static.scholarslab.org/wp-content/uploads/2011/08/muleCar-1024x520.jpg)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/mulecar/) Mules pulling streetcar on Main St. - Special Collections, University of Virginia Library I first found the following 1890 map which shows the holdings and plans for the Charlottesville Land Company.  The map highlights the existing streetcar system and plans to extend the system into their new neighborhoods.  We also found a large number of streetcar-related images from the Holsinger Photo Collection . [![](http://static.scholarslab.org/wp-content/uploads/2011/08/venable-1024x605.jpg)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/venable/) Charlottesville Land Co., 1890 - Special Collections, University of Virginia Library The Charlottesville Land Co. map is intriguing.  Not only does it show the eventual street grid for Belmont and Rose Hill, it has unrealized plans for the southwest quadrant of the city.  The extensive streetcar plan designed to service these new developments piqued my interest. After a little research, I found a book called  “Forward is the Motto of Today”: Street Railways in Charlottesville, Virginia, 1866-1936 by Jefferson Randolph Kean.  The UVa Library has two copies, one in Special Collections and one in general circulation.  The book gives the entire history of the rail system from its modest beginnings with mule-drawn cars down Main St. to an extensive electric system.  One of the best qualities of the book is that it has maps drawn by the book’s publisher, Harold E. Cox. [![](http://static.scholarslab.org/wp-content/uploads/2011/08/1891map-1024x836.jpg)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/1891map/) Charlottesville streetcar system 1891 from \"Forward is the Motto of Today\": Street Railways in Charlottesville, Virginia, 1866 - 1936 - H.E. Cox, reproduced with permission The 1891 map shows the main trunk line which connects the UVa Corner to the C &amp; O station on the east end of downtown via Main Street. Notice the Fry’s Spring R.R. line from the train station area to the Hotel Albemarle.  This line was eventually abandoned and replaced with one following the Jefferson Park Avenue route.  The Belmont R.R. line servicing “The Grove” (now Belmont Park) was also later abandoned. [![](http://static.scholarslab.org/wp-content/uploads/2011/08/1895map-1024x684.jpg)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/1895map/) Charlottesville streetcar system 1895 from \"Forward is the Motto of Today\": Street Railways in Charlottesville, Virginia, 1866 - 1936 - H.E. Cox, reproduced with permission The 1895 map shows planned expansion to Woolen Mills, Fifeville, and 10th and Page neighborhoods.  The section downtown (South Street, 1st Street, and Market Street) that was built but abandoned is particularly interesting.  The entire section was never more than two blocks from the main line, which probably explains why it was removed. [![](http://static.scholarslab.org/wp-content/uploads/2011/08/1920map-1024x705.jpg)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/1920map/) Charlottesville streetcar system 1920 from \"Forward is the Motto of Today\": Street Railways in Charlottesville, Virginia, 1866 - 1936 - H.E. Cox, reproduced with permission The 1920 map shows an expansion through UVa to service Lambeth Field and down Jefferson Park Ave. to Fry’s Spring.  This map also depicts the “pass-bys” (represented with the double lines in five locations), car barn (on Ridge St.), and turn-arounds at the end of all the lines. GISing the Data Reviewing Kean’s work and other maps led me to some interesting questions.  What exact streets did the trolleys use?  Are any of the facilities remaining?  If the Charlottesville Land Co.’s streetcar plan was implemented and still existed, how many Charlottesville citizens would be served?  How do you answer these questions?  Here is where GIS can help. Steps in spatial analysis: Georeference the maps.  Georeferencing is the process of taking scanned maps and geolocating them - generally through the use of control points - for use in GIS or other tools.  Check here for more information.\n2  Digitize the relevant features using the ArcMap editor.  More information here . Perform spatial analysis.  In this case, creating a service area around the lines using 1/4 mile buffer.  That distance is considered a serviceable walking distance. [![](http://static.scholarslab.org/wp-content/uploads/2011/08/CLC-1024x768.png)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/clc/) Georeferenced Charlottesville Land Co. map The above image shows the georeferenced Charlottesville Land Co. map with 40% transparency overlaid on an Open Street Map  (OSM) layer. [![](http://static.scholarslab.org/wp-content/uploads/2011/08/1920-1024x768.png)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/1920/) 1920 map with OSM base map This copy of the 1920 map was the first one I acquired using a handheld camera (opposed to scanning as with the other version).  Notice my fingers in the lower left holding the book open.  While nowhere near optimal, this shows that even poor quality photographs of maps can be georeferenced with decent results. Once all the maps were georeferenced, I was able to digitize all the pertinent streetcar system features.  I chose to show all lines from the maps including conceptual, planned, and abandoned.  I also included all the pass-bys, turn-arounds and support buildings including the electric generation plant on the Rivanna River near Woolen Mills (not on the map above but found using aerial images). [![](http://static.scholarslab.org/wp-content/uploads/2011/08/trolleyFeatures-1024x791.png)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/trolleyfeatures/) Digitized Charlottesville streetcar system overlayed on modern aerial imagery The above map shows all the streetcar routes, realized or otherwise.  It includes all the system features except the power plant adjacent to the Rivanna River.  A PDF showing the full extent of the system (with the ability to toggle layers on and off) is available here . So what if the Charlottesville streetcar system was fully realized and still existed today?  How many Charlottesville citizens would it serve?   Using spatial analysis techniques, we can answer that question.  To do this, I downloaded the 2010 census population counts and joined them to a block-level boundary layer.  I then created the walking buffer of 1/4 mile around the streetcar routes and aggregated the census blocks within the buffer to get the answer. [![](http://static.scholarslab.org/wp-content/uploads/2011/08/trolleyCensus-1024x768.png)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/trolleycensus/) Proposed Charlottesville streetcar system with 1/4 mile service area buffer overlayed on 2010 census blocks The above map shows the full streetcar system in black with the service buffer around it in red.  The background is the 2010 census blocks.  Notice the two donut holes and the white areas.  The white areas in the map are Albemarle County.  At this point, we are only interested in city residents.  However, if you wanted to get the full extent of the service population, the UVa Grounds would have to be included (the eastern section of the service area is mainly over the river and unpopulated areas).  The next step is to use a technique called a spatial join to aggregate the total population for each block to the streetcar service area.  More on that here . [![](http://static.scholarslab.org/wp-content/uploads/2011/08/bufferJoin.png)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/bufferjoin/) Showing attributes of spatial join aggregation You can see by the last column in the above table that the streetcar service area covers around 33,267 city residents.  I say around because any block that intersected the service area buffer boundary was counted in full even though a percentage of people living within that block may not be within the buffer.  Using our block level tabular data, we can gather some basic statistics including total population for the city. [![](http://static.scholarslab.org/wp-content/uploads/2011/08/sum.png)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/sum/) Using the basic statistics feature in ArcMap The 2010 total population for Charlottesville is 43,475 which mean that approximately 76% of the city’s population would be served by the streetcar system. Some More Pictures The Holsinger Collection in the UVa Library’s Special Collections has many great pictures of the streetcar system.  Here are some of my favorites along with a few other images. ![](http://static.scholarslab.org/wp-content/uploads/2011/08/X02377B-1024x809.jpg) Streetcar near Rotunda - Holsinger Collection - Special Collections, University of Virginia Library [![](http://static.scholarslab.org/wp-content/uploads/2011/08/Y20866B-1024x801.png)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/y20866b/) Main St. looking east, electric street car about to pass Christian's Pizza (third full building on the right) - Holsinger Collection - Special Collections, University of Virginia Library [![](http://static.scholarslab.org/wp-content/uploads/2011/08/X1803B.jpg)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/x1803b/) Looking north towards power plant, Rivanna River on the right - Holsinger Collection - Special Collections, University of Virginia Library [![](http://static.scholarslab.org/wp-content/uploads/2011/08/X06208B1-copy-1024x861.png)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/x06208b1-copy/) Working on the tracks, looking north up JPA Extended - houses in background exist today - Holsinger Collection - Special Collections, University of Virginia Library [![](http://static.scholarslab.org/wp-content/uploads/2011/08/X06114B11-1024x837.png)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/x06114b1-2/) Troops marching north up Rugby along tracks, Fayerweather Hall on the left, Mad Bowl on the right - Holsinger Collection - Special Collections, University of Virginia Library [![](http://static.scholarslab.org/wp-content/uploads/2011/08/X06208B3-1024x854.png)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/x06208b3/) Ridge St. with car barn in background, bridge over tracks and church exist today, car barn now the Greyhound station bus entrance - Holsinger Collection - Special Collections, University of Virginia Library [![](http://static.scholarslab.org/wp-content/uploads/2011/08/Y08206B-1024x810.png)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/y08206b/) Installing tracks on University Ave. near the Corner - Holsinger Collection - Special Collections, University of Virginia Library Of course once you know where to look, evidence of the streetcar system is around. ![](http://static.scholarslab.org/wp-content/uploads/2011/08/trolley-005-1024x768.jpg) Intersection of University and Rugby looking southeast If you look closely in the crosswalk there, you can see the old track as it turns off University Ave. onto Rugby Ave. ![](http://static.scholarslab.org/wp-content/uploads/2011/08/trolley-008-1024x768.jpg) Closeup of exposed streetcar track in crosswalk across Rugby at University"},{"id":"2011-08-24-announcing-the-praxis-program","title":"Announcing the Praxis Program","author":"bethany-nowviskie","date":"2011-08-24 06:16:48 -0400","categories":["Grad Student Research"],"url":"announcing-the-praxis-program","layout":"post","content":"We’ve radically re-imagined the teaching and training we do in Scholars’ Lab R&amp;D, and today are excited to launch the Praxis Program ! This is a pilot, we hope, of big things to come, and it is meant to complement our work with Graduate Fellows in Digital Humanities, a program now entering its fifth year at UVa Library. In pilot mode, the Praxis Program will fund six University of Virginia graduate students from a variety of disciplines to apprentice with us for an academic year as we design and build Prism, a new tool for “crowd-sourced” textual analysis, visualization, and humanities interpretation. (More about that, later.) Our goal is fairly lofty: recognizing that methodological training in the digital humanities is often absent or catch-as-catch-can at the graduate level, we are using the Praxis Program to experiment with an action-oriented curriculum live and in public, – hoping to attract local allies as well as partners in labs and centers at other institutions (which could, in future, work as nodes in a larger Praxis Program network). Above all, we want to situate our contribution to methodological training within a larger conversation about the changing demands of the humanities in a digital age. The Praxis Program will equip knowledge workers for emerging faculty positions and alternative academic careers at a moment in which new questions can be asked and new systems built. We’ll share our evolving curriculum and our faculty, staff, and students alike will be blogging about their experience . So watch us this year as we see what it takes to produce thoughtful DH scholars who are comfortable designing effective user experiences, writing and working with open source code, engaging broad audiences, managing teams and budgets, and theorizing their work within the rich tradition of humanities computing."},{"id":"2011-08-24-mapping-the-earthquake","title":"Mapping the Earthquake","author":"chris-gist","date":"2011-08-24 12:40:08 -0400","categories":["Geospatial and Temporal"],"url":"mapping-the-earthquake","layout":"post","content":"One good thing about living in this age is instant access to information.  What could be better than that?  Maps! The USGS has up-to-the-minute maps for earthquakes all over the world.  For the latest Virginia events click  here .  You can find their main earthquake page  here . The USGS also has a crowd-sourced program - called Do You Feel It? - where users can gauge the quake at their location and report back to help build the map below.  More on that program  here . [![City map](http://earthquake.usgs.gov/earthquakes/dyfi/events/se/082311a/us/se082311a_ciim.jpg)](http://earthquake.usgs.gov/earthquakes/dyfi/events/se/082311a/us/index.html) Crowd-sourced intensity map - USGS The good people at Development Seed  created some cool maps just after the largest Virginia quake using publicly-available data and some tools from the guys at MapBox .  Please click here to see how they quickly mashed up the earthquake data to make some great maps. [![](http://farm7.static.flickr.com/6202/6074536202_f060ba45dc.jpg)](http://tiles.mapbox.com/mapbox/#!/map/map_1314132938521) http://tiles.mapbox.com/mapbox/#!/map/map_1314132938521 Though, I think the most interesting visualization is this animated one, showing the earth rippling like a pond. [![](http://static.scholarslab.org/wp-content/uploads/2011/08/earthquakeWave.png)](http://www.youtube.com/watch?v=IKE7MLNdtcg) YouTube: http://www.youtube.com/watch?v=IKE7MLNdtcg"},{"id":"2011-08-26-praxis-program-ethos-and-charter","title":"Praxis Program Ethos and Charter","author":"jeremy-boggs","date":"2011-08-26 07:25:35 -0400","categories":["Grad Student Research"],"url":"praxis-program-ethos-and-charter","layout":"post","content":"As Bethany already explained in her last post, the Praxis Program will give a few graduate students the opportunity to learn on-the-job in a humanities shop. The program centers around building a project, named Prism, for public use and critique. For the next academic year, students will work on that project, and in the process critically engage a host of activities we think vital for knowledge workers in the digital age. No doubt this is hands-on learning: Wayne, Eric R., and David will have students eyeball-deep in programming (and those oh-so-important unit tests!), while Joe and I talk about design and usability. Bethany, Julie, and Eric J. will take turns preparing students for project management and outreach, budgets, and team-building. There’s a rough schedule and list of topics on the Praxis site, but the direction and shape of the year will depend on how each week goes. Much more than learning to design, code, and manage grants, the Praxis Program strives to realign attitudes about humanities graduate training. We have a developing ethos for this project, one we hope Praxis participants adopt, adapt, and expand: Work is finished only when it’s as good as it can be, which is usually not when you think it is. Like most DH work, learning is on-going and interactive. While this can be frustrating, it can also be exciting and liberating. Embrace it. Don’t ask permission to look into or try something. Just do it, and let us know how it goes. Be willing and ready to share anything you learn and anything you produce. If you’re not learning, you should be teaching, and vice versa. Students in the program are not code monkeys doing what we tell them to. They will have an incredible amount of agency in shaping the final project and the program as a whole. That agency is reflected in the very first thing we asked Praxis students to do: Create a project charter. The charter is an informal agreement among the group about how the project will be built and maintained and how credit on the project will be given. To prompt thinking about the charter, we asked the Praxis team to address a few questions: What conflicts might arise from this collaboration, and how would you recommend resolving those conflicts? What would be your policy about authorship and credit for works derived from this collaboration, and why? (E.g. a journal article about the project, an article about your personal contribution to the project, a credits page on the project website) What would be your policy about maintaining/sustaining the project once this is over? How might you deal with colleagues leaving in the middle of the project, or new colleagues coming into the project? What kind of license would you want to apply to this project, and why? Who is, or should be, the audience for this charter? Would you publicize it? If so, how? If not, why not? What are the desired outcomes of the project? Which outcomes do you consider to be required rather than merely desired? (examples: release of working software, release of open source code, formal and informal scholarly publications, professional development / CV enhancement) Who are signatories to the charter? What does this decision imply about the working relationship between Praxis students and Scholars’ Lab faculty and staff? Look for their first-draft, individual answers on this blog in the coming days. Next week, we’ll draft and post our collaboratively-written charter!"},{"id":"2011-08-29-let-the-process-begin","title":"Let the process begin:","author":"brooke-lestock","date":"2011-08-29 17:21:02 -0400","categories":["Grad Student Research"],"url":"let-the-process-begin","layout":"post","content":"Hello, digital universe! (You’ll have to excuse my childlike enthusiasm; this is all fairly new to me.) I have to admit that I’m a bit nervous, because of my novice DH status, to have so much control over the design and progress of a program that has the potential to really transform graduate student training at a time that seems ripe for it. On the other hand, I realize how lucky I am to enter this program and the DH “field” (if it can/should be defined as a field) with a blank slate - or at least a slate that has only some rudimentary XML chalked into the corners along with a lot of cloudy eraser marks. This year will take us from 0 – 60, and for my own professional development and CV adornment purposes, I’m not ashamed to say that I’m thrilled. But if that was my only motivation for joining the Praxis Program, I’d be doing myself and the program (and graduate students everywhere) a huge disservice. Likewise, if as a group we Praxisers focus only on our project on our terms (i.e., the success of Prism ), we’ll do a disservice to any of the communities of scholars or researchers who could benefit from our work. In this last sentence I realize I’ve addressed two of the charter prompts which started me scribbling notes like a mad scientist when I first read them: the prompts relating to the Praxis Program’s audience and outcomes. As for the audience, because the specifics of the project are not completely determined as this point, the design of this charter must and naturally will move beyond a project-specific ethos or modus operandi. In writing this charter at the very outset, we are setting guidelines that can be adopted by any project of this nature, and thus making our “work” available and relevant to the interests of a larger community. To readdress the word “work”: In Bethany’s article, “ Where Credit Is Due,” she speaks persuasively of the responsibility of tenure and promotions committees to “assess quality in digital humanities work – not in terms of product or output – but quality that is embodied in an evolving and continuous series of transformative processes .” The evaluation of the Praxis Program should be undertaken with this in mind, and so I would hesitate to apply specific end-product requirements to a pilot program like this. That being said, we will be working on a digital tool and should aim at being able to release software and open source code. Our ultimate goal, though, should be to account for the process, which absolutely necessitates publicizing and archiving along the way (though like Sarah, I can’t quite speak comfortably about what archiving entails). Accounting for the process also requires that we aim to produce formal and informal scholarly publications on the project’s development, and that we pursue every opportunity to make our process available to the public and open for discussion. Long live the process."},{"id":"2011-08-29-live-and-in-public","title":"Live and in public!","author":"sarah-storti","date":"2011-08-29 15:19:30 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"live-and-in-public","layout":"post","content":"To prepare for our meeting last week, all Praxis Program participants read the following pieces: Bethany Nowviskie, “Where Credit is Due.” Stan Ruecker and Milena Radzikowska, “The Iterative Design of a Project Charter for Interdisciplinary Research.” Siemens, et. al. “INKE Administrative Structure, Omnibus Document.” These links are also available here, but so that you, my reader, may easily follow my references (and explore for yourself) I resubmit them now. First of all, I would like to express my delight that, as Bethany put it in her blog post of the 24th, “we are using the Praxis Program to experiment with an action-oriented curriculum live and in public .” That emphasis on “live and in public” had me searching back through the above-posted materials, specifically Ruecker and Radzikowska’s piece, as I thought about my blog post for this week. I recall nodding vigorously and perhaps even whisper-shouting “yes!” to myself in the library last Monday when I came across this policy, under the subheading “Professional dignity”: “We will attempt to keep communications transparent, for example, by copying everyone involved in any given discussion, and by directly addressing with each other any questions or concerns that may arise.” The prioritization of transparency, here meant to ensure that members of the project maintain healthy professional relationships, sounds like an excellent strategy. I would love to see something similar in our charter: it seems like an easy and probably very effective way to reduce conflict and encourage mutual respect. But I become even more excited when I imagine the “everyone involved” to include the general public—that is, you, dear reader. Ruecker and Radzikowska’s paper continues as follows: “This policy of transparency is another simplifying strategy. If too much back-channel discussion takes place, it can become very difficult for everyone to understand what decisions are being made and why, especially on a geographically distributed team.” The audience for this blog could easily be described as a “geographically distributed team” of commentators and interested parties. Though the central Praxis Program group consists of local participants who are fortunately able to get together every single week for two hours, we are very interested in sharing what we do—whether it succeeds brilliantly or fails just as brilliantly—with you . I feel exceptionally lucky to have been selected to work with this talented and enthusiastic group, and desire passionately to make this kind of thing happen for other graduate students.  What decisions are being made, and why? You, reader, should feel that you are able to answer these questions as we go along. I am sure that some kind of policy about public access was always intended to go into our charter, but here is my formal declaration of support for it. The more we put out there—the more publicly we do this thing—the better for our geographically distributed team. It might not always be pretty, but at least we’ll have a record of how, exactly, we got wherever it is we end up. Last week, Jeremy called these blog posts “first-draft” components of the charter: it is a thrill to know that this is the beginning of our live and in public adventure. Epilogue: Archiving was the other topic I wanted to write about, though with my relative newness to most things DH I felt less than qualified to do so in an intelligent way: I’m not exactly sure how it works. I would, however, like for us to talk about it as we consider what will go into the charter."},{"id":"2011-08-29-owning-up-the-praxis-program","title":"Owning up the Praxis Program","author":"alex-gil","date":"2011-08-29 11:59:05 -0400","categories":["Grad Student Research"],"url":"owning-up-the-praxis-program","layout":"post","content":"On joining the Praxis Program, I knew I was in for something new. As part of the most recent generation of DH’ers at Uva, I’ve had time to develop a healthy dose of envy for the heroic age of SpecLab or the early years of NINES (not so long ago to be honest), when the DH demi-gods were said to roam the halls of Bryan. In the past couple of years, there have been informal attempts to revive the gall and vision of those who (just) came before us… without much success. Perhaps it was time to give up. After all, UVa continues to be a DH powerhouse without the shop-apprentice model of (not-so) yore. Perhaps the problem was that our impromptu efforts were tinged with nostalgia. When I was invited to become part of the Praxis Program, I knew this was something different, something new. Finally, we had a shop-apprentice model that I could make my own, that we could make our own. To talk about ownership in the hour of open access and crowdsourcing may seem oxymoronic, but I beg to differ. Before I came to the academy I was a salaried worker for more companies than I would care to enumerate. Though the service-industry’s book of mantras includes a line or two on how the company belongs to everyone, no one really buys that. Once in the academy, I’ve had a chance to help several faculty members with their projects where all I got in return was a footnote of appreciation. Even the countless ENWR courses I’ve been deputized have felt alien. In the end, the only thing I felt belonged to me where my most solitary scribblings and my toothbrush. I realize now that what makes the difference is creative direction. I too want to own what I create, but in my previous brushes with collaboration, I’ve always felt the only ‘I’ came from the top. When I saw that the first assignment of the Praxis Program was for us to design our own charter, I knew I was in for something new, something that is already starting to feel like my own. With that in mind, here are some of the things I would like to see in the final version of our charter: Credit should be non-hierarchical : Though the program that’s allowing us to build this project has a steward, the project itself should be credited to all of us. Detailing the contributions : Though we all get credit for the project, we should still publish a list of detailed contributions for audiences which require more details. One for all and all for one : Though we each can end up focusing more on those things suited to our individual calling, we should all be equal partners in the overall progress of the project. Departures : In the unlikely event that one of us leaves the project, that person should always receive credit by dates worked and contributions made, and have the right to reference the project on their vita. New Members : New participants should be given credit by date of arrival. License : I vote wholeheartedly that we offer everything we make open-access, open-source through a Creative Commons Attribution license. Bethany provides excellent rationale in “Why, of why, CC-BY?” . Taking ownership : I say we codify in the charter our commitment to promote the project publicly, to link it to our online personas, to make it truly our own. Each member should be allowed to list the project on their Vitas or Webpages under current projects or whatever appropriate equivalent Non-representative democracy : All major aspects of the project should be decided on a 2/3 vote with full quorum. This means we should also codify what we consider to be these major aspects."},{"id":"2011-08-29-preliminary-praxis-charter-ideas","title":"Preliminary Praxis Charter Ideas","author":"ed-triplett","date":"2011-08-29 11:54:45 -0400","categories":["Grad Student Research"],"url":"preliminary-praxis-charter-ideas","layout":"post","content":"In only its second week, the Praxis Program does not yet have a well established Identity. Creating a project charter will help narrow our focus, and allow us to establish rules of operation. As a group, we are subject to a number of challenges inherent in being the first members of the program, as well as the added hurdle of learning to work in an interdisciplinary group. The umbrella of the “humanities” does a poor job of representing who we are as scholars, and we each bring something different to the Praxis table. As we discussed last week, even “English” does not properly unite the vast range of interests held by five members of our group who work in that department. We should therefore be wary of the view that we all speak the same humanist language even before we add our new “digital” vocabulary. On one hand, as someone not as well versed in textual analysis as other members of the program, I would find it very valuable to read an article each week chosen by one other member of the group that they believe represents a methodology that has been helpful in their field. On the other hand, I do not think that our individual skills should be rounded off in the desire to create a common experience and contribution for everyone. As collaborators, we do not need a formal hierarchy, but we should have individual responsibilities that play to our strengths. We all had individual goals when we applied to this program, but I don’t believe I am alone in assuming that professional/ CV development lies at the heart of our interest.  That said, one of the strengths of this program is its ability to introduce some of us to the rules, positives and negatives of online publishing. The mystique of traditional publishing can act as a deterrent for graduate students to publish their work, yet it is always in our best professional interest to do so. In the hope of breaking down my own hesitance with regard to publishing I hope to digitally publish at least one well-crafted and edited article relating to my experience in this program. The Praxis blog should serve as our collaborative identity, and there should be a credits page linked to the site, yet we are still hired as individuals, and we cannot simply expect our future employers to be enlightened with regard to collaborative authorship as they look over our CVs. The final issue that I have not yet come to a conclusion on is the Praxis Program’s level of self-promotion. This program has the ability to be our “boldest” representation online as scholars and there is a fine line between keeping our audience interested in our progress and shouting every minor accomplishment. I am personally rather new to using social media for professional development, and I would greatly value a discussion of how this will work in our program."},{"id":"2011-08-29-thoughts-on-our-charter","title":"Thoughts on our Charter","author":"annie-swafford","date":"2011-08-29 19:09:57 -0400","categories":["Grad Student Research"],"url":"thoughts-on-our-charter","layout":"post","content":"Although I have experience in designing my own digital project (through NINES) and in working on pre-existing ones (through Documents Compass’s Adams and Madison papers projects), I’m getting my first taste of being on a team that will work together to design and manage a project.  Since my thinking tends to be more detail-oriented than abstract, the act of coming up with a charter to govern our interactions before we have a sense of the project itself is a challenge, but a welcome one. Here are some thoughts on our charter: **Equal Credit: **Like Alex, I think that we should find a way to have all Praxis Program participants get equal credit, although I’m not sure of the best method.  Would designating authorship with alphabetical order be the clearest, or should we just say “by the Praxis Program” and add our names in a footnote? Publicity Opportunities:   I think we should each be able to submit articles or attend conferences about our personal contributions to the project, and we should be able to add it to our CVs, but we should always openly acknowledge that this project is a joint venture.  We shouldn’t be able to put someone else’s article about the project on our CV unless we actually helped that individual write the article. Sharing the Work: Although I think we should all try our hands at every aspect of the project, it seems reasonable that some people might prefer to spend more time on some elements than others.  Therefore, once we’ve settled on the features we want our project to have, maybe we should divide it into subsections and appoint each member to head a subsection. **Conflict Management: **Since conflicts may arise regarding features or the intended audience of Prism, setting up some sort of voting system to resolve conflicts might be a good idea.  However, I feel as though the head of a subsection should maybe have a larger say in matters directly involving the subsection, since we don’t want someone to be forced to work on a feature he/she believes is doomed to fail. Maintaining the Project: I would imagine that we as a group would not want to run the project for the rest of our days, so we should maybe see if one person wants to be in charge of it after the project is over, at which point the rest of us would become the “advisory board,” or to see if Scholar’s Lab would like to take charge while still listing us as authors/founders. Outcomes: In addition to releasing working software, I would also expect us to release the open source code and to use the project for our professional development."},{"id":"2011-08-30-chartering-the-unknown","title":"Chartering the Unknown","author":"lindsay-oconnor","date":"2011-08-30 09:53:20 -0400","categories":["Grad Student Research"],"url":"chartering-the-unknown","layout":"post","content":"Like Alex, I’m excited for what seems to be a true shop-apprentice approach to learning this whole list of skills, methods, and programs. Before I came to UVa I held a few jobs with a company that hired people based on general knowledge and potential rather than specific technical know-how, and I learned to use new technological tools as my responsibility increased with each position. While I assume we’re accepted to graduate school not for our current knowledge but for our potential to do great work later on, the generally ability v. specific skills parallel doesn’t quite work for a field in which the product is criticism, scholarly writing, or humanistic knowledge (whatever that is). But DH refigures or even expands the endpoints of humanistic inquiry and allows for different forms of education and training along the way. Somebody decided I have the potential for insightful literary analysis, and I’m glad the kind folks in the SLab have decided there’s hope for me to add some technical and managerial tools to my professional toolkit. But it’s also quite new and difficult to have to draft a charter for a project I know almost nothing about so far. How much will the knowledge and skills we gain through the Praxis Program’s weekly workshops change our ideas about  self-governance and credit? Like Edward, I’m interested in and concerned about interdisciplinarity. One of the model charters we examined suggests an interdisciplinarity that is necessarily collaborative, with participants both “thinking across boundaries” and “communicating with people unlike [ourselves].” I’m much more used to the former but am definitely looking forward to the latter, and to working with people from the same disciplinary background as me (literature) who are now working in “non-traditional” positions. To make this work, I hope we can take the advice of the charter linked above and “map the relevant conceptual territory” early on in order to de- and re-territorialize not only our disciplinary backgrounds but also our individual theories, methodologies, and working idiosyncrasies. (Mine seem to include a tendency toward spatial thinking and obnoxious theory references. Sorry, team.) Like Sarah, I hope for professional dignity and transparency at all steps in the process. We’re lucky to have our core group all here in Charlottesville, but Sarah is right to point out that our working group doesn’t end there. Since we’re going to be working on crowdsourcing interpretation and since Brooke emphasizes “accounting for the process,” I wonder if we might invite blog readers to help us with our charter as we draft it over the next few weeks. Some crowdsourced input and interpretation by experts and amateurs near and far might keep us apprised of issues we’re overlooking, mishandling, or failing to anticipate. I admire Annie for looking forward to sharing work and sharing credit. I can already see that she’ll be great at keeping details straight and bringing us down from abstractions and What Ifs. I too hope we can share credit equally, and taking leadership on individual components, whether technical or managerial, seems like the right idea. I’m still hesitant to say much more about how this will all work without talking more about what the particular goals of this project will be. I’m all for various forms of publication and the release of open-source software, but I’d like to talk more about what that software is going to do before we get too far into this charter. This requires even more looking ahead to unknown outcomes than the dissertation prospectus I’m struggling with right now!"},{"id":"2011-08-31-vim-gui-font-magic","title":"Vim GUI Font Magic","author":"eric-rochester","date":"2011-08-31 07:25:00 -0400","categories":["Grad Student Research"],"url":"vim-gui-font-magic","layout":"post","content":"Yesterday, Wayne tweeted this : @erochest just dropped some vim wizardry on the @scholarslab: open system fonts, set the guifont from the settings selected in gui I thought I should describe what I did, for several reasons. First, it’s kind of cool, in a geeky vim-lover kind of way. Second, it’s not something you do everyday, so it’s helpful to have it written down. I’m pretty surprised that I remembered it as quickly as I did. Step 1: Select the Font Using the Standard Font Dialog In Vim, pop open the GUI to select the font by typing in this command: :set guifont=* . It should open up the standard font dialog box for your platform. Select the font you want to use and close the dialog. Step 2: Add the Font to Your .vimrc file. Now you need to make the change permanent by adding it to your .vimrc file. Open this file using the command :e ~/.vimrc (on Linux, UNIX, and Mac) or :e ~/_vimrc (on Windows). Then add this to the bottom: set guifont= At this point, you need to insert the current value of guifont . Do this by pressing Ctrl-R (while still in insert mode) and entering this in the command bar at the bottom of the screen: =&amp;guifont; Hit enter. The name and size of the font should be appended to the set guifont line you started. If the font name has spaces, however, you’ll need to escape those. This command should do the trick: :s/ /\\ /g` Unfortunately, this does a little more than you want. It also escapes the space after set . Take that out manually. Now, the line should look something like this: set guifont=DejaVu Sans Mono:h13 Bonus Some nice fonts I’ve used with Vim over the years include these: DejaVu Sans Mono ; Bitstream Vera Sans Mono ; and Lucida Sans Typewriter Regular (free with many Microsoft products)."},{"id":"2011-09-01-praxis-program-week-2","title":"Praxis Program week 2","author":"eric-johnson","date":"2011-09-01 06:00:03 -0400","categories":["Grad Student Research"],"url":"praxis-program-week-2","layout":"post","content":"Tuesday saw the second weekly meeting of the team involved in the Scholars’ Lab’s Praxis Program.  The conversation largely revolved around coalescing the groups’ thoughts on their project charter. While the specifics of the formal charter are still being ironed out—and will be shared shortly—the group identified four high-level principles to help guide the approach of the project: Open and Frequent Communication When in Doubt, Ask (both inter-personal &amp; training/help) Getting to Know You/Staying Conscious of Interdisciplinary/Weekly Show &amp; Tell (in other words, sharing individual expertise and interest) Entire group reviews claims for credit for all derivatives So what’s on tap for the rest of this week?  First off, the students got a crash course on installing Vim, a text editor used by members of the Scholars’ Lab’s development team for writing code.  Now they have to figure out how to use it . Secondly, they’ve got another blog post to do.  If they choose to, they can tackle some of the issues we also discussed during the meeting on evaluating digital work .   In particular, Jeremy posed them this prompt: take a look at one or more the sites we listed.  What’s one big thing that you would change?  Why would you change it?  And, perhaps most interestingly, don’t forget that your change isn’t just for you alone.  Keep the other members of the site’s audience in mind.  How would your suggested change improve/alter their experience?"},{"id":"2011-09-01-vim-config-and-windows","title":"Vim Config (and Windows)","author":"eric-rochester","date":"2011-09-01 02:21:58 -0400","categories":["Grad Student Research"],"url":"vim-config-and-windows","layout":"post","content":"This is a set of instructions for configuring Vim for the Praxis Program. Most of it will only apply to Windows, but I’ve included some notes for any UNIX-like system (LINUX or Mac) at the bottom. Windows The bulk of this is just downloading and installing things that aren’t included by Windows or by the non-existent Windows package manager. Installing Everything Install Vim . Install Ruby 1.9.2 . (You can get the preview release here ). Make sure you click “Add Ruby executables to your path.” Install msysgit . Look for “Full installer for official Git.” During the installation select “Git Bash Here.” Later either select “Use Git Bash only” or “Run Git from the Windows Command Prompt.” Finally, also select “Checkout as-is, commit Unix-style line endings.” Download the Windows binary for Exuberant CTags . It comes in a ZIP file. Open it and put ctags58ctags.exe (it may be display as ctags58ctags with type Application) into C:Program FilesVimvim73 . Finally, download cURL . Download the one at the bottom for “Win32 - Generic” labelled  “Win32 2000/XP binary” (it also says that it’s 1.32 MB at the moment). Open and put curl.exe, libcurl.dll, libeay32.dll, and libssl32.dll into C:Program FilesVimvim73 . Vim Configuration For this, you’ll need to work from a command prompt. This may be a new experience for Windows users, but don’t worry. It won’t bite you, and it’s easier than it may seem going in. First in Windows 7, click on the Windows button and search for “bash.” Open up the program it returns (“Git Bash”). In XP, look under the Start menu for the Git program group and select “Git Bash”. Now, open this script . Copy-and-paste each line into the Git Bash Console window that’s open. Run Vim That’s it. Give it a spin. Look under the start menu, for the “Vim 7.3” group and select “gVim.” In Windows 7, you may want to pin it to your taskbar. Fonts Check out Vim GUI Font Magic for instructions on how to change the font from the truly yeechy Windows default. This also has some suggestions for nicer fonts. UNIX (LINUX or Mac) The process for installing this for Linux or Mac is the same. You’ll want to install Vim, Git, and cURL. Chances are, they’re already there. For Mac, you’ll probably want to either download the DMG file from the MacVim project or use something like Homebrew and Xcode . For Linux, you’ll probably want to just use your distribution’s package manager ( sudo apt-get vim-gnome for Ubuntu). After that, basically the same script as above should work. You’ll want to use the version here, however, which is modified for UNIX."},{"id":"2011-09-05-evaluating-digital-work-suggestions","title":"Evaluating Digital Work: Suggestions","author":"brooke-lestock","date":"2011-09-05 18:31:42 -0400","categories":["Grad Student Research"],"url":"evaluating-digital-work-suggestions","layout":"post","content":"For this week’s post, we were asked to evaluate a few of the digital tools we looked at for last week’s meeting. This time we were armed with a list of questions, which I was particularly eager to have as someone with a fair amount of experience evaluating literary work and none whatsoever evaluating digital work. I was relieved to find that the questions are similar to those that must be asked of any scholarly article or book. What problem does it try to solve? What contribution does it make? How would it affect yours and others’ work? The big one: Does it work? And so on. Following these guidelines, we were asked to identify one thing we’d like to change, how we’d change it, and who would benefit from the change. That being said, here are my suggestions (humbly submitted): Linguistic Atlas Projects: The site compiles a group of projects studying English dialects in the US submitted by institutions across the nation, but I think the site would benefit from some kind of uniformity of layout and content. It’s intended to be a collection of these linguistic studies, but they’re treated very separately. Once you’ve selected one project from the side column, the other projects disappear. Some projects’ links will take you to to a map in which you can select a state to review, while others take you to a basic info page. Some projects’ pages offer images, recordings, and detailed project descriptions complete with bibliographies, while others offer only maps and subject data. I imagine it’s quite difficult to put projects with different methodologies in context with one another, but a uniform format would allow users to move between the different projects without becoming disoriented. Visualeyes: I was so excited by the possibilities of Visualeyes and so anxious to figure out how to use it, the only suggestion I’d make would be to get more basic with tutorials. Visualeyes makes itself available as a non-programmers’ tool with many possible uses, and so there are many tutorials featured, but I had difficulty finding a sort of “Visualeyes for Dummies” video that would give me an overview starting at ground zero. The “Visualeyes Project Guide” document looked like a promising written guide, but its page was unavailable. I wasn’t quite sure where to begin. OldWeather: I enjoy the interactive, collaborative nature of sites like this one and What’s on the Menu. OldWeather in particular pushes for public involvement, but so much so that the bigger picture becomes unclear. The home page highlights the transcription “game” while providing just a few introductory sentences and a short intro video for the program itself. There seems to be little information available on how the data will be interpreted, what the projected outline or timeline of the work will be, how the project responds to other research being done in the field, etc. The goals of the project should be clearly advertised and delineated on the site, which will then give users a better idea of why they should participate."},{"id":"2011-09-05-further-evaluation-of-digital-work","title":"Further Evaluation of Digital Work","author":"annie-swafford","date":"2011-09-05 19:55:36 -0400","categories":["Grad Student Research"],"url":"further-evaluation-of-digital-work","layout":"post","content":" Although I feel a bit silly evaluating digital work when I’m still a comparative newbie to the DH world, I found that looking at these projects helped me not only improve my ability to imagine how projects could be altered to suit different purposes, but also understand what sort of features or documentation I would like us to incorporate in our own project.  In general, I’m a big believer in explicit instructions, and wherever possible, an explanation of why the project is important.  I also prefer projects that can be accessed/used in multiple ways (For Better For Verse), but I also like projects that have a single but incredible clear purpose that is easy to follow (What’s on the Menu).  Group 1:   For Better For Verse : This is a fantastic project that can help teach students all elements of poetics, including rhyme scheme, meter, and how they relate to interpretation.  I was also very impressed by the sorting principles (you can organize the poems by title, difficulty, and author).   I can only think of a handful of features that I would like to see added:  I wish there were a way to sort by time period, an explanation of why a given scansion is correct. Also, as a TA, I wish my students could practice scanning poems they’re studying in class, so it would be nice if someday there were a way for people to contribute content so that teachers and students wouldn’t be limited to the (extensive collection of) poems there if they want to use it for class. Group 2:   TILE: This seems like a really useful tool for creating digital editions where you want to clearly show which lines of a transcript correspond with which lines of the image.  I thought the sandbox feature, which lets you try the tool without downloading it was a great idea, and once I found the instructions, it became easy to use.  Although I think TILE is important, I think it would be clearer if the website had links to projects that had used this tool, or maybe a video to show what it’s capable of, because it took me some time to be able to understand its purpose and to visualize how it should be used. Group 3:   What’s on the Menu : This archive features crowdsourced transcription of menus held in the New York Public Library’s rare books division.  It’s easy to add content; you just have to click on each dish and type the text as it appears on the menu, and it instantly becomes part of their searchable text.  I also thought that the site was incredibly well designed from a user perspective; the instructions were clear, and the rationale behind the project made me more interested in it than I had been when I was just looking through the menus.  I hope we can be influenced by their documentation in our own project.  I could barely come up with anything I wish were different; my only complaint is that the instructions say to ignore some typographical features, like accents, but not about other irregularities (ie. should we preserve line breaks within menu items).  Other than that, I think it’s excellent."},{"id":"2011-09-05-in-which-a-novice-evaluates-digital-work","title":"In which a novice evaluates digital work","author":"sarah-storti","date":"2011-09-05 19:18:14 -0400","categories":["Grad Student Research"],"url":"in-which-a-novice-evaluates-digital-work","layout":"post","content":"After spending a good forty minutes browsing the Valley of the Shadow digital archive, I must admit that I’m defeated by the prompt Jeremy gave us last week for these blog posts. I simply could not find anything about the archive I would care to change. I approached it aggressively, determined to be disappointed or confused, or perhaps annoyed by the difficulty of navigating the project, but each time I thought “aha! that’s it!” a page scroll or more careful reading revealed the solution to my imaginary problem. In lieu, therefore, of explaining what about the archive I think could be changed to improve the user’s experience, I want to point out very briefly how the folks at the Valley of the Shadow impressed the pants off me. Firstly, the archive is prefaced with an excellent description of its raison d’être . Upon entering, a blueprint of sorts (oooh!) describes very succinctly and logically (in my humble opinion) how the information in the archive is organized. I dove in right away, and discovered through experimentation how to navigate the archive. Those who prefer a less visual or more guided navigational experience, however, can consult the “walking tours” for each of the topic sections. My favorite discovery, though, was the “Using the Valley Project” section, which contains examples of how previous visitors have put the material to use in various (and very different) ways. At no point did I feel confused, disappointed, or annoyed. I just felt eager to continue exploring the American Civil War as witnessed by Augusta (VA) and Franklin (PA) counties. I also looked closely at the September 11 Digital Archive, which is an incredibly enormous collection of digital items relevant in some way to the September 11, 2001 terrorist attacks on the United States of America. Selecting “Browse” from the menu on the main page takes the user to a list of links which fall under the categories Stories, Email, Documents, Images, Digital Animations, Interviews, and Audio/Video, and clicking on any link will take the user to a group of related documents… or that’s the idea: the documents should be related, somehow. I suppose that the question I would pose to the curators of this project would be: what is the archive supposed to do? Is it merely meant as a permanent storage site for digital items that are in some way associated with September 11, 2001? And here are some emails? And here are some comment cards, and here some animated political cartoons? I suppose I would recommend that the archive might be more useful and navigable were it presented with more thematic nuance: as is, items are grouped into large categories that do not really encourage the exploration of the rest of the site. With the Valley Project on my mind, I couldn’t help but feel that a chronological approach (with separate sections for items connected to the country before, during and after the attacks) might be one very effective method of organization for these materials. Unfortunately, I was unable to login to the “researcher” account I tried to register, because I wanted to check out the “enhanced access to the entire public and researcher-only collections, including personal notes and favorites storage” which comes with such an account. I imagine being able to mark or store items inside your personal account might help one work through the collection more easily, though I stand by the assertion that the archive could do with a more carefully thought-out method of organization."},{"id":"2011-09-05-project-reviews","title":"Project Reviews","author":"alex-gil","date":"2011-09-05 19:54:08 -0400","categories":["Grad Student Research"],"url":"project-reviews","layout":"post","content":"As part of our second week assignment we were giving the task of reviewing three projects from the following list . The list is interesting because it groups projects that have been around for a while with more recent contributions. The three that I chose actually belong to the former category. I thought it would be refreshing to re-view the familiar. I apologize beforehand if what I say below echoes criticism and praise already directed at these projects. Zotero I’ve been a user of Zotero for a while. When I discovered it a few years ago it changed the way I collected my research materials. It’s been three years now that I’ve been compiling a large bibliography of Aimé Césaire primary sources and secondary sources. I must here confess that I use Zotero somewhat off the beaten pass as a content management system of sorts. I have transcriptions for many of my items in the note section. This allows me to use the search function through out all my items, returning the bibliographic items. I’m aware that there are better ways to search through a corpus (Solr for example), but I like the fact that I can keep all my research in one single work-space. The tags and related features allow me to record connections between primary and secondary sources that an algorithmic solution might not discover. Being able to organize the database by different categories can also be a great way of visualizing a collection. My collection at this point is enormous, and it is true that the task of adding transcriptions and making connections can be overwhelming as the collection grows, but I figure there is no hurry. As the community continues to develop around the Zotero API, I’m hoping that I will eventually be able to share my bibliographic work in the form of an online publication. It is true that the collection can be offered through the Zotero website, but this does not allow for a personalized design around particular collections. As you will hear often from me, “analytic” tools should always move in the direction of publication. I’m happy to see Zotero is laying the groundwork for these kinds of projects. Whitman Archive In my mind the Whitman Archive has always been part of an imaginary triumvirate next to The Rossetti Archive and The Blake Archive . Like the other two it has always suffered from the “tree” structure. In order to get to a text you must click through narrowing categories. Once you do, you have arrived at… a text. True, you can use the search box.  It uses the Google algorithm. The logic of the Google algorithm is not necessarily the logic I would like to use for an archive. The archive is the work of Ed Folsom and Ken Price. At all levels of the work we find their imprint, criticism included. There is nothing inherently wrong with featuring the work of the principal investigators, but an archive of this scope and prominence should strive to feature the work of other scholars as well. Another question which is extremely important to me is the presentation of texts. The typical WA edition of Leaves of Grass for example will include images and lightly formatted text. The design of the pages is not very conducive to reading online. Although the edition remains a great resource for the study of Whitman, it still does not provide an aesthetic experience comparable to a Penguin edition. I believe the two should not be mutually exclusive. Finally, there is the question of what you can do with the texts. So far, the Archive has partnered with TokenX to allow users to do some textual processing with the materials. You can also download the XML files to use with other tools. What I would like to see in the archives of the future is more play within the space of the archive. With a bit of cleverness and resources, design can invite you in to play, and play can lead to better connections, which brings me to… TAPoR I started using TAPoR first through Voyeur Tools, and it wasn’t until this assignment that I started using their web interface directly, which if I’m not mistaken, is a recent offering. I discovered that the tools do not necessarily overlap, and that in general the set of tools in Voyeur Tools is more complete. Voyeur Tools also seems to be designed in such a way that the tools connect with each other, something TAPoR does not do very well. That said, TAPoR is a great tool for beginners who want to experience what textual processing can reveal about their texts. Much of the debate that I have heard about the use of statistical tools to examine texts has been directed precisely at the sorts of things that TAPoR and Voyeur Tools do well. I believe that criticism to be a result of poor usage, rather than a flaw in the tools. The conceptual claims of collocation and distribution graphs are never overstated by these projects. In this sense, the tools deliver what they promise. That said, there is plenty of room for misuse, false assumptions and ‘naive’ evidence. In short, use with caution."},{"id":"2011-09-06-a-simple-critique-of-tile","title":"A simple critique of TILE","author":"ed-triplett","date":"2011-09-06 08:36:57 -0400","categories":["Grad Student Research"],"url":"a-simple-critique-of-tile","layout":"post","content":"I would like to echo Annie’s thought that I feel a little awkward critiquing DH projects of this kind given how new a lot of this is to me. I was drawn to T.I.L.E. as a user, and so while I may not yet be able to discuss the project’s inner workings, I can assess its usability. I cannot come up with any single idea that would improve the project other than that it must improve the exposition of how it works and what its purpose is. I will begin by listing a number of general mistakes that occur in the “Sandbox” version of the TILE tool: There is no home button to return you to the TILE site once you have clicked on the Sandbox OCR appears to be a critical aspect of the project, yet there is no explanation of what attention was paid to improving its results. Often but not always, when attempting to create an “Image tag” box, the selected area appears off the scanned page instead of where the user’s cursor is creating the box or ellipse. Once this incorrectly created image tagging square appears off to the right of the scanned image, there is no way to move it back to its proper location. After performing a line recognition, there is no way to zoom in to judge how well this was done. A composite scan containing a text/image composite would have been very helpful as a demonstration of the image tagging tool. I have trouble understanding the choice of the phrase “Semi-Automated line recognizer.” Especailly when combined with the shaky functionality of the Sandbox, it gives the impression of an “almost working line recognizer.” There really MUST be some visual connection between the dialogue box that pops up when image tagging, and the specific location of the image-text being tagged. I would advise looking at software such as Nowcomment for the functionality/UI for TILE. To take a wider scope, there appears to be a general lack of exposition. MITH must be aware of how traffic will get to their site. I think it is not uncommon for users to discover the site in just the way I did; by clicking on one of a series of links relating to text-based DH projects. With this in mind, the problem that TILE is attempting to solve must be placed in the foreground. A lot of these problems could be solved with a simple video demonstrating the project at each stage; from scanning to OCR, to line recognition and finally image tagging. TILE introduces a lot of issues in terms of exposition of a DH project. More text explaining what inspired the project, and what it does is not always the best way to introduce and “hook” new users. Still, MITH’s decision to use such minimal text does not help. It should also be pointed out that the small screenshots devoted to the various tools are poorly chosen and fail to demonstrate anything. TILE easily could accomplish its stated goals, yet it seems the presentation and functionality of the sandbox would scare off or confuse most users before they realize the strengths of the project."},{"id":"2011-09-06-temporal-mapping-crowdsourcing-and-standardization","title":"Temporal mapping, crowdsourcing, and standardization","author":"lindsay-oconnor","date":"2011-09-06 08:44:01 -0400","categories":["Grad Student Research"],"url":"temporal-mapping-crowdsourcing-and-standardization","layout":"post","content":"For my first DH project review, I picked the Linguistic Atlas Projects because I’m interested in regional dialect and linguistic change and because I liked the idea of a linguistic map that the name “linguistic atlas” invokes.  But when I looked closer at the site, I wanted to make it into more of a globe instead of just a collection of maps. Comparison across regions could be easier and less time-consuming for the user if the analyses had more overlap or formal standardization. Right now, each study appears differently on the website. Some provide analyses and some only provide data and a basic description. The LAMSAS Density Estimations Maps provide density across the region while the LAPNW maps provide isolated community locations, and these two analyses map none of the same terms. The extensive data files show many similar questions and terms, so coming up with similar analyses would not require additional research. I know we’re only supposed to suggest a single change, but I want to suggest an additional change that would also serve the goal of more extensive and accessible comparisons, this time within regions instead of just between them. Much of the data is already dated, so newer surveys would make for a more current version of the Atlas as it is, and they would also give these linguistic maps an additional dimension by allowing for comparisons over time. Users could see how dialects are changing within and across regions and demographic groups.  I realize this isn’t so much a shortcoming of the project as much as it is a next step it could take, and with Scholars’ Lab developing Neatline, Linguistic Atlas might be a good candidate for an update. I also want to reflect briefly on crowdsourcing via What’s on the Menu .  Annie helpfully pointed out some inconsistencies in the instructions on what to include and what to ignore when transcribing menus, and the “About” page mentions “cleanup” before the project is complete. I’ve seen so many restaurant menus with typos and misspellings that I’m sure there will be many variations in the language used to name and describe many menu items, and those variations and “errors” are part of the beauty of these objects. It would be a shame for those things to get “cleaned up;” a project like this that documents and preserves ephemera should maintain all the quirks in its archive. I’m ambivalent on crowdsourcing here; it’s of course great for getting things accomplished quickly and cheaply and it will be much better than OCR for all the different menu formats, but like OCR it might still lead to inconsistent representation or interpretation of items in the archive. This possible problem could also become a strength, however, if the archive of menu items and prices could also serve as an archive of the transcription process. If information about each transcriber were preserved and associated with the items they transcribe, What’s on the Menu (or maybe any crowdsourced project?) could serve the secondary function of demonstrating how different people interpret different objects. This issue will probably come up again as we work on Prism, so I look forward to reviewing other crowdsourced projects along the way. Now I’m asking for more standardization in one project and am resisting standardization in the other. The difference is in what level of analysis or representation is being standardized, and in a way, the Linguistic Atlas Projects might serve as a model for the data in What’s on the Menu. The LAPs document linguistic variation, and I hope What’s on the Menu will document variations in names, descriptions, spelling, and punctuation the way the LAPs document variations in word choice and pronunciation. The standardization I think the LAPs lack is at a higher level of analysis, not at the level of data collection. It seems that many different projects share the challenge of representing variation in the data or objects in their archives in formats that are consistent and user-friendly."},{"id":"2011-09-12-crowdsourcing-interpretation","title":"Crowdsourcing Interpretation / Praxis and Prism","author":"bethany-nowviskie","date":"2011-09-12 08:34:10 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"crowdsourcing-interpretation","layout":"post","content":"Our goal in the Scholars’ Lab Praxis Program is to address methodological training in the humanities not just through workshops and courses, but by involving graduate students in digital projects from the ground up. This means learning by creating something – together – with all that entails: paying attention both to vision and detail; building facility with new techniques and languages not just as an academic exercise, but of necessity, and in the most pragmatic framework imaginable; acquiring the softer skills of collaboration (sadly, an undiscovered country in humanities graduate education) and of leadership (that is, of credible expertise, self-governance, and effective project management). All this also involves learning to iterate and to compromise – and when to stop and ship. To do this, our Praxis team needed a project. We wanted it to be a fresh one, something they could own. It was important to us that the project only be in service to the program – that its intellectual agenda was one our students could shape, that they set the tone for the collaboration, and that – as much as possible – it be brand-spanking-new, free from practices and assumptions (technical or social) that might have grown organically in a pre-existing project and which we might no longer recommend. In this inaugural year of the Praxis Program, the Scholars’ Lab, in consultation with some colleagues from UVa’s College of Arts and Sciences, is providing the central idea for the project. It’s just too much to ask that students new to digital humanities work invent a meaningful project from whole cloth on Day 1 of the program – especially one that, we hope, will make a meaningful intervention in the current scene of DH research and practice. That said, by the end of this year, our current Praxis team plans to have conceptualized a second project (or perhaps an extension of this one) to pass on to next year’s group. Here endeth the preamble. What are we up to now? This year, the Praxis Program is building a web-based framework, codenamed “Prism,” for collective marking of texts according to small and constrained (but flexible) interpretive vocabularies. Prism will enable visualization of those marks – made by many users on the same document – as zoomed-out, rainbow-like spectra. It will also (should we get so far!) allow for comparison and analysis of the results of users’ activity (that is, their collective attention paid to certain passages of text, and the categorizations they make of those passages) by treating them as input for the data-mining techniques we can apply against large corpora of digitized texts. In other words, Prism will be a blunt but very interesting and user-friendly tool for crowd-sourcing humanities interpretation. The basic concept has several sources. It stems in part from conversations on categories of textual interpretation, led by Johanna Drucker and Jerome McGann, in which I participated as a graduate student at SpecLab (see especially chapter 2.5 of Drucker’s book), as well as from a fond memory of markup games I played in my UVa Media Studies classroom and with SpecLab colleagues, including (among several others) Drucker, McGann, Andrea Laue, Worthy Martin, and Nathan Piazza. These games and discussions fed into McGann’s “Marking Texts of Many Dimensions,” and Jerry and I spoke about our experiences of them last year, in response to a Scholars’ Lab talk on “N-dimensional Archives” by Julie Meloni. SpecLab participants called this (quite complicated) thing “the ‘Patacritical Demon.” It also stems from work on folksonomy which I undertook in designing the NINES/Collex software as a postdoc with McGann, and more recent Scholars’ Lab discussions about color-coded text visualization with Alison Booth, in the context of her project to define and mark narrative structures in biographies of women. The concept I presented to the Praxis team last week as an inspiration for Prism is simpler in scope and beholden more to the material and pedagogical markup exercise than to text-theoretical debates. I’ll say no more here than that the original game involved shared, Xeroxed page images, transparent overlays, dry-erase markers, a common interpretive prompt, and a moment in which somebody yelled “Stop!” and the transparencies were stacked up for discussion. Members of the Praxis team will be describing their vision for the user interface of Prism in more detail as the weeks and months progress. In our early conversations, the whole team has seemed energized by the potential of the tool for classroom use. But it’s important to say that we’re not just replicating an offline pedagogical exercise in the browser. Prism updates the concept in some important – and we think timely – ways, some of which are meant as interventions in the current scene of DH project development and conceptualization: We recognize that there’s a huge vogue for “crowd-sourcing” in the digital humanities right now, but have been feeling like there’s potential for much more interesting work in this domain. We don’t want to treat the “crowd” only like robots or mechanical turks – asking for transcription labor, or refinement of OCR output, as valuable as those products may be. What would happen if we could systematize, capture, and build collective interpretive energy – on shared understandings and unexpected disagreements? We also feel ready to build on design lessons from citizen-science/citizen-scholar projects, like those created by the Zooniverse group, to create a DH tool that appeals to the general public and is easy and fun and effective for pedagogical use. We’d like to be able to use Prism as a laboratory exercise for thinking about design and development in the public humanities, and on the relation of audience and user communities to the questions we can ask in DH research. Finally, in an era of mass digitization, we’re keen to engage with big data in the humanities. Once the basic framework for Prism is established, we want to be able to experiment with the flow between user-friendly input and “easy” and attractive visualizations (like our spectra) and the deeper questions that can be asked and harder information design problems that are encountered when we move into computational linguistics &amp; text mining techniques such as sentiment analysis. This is a tall, tall order – but neither the Scholars’ Lab staff nor our Praxis students are the sort to be attracted to an unambitious project. We hope you’ll follow along this year as we see just how far we can get, and what we can learn along the way."},{"id":"2011-09-12-processing-praxis","title":"Processing Praxis","author":"brooke-lestock","date":"2011-09-12 18:15:43 -0400","categories":["Grad Student Research"],"url":"processing-praxis","layout":"post","content":"Since we’ve been let loose, prompt-free, to blog as we please this week, I’d like to take the opportunity to get back to “the process.” As the Praxis Program garners more and more attention, and we begin to produce and publicize documentation, I’m realizing how little I knew when I boldly stated “Long live the process!” in my first blog post just a few weeks ago. I am still convinced that the transparency of our progress is integral to the success of the program - its success as a model for similar graduate training programs in other universities and as a model for collaborative DH work - but I’m only now understanding why . The process is something that makes most academics, myself included, a bit uncomfortable because we spend the majority of our time and energy, to use Bethany’s term from our first meeting, “polishing.” As graduate students, we are trained to cultivate increasingly more specific areas of expertise, to hold onto and constantly refine a piece of work before making it public, and even then we’re often uncomfortable with releasing a work-in-progress to our peers. Sure, classrooms, conferences, and trusted advisors or colleagues are acceptable venues for “testing out” work, but there is still bound to be a considerable amount of refinement anxiety before the work is shared. Of course I’m speaking from my experience and my academic career has only just begun, but these are my impressions reinforced by what I’ve gleaned from more seasoned academics and professionals. That being said, the transparency and publicity expected of us in the Praxis Program is a huge source of anxiety for me. But it’s good anxiety.  I spend way too much time polishing (read: agonizing over) what I put “out there” for the group or the website - from our weekly blog posts to our working GoogleDocs for the charter or requirements gathering - that it would normally be paralyzing, but in a program as fast-paced as Praxis, there’s no time for solitary perfectionism. As uncomfortable as it is to contribute an unpolished idea, sentence, post, etc., there’s more at stake than my ego and more to gain than the perfect blog post. Now that Bethany’s explained the basics of Prism, I’m sure there are plenty of people out there who responded exactly as I did, that is, wishing it was already a year from now so we could use it! But since I don’t have a DeLorean and opportunities to develop and build a tool as potentially awesome as Prism don’t present themselves every day, I’ll stick to my guns and I won’t betray the process. So, in addition to all the logistical training and DH street cred that the Praxis Program will provide, it will also force me to resist my urge to hoard and over-polish work, which is exactly in tune with the program’s mission to “realign graduate methodological training with the demands of the humanities in the digital age.” Realignment: in process!"},{"id":"2011-09-13-a-disclaimer-and-a-declaration","title":"A disclaimer and a declaration","author":"sarah-storti","date":"2011-09-13 04:02:33 -0400","categories":["Grad Student Research"],"url":"a-disclaimer-and-a-declaration","layout":"post","content":"Kudos to Brooke for her excellent blog post  this week. I believe she was writing it in the midst of a texting session with me, during which we each gave vent to plenty of grad student perfectionist angst (along the lines of “what on EARTH can I say this week that will be worthy  the highly visible platform that is the SLab/Praxis blog??”). I’ve been inspired by her honesty in writing about what many of us are thinking: transparency sounds great in theory but is pretty terrifying in practice, especially for those of us relatively new to both the academy and to the DH community. But Brooke is also right about the speed of the Praxis program: we’re flying through the basics, and there is simply no time for perfection. This, I think, should count as our disclaimer for anything we may put up here, in the coming months, that may sound slightly crazy, impossible, or (heaven forbid) ill-written. We’re doing our best to keep up, and in exchange, we appreciate your indulgence. That said, I do want to add a simple paragraph’s worth of unadulterated enthusiasm to the blog tonight with regard to Prism. In our meeting last Tuesday Bethany explained the basic concept, beginning with its origin in a text-analysis game involving Xeroxed texts and transparency overlays. The most exciting part about the proposed project, in my opinion, is so well-put by Bethany that I despair of rephrasing it more effectively. Here she is in her blog post this week : “What would happen if we could systematize, capture, and build collective interpretive energy — on shared understandings and unexpected disagreements?” Yes. What would happen?  We are going to build something that allows us to peek into other readers’ interpretive, individual, creative minds. Any given book club discussion, literary journal, or scholarly society gathering  testifies to the fact that everybody reads and processes texts differently: to think that with Prism we will be able to pin down a little bit of how and when that happens… I want to shout this from a rooftop somewhere! (And I suppose this blog is as good a place as any!) I’m with Brooke–I wish we could have Prism up and running tomorrow, as long as that didn’t mean my internship at the Praxis Program was over. Fortunately (and I mean this in all seriousness), we have a long way to go. I’m looking forward to every stress-inducing, joyous minute."},{"id":"2011-09-13-fall-2011-newsletter","title":"Fall 2011 newsletter","author":"eric-johnson","date":"2011-09-13 13:04:07 -0400","categories":["Announcements"],"url":"fall-2011-newsletter","layout":"post","content":"Our Fall 2011 newsletter (PDF) is now available for your reading pleasure. It’s chock full of introductions: of our three new graduate fellows, of our new Praxis Program (rethinking methodological training in the digital humanities), of our new cohort of Praxis fellows, and of four new staff members. You’ll also find the full schedule of fall programs and workshops and more news from and about the SLab. Get your copy here ."},{"id":"2011-09-13-getting-to-know-our-praxis-peers-samples-of-our-digital-work","title":"Getting to Know our Praxis Peers: Samples of our Digital Work","author":"ed-triplett","date":"2011-09-13 09:24:39 -0400","categories":["Grad Student Research"],"url":"getting-to-know-our-praxis-peers-samples-of-our-digital-work","layout":"post","content":"I would like to mention to you all that I have added two files to our collab resources that I hope will give a better idea of the kind of work I am putting together as a Scholar’s Lab fellow this year. There is a transcript of the very short presentation I gave two weeks ago which describes my GIS project and briefly outlines my dissertation topic. At some point there will also be a podcast of the presentation on the Scholar’s Lab page. My purpose in posting these two files is to begin the process of making the entire group more aware of what each of us works on as individual scholars. As I discussed briefly when we began talking about the Praxis charter, I believe it would be very helpful for us to better understand our individual motivations for joining the Praxis Program. As a multi-disciplinary group, we should be aware of each others’ strengths and interests as scholars, as well as the particular methodologies and approaches that are common to our particular fields. As such, I am proposing that if we do not have a digital, or “traditional” humanities project that we are ready to share with the program, each week one of us can submit a sample from another scholar that we believe approaches their subject in a way that represents our field, or may be relevant to PRISM. This can even take the form of a “favorite” article."},{"id":"2011-09-13-on-demons-and-prisms","title":"On Demons and Prisms","author":"alex-gil","date":"2011-09-13 06:30:20 -0400","categories":["Grad Student Research"],"url":"on-demons-and-prisms","layout":"post","content":"Prism is not many things, one of them is itself… for now. There is a history behind the identity crisis. As Bethany pointed out in her flagship post, Prism began as a Demon. Hearing McGann talk about it nowadays, you would think that we have found Richard Rorty’s ultimate intellectual ring, the one eye that encompasses all other. The pata-critical Demon owes its name in part to Alfred Jarry’s pataphysics, “the science of imaginary solutions,” from which we also get Pablo Lopez’s  pataphore, “an unusually extended metaphor.” When the folks at the SpecLab began playing around with markers and transparencies, they were in a sense blending science with play by making literal the idea that we all read differently. Although we all knew for centuries that there was room for interpretation, footnotes and marginalia safely occupied different places on the page, reinventing the author at the moment of its undoing. The Copernican move was to take the idea of difference seriously enough to overlap it. McGann, in most other cases a visionary, hesitated before the chasm. Today, he still wants to feed commentary to the Demon. Our prism ventures out on a different path. [![](http://static.scholarslab.org/wp-content/uploads/2011/09/prism-300x167.png)](http://www.scholarslab.org/praxis-program/on-demons-and-prisms/attachment/prism/) Prism ray trace Then there is the knack that some folks have to try to reduce it to the most mundane digital tools. Two in particular surface often: Diigo and NowComment . I hope I am clear when I say, Prism they are not, and they are not for the same reason: They are not focused. A prism refracts light according to a specific set of rules. Diigo and NowComment allow for a very diffuse set of comments and monotone highlights that cannot be wrangled easily for analysis. Both are helpful to provide feedback for one reader who has a vested interested in reading the comments. If we were to read interpretation as a social phenomenon, their usefulness runs its course. Our prism understands that we all wiggle under controlled vocabularies and that it is there that our differences thrive. So now that I’ve said my peace about what I think prism is not, let me leave you with a vision: On one of those slow dry desert days where Saint Anthony receives his motley crew of visitors, he has a vision. He sees a man wearing a wig before a strange glass pyramid. He sees strange markings on several pages strewn about a table rife with even stranger machines. A ray of light flashes through the window and the wigged man fumbles for the triangle. He offers it to the light like a bishop offering the host. Miracle of miracles. The crystal gives birth to a rainbow which fills up the room with the brightest colors, like the garments of his demons."},{"id":"2011-09-13-programming-for-prism","title":"Programming for Prism","author":"annie-swafford","date":"2011-09-13 08:36:35 -0400","categories":["Grad Student Research"],"url":"programming-for-prism","layout":"post","content":"I’d like to echo the blog posts that Brooke and Sarah made about their excitement regarding Prism and also the associated angst of blogging and transparency.  In addition to the pressure of worrying about “living up to the Scholars’ Lab blog,” I also feel discomforted by the very act of blogging itself; perhaps because I am used to imagining typed text as formal expressions of carefully thought-out ideas for journals, conferences, or a dissertation committee, it feels odd to type and make public thoughts I would more likely share in conversation.  The blog format itself seems to ask for more profound thoughts than my ideas here justify.  However, I look forward to the time when we all will have acclimated to this new way of sharing our thoughts, work, and progress in public! Although I am certainly looking forward to brainstorming the features of Prism and to reaping the benefits of the finished product in the classroom, I am most excited by the prospect of helping to build it.  While I have some HTML, CSS, and Javascript experience, my actual coding skills are practically non-existent (other than a preliminary knowledge of Python), and I am eager to learn and to improve my DH skill-set.  Over the last two weeks, we have learned the basics of VIM and of Bash and the command line, which, while a bit daunting, is also incredibly empowering.  My husband is a programmer, and I have watched him write code and work in Terminal with ease, and I look forward to gradually understanding more and more of what had previously seemed like multi-colored gibberish on a screen.  Although it’s hard to imagine now, at the beginning of our programming venture, that we will be able to make anything useful to anyone, I have faith that our learning speed will increase and in a few weeks, what now seems daunting will soon seem simple."},{"id":"2011-09-18-imagining-end-users-for-requirements-gathering","title":"Imagining end users for requirements gathering","author":"lindsay-oconnor","date":"2011-09-18 19:51:59 -0400","categories":["Grad Student Research"],"url":"imagining-end-users-for-requirements-gathering","layout":"post","content":"Bethany writes, “It was important to us that the project only be in service to the program — that its intellectual agenda was one our students could shape, that they set the tone for the collaboration, and that — as much as possible — it be brand-spanking-new, free from practices and assumptions (technical or social) that might have grown organically in a pre-existing project and which we might no longer recommend.” This sounds like a great approach, but I worry that it leaves out a few important details. Prism does fit this description—there are many possibilities for what it will look like and how we will work together and separately to create it—but it also brings with it some theoretical assumptions that we have less freedom to critique or modify. The idea has been around for a while now, as Bethany and Alex explain, and our discussion about it last week was more Q&amp;A and less brainstorming session than I had expected. Bethany writes, “The version of the Demon I presented to the Praxis team last week as an inspiration for Prism is simpler in scope and beholden more to the material and pedagogical than to the text-theoretical.” A few weeks ago Bethany helpfully talked about how all DH projects have implicit theoretical stances and how programming performs critical work, so I do think Prism will have a “text-theoretical” aspect.  It’s an aspect of PRISM that seems non-negotiable right now. I left last week’s meeting thinking that a text-mining functionality is a goal we have to adopt, despite any reservations we might have about it, and that’s a part of Prism that is bringing out some of our differing ideas about interpretation. I realize that we are creating a tool for researchers to use in ways we anticipate and ways that we don’t, so it’s not up to us to interpret the crowdsourced interpretations. Yet I still worry about enabling facile conclusions from quantified data under the name of “literary” interpretation and what an increase in this kind of work would mean for the future of the non-alternative academy. Now that we’re working on our requirements and expectations for Prism, I support making text-mining a much later goal and focusing on creating user accounts and communities. Thinking through requirements has also gotten me excited about how Prism could work as a tool for the social sciences. If Prism can collect information (demographic and otherwise) about the people tagging or highlighting text, social scientists could use it for research questions about reading communities, literacy, and education. Focusing on this possibility helps me get away from worrying about text-mining, but I’m not sure if this takes Prism out of the scope of digital humanities ."},{"id":"2011-09-19-a-transdisciplinary-ethics","title":"A Transdisciplinary Ethics","author":"brooke-lestock","date":"2011-09-19 14:40:57 -0400","categories":["Grad Student Research"],"url":"a-transdisciplinary-ethics","layout":"post","content":"Coincidentally (or maybe not-so-coincidentally), part of Lindsay’s post directly echoes the opening concerns of an article I’m reading for the EELS (Electronic Enabled Literary Studies) group led by Profs. Stauffer and Pasanek here at UVa. In “Learning to Read Data: Bringing out the Humanistic in the Digital Humanities,” Ryan Heuser and Long Le-Khac discuss the conflict between excitement and anxiety in DH work (sentiments I echoed in my last post and feel every time I approach my computer these days). The essay asks first whether we can use the quantitative methods employed in DH work while “respect[ing] the nuance and complexity we value in the humanities,” and then confronts the much deeper issue at-hand: “Under the flag of interdisciplinarity, are the digital humanities no more than the colonization of the humanities by the sciences?” (2). Even more coincidentally (scholarly synchronicity at its finest), Jahan Ramazani, in his Modern Poetry course, just assigned us the second chapter of his game-changing book, A Transnational Poetics . Now, I’m making a theoretical leap, but I can’t help approaching Lindsay’s concerns with Ramazani’s words in mind. In the opening page of his second chapter, he explains that “humanistic disciplines must draw artificial boundaries to delimit their object of study - nation, language, period, genre, and such - and so must allow for anomalies” (23). The chapter goes on to debate the mononational narratives literary scholars build around modernist poets, and the solution Ramazani offers is one of expansive compromise: “to begin to explain how poetry helps newness enter the world,” he writes, scholars must investigate “complex intercultural relationships across boundaries … without erasing those boundaries” (47). Now that I’ve sufficiently piggybacked off of other more experienced scholars’ work, I’ll attempt to address our group’s concerns about data-mining with Prism. Though I admit that this quantitative approach to literary work gives me the nervous-sweats because of its potential for producing data that ignores the “nuance and complexity” we worship as humanities scholars, the idea of  drawing “artificial boundaries” delimiting the scope of the project is something I’m even less comfortable with. I barely understand the potential implications of a tool like Prism at its most basic level, so I’m hesitant to demarcate how Prism should be used by other disciplines that could potentially do interesting things with it (Lindsay mentions the social sciences, for instance). That being said, I completely agree with Lindsay that the text-mining feature should be a second-tier priority until our Alpha version is running. We already have our work cut out for us. What I hope I’ve made clear is that I support not a “colonization of the humanities by the sciences,” but instead a transdisciplinary ethics for our project and the DH field-at-large. By that I mean we must be conscious of our methodological differences while constantly questioning disciplinary boundaries (shout-out to Ed for his interdisciplinary show-and-tell idea ). Ramazani quotes Edward Said’s  Culture and Imperialism, and it bears repeating here: “The fact is we are mixed in with one another in ways that most national systems of education have not dreamed of. To match knowledge in the arts and sciences with these integrative realities is, I believe, the intellectual and cultural challenge of the moment” (Ramazani 49, Said 331). P.S.: Stay tuned for next week’s blog post, which I’m sure will be a mental breakdown à la Ruby."},{"id":"2011-09-19-on-interventions","title":"on interventions","author":"bethany-nowviskie","date":"2011-09-19 03:47:31 -0400","categories":["Grad Student Research"],"url":"on-interventions","layout":"post","content":"I started to write this as a comment on Lindsay’s latest post, but then thought I should boost it a bit, so that it becomes a part of the overall conversation about next steps for Prism. I’ve been out of the mix of the discussions you guys have been having, so it may be that Lindsay is responding more to a building group consensus about how to use textual data than to the model “reference interview” introducing Prism that Jeremy conducted two weeks ago. In case the opposite is true, I want to jump in with some clarifications. The first is that there is no aspect of this project that is not up for consideration, critique, and potential re-casting.  Because we are building the tool collaboratively, under a charter I hope you guys will propose to us for adoption on Tuesday, user requirements and overall vision for Prism will need to be negotiated with the group. Project-building in DH invariably requires compromise, but to get there in a healthy way, it first requires a great deal of clarity about team-members’ stances and goals, and the basic theoretical orientation of the work they are trying to conduct (and enable!) together. In the case of Prism, Lindsay’s (and perhaps others’) skepticism about the utility of that potential text-mining piece – and, I think, about what algorithmic approaches mean in the overall scene of literary studies in an age of “big data” – is certainly being heard. What I’m less clear on is what the team might be imagining is possible to do with the marked passages of Prism’s crowd of users. I would encourage you, at this point in the planning, not to reject approaches without exploring them – without feeling fully informed as to their potential and (more importantly, in cases where you intuit a problem) the intervention you might make, or the twist you might bring, to their use in the scholarly community. Our suggestion about the potential of this project to re-cast the vogue for mechanical “crowd sourcing” in DH in more interpretive and pedagogical terms is an example of one such intervention. The role of the more experienced members of the Prism team will be to make sure we’re all aware of (sadly, well-trodden) pitfalls in project design and execution. Examples of this from our first requirements conversation were the design and UX dangers of starting the project with a large or unconstrained number of user-extensible color-codings and, likewise, diving into a complex user-accounts system before specifying and architecting the basic functionalities of the tool. You can think of those as an example of some negative course-corrections we may try to make as the group moves forward. A positive one would be my suggestion that you learn about and think fully through the possible value of data-mining approaches – and the potential for this project to model good ones, or critique from within – before rejecting the notion out of hand. If you ever feel us pressuring you to move in a certain direction, it will be out of a desire to help you and the overall community grow. If that’s not evident in the moment, it’s your job (as Lindsay has done admirably!) to tell us what you’re hearing and pressure us for clarity. In terms of the overall vision for Prism, it’s true that it stems from longstanding conversations and experiments, and that we came in with some big ideas for the kinds of scholarly intervention it might make, and the directions it was possible to go. But if we had really wanted to build it exactly as specified, we’d have done so, and made up another project for you guys to sink your teeth into. This one’s just an offering, for the whole group to make of what we will."},{"id":"2011-09-20-introducing-our-digital-work-songs-of-the-victorians","title":"Introducing Our Digital Work: “Songs of the Victorians” ","author":"annie-swafford","date":"2011-09-20 06:55:31 -0400","categories":["Grad Student Research"],"url":"introducing-our-digital-work-songs-of-the-victorians","layout":"post","content":"In response to Ed’s suggestion that we introduce ourselves and our digital work, I thought I would spend some time explaining my own project on Victorian musical settings of Victorian poems, which I developed through the NINES fellowship program here at UVa. I study the intersections of music and Victorian poetry, and the final chapter of my dissertation focuses on Victorian musical settings of contemporaneous poems.  I have written papers on such topics in the past, and have discovered that it’s nearly impossible to make arguments that involve music through traditional print media unless one writes for a musical audience.  I have included excerpts of the musical score, which helps the handful of people who can read music, but even fewer of them can actually hear in their mind the music they see on the page.  I’ve seen instances where scholars have included a cd along with their article, but even that is no guarantee that the reader will listen to the music or be able to follow the audio or the argument without guidance.  As a result, I’ve developed a website that can help solve this problem. My website features scans of the first edition printings and audio files of the songs as well as an analysis of the ways the song interprets the poems they set.  Although users can focus on each part separately (view the score, listen to the audio, or read the commentary), they will be best served by the interactive functionality of the site.  When the audio file is played, a box highlights each measure in time with the music so a user can follow along.  Additionally, when the commentary explains a particular musical effect that augments the meaning of the text, the user can click on a parenthetical note at the end of the sentence that will play that portion of the audio file and highlight the score in time with the music, so the user can follow the argument regardless of their musical prowess. I hope that this will be useful not just for my own work, but also for anyone who wants to present an argument involving music for non-musicians. This project uses html, css, and javascript, and I am in the process of figuring out how to incorporate MEI, the scholarly music XML standard, invented by Perry Roland here at UVa.  I had hoped to be able to incorporate score following so I wouldn’t have to generate the information for each measure box by hand, but it seems as though that is too substantial a project for me to tackle, as it is not at all a solved problem. The project is not yet live, but if you would like to see it in its current incarnation, then feel free to view the demo video I made. If any of you have suggestions on MEI incorporation, design, or score following, then please let me know! I’m always open to suggestions."},{"id":"2011-09-20-mimesis-and-computer","title":"Mimesis and Computers","author":"alex-gil","date":"2011-09-20 09:52:22 -0400","categories":["Grad Student Research"],"url":"mimesis-and-computer","layout":"post","content":"“Computers are inherently dumb.” I hear this all the time, even from folks in computer science. I like to think of them as marionettes. After Wagner called for a Gesamtkunstwerk, many European artists and thinkers reacted strongly to it (Nietzsche being the most famous case). This reaction eventually led to a modernist distrust of theater in general, and of human actors in particular. Think for example of Bertolt Brecht’s Verfremdungseffekt . Somewhere in between Wagner and Brecht, the English artist Edward Gordon Craig suggested that human actors should be replaced by marionettes. As you can imagine, this did not go well with the actor’s guild. I hear echoes of those debates and cultural shifts in our moment, when computers are starting to resemble us more and more. Computers don’t replace us always in the way that machines replaced farmers or smiths, although there are still parallels between ours and the anxieties of the industrial and agricultural revolution. And just like machines then generated monstrous forms of mechanized human labor, computers do the same (If you don’t believe me, ask any of my students for Project Tango ). However, there is another anxiety I see which is not necessarily that of machine iteration replacing familiar mechanical tasks with unforeseen ones. I’m talking about our fear of marionettes. Even more specific, the fear that we will confuse the marionettes for human beings. [![Quixote fights the puppets](http://static.scholarslab.org/wp-content/uploads/2011/09/quixote-300x225.jpg)](http://www.scholarslab.org/praxis-program/mimesis-and-computer/attachment/quixote/) Quixote fights the puppets The true marionette is always controlled by a human, so are computers… ultimately. We ventriloquise through them, and they only talk back to us according to our ridiculously precise instructions. I’m not talking about Bina48 . She’s kind of creepy. I’m talking about the ways in which a google search acts like an operator at the end of a 411 call; or the way that netflix suggests what we might like. There are two approaches to figuring out what counts as a title in a large repository: we can tag it, or we can write an algorithm that does it for us. Don’t worry, we’re not there just yet. At some point that meta-data might pass the Turing test . If it does, by definition, users will think a human did the work… but wait. When it does, by definition, users will think a human did the work… but wait… Prism is not really interested in how humans might be fooled by the marionettes more than it is in how we can fool the marionettes to behave like us. Sometimes that line is blurred. The ‘text mining’ component, as I have understood it, seems like the bastard child of natural language processing and web crawling. The goal here is not to count words (although that is a time-honored human activity), but to abstract semantic relationships that can be used to query large data. When we Google something, we are doing something akin to that, except we never think Google is run by a million efficient munchkins . When we start getting results for our perhaps-to-be Prism queries, we use those results in public at our own risk. That there will always be Quixotes in the audience… well… Take home tweet: Even if we replace actors with marionettes, the plot stays the same."},{"id":"2011-09-20-prism-is-looking-for-john-connor","title":"Prism is looking for John Connor","author":"ed-triplett","date":"2011-09-20 08:57:41 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"prism-is-looking-for-john-connor","layout":"post","content":"It seems the text mining issue has struck a chord with our group, so I will jump in as well. Specifically, I want to refer to Sarah’s thought that the potential danger lies with the scholar interpreting the data prism could potentially collect, not with “the machine.” This allows us to do what comes natural in the humanities: critique the conclusions a scholar makes when they attempt to make use of the data that Prism “collects.” There is a well established system in place that – especially given the growing understanding of DH processes among humanities scholars – can “sift” through scholarship that uses a tool like Prism to make poorly supported conclusions. I’d like to propose that we think about what kinds of things we can learn through Prism. Thus far we have defaulted to describe our “colors” with tongue-in-cheek phrases like “Happy passages” or “Daddy Issues passages.” I may be wrong, but I don’t think we are truly trying to “map” happiness in literature.  To make this process more tangible for our group, I think we should make it a priority to decide on a sample piece of literature and at least two possible “colors” or “expressions” that we deem valuable as crowd-interpreted data. Some of the debate about using prism to “quantify” and “mine” literary expressions or feelings may diminish if we were had a more concrete problem we’d like to solve as an example. We will thus avoid thinking about Prism as a method for mapping “happy.” Clearly there is an aspect of prism that is designed to be “hands off” and allow the crowd sourcing to be less directed and therefore more “honest.” However, many of us are simply not comfortable sending a powerful robot into the world which could be easily told to shake its metal hands and help people draw spurious conclusions before we see an example of this robot saving kittens from trees and making life better for literary scholars. Think Schwarzenegger in the original Terminator versus Schwarzenegger in Terminator 2…"},{"id":"2011-09-20-the-pleasures-of-programming","title":"The Pleasures of Programming","author":"lindsay-oconnor","date":"2011-09-20 13:13:47 -0400","categories":["Grad Student Research"],"url":"the-pleasures-of-programming","layout":"post","content":"Instead of continuing the text-mining debate, I want to reflect on our programming lesson last week and our homework this week. I’m writing this from the Scholars’ Lab grad fellows lounge, where about half of the Praxis team is working through some of our Ruby homework with Eric and Wayne’s expert, patient assistance. Annie just exclaimed with happiness when she figured out how to calculate grades with an array. We all smiled and Wayne seemed especially happy to see his student catching on and feeling good about her progress. There is something so satisfying about programming, about telling a computer to do something and having it do what you want, even if you’re just telling it to spit out a few numbers under certain conditions.  We are learning to create a program and make it respond to our commands, and that seems to give us a more intense or more satisfying pleasure than we get as users telling Google to execute a search or telling Windows to open a file. This satisfaction is particularly notable to me, and I presume to many of the other grad students in the Praxis program, because our work in the humanities is never so cut and dry with such easy, absolute measures of success and failure and with such ownership of the process from beginning to end. While I’ve never programmed before, I recall a feeling somewhat like this from my time in middle management using a CRM system in a large call center. I was good at my job because I was just tech savvy enough to learn new functionalities quickly, to intuitively find ways to make our computer programs do what I needed them to do. I was so pleased with myself when I found the most accurate search terms or devised a shortcut that made workflow faster or easier. But when I was in a supervisory role, the skills I needed to succeed were much less technical. I needed to be kind enough and flexible enough to handle whatever issues arose with the people I supervised, from tardiness and time off to explaining why we were managing human relationships with a computer system. Success on this front was much harder to measure and I rarely felt such unqualified joy at my successes because I was never quite sure when I was successful. I wonder if this will prove roughly correlative to humanities computing. Learning to program is slow and tedious and difficult, but success feels hard-earned and justified and thus pleasurable. Humanities scholarship hasn’t ever brought me such a clear-cut sense of success, but it has brought me pleasures that I think mastery of computer algorithms cannot. I hope we can all enjoy the pleasures of mastery over our computers, but I hope we can do so without losing sight of the academic interests that brought us all here. As Annie worked through the grade calculation task, she dealt with the gray areas of rounding up and rounding down and admitted that these decisions always vary. That very simple program had its limits when applied to an idiosyncratic situation, a lesson I will keep in mind as I learn more about and engage in humanities computing."},{"id":"2011-09-20-vive-la-difference","title":"Vive la différence!","author":"sarah-storti","date":"2011-09-20 04:41:07 -0400","categories":["Grad Student Research"],"url":"vive-la-difference","layout":"post","content":"When I signed up for  David Hoover’s “Out-of-the-Box Text Analysis” course at last summer’s Digital Humanities Summer Institute, I had absolutely no idea what I was getting into. Text analysis… with computers? Data mining? What? The first of our meetings felt akin to culture shock, I think, or to having a bucket of ice water thrown over my head (We are going to do what with the texts? Cut them up into word frequency lists??), but once I recovered myself, as it were, the rest of my fast-paced DHSI week provided me with a basic understanding of not only how to use text mining tools, but, perhaps more importantly, why one might want to use them. I thought now might be the perfect time to share some of what I learned in Victoria. Firstly, yes, a text mining operation can give you a set of numbers that supposedly correspond in some way to a given text. Oooooh, isn’t that scary? Well, okay, maybe it is. But what happens after that is entirely dependent upon the scholar interpreting the numbers. The machine is only a machine. It is not going to change the text. Text mining tools can, however, change the ways we look at our texts. In the course, we certainly considered questions that I had never before considered about a set of texts. They included: how well (according to word frequency lists) does the author distinguish between the “voices” of his/her different characters? What about the difference between those number sets and the number sets we get from doing the same kind of test with a different author? Is one “better” at differentiating vocabulary than another? Does this even matter? Can we make an educated guess about authorship of a disputed text based on comparisons of that text with other texts written by the two authors in question? Frequently other members of my class would push back, pretty forcefully, on the idea that we could draw hard and fast conclusions about a text using only the tools on our computers. Nobody, it seemed to me, was about to publish a paper on why Author 1 is superior to Author 2 because Author 1 uses a richer vocabulary. And these people were in the class voluntarily—they (in theory) wanted to learn how to use such tools, to what various ends I could not say. I do know, however, that I never once felt that the group lost sight of the difference between an author’s text and a set of numbers. The kind of data results we got from running these tools on our texts were simply that: data results. Mining texts for word frequency (or what you will) does not inherently devalue or damage the text. Sure, it’s possible to come at a text with a preconceived notion and repeatedly run different tests to try to prove that theory right, but the same holds true, certainly, for traditional literary criticism. I suppose my question is this: do we have any reason to be concerned, really, about what will happen if Prism does eventually allow for some kind of data mining? What is the worst that can happen? And by allowing our fears (e.g. someone will draw an irresponsible conclusion based on numbers) to dictate the direction we take Prism, aren’t we obstructing the possibility of unimagined positive outcomes? While I do think it is important for us to feel good about the tool that we are building, I would also echo Bethany’s cautionary (and immensely helpful) advice “not to reject approaches without exploring them — without feeling fully informed as to their potential and (more importantly, in cases where you intuit a problem) the intervention you might make, or the twist you might bring, to their use in the scholarly community.” “Text mining” may be a Bad Word to some of us, but the way our as-yet unrealized tool works could very well make an interesting intervention in the text mining (and DH) world, regardless of our personal preferences. Isn’t that the kind of thing we’re here at Praxis to do? To conclude, I will make another admission: even after a week of thought-provoking and congenial collaboration between myself and my colleagues at DHSI, I would not consider myself to be a data mining kind of scholar. In fact, I would consider myself to be more bibliographically-inclined than anything else. I own both of the Tanselle syllabi . I am invested in methodologies which make it difficult for me to see how I could implement text mining in my own work right now. These inclinations of mine do not, however, mean that I think everybody else needs to lean my way. I look forward to seeing what other people want to do with Prism, and I hope text mining does eventually become part of that. I would like to see what kind of questions people will ask about the interpretation of texts thanks to our intervention via Prism, especially because those questions are ones I would probably never ask if left to myself. After all, as my grandmother says, variety is the spice of life."},{"id":"2011-09-22-richmond-virginias-place-in-gis-and-racial-discrimination-history","title":"Richmond, Virginia's Place in GIS and Racial Discrimination History","author":"chris-gist","date":"2011-09-22 07:34:45 -0400","categories":["Digital Humanities","Geospatial and Temporal"],"url":"richmond-virginias-place-in-gis-and-racial-discrimination-history","layout":"post","content":"Richmond, Virginia is a city steeped in history.  It is the home of the first commercially viable electric street car system,  the world’s only triple train crossing ;  the first woman-owned and African American-owned bank, and some great Americans including Bojangles Robinson and Arthur Ashe .  Not exactly the history you were thinking about, correct?  There is much more hidden history in Richmond. ![](http://www.csiss.org/classics/uploads/mcharg-image1.jpg) Ian McHarg Ian McHarg was born around the industrial town of Glasgow, Scotland in 1920.  After World War II, he came to the U.S. and started a career in city planning and landscape architecture.  He founded the Department of Landscape Architecture at the University of Pennsylvania and is considered innovative for his notion that design should work with the landscape instead of fighting or changing it.  He has also been credited with coming up, in the 1960s, with the idea of map overlay which is a fundamental GIS technique. So, what do Richmond and McHarg have in common?  Before I can tell that story, I have to tell this one.  When I was a  young struggling grad student, I happened to be at work in the Urban Planning Department at VCU one summer day when a PhD student from the University of California, Santa Barbara – John Cloud – strolled in. He told a small gathering of a few grad students and professors the story of how during the Great Depression the economic conditions were similar to those we face now. There was a foreclosure crisis and banks were not offering mortgages.  In an attempt to get the industry back on track, the Federal Housing Administration seeked for ways to estimate neighborhood risk for mortgages.  They looked for indicators to predict how neighborhoods would fare at future dates. In partnership with the Richmond Planning Commission (RPC), the FHA used Richmond as one of its major study sites.  Mr. Cloud showed us a report he pulled from the National Archives (NARA) called Statistical Data Relative to Housing and other Planning Matters . ![](http://static.scholarslab.org/wp-content/uploads/2011/09/planninManners-732x1024.jpg) December 1935 Version This report, produced as early as 1935, uses a series of tissue paper overlays to show various themes.  A color, loose-leaf card stock map of rent by block is used as the underlay.  Mr. Cloud has found this report to be the earliest American example of the use of such overlays during his research.  He also found parallel lines of work going on in Germany during the same time period.   Please find John Cloud’s detailed article on this story  here .  There may be older examples out there waiting to be discovered! ![](http://static.scholarslab.org/wp-content/uploads/2011/09/IMG_0951-768x1024.jpg) Rental map under housing study area map ![](http://4.bp.blogspot.com/_D7-iSeG_eP0/SwGJ0Qs_hiI/AAAAAAAAABI/lOpSGsLH7ks/s1600/Homer%282%29.jpg) Homer Hoyt Another player in this drama, economist Homer Hoyt, was an influential researcher at FHA.  He wrote a book in 1939 about housing research techniques called The Structure and Growth of Residential Neighborhoods in American Cities . In that book, Hoyt demonstrated the value of techniques developed during the FHA’s neighborhood forecast research including the  sector model  and – you guessed it – map overlays.  He used mylar sheets in the book to do a series of overlays for Richmond, which is clearly a distilled version of the RPC report maps. ![](http://static.scholarslab.org/wp-content/uploads/2011/09/hoytOverlays-1024x599.jpg) At right, all four of Hoyt's overlays together: Race, Age, Condition, and Rent So, it wasn’t really Mr. McHarg who pioneered the use of overlay at all (sorry to all you landscape architects).  At least, the RPC, Mr. Hoyt, and German researchers did it some twenty-five years earlier. On a related topic, in my recent correspondence with Mr. Cloud he informed me that the Library of Virginia (LVA) had another copy of the RPC report that was missing at least one overlay.  We agreed that I would scan the LVA’s copy and send him a copy.  In return, Mr. Cloud promised to get the missing pages from the NARA copy.  However upon further investigation, I have discovered that there are at least two versions of the document.  The NARA version is dated December, 1935, and the LVA version is dated January, 1938.  There are a few discrepancies between the versions and it is hard to tell whether they were produced with different overlays or whether some layers have been lost over the years.  Specifically, there are unique overlays in each document.  The following table is an inventory of the overlays for each document. Theme Name\n(from report)\nNARA Version\n(1935)\nLVA Version\n(1938) Rental Map\n(by city block - card stock color underlay) Yes Yes Rental Map (by area) No Yes Relief Cases (point data) Yes Yes Juvenile Delinquency (point data) Yes Yes Adult Delinquency (point data) Yes Yes Infant Mortality (point data) Yes Yes Tuberculosis - 1934 (point data) Yes Yes Population - 1930 (dot density) Yes Yes Areas Inhabited by Negroes (area) Yes No Certain Statistical Data - 1935\n(combination of TB, relief and delinquency) No Yes Principal Thoroughfares Yes No Housing Studies\n(shows specific study areas within report) Yes Yes Territorial Growth Yes No Census Tracts - 1935 Yes Yes Mr. Cloud had concluded that purposeful reasons (i.e. a tear out) were to blame for the missing African American overlay from the LVA version of the RPC report. However, based on my comparison of the two different texts, I am not so sure.  I believe Mr. Cloud did not have all the information.  Did the 1939 version of the report originally have different overlays than the 1935 report?  Or were overlays removed/lost from each copy in the subsequent decades?   Of course, it wouldn’t be a surprise to find out that someone purposely removed specific overlays from the PRC report given the amount of revisionist history around Richmond. Hoyt listed four factors (race, age, condition, and rent) in determining neighborhood risk.  According to Hoyt’s analysis, we have race, specifically the number of African Americans, as a major indicator for forecasting neighborhood risk.  Is that enough to cement Richmond’s place in racial infamy?  Not quite.  However, much of his research went into a survey which concluded in a series of maps produced by the Home Owners’ Loan Corporation (HOLC).  The HOLC maps were used as proof of  redlining,  as term coined by John McKnight in the late 1960s. ![](http://static.scholarslab.org/wp-content/uploads/2011/09/07-HOLC-map-1024x801.png) \"Redline Map\" of Richmond by the Home Owners' Loan Corporation Undoubtedly, race – specifically the percentage of African Americans – played a large role in determining the hazard values for the HOLC maps.  The question is whether or not the HOLC maps caused institutional discrimination against African Americans after their production.  Amy Hillier, a researcher at the University of Pennsylvania, says no, at least not in Philadelphia.  According to her analysis, banks were giving mortgages in the redlined districts after the Philadelphia HOLC map was produced.  She surmises that the HOLC maps were not widely circulated outside the federal government and therefore were probably not known or used by lenders.  In fact, a majority of HOLC’s mortgages were in the hazardous end of their assessment scale.  However, Hillier does conclude that the FHA policies, which were formed partially from Hoyt’s research, were influential in the arena.  The legacy of these maps and policies must not be understated. The University of Richmond has a excellent  site that shows the HOLC map for Richmond and explains in detail the criteria and survey data used to determine hazard rankings of which presence of African Americans trumped all other factors.  The handwritten survey reports are shocking. Next Steps We have now finished the process of digitizing all the RPC and Hoyt overlays.  We next will do some geostatistical analysis to compare them to the Richmond HOLC map to see how well the data fit together.  I would also like to map the new mortgages and refinancing given from the 1940s to the 1960s, à la Amy Hillier, to see the rates in the different hazard zones of the HOLC map.   Jeremy Boggs, a historian in our group here at the Scholars’ Lab, has an interest in looking at the City of Richmond’s policies during this period to gauge how they were affected by Hoyt’s and HOLC’s research and maps. ![](http://static.scholarslab.org/wp-content/uploads/2011/09/map-1024x727.png) GIS map showing overlay from RPC report and HOLC hazard areas Article edited to make corrections to errors pointed out by Mr. Cloud in the comments section."},{"id":"2011-09-25-ruby-slippers","title":"ruby slippers","author":"bethany-nowviskie","date":"2011-09-25 10:21:42 -0400","categories":["Grad Student Research"],"url":"ruby-slippers","layout":"post","content":"It’s been an excellent Sunday morning for posts about DH and the profession(s). First, Desmond Schmidt crunches the numbers from a decade’s worth of job postings on Humanist, which is the primary and longest-standing international discussion list for the digital humanities. (If you think there’s a DH boom in the US, check out Desmond’s per-capita analysis.) Interestingly, this survey only took PhD-level positions into account.  How have job requirements in this field evolved? Tomorrow’s Humanist should have a response from Dot Porter, citing an #Alt-Academy essay she wrote with Amanda Gailey on “ Credential Creep in the Digital Humanities .” And here’s Kathleen Fitzpatrick in the Chronicle, on what is really required of institutions and departments who encourage junior scholars to ‘Do the Risky Thing’ in Digital Humanities . Kathleen is amplifying and contextualizing a concern frequently voiced in the past two years, around the spate of “cluster hires” in DH – which sometimes seemed to happen without thought given to the suport structures, both departmental and institutional, that new faculty would need. (I remember Patrick MurrayJohn as the first to start squawking about this on Twitter. I couldn’t find his much-earlier tweets, but there’s this thread at DH Answers.) On the Chronicle piece, Kathleen and Ian Bogost make two important further points that may resonate with our Grad Fellows and Praxis group: regarding “ mentoring up,” and pressing forward . Finally, Natalia Cecire responds with the most acute blog post I’ve read on the whole so-called “rise” of digital humanities and its political and professional consequences: “It’s not “the job market”; it’s the profession (and it’s your problem too).” And what am I doing on a quiet Sunday afternoon (besides linking together this distributed conversation)? I’m following along with our Praxis students as we learn Ruby from the ground up. This has been really satisfying to me, and not only because of every way in which I agree with Steve Ramsay on “building.” It’s also because, like so many digital humanists of my generation, I learned every ounce of what I know on the job, rather than in the classroom or through any formal or institutionally-supported training program – and most of the time my learning involved being confronted with something half-built or even jury-rigged by other humanities scholars who only marginally knew what they were doing. I’m not disparaging this experience! The soft skills and improvisational confidence you learn on real-world collaborative projects are invaluable – but the rationale for addressing programming more clinically may be akin to the one for learning Latin. (Look what has been built upon it – what you will understand! And you’re not really going to pick it up as an exchange student.) I’ve always felt like I could hack around (read: extend, modify, steal) on the spot with a decent level of fluency – if supplemented by a small amount of magical thinking – but that I lacked the basic and thorough grounding that would serve me well in a variety of situations, and that would make me less dependent on others when starting from scratch.  It’s time I did something about that. The R&amp;D; staff of the Scholars’ Lab are providing our Praxis colleagues (and those of us in the SLab who need it!) with exercises, tutorials, lessons, and one-on-one sessions on learning to code. They’re also sharing the materials they create with the wider world. Pretty soon, the Praxis team will move out of lesson-ville and back into on-the-job learning, as they collaboratively design and build a tool called Prism .  For now (for me, anyway), taking the time to complete a set of rudimentary Ruby exercises feels like the biggest gift I’ve given myself in a long while. What does this have to do with Fitzpatrick on risk? With Cecire’s sharp look at the present scene? With Schmidt and Porter &amp; Gailey and the trends? With the title of this post? I’m making a cup of tea and moving off the Praxis site into a third set of exercises – so let’s leave that as an exercise to the reader."},{"id":"2011-09-26-play-in-the-praxis","title":"jugando a praxis","author":"alex-gil","date":"2011-09-26 10:41:54 -0400","categories":["Grad Student Research"],"url":"play-in-the-praxis","layout":"post","content":"I third Lindsay ’s and Bethany ’s motion to declare coding a pleasurable activity. I have been tingling with guilty pleasure for the past couple of weeks doing the exercises that the Slab developers have prepared for us. The guilt comes from the distance between these puzzles and the dissertation chapters I have spent the better part of this year writing. I know I will not feel guilty for long. Very soon I will begin coding for my digital edition of Aimé Césaire’s Et les chiens se taisaient and coding will just be what I do in lieu of writing for a while. My pseudo-catholic reaction to code was very revealing to me. After all, why should anyone feel guilty to learn a new language? Why does it feel like a forbidden art? These questions got me thinking about the alleged differences between natural languages and computer languages. In the English Department, nobody would bat an eyelid if you said you were learning Farsi. Somehow Farsi could be integrated to your work as a scholar. Coding on the other hand still feels alien. It shouldn’t. The fact that it does speaks to that imaginary border between scholarship and service. As English graduate students, well-steeped in the scholarship of Benedict Anderson or Walter Mignolo, we should recognize by now that borders are reified in the performance of a nationalism. I belong to the literary scholar nation, you belong to the librarian nation, etc. I am a migrant in the fullest sense of the word. To me these borders seem silly… and dangerous. [![Git Languages](http://static.scholarslab.org/wp-content/uploads/2011/09/git-lang-300x136.png)](http://www.scholarslab.org/praxis-program/play-in-the-praxis/attachment/git-lang/) The languages of Git Another (false) difference between natural languages and code got me thinking further about what we’re doing here. If a speaker of two languages sees a paragraph where those two languages intermingle, they recognize the meaning immediately. At first sight, computer languages don’t seem to behave that way. After all, you can’t really insert a PERL line at random in a Ruby script and expect the interpreter to recognize it. This difference is also artificial. There is a right way and a wrong way to mix Spanish with English, just as there is a right way to mix Java with PHP. In order to produce meaning you just have to do it the right way. Git, for example, is written mostly in C, but with substantial contributions by other languages. Junot Diaz’s The Brief and Wondrous Life of Oscar Wao is written mostly in English, with some Spanish thrown in the mix. Even for Spanglish speakers the text follows certain rules to make itself clear. All that to say, that for those of us who are becoming bilingual (again), it is not caos total, but just a new set of procedures."},{"id":"2011-09-26-subject-and-object-required","title":"Subject and Object Required","author":"lindsay-oconnor","date":"2011-09-26 06:25:32 -0400","categories":["Grad Student Research"],"url":"subject-and-object-required","layout":"post","content":"Despite the title, this won’t be about objects and coding. It’s about the subject behind the requirements we gather, the people we gather those objectives from. For the past couple weeks, we’ve been bogged down in some of the practical instruction we’re getting as part of the Praxis program, so I had forgotten about some of the project planning and management instruction we got a few weeks ago. When introducing us to requirements gathering, Jeremy and Bethany modeled the methods we might use as project planners when meeting with a client for whom we’re building a tool. Jeremy asked specific questions and redirected Bethany to the issues he wanted to cover. As I wrote a few weeks ago, perhaps over-critically, this is how some of Prism’s history, and what I took to be its existing expectations, were revealed to the Praxis team. Now I want to step away from the content of that discussion and look at the form — what methods and approaches it modeled rather than what particular information it revealed. If we were to extend this model of client-focused project planning into project management and evaluation later on, we might continue to check back with our “client” Bethany to make sure the tool we build for her is coming along as she would like. It would be her goals as a user that would become our end goals as tool-builders. But this instruction on how requirements gathering works is at odds with the “Prism is whatever you want it to be” message of the Praxis program. Is our job to learn the many skills necessary in DH by building a tool for a given client’s needs, or is our job to create a tool for the broadest possible audience, being cautious not to assume too much about how it will be used? I wonder if this question is similar to a question about what kind of role in the DH community we want, or what role Praxis is preparing us to take. Are we collaborating on scholarship or are we in a support role, providing a technical service that shores up someone else’s scholarly project? If we’re creating a tool for a very wide audience, how does this activity fit into our own scholarship? Are we academics or “alternative academics”? Now I realize that I was wrong to point to the specifics of Bethany’s history of Prism as the source of my confusion over just how much Prism comes with particular expectations and how much it’s really ours to create. I came into that meeting with the expectation that we could make Prism however we wanted, so I was surprised to see that we might be gathering requirements from outside the Praxis fellows. So now I’m interested in the higher level question of where we’re gathering requirements from and to whom we’ll be accountable throughout the process. Are we acting like Bethany is our client who has a scholarly project in mind but needs a technical team to help think it through and make it happen? Or do we get requirements from the entire Praxis team, with all our individual hopes and expectations but a potential user population so wide we have to somehow build for uses we will never anticipate? If it’s the latter, we would do well to take Sarah’s advice on our requirements document to heart: “we want to be careful about pigeonholing Prism” because it could have uses beyond the primary pedagogical and second-level interpretive ones we’ve talked about so much. I’m not trying to disparage either framework, but I am hoping we can talk about which one applies to the Praxis program since this seems to cut right to the point of it all. This dichotomy makes a lot of sense to me, helps me orient Praxis in relation to the solitary academic scholar I’m so familiar with, but I realize that it’s a dichotomy that Bethany’s most recent post and some of the people whose work she cites all seek to break down. We all want to be visionaries and risk-takers, but sometimes I react quite critically to new things and, much like Kathleen Fitzpatrick ’s grad student questioner, I might be a little too worried about the current academic “ecology” to feel confident in disrupting it. But it has only been a month."},{"id":"2011-09-27-a-belated-love-letter-to-ruby","title":"A belated love letter to Ruby","author":"sarah-storti","date":"2011-09-27 06:02:30 -0400","categories":["Grad Student Research"],"url":"a-belated-love-letter-to-ruby","layout":"post","content":"Today in the Scholars’ Lab Grad Fellows office I had a brief conversation with Alex about learning programming. Me: Hey, Alex. Alex: Hi. How are things going for you? Me: [exasperated sigh] Alex: Ha! I actually really enjoy it. It’s soothing… Me: [incredulous eyebrows] Alex: …like Sudoku! Soothing is not the first word I would use to describe my programming language skills acquisition experience. I realize that much of the problem has to do with my schedule: trying to work my weekly Praxis hours into an even spread has proven all but impossible thus far. I haven’t given up on trying to resolve this problem, but because I’m “still in coursework” (akin to “still in diapers,” I think?) as well as a first-time TA, I often find that the weekly round of course reading, discussion section prep, and meeting scheduling pushes Praxis from the top third of my to-do list until after the Tuesday-through-Friday crunch. Like Alex, I am convinced that learning how to use Ruby is very much like learning any other language—but when I was learning French I had class (and thus practiced) every day. Such is not the case for me and Ruby at this point in time. And I’m feeling guilty. I should reiterate that I do take full responsibility for my Ruby angst. The hours I spend with colleagues and SLab folks every week are always productive, and the direct instruction we receive at our weekly Praxis meetings has been well-structured and richly informative. But I will say that in spite of these advantages it has still been just plain difficult to make enough time to do this job as well as I would like to do it. Both Alex and Lindsay bring up the separation of Academy from Other this week in their posts, and I suppose I feel as though the middle ground between these two groups is at times the most difficult to navigate. I am proud to be part of the Praxis Program; I also feel honored that I am being allowed to earn my literature degree at this prestigious institution. I just hope that I can remember why the second honor should not be incompatible with the first; that I can find enough courage to spend one extra hour per day pursing knowledge which I’m convinced is at least equally important to my scholarly and intellectual development as are my more traditional studies. It’s always difficult to step away from the volume of poetry; by the end of my morning session in the SLab today I found it really was equally difficult to break off my programming exercises practice, though I still don’t think I’ve become proficient enough to feel soothed by Ruby. Provoked is more like it. But then I enjoy a little bit of confrontation. So here’s to making time! Cheers to the newly bilingual."},{"id":"2011-09-27-the-joys-of-ruby","title":"The Joys of Ruby","author":"annie-swafford","date":"2011-09-27 08:00:21 -0400","categories":["Grad Student Research"],"url":"the-joys-of-ruby","layout":"post","content":"It’s official: I think I might like Ruby.  Granted, I think I still slightly prefer Python, but I’m reaching the point where Ruby syntax seems to make sense and I understand methods, variables, the different types of loops, conditionals, and iterators, and I’m ready to learn more about classes, attributes, and instance variables.  The exercises assigned for this week took substantially less time than those from the week before, and I hope that this is a sign that my dream of coding proficiency from my blog post of two weeks ago will one day come true. I’m hesitant to completely proclaim my progress, however, since the exercises for this week told us what elements we needed to use (ie. write a method using a while loop), and I’m much better at following directions than I am at figuring out the directions myself (ie. discovering when and why I would need to use a while loop).  However, I suppose that knowing how to use all the building blocks of programming is half the battle, and figuring out exactly when to use them will become easier with time. Now that we know the basics, I’m looking forward to seeing how we will move from defining short, straightforward methods to building a digital tool, since it still seems like we’re a long way away.  I’m sure learning Ruby of Rails will help with the process."},{"id":"2011-09-27-waxing-metaphorical-with-ruby","title":"Waxing metaphorical with Ruby","author":"brooke-lestock","date":"2011-09-27 09:55:44 -0400","categories":["Grad Student Research"],"url":"waxing-metaphorical-with-ruby","layout":"post","content":"I must preface this post with a few disclaimers: First, The Wizard of Oz  is my all-time favorite movie. Second, I am an English graduate student, so it’s in my nature to wax metaphorical. And last, I’m currently in a Ruby-induced fever which has severely limited my ability to think/write clearly, so this post will be one of my more ridiculous. (I actually dreamed in code last night.) Disclaimers made, Bethany threw out a metaphor that I can’t get out of my head, though it was probably intended as just a witty, catchy title: She named her blog post, “ ruby slippers” (my emphasis). Because I’ve seen The Wizard of Oz  at least 100 times and Ruby is still an alien language to me, I grasped at the ruby slippers as a familiar way to allegorize my experience learning Ruby. Brilliant, I know, but bear with me here: The Ruby slippers are magical and powerful. You have to ride a twister over the rainbow and drop a house on a witch to get them on your feet. Then you’ll spend roughly an hour battling a witch to keep them on. Now, rather than twister-hopping and murdering someone, I joined the Praxis Program, but that move for me could easily be likened to riding a DH tornado to the other side of the academic rainbow. The Scholars’ Lab is Munchkinland, which I guess would make the staff Munchkins (but of average height, superior intelligence, and much less prone to musical outbursts), and Bethany is (of course) Glinda the Good Witch. I’ve bridged the imaginary academic divide, dropped a house on the Wicked Witch of the East, and Bethany and the SL staff have convinced me to keep the Ruby slippers on my feet rather than frustratedly discard them and run screaming. But now I have to battle the Wicked Witch of the West and her flying monkeys of programming frustration. The shoes don’t fit and I’m not quite comfortable on the other side of the rainbow yet, but they’re on my feet, I’m on the yellow brick road, and I have a goal: Prism. And even if Prism turns out to be just an old man behind a curtain (it won’t), it will never disappoint because the journey is significant in itself. BUT: The Ruby slippers are more complicated than they seem. Wearing them isn’t sufficient; they come with instructions that are neither obvious nor intuitive. For Dorothy, the method for using the ruby slippers’ magic is to tap her feet together three times while repeating, “There’s no place like home.” If only it were that simple. For me to master my Ruby slippers, I need to resurrect long-dead mathematical and practical/logical thinking, then pair that with a brand-spanking-new vocabulary of strings, loops, and methods that currently makes very little sense to me. I know Ruby is magical and powerful, but I don’t think I’ll understand that power or my own potential for harnessing it until I find my way to the Prism “Wizard,” when I’ll move out of theory and into (pun intended) praxis. But from what I’ve heard, we’ll be off to see the Wizard soon enough."},{"id":"2011-09-30-fall-2011-workshop-series","title":"Fall 2011 Workshop Series","author":"ronda-grizzle","date":"2011-09-30 09:35:50 -0400","categories":["Announcements"],"url":"fall-2011-workshop-series","layout":"post","content":"We’d like to invite you to our workshop series this fall. Chris Gist and Kelly Johnston have created eight GIS workshops designed to take attendees from learning to create their first map using ArcGIS10 to mapping the world with Open Street Map. The series starts Tuesday, October 11, and will run weekly through November 29. Download the complete schedule of GIS workshops (PDF) here . Nancy Kechner and Kathy Gerber of UVa ITS and the Scholars’ Lab are offering six workshops introducing statistical software packages and data visualization. The series started on September 28, and runs through Wednesday, November 9. Download the complete schedule of software workshops (PDF) here . All Scholars’ Lab workshops are free, open to all, have no prerequisites, and no registration is required. We hope you’ll join us!"},{"id":"2011-10-02-elotroalex-re-mixed","title":"elotroalex re-mixed","author":"alex-gil","date":"2011-10-02 13:08:12 -0400","categories":["Grad Student Research"],"url":"elotroalex-re-mixed","layout":"post","content":"[We recently decided to get to know each other by way of the blog. There is something really odd about auto-bio pieces.  They only have meaning at the beginning of a relationship. In the spirit of the conversation starter, I thought I’d play with the genre. If I am the subject, here are my conversations.] I am a mix of places: Charlottesville, Miami, Paris, Santo Domingo, Beirut, Amsterdam, Valencia. I am a mix of places: The library, the department, the street, the salon. I am a heteroglossic fantasia in 7 languages and 7x7 idioms. Everywhere I am, I am other, el otro Alex. No kidding. I study Aimé Césaire. In particular, his drama, “And the Dogs Were Silent.” The plot: A man rebels against everything and everyone, radically, uncompromisingly. Simple. The textual history: The author rebels against himself, radically, uncompromisingly. Not so simple. I study machine languages. Ruby, now. I am searching for the boundary between humans and machines. It is not where most people think it is. I study texts. They are strange machines, simultaneously covered and exposed. They are patient teasers. Before I had mechanical questions, I read an inordinate amount of texts. I remember most of them. In my thesis I argue that material conditions of production over-determine texts. I am not the first to argue this, nor the last. My research on Césaire is original. I court three professional camps: Digital Humanities, Caribbean Studies and Textual Studies. I am about to court a fourth: Library Studies. I wish I had time to court Continental Philosophy and be a local community organizer. This year I am a fellow at the Scholars’ Lab, HASTAC and NINES. I publish in different places. I publish in print and on the web. I gave away my copyright once. That was a mistake. Now I give away my scholarship and ideas instead. I am one of the editors of the Planète Libre critical/genetic edition of the complete works of Aimé Césaire (2013). I am the sole editor of the digital version. On a personal note: I am a father of two clever sons. I am cleverly married.  I have a large family. They are a mix of places too."},{"id":"2011-10-03-spatial-humanities-step-by-step-mapping-wikileaks-using-google-fusion-tables","title":"Spatial Humanities Step By Step - Mapping Wikileaks using Google Fusion Tables","author":"kelly-johnston","date":"2011-10-03 10:03:36 -0400","categories":["Geospatial and Temporal","Visualization and Data Mining"],"url":"spatial-humanities-step-by-step-mapping-wikileaks-using-google-fusion-tables","layout":"post","content":"Are you ready to participate in the Golden Age of online mapping? June, 2009 - Google Launches Fusion Tables, an online tool for mapping places July, 2010 - Wikileaks Publishes the Afghan War Diary, a massive dataset full of place names April, 2011 - Scholars’ Lab Launches “Spatial Humanities Step By Step”,  a source for peer-reviewed geo-tutorials October, 2011 - Devin Becker, University of Idaho Digital Initiatives Librarian, publishes Step By Step tutorials on mapping Wikileaks using Google Fusion Tables So many new mapping tools.  So many online data sources.  So much interest in the Spatial Turn across disciplines.   Where to begin? Participants in the NEH funded Institute For Enabling Geospatial Scholarship made it clear they wanted a reliable source of helpsheets and tutorials for working with data sources and mapping tools.  Spatial Humanities Step By Step is that growing resource. We created Step By Step to help aggregate high-quality (yet simple-to-follow) tutorials and provide an opportunity for folks to receive professional acknowledgment for the work that goes into creating them. In addition to Devin Becker’s newly published contribution on Mapping Wikileaks with Google Fusion Tables, we have in the pipeline tutorials on extracting and quantifying information from historic maps, using Google Earth as a gazetteer, and calculating least cost paths across a landscape.   Already posted are helpsheets for georeferencing historic maps, converting addresses to mapped locations through geocoding, mapping Global Positioning System datasets, and using Google Maps to create a HyperCities project. We believe folks who invest the time to write easy-to-follow tutorials should receive credit for their work.  Is that you?  Then submit your own work to our supportive review board: Patrick Florance-Tufts University Chris Gist-University of Virginia Tracey Hughes-University of California, San Diego Kelly Johnston-University of Virginia, Editor Scott Nesbit-University of Richmond Bethany Nowviskie-University of Virginia Diana Stuart Sinton-University of Redlands Ginny White-University of Oregon Our simple process: two reviewers take each submission for a spin to confirm the process is easy to follow, jargon free,  and just works.  Then with consensus, we publish.  If you’d like to volunteer to be called upon as a peer reviewer alongside our current review board please contact me . I serve as the Step by Step editor and along with my colleagues on the review board we look forward to your contributions.  Check the site for more information: http://spatial.scholarslab.org/step-by-step"},{"id":"2011-10-04-what-ive-learned-from-my-kindle-part-i","title":"What I've learned from my Kindle: part I","author":"sarah-storti","date":"2011-10-04 08:35:04 -0400","categories":["Grad Student Research"],"url":"what-ive-learned-from-my-kindle-part-i","layout":"post","content":"Though I believe the idea is to fill the “getting to know you” blog spot in turns one week at a time, events conspired this week to set me up perfectly for part one of a relatively brief expostulation on Some Things I Hold Dear as a Scholar in This Age of Digital Texts, a topic I’ve been reflecting on almost nonstop ever since we learned about the basic idea behind what will become Prism. Like Alex, and as I’ve acknowledged previously, much of the work I do as a scholar is centered in textual studies. I frequently find myself drawn to problems that concern the production, dissemination, transmission and translation of texts. Partly for this reason, and perhaps perversely, I have always had a very difficult time imagining myself happily using an e-reader for scholarly work (or, to be honest, even for leisure reading). However, this aversion to e-readers, which is not entirely unfounded as far as textual concerns go, caused me distress when I considered that though I do enjoy studying physical books and printed texts, I also frequently profess to be a member of what Alex calls the Digital Humanities “camp.” I had always felt that these two areas of interest mesh quite well and are complementary. But how pro-digital could I really be, I used to wonder, if I couldn’t even bring myself to come within five feet of a Kindle or a Nook? The e-reader problem went unaddressed until I graduated from UVa with my M.A. last spring. I opened the door of my apartment one day to find an unexpected box from Amazon sitting innocently on the front step. Inside the box was a very thoughtful and generous graduation gift from my aunt and uncle: a Kindle. Magnificent! I thought. Now I don’t have to pay for an e-reader, and I can finally rid myself of the guilt I’ve been feeling about assiduously avoiding contact with any and all e-reader devices since they first appeared on the market. Of course, I decided that I would start out on the Kindle with something unrelated to what I studied. Some light summer reading, perhaps. I had a number of international flights booked in the upcoming months. Perfect. To make a long story short, I did not take the Kindle with me to British Columbia, Greece, or London. It disappeared during the move into my new apartment and only recently, with a pang of remorse, did I discover it again. I determined, however, that this time I would actually read something on it before the week was out.  Quite happily, a favorite reading group of mine provided the perfect opportunity. At our meeting last Friday we decided that our next text for discussion would be John Henry Newman’s novel Loss and Gain. Copies seemed scarce, I needed one quickly, and I refused to pay for print-on-demand. Enter: the Kindle option . I was ready. I was even excited. I “purchased” my copy for free, and it instantly appeared on my Kindle via Amazon “Whispernet.” It was magical. But would the reading process prove to be equally satisfactory? The saga continues next week… (and will conclude with an explanation about why all of this matters in a Praxis/Prism context!)"},{"id":"2011-10-07-ada-lovelace-day-2011","title":"Ada Lovelace Day 2011","author":"bethany-nowviskie","date":"2011-10-07 10:59:43 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"ada-lovelace-day-2011","layout":"post","content":"On this Ada Lovelace Day, I’m looking forward and back.  Here’s my full post in honor of humanities computing pioneer Susan Hockey (where you can also find links to past years’ posts on Johanna Drucker, Bess Sadler, and Leah Buechley). But I’m also spending today feeling appreciative of the a fantastic group of young women – emerging humanities and social science scholars, technologists, and cultural heritage or scholarly communications workers – with whom we’ve been privileged to collaborate in the SLab. So here’s a little post in honor of our grad school gals: Scholars’ Lab Graduate Fellows in Digital Humanities and Praxis Fellows past and present. Jean Bauer\nBeth Bollwerk\nAbby Holeman\nDr. Wendy Hsu\nBrooke Lestock\nRandi Lewis\nLindsay O’Connor\nSarah Storti\nAnnie Swafford\nand Dana Wheeles Thanks for inspiring us all!"},{"id":"2011-10-11-riding-the-rails-and-learning-not-to-fall-off","title":"Riding the Rails (And Learning Not to Fall Off)","author":"annie-swafford","date":"2011-10-11 09:58:49 -0400","categories":["Grad Student Research"],"url":"riding-the-rails-and-learning-not-to-fall-off","layout":"post","content":"It’s hard to believe that only a few months ago, I thought the idea of using a web framework to generate a website seemed like taking the easy way out.  Although I’d heard of Django and Rails, I didn’t really see the point of them.  Apparently I had a lot to learn.  Last week was our first experience of Rails, and I think I’m being converted.  It’s nice to type a few lines of code into the terminal and to have a barebones working framework for a site. And although the syntax initially seemed as clear as gibberish incantations, I’m starting to see how to have it build what I want it to, even without the scaffolding.  I’m finally understanding MVC architecture patterns, and I think that many features of Rails, including partials, will be incredibly helpful.  I’m even thinking about rebuilding my digital project ( I blogged about it a few weeks ago ) in Rails.  I think that it might still take awhile before I feel comfortable creating my own site in it without step by step instructions, but I’m looking forward to trying it.  Maybe next time I’ll blog about transitioning my project from its current framework (html5, css, and javascript) to Rails!"},{"id":"2011-10-11-towards-a-geo-textual-humanities","title":"towards a geo-textual humanities","author":"alex-gil","date":"2011-10-11 06:32:28 -0400","categories":["Grad Student Research"],"url":"towards-a-geo-textual-humanities","layout":"post","content":"Maps are texts, and texts are maps. At the beginning of the movie The English Patient, as Márta Sebestyén’s “Szerelem, szerelem” overcomes our senses, a paintbrush traces the figure of human swimmers on a yellowing page. The black-ink soon gives way to a skin-colored desert landscape sifting beneath our aerial view, evoking hands moving over human curves. Skin, page, territory all united by the theme of lost love. I can’t think of a better image to describe how we are wedded to the n-dimensions  of the textual condition. As we get ready to think about design I wanted to outline a few of the ways we can abstract the material reality of print to a totality of 1’s and 0’s. In my own work I have been trying to create a digital edition of Aimé Césaire’s Et les chiens se taisaient that is both pleasant to read and that allows for some algorithmic manipulation of the textual territory. My goals lead me to seek the chimera of html forgeries  as opposed to the classic images with texts beneath them. The experience taught me an enormous deal about the process of remediation. There are many ways we can remap texts online. We can have a simple image. We can have text behind that image, like your typical PDF. We can map out the position of text and white space on that image by overlaying a basic Cartesian x and y grid on top (or is it below?). We can name areas on that grid like land-grabbers use contracts to justify their fences. We can query  the areas, we can query the points, we can query the text. We can overlap those areas, like the map of Aztlán tensely overlaps with the map of the United States, like our Prism diffracts difference. We can create replicas from scratch using HTML, using Canvas, and trade grain for the possibility of playful deformation and a digital audience born into cool media. We can standardize our geo-textual mark-up, make a TEI out of HTML/CSS, opening the door for large scale analysis  of page design in book-history. Heck, we can just put our UTF-8 txt’s out there and just sit back and wait for our computer overlords to tell us that the eternal present of spotless text was all we ever needed. Lord knows, most literary scholars haven’t done better than that. (I will rebel against that last possibility the way I rebel against propaganda, the way I rebel against the early Wittgenstein, who wanted to get rid of love because we couldn’t fit it in just one map). Let us move towards a geo-textual humanities conscious there are swimmers in the desert of the page. (to be continued…)"},{"id":"2011-10-11-wayne-graham-leader-of-lemmings","title":"Wayne Graham: Leader of Lemmings","author":"ed-triplett","date":"2011-10-11 09:55:16 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"wayne-graham-leader-of-lemmings","layout":"post","content":"Last week from 2-4pm we jumped into a new field with the assistance of Wayne Graham. The same Wayne that has designed many of our exercises was stuffed into a corduroy jacket with leather elbow-patches and asked to do something very difficult: teach the Praxis group how to implement Rails. I want to take this opportunity to describe the challenges set before an instructor attempting to get our group up to speed. First, lets set up the immediate context before our last lesson. The time designated to teach something new is two hours per week. Building up what Wayne and others have called our “muscle memories” takes place during the hours before our next meeting, usually on Monday and Tuesday. As Praxis fellows come and go during the course of their other responsibilities, we eventually arrive at a semblance of even proficiency on the previous lesson about 15 minutes before we begin a new set of challenges at 4:00. Now eleven of us are in a room staring at Wayne’s first slide like it’s an RCA television. Professional DH programmers, researchers and developers, English graduate students and a token architectural historian each plug in their laptops and gear up for a lesson that is new for at least nine of us. Then the fun begins for Wayne. Roughly every seven words of Wayne’s lesson he has to stop for a question. Twice per slide, Wayne must ask “Is everyone here?” while holding up a hand to a line of code. The answer is very rarely affirmative.\nBeginning with the second slide, it becomes clear that the PC users, Mac users and Unix user have unique challenges that hamstring attempts to keep us all going at the same pace. As I sit on my Toshiba netbook, I have to admit this is not the best tool for the job. Most of us will be working with at least VIM and Git Bash open while programming in Ruby on Rails. Add to this several essential tabs open in a web browser to test our changes to the code online. For us PC users, we also had to have a command prompt open because there are a host of permission and directory issues that came up during this process. While performing all of these tasks on a netbook, I am reminded of these weirdos that attempt to write the declaration of independence on a grain of rice, or paint the Sistine chapel on a post-it note. So while Wayne is walking us through the process of installing rails, setting up our servers, making changes in VIM, committing changes in GIT, and testing the changes in our browser, some are following like dutiful lemmings. We happily jump off the cliff thinking “Wayne is our leader, he obviously wants us at the bottom of this gorge for a reason… weeee!” Others of us feel like we wore the wrong shoes and become jealous of the other lemmings that seem better suited for the jump. Still others want to know where we are now, and how we define “cliff.” At various times, some of us are distractedly wondering if we left the server on. While Lemming-leader Wayne is herding these unruly lemmings, the remaining lemmings just keep trying to touch the third rail with our tongues. I don’t have to tell our group that this is difficult stuff to learn. We should also understand that it is equally difficult to convince a group of naturally inquisitive people to have the patience to “wait and see.” Hopefully we can make the next lesson a little easier on Wayne and ourselves by doing some preemptive strikes on our laptops before we start. Finally, we will have a lot better idea of what cliff diving feels like when we are in mid air, rather than asking Wayne all about it back in lemming-town."},{"id":"2011-10-12-tim-powell-revitalizing-jeffersons-vision-for-preserving-native-american-languages","title":"Tim Powell, Revitalizing Jefferson's Vision for Preserving Native American Languages","author":"ronda-grizzle","date":"2011-10-12 08:35:34 -0400","categories":["Podcasts"],"url":"tim-powell-revitalizing-jeffersons-vision-for-preserving-native-american-languages","layout":"post","content":"Revitalizing Thomas Jefferson’s Vision for Preserving Native American Languages On September 28th, the Scholars’ Lab welcomed Tim Powell, Director of Native American Projects at the American Philosophical Society where he oversees the Native American Endangered Languages Digital Archive. Dr. Powell is also a Senior Lecturer in the Department of Religious Studies at the University of Pennsylvania, and a Consulting Scholar for the Penn Museum. He has worked closely for the last ten years with the Eastern Band of the Cherokee and Ojibwe Bands in northern Minnesota. He has won three NEH grants to create Gibagadinamaagoom: An Ojibwe Digital Archive . Talk Summary:\nThomas Jefferson began the Native American language preservation project while President of the American Philosophical Society (APS) from 1797-1815. The APS recently received two Mellon Foundation grants to digitize its entire Native American audio recordings collection, totaling more than 3000 hours. The collection contains invaluable linguistic and historical recordings that scholars and Native American communities are using to bring languages back from extinction. The project also raises important questions about the meaning of Digital Humanities in august archives. In his talk, Tim introduced us to these efforts and discussed how cross-institutional work in the world of cultural heritage organizations might serve as a model for academic digital humanities writ large. As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.10550711093/enclosure.mp3”]"},{"id":"2011-10-17-in-the-neutral-ground","title":"In the Neutral Ground","author":"lindsay-oconnor","date":"2011-10-17 18:35:19 -0400","categories":["Grad Student Research"],"url":"in-the-neutral-ground","layout":"post","content":"With all the coverage of Occupy Wall Street, I’m hearing a lot about urban space and how we live in it. When space arises as a site or source or critique, I always wonder about the ways DH could help analyze the spatial and make spatial thinking more available to humanities scholars, but it also makes me think about the spaces of academic work itself. I’ll leave the digital question until I have a little more technical skill to bring to it, and for now I’ll try to think through the space of collaboration. I want to do that through a story about another interesting experience of public space that seems just as relevant as the protests. I guess this is a “getting to know you” autobiographical blog post as well. I was recently in New Orleans for UVa’s fall break, and I participated in my first proper second line. My teacher and friend Joel Dinerstein is a member of the Prince of Wales Social Aid and Pleasure Club, and he proved a great tour guide for a surprised and amazed grad student who has been in prim and proper Charlottesville for too long. As Joel will tell you, a second line is no quaint neighborhood shindig; it instead involves dancing-walking-grooving through a winding 5 mile route that crosses racial and socioeconomic boundaries. When you’re with an American Studies mentor, it also involves waxing ethnographic about African American working class culture and marveling at how this experience can only be lived, never represented.  Buying bottles of beer from people wheeling coolers through the street helps with both the dancing and the analyzing, and there’s barbecue on truckbed grills if you get hungry. I danced and clapped my way down Magazine St., stopped streetcars full of amazed tourists on St. Charles, and ducked into the Sandpiper Lounge for a beer. Around the corner on LaSalle St., the Dew Drop Inn sign remains across the street from where the iconic Magnolia projects have become a mixed-income housing development featuring cookie-cutter townhomes and treeless courtyards.  This is central city, a neighborhood that white folks generally avoid and that few tourists know exists. I had driven through this area countless times before, but it’s a place you pass through on the way to somewhere else, not a destination or a place to linger. But last weekend I found myself buying lemon cake from The Pie Lady in the middle of Louisiana Avenue, meeting the King of the second line while we jostled toward the bar for a beer, and chatting with the hodgepodge of Tulane (my alma mater) English faculty, post-docs, and alumni who all came out for the afternoon. In New Orleans, roads don’t have medians; they have the neutral ground. There’s interesting geographic history to this term that work like Rich Campanella’s makes accessible, and just the name itself sheds some light on what makes this place so special. The Second Line made me slow down, made me stroll or dance through places I had never noticed as places. Dancing is a way to move through urban space that I had never quite thought about before. If you’re doing it right, it makes you more aware of your body even as you stop worrying about exactly what your body is doing, and it makes you more aware of the place where you are and where you’re going as you negotiate everything from potholes in the road to your previous abstract ideas about the space you’re in. It’s inspiring and it’s reparative—second lines derive from jazz funeral traditions that turn loss into celebration—and it’s making meaning out of music and dance.  Now I understand the metaphor of a political “movement,” and I hadn’t even noticed that it was a metaphor before. This second line brought people together who wouldn’t normally be together. Identity categories and the implicit rules that separate us are transgressed just like neighborhood boundaries and traffic laws, if only temporarily, and there was also me with all these Tulane English folks past and present.  I was less self-conscious than I’ve ever been at a faculty-student social event; my “elevator story” wasn’t a source of tongue-tying anxiety, and I felt OK offering half-baked ideas about literature and culture and second lining. As I ate red beans and rice and sweet potato pie, I felt that we were all equal.  This is something I rarely feel as a graduate student, and I’m sure that’s my fault as much as anyone’s, but it seems at least partially related to the spaces in which we live and work. In New Orleans, I felt a spirit of collaboration I didn’t know I had been looking for. But I think maybe you can’t actually look for it; it’s improvised, and that might be where some of the frustration of my previous blog posts came from. The planned Praxis collaboration seemed less organic that I would have hoped. But I’ve been inspired by recent serendipity and am looking forward to working with the Praxis team even more. I hope we can remember the “healthy dose of play” that we mention in our charter and make Praxis even more of a space of encouraging and productive boundary-crossing. On NPR’s Studio 360 this weekend, architecture critic Michael Kimmelman pointed to Aristotle’s Politics for evidence of what’s exciting about Occupy Wall Street: “[Aristotle] talks about a polis shaped by the distance of a herald’s cry — meaning a civic space, a city functioning in which people have the ability to meet face to face to speak with each other.” He goes on to point out how Zuccotti park has been transformed from a place people hurry through to a place where people linger and connect with one another. This seems to me like one of Occupy Wall Street’s most exciting accomplishments, but that’s just any Sunday afternoon in New Orleans."},{"id":"2011-10-17-praxis-program-charter-available","title":"Praxis Program Charter Available","author":"jeremy-boggs","date":"2011-10-17 06:51:45 -0400","categories":["Grad Student Research"],"url":"praxis-program-charter-available","layout":"post","content":"After a bit of time discussing and editing, we’re happy to announce that the project charter for the 2011–2012 Praxis Program is available on our site . The charter outlines the team’s broader goals for Prism and the program, as well as our approaches and responsibilities for achieving those goals collaboratively. Feel free to read it, and let us know what you think!"},{"id":"2011-10-18-charter-and-design","title":"Charter and Design","author":"annie-swafford","date":"2011-10-18 08:48:49 -0400","categories":["Grad Student Research"],"url":"charter-and-design","layout":"post","content":"This has been an eventful week in Praxis, since we both finished our charter and started trying to design Prism.  The charter took longer than anticipated to produce, primarily because we had some difficulty figuring out when we needed to be incredibly specific and cover all possible scenarios, and when we needed to be very general and deal with problems in a case by case basis.  For example, we eventually decided that we didn’t need to include a policy covering credit for departing members because we thought our general statement about credit would cover it.  We also had a tough time deciding on the tone for the document; we didn’t want to sound paranoid about potential problems by assuming that we would have conflicts, but we also didn’t want to sound too naïve or optimistic and assume that it would all be smooth sailing.  Eventually we decided on language that seemed suitably neutral.  Additionally, we had tried to use language that covered all our job titles (graduate students, faculty, and staff) in an attempt to be as inclusive and respecting of difference as possible, but we decided that we wanted to emphasize our unity above all, since the charter applies to the whole team, so we made such to always say “all Praxis team members” instead.  It was an excellent exercise in team building as well as project management. Our attempts to start designing Prism have been both rewarding and frustrating for me.  It’s rewarding because we have started discussing features and the philosophy behind the possible visualizations for Prism: today in our morning meeting we talked about how to visualize overlap, whether a user can mark a word with two colors, and whether the visualization should be tied to an image of the text, or whether it should be colored lines by themselves without the text.  The frustration for me comes from my own negligible artistic skill; although I’m intimately involved in the other arts, I have never been adept at drawing.  Consequently, I end up having to describe the image of the project I have in my mind with words instead of letting a pencil do the explaining.  Now more than ever I am reminded of one of our charter’s goals: “We aim to recognize, respect, celebrate, and leverage the differences in our intellectual convictions, our academic backgrounds and experiences, and our talents and skills.”"},{"id":"2011-10-18-design-to-play","title":"design to play","author":"alex-gil","date":"2011-10-17 20:08:05 -0400","categories":["Grad Student Research"],"url":"design-to-play","layout":"post","content":"Full disclosure: I have not used coloring pencils in over a decade… that is, until we got down and dirty to brainstorm some possible designs for Prism last Tuesday. Word has it, there will be some transparencies and markers tomorrow. Funny thing is my first salaried job was as a caricaturist for the newspaper El Caribe in Dominican Republic. I remember sitting down with coloring pencils every night and cranking out a few designs before the 2am deadline. That was around the time when I was supposed to be an architect. El Caribe paid for my full tuition to go to architecture school. I know. Why did I drop? Short answer: I didn’t want to draw for the rest of my life. Those pencils felt like old friends in my hands last Tuesday. Full disclosure: I don’t highlight books. I use light pencil markings. Somebody must have instilled the fear of the biblio-gods in me early on because I even feel bad when I see someone else do it. I started highlighting for the first time with computers. Now there’s a place where I don’t feel like I’m defiling anything by coloring it up (I use Diigo, Word and Adobe Pro nowadays). Until now, I never stopped to think about the kind of highlighting we do when we highlight web pages, doc’s or pdf’s. I realize right away that it is a very different activity from highlighting paper. For example, when was the last time you could erase highlighting from a printed article? Prism highlighters also seem different than your average random markup. I’m not only talking about the controlled vocabulary. Any organized researcher can device his or hers color system and use the tools available already to give meaning to their markup. What feels different about Prism is that you are being asked to play. Unless it is a reviewer trying to call attention to some text for you, highlighting is a very personal and solitary activity. Prism is nothing of the sort. It is hard to say what the effect would be before actually doing it with the real thing when we roll it out, but I can already tell it will be a new kind of play. In a ludic spirit, I suggest we make the color palette big and bright, almost child-like: four circles below the text. The text should have nothing to the left, nothing to the right. The horizontal space of the text should be clear to keep the peripheral vision in check. When the pointer goes down to dip in the palette, I would like to feel the danger of wet ink over the text. Highlighting and painting combined into one? Me to play!"},{"id":"2011-10-18-highlighting-limitations","title":"Highlighting limitations","author":"brooke-lestock","date":"2011-10-18 12:27:37 -0400","categories":["Grad Student Research"],"url":"highlighting-limitations","layout":"post","content":"In his blog post, Alex mentioned that we’ll soon (I think today) be coming face to face with the Patacritical Demon - that is, we’ll be doing the exercise that awoke the demon - in all of its highlighted transparency glory. I have been desperate to do this with the group since Bethany first mentioned it. Partly for a reason Alex mentions in his post in that I never highlight literature as I read. I don’t necessarily have the bibliographic prejudice against it that Alex does, but highlighting literature goes against all of my instincts. I highlighted in my science, math, and history textbooks because they offered rules, laws, facts, and dates to be memorized. I highlight criticism now because there’s a similar feeling that you’re being offered more straightforward information that you’re not necessarily interpreting. The critic, of course, is interpreting a text and you interpret the critic’s argument to determine its validity or usefulness for your own purposes, but I feel more comfortable highlighting criticism because there’s a sustained, usually (hopefully) unified argument, and thus there’s less of a chance that I will return to it later with a different reading or highlighting for different things. Also, in those cases, I only use one highlighter because I’m not necessarily categorically interpreting the information I’m reading. When I read literature I will obviously mark for certain things, only rather than highlighting, I create headings in a OneNote notebook page and then add quotes and page numbers for passages or lines that I feel the need to make note of or categorize. This way my notes are always in flux: I can add new categories as they strike me as necessary without the fixed mark of what I thought was noteworthy in my first reading, and without the worry of running out of highlighter colors. Prism addresses my highlighting fears in an interesting way. The highlights are fixed and preserved, yet also necessarily in flux because they’re not being evaluated or examined on an individual level (and there’s even the possibility of having users mark texts over time). The highlighter colors are limited, but only so as to allow for maximum “aesthetic provocation,” as Bethany once said, and interpretive possibility on the user end. Even as I write, there’s a conversation happening in the grad lounge about whether or not people would be able to mark a piece of text with multiple highlighters, and how that could be visualized. I think one way of generalizing some of these debates  is a fear of limitation that ignores or undermines nuance, a practice that most humanities scholars are very uncomfortable with, even at the most basic level of highlighting a novel at home with the four or five highlighters that come in the pack. Alex uses light pencil-marks, I use OneNote, whatever the strategy there are many ways of noting  and interpreting the nuance and complexity of a literary text, but what I’m struggling with (and what the group seems to be working towards) is devising a way to productively limit the texts in question, and productively limit Prism itself, and be comfortable with it as a group. We’ll see how it goes, but I’m hopeful."},{"id":"2011-10-18-prism-images-and-binaries","title":"Prism, Images and Binaries","author":"ed-triplett","date":"2011-10-18 18:55:27 -0400","categories":["Digital Humanities","Grad Student Research","Visualization and Data Mining"],"url":"prism-images-and-binaries","layout":"post","content":"Several of us were recently asked to come up with sample texts to use for a simulated Prism experiment. As the token art historian of our group, I volunteered to find an example that included images as well as text. My initial efforts were spent imagining how I would use Prism as a teaching tool in an art history course. I thought that the clear cut nature of prism,  i.e. its requirement that the reader/viewer make a sharp distinction between ideas, would be a great method to teach students about their own preconceptions with regard to art. For example, I am very interested in what a crowd sourced application could tell us about what a group of students believe are the formal qualities that represent “Islamic” or “Christian” art or architecture. Another simple example might be for a group of students to mark up images that appear “Eastern” or “Western,” or more problematically, “Oriental” versus “Occidental.” How would the crowd mark up a series of deliberately multicultural images if a variety of the above terms were offered as markers? There is of course a problem in asking students to apply a binary they may not agree exists. However, would allowing a “combination” marker defeat the purpose of the exercise? In a sense this is a problem that is inherent in any cultural binary, but I couldn’t help wondering how this potentially useful application for Prism might work. In the process of searching for a published example that might be applicable for one of the binaries stated above, I came to the conclusion that Prism might be as useful in a design context as an art-historical one. I looked through Print magazine’s 2010 regional design annual and noticed that the editors’ reasons for selecting the works in each regional collection were rarely specific or clearly observable. In many cases, three or four adjectives were deemed sufficient to loosely hold the collection together. These adjectives, when separated into Prism “markers” seemed to be excellent vehicles to analyze “art speak,” editing, and curatorship. I was then struck by the idea that we could also use Prism to ask a group of students which images in a group seem more “Midwestern” in style versus “Far West,” and “Eastern” by selecting images from across Print magazine’s regional categories. For our exercise, we only had time too work with a single page, and I did not want to black out the studio locations on the captions, so I selected the Midwest section and asked the group to mark the works that exemplify what the editors of Print magazine called “Narrative,” “Organization,” and “Viewer Interraction.” The exercise went well, and without a lot of time to discuss the results here, I will wait until we have scans of our marked up texts and images. I was tempted to have a marker devoted to “Art Speak” but that might be a little too snarky for a marker."},{"id":"2011-10-22-overlapping-anxieties","title":"overlapping anxieties","author":"alex-gil","date":"2011-10-22 11:45:39 -0400","categories":["Grad Student Research"],"url":"overlapping-anxieties","layout":"post","content":"This week has brought back the question of image/text to my thinking with a vengeance. First, as Lindsay points out in her latest blogpost, our group has been occupied with the question of overlapping markup; second, during our weekly meeting Annie asked “When can we start building Prism,” to which Wayne responded, and I paraphrase, “You already have. Next, you need to choose what you will upload to Prism, and learn what it takes to do so”; third, right after our meeting, Lev Manovich presented some of his work with ImagePlot and the question of “what to do with a million images?” In his talk he made what I think is a troubling distinction between image-as-continuous and text-as-categorical; fourth, at the ADE 2011 in Salt Lake City, where I write these lines, the old anxiety of crowds versus scholars crept in (as expected) on a session on the design of scholarly editions online. As a result, I am torn between writing on the question “what is a (digital) text?” or the question of overlapping representation. Since jet-lag prevents me from making a choice, I will try to combine both instead. What is a (digital) text? A text is both matter and language. Leaving Braille and voice aside for now, let us concentrate on the visible side of textual matter. An image is textual in as much as a text can be an image. Something wicked happens to this binary when you move from analog to digital. We are under the illusion that a text becomes ASCII and an image a bitmap. We have naturalized these two formats (there are others) without taking into consideration the fact that a representation of ASCII on the screen is an image, or that the representation of a bitmap is textual. We’re almost confusing the manuscript for the ink on the pen. I say almost because the way we use a digital text can be very different from the way we use analog text. Furthermore, there are different kinds of digital texts, each with different uses. A close-enough HTML replica and a JPEG image of a text, for example, provide similar images and similar texts, but we use them in different ways… and we must make choices. Which brings me to, the first Prism If indeed we want to have a working Prism model by the end of the Spring semester, we might have to separate bitmaps from ASCII. The problem of satisfactorily having an image/text combo that we can prism successfully seems insurmountable at this juncture and what I understand to be current web technologies. I propose we do one textualey side for ASCII (inside HTML) mapping and a imagey side with bitmaps (inside HTML). That would limit our uploads to HTML and CSS for now. Of course, the manipulation of those texts and images is a whole different matter. Nevertheless, if we know we are going to be working with simple HTML, we can start focusing our Ruby and JavaScript learning (self + slab) to work in this environment. If you ask me, I would just start with the ASCII and postpone the bitmaps for later. If we do decide to do that, the question of overlapping colors becomes somewhat of a linear (and well-trodden) problem. I can’t even begin to imagine how you could solve overlapping bitmap data. If all we have to do is 4! colors that would also help. I understand there is a question of intensities when we try the “stacked” visualization. I say chuck it. Let’s start with the side-by-side visualization which to me is more interesting and what’s more important, easier to set up."},{"id":"2011-10-22-the-transparent-crowd","title":"the Transparent Crowd","author":"lindsay-oconnor","date":"2011-10-22 06:45:29 -0400","categories":["Grad Student Research"],"url":"the-transparent-crowd","layout":"post","content":"Ed, Annie, and Brooke have each mentioned our discussion of whether to allow users to mark parts of a text or picture in more than one color. In the grad lounge on Tuesday morning, Ed and I went back and forth about the implications for interpretation and for possible visualizations. He was concerned about preserving and representing the particular instances when a user marks something as more than one color while I suggested that this would not matter nearly as much as the aggregate markings in each color. He drew on the dry erase board. I raised questions about his ideas and then questioned my own questions and probably contradicted myself. We were each of course projecting assumptions about what kinds of texts we will have in Prism, what we might ask people to highlight, and how the resulting interpretation might be used. I don’t think we realized all that, though; in retrospect, I can see how conversations like this one put pressure on the ideas and images of Prism that I’ve been forming since August. The same goes for the transparency exercise (quite a metaphor!) we did in our full group meeting Tuesday afternoon. I hadn’t thought much about interpreting images (Ed’s example) or about highlighter categories created to emphasize overlapping interpretations (Alex’s example). And for the first time, I recognized that Prism could be used to interpret the highlighter categories as much as or even more than the text (“modernism” in Sarah’s example). This exercise made me feel a little foolish for having spent so much time on the issue of overlap without a fuller sense of what kinds of categories and texts might be possible, but it also contributed to my realization that exercises like this are what we need to remind us that we still don’t know what exactly Prism is, and maybe we never will. Fittingly, I think the Fight Club reference finally emerged last week (first rule of Prism…) Talking even hypothetically about multiple markings taught me something about crowdsourcing as well. Preserving the individual instances of multiple highlights does indeed seem like it could serve many intellectual interests, but it does not seem like “crowdsourced interpretation” as I understand it. If we privilege crowdsourcing as the method that Prism makes possible (or at least much, much easier), should we limit the questions researchers can ask Prism to those that a large number of non-individuated responses can answer? When Bethany piled all our transparencies on top of one another, no one’s individual transparency was easy to see. One person’s double-markings were indistinguishable from the markings of one transparency beneath another. Will Prism recreate this exercise on a larger scale, or will it allow for a kind of crowdsourcing that preserves individual interpretation somehow? In my mind, this question parallels the ideas I was struggling with recently about equality and collaboration versus the solitary, masterful scholar. I think I’m coming down on the side of the wisdom of the crowd instead of the nuance of the individual, so maybe we can call that progress."},{"id":"2011-10-24-international-open-access-week-2011","title":"International Open Access Week 2011","author":"ronda-grizzle","date":"2011-10-24 08:28:36 -0400","categories":["Announcements","Digital Humanities"],"url":"international-open-access-week-2011","layout":"post","content":"October 24 - 30 is International Open Access Week 2011. Now in its fifth year, International Open Access Week, sponsored by openaccessweek.org, is dedicated to educating scholars and university administrators about authors’ rights, copyright, and the importance of creating and maintaining free access to scholarship. Openaccessweek.org encourages scholars to allow unfettered access to the products of their scholarship by publishing in open access journals and depositing their pre- and post-prints in open access institutional repositories. Additionally, openaccessweek.org encourages university and departmental administrators to update policies for tenure and promotion so that publication in peer-reviewed, digital, open access journals is given the same weight as publication in traditional print publications. In support of open access here at UVa, the University has implemented an institutional repository called Libra, maintained and administered by the Library, in which faculty members are encouraged to deposit their research. More information about Libra, open access at UVa, and instructions for depositing articles is available from the Frequently Asked Questions page on the Libra website."},{"id":"2011-10-24-what-ive-learned-from-my-kindle-part-ii-and-other-thoughts-on-prism-and-markers","title":"What I've learned from my Kindle: part II, and other thoughts on Prism and markers","author":"sarah-storti","date":"2011-10-24 06:30:39 -0400","categories":["Grad Student Research"],"url":"what-ive-learned-from-my-kindle-part-ii-and-other-thoughts-on-prism-and-markers","layout":"post","content":"A while back I posted what I thought would be the first of two blog contributions about my (relatively) new Kindle—I figured that in the second of these I would address the way the “pages” of a Kindle book work and perhaps offer some thoughts on how I think Prism “pages” (or whatever we’re going to call them) should work. Since then, as will be made clear by the many posts immediately preceding this one, there has been a flurry of conversation about a cluster of topics all related to how the marked-up documents should appear (in theory) in Prism. Some of this began before our meeting last week, but doing the transparency exercise presented us with a whole new set of problems to consider. Though I think a few of my original reflections on the Kindle are still worth posting, I’d also like to (briefly) address some of the bigger questions I’ve had from the beginning of this project about what kind of marking our crowd-sourced crowd will be able to perform on Prismed documents. I left you, dear reader, with quite the cliffhanger in my last post . I had downloaded a free Amazon Kindle edition of Loss and Gain, and was prepared, cup of tea in hand, to devour some John Henry Newman. What happened next? To keep things simple, I’ll just say that I still prefer books, and perhaps even print-on-demand books, to the Kindle. In her comment on my first Kindle post, Brooke mentioned some of the same problems I encountered, the most frustrating being the difficulty not of making comments on the text but of using them later. The general discomfort I felt during our discussion group when I was not able to page through the physical book is, of course, something one must get used to if one wants to use e-texts, but I do second Brooke’s wish for page numbers at the very least (the book I was reading was organized by “locations,” whatever those are. I’m afraid I didn’t do much Kindle research before I began this adventure). But these objections mostly arise from my preference for the physical form of the codex. The codex is an excellent machine, not ever quite satisfactorily reproducible digital form. An e-reader is not, and cannot be, a codex. Of course, Prism is not concerned with the codex, or with written comments. The only way to “comment” on a document in Prism will be to highlight it: to color it. You aren’t allowed to explain yourself. The project wouldn’t be half so exciting (or provocative) without this limitation.  Strangely enough, there is something similar going on in Kindle e-texts.  As I read Loss and Gain, I began noticing that some sections of the text appeared to be underlined, and at the beginning of such sections, a grey subscript note indicated the number of people who had previously highlighted that phrase or sentence or (in some cases) paragraph. I next discovered that through the menu I could easily access what Kindle calls the “popular highlights” in the novel all at once. There are no comments attached to these highlights: you merely know that some anonymous group of thirty-six people all found this or that sentence interesting enough to underline. Perhaps Newman has something to do with it, but I thought the vast majority of these obviously favorite segments would do well on a greeting card: “we must measure people by what they are, and not by what they are not;” “our strength in this world is, to be the subjects of the reason, and our liberty, to be captives of the truth;” “in the choice of friends, chance often does for us as much as the most careful selection could have effected;” etc. In any case, this did get me thinking a bit more about what I’d like to see from Prism. The only way to “mark” a text on the Kindle is to put your cursor in front of a word and then “highlight” until you reach the end of your section-of-interest. I could be wrong, but I had the impression that many on the Praxis team envisioned Prism working in a similar (linear) fashion—that is, before we got out the markers last week. After all, how could one possibly “highlight” something like this and achieve a satisfactory result? Ed’s transparency exercise involved an even more complicated image than the one above. But even documents we consider to be more “text” based, or prose-y (such as pages from novels or poems), do not necessarily call for straightforward, linear highlighting. Consider the following excerpt from William Carlos Williams’s Paterson, which I’ve taken the liberty of marking up with my snipping tool: Alex’s post demonstrates his obviously more sophisticated understanding of the technical limitations we’re up against as we try to build a basic working version of Prism, but I figure that as long as Wayne keeps saying “right now, the sky’s the limit,” it can’t hurt to push things a little bit!"},{"id":"2011-10-26-facing-the-demon","title":"Facing the Demon","author":"brooke-lestock","date":"2011-10-26 17:46:52 -0400","categories":["Grad Student Research"],"url":"facing-the-demon","layout":"post","content":"I mentioned in my last blog post that many of our recent debates have been about how to productively limit Prism, but I don’t think I realized how difficult that would be until I had the transparencies and highlighters in front of me last Tuesday. We started the exercise with Alex’s selection from Plato’s Allegory of the Cave and his parameters were something like, “Mark for allegories of the self, politics, and economics.” I completely blanked. I’ve read the Allegory of the Cave before, but I kept wanting to ask Alex to define the parameters more clearly or asking Bethany what we’re allowed to do, for example: “Can we highlight margins?”; “Can we highlight something twice?”; Or in Ed’s selection of images, “Should we be looking at the captions under the images?” All of these questions either went unanswered as part of the exercise or were promptly answered by a cryptic shrug from Bethany. That’s when I fully understood how frighteningly open Prism is to interpretation, not only the crowdsourced interpretation of texts  uploaded to Prism, but our personal interpretation of what Prism is and should do. I think that’s why I tend to lean towards limiting Prism as much as possible while we’re still defining what it is. The sky is the limit at this stage as we imagine what Prism can become (when it grows up), but we need one clearly defined use of Prism to begin to build it. My technical knowledge is obviously limited, so I keep framing this issue of limitations in a context I can understand, that is, I think of developing Prism as writing a paper. Last week, I turned in a paper on The Waste Land and, to be honest, I was unhappy with what I turned in. I decided to write on The Waste Land for a course primarily because it’s so incredibly open to interpretation. Where I went horribly wrong was in allowing myself to get lost amidst all the interpretive possibilities of the poem and in the seemingly bottomless pit of criticism. I must’ve spent hours just book-hunting here in Alderman library and when I was finished taking notes, I had twenty single-spaced pages for a ten-page, double-spaced paper. I was paralyzed by possibility, and I was focusing on the shortest, most overlooked section of the poem (“Death by Water”). I spent so much time pursuing other critics’ interpretations of the poem that I not only lost my own focus, but I couldn’t bring myself to begin writing  and ended up producing something I didn’t want to own. That being said, I find myself needing to impose limits on Prism so it doesn’t turn into some kind of shape-shifting, waste-landish, baby-faced bat monster (the _Bat_acritical demon, just in time for Halloween). Of course I trust that we wouldn’t allow that to happen, but I just don’t want to spend too much time parsing out Prism in all its possibilities, feeling the need to develop it to allow for anyone to use it any way. As I said in my “Transdisciplinary Ethics” post, the possibilities absolutely must be considered eventually, but we need to start somewhere specific or we’ll get lost in the scope creep. We addressed this issue in the Scholars’ Lab grad lounge today and it resurfaced at our meeting this afternoon, and I think we came to a consensus: start small and generic, then build on that. We’re still figuring out exactly what that means, but it seemed to be that we were headed toward the initial goal of building Prism on the most basic level with one HTML text and one to four highlighters, and defining our audience as college instructors and students. Simple enough, right?"},{"id":"2011-11-01-building-prism-let-there-be-light","title":"Building Prism: Let There Be Light!","author":"annie-swafford","date":"2011-11-01 10:20:16 -0400","categories":["Grad Student Research"],"url":"building-prism-let-there-be-light","layout":"post","content":"On Friday, David, Eric, and I spent some time drafting the data model for Prism.  I had never done anything like this before, so it was quite the experience.  Since I’m new to this sort of work with databases, I had to get over my initial hurdle of thinking about the user interface in order to think about the individual components.  Once we figured out that the component parts were the tags, text, and the image and document classes, we had to figure out how they were related.  Alex did the first pass of constructing the data model in Rails, and today we met to polish it up.  We learned some interesting things about Rails in the process: for example, it seems as though Rails scaffolding sometimes changes the symbol names without warning, but now that we know, we can keep our eyes open for it.  We haven’t yet added anything to our database, but that might be the next step. We’ve been cautioned against getting  too attached to our data model yet as we’ll probably have to change substantial portions of it, but at least we’ve finally started building!"},{"id":"2011-11-02-report-from-the-rails-trenches","title":"report from the rails trenches","author":"alex-gil","date":"2011-11-02 07:39:07 -0400","categories":["Grad Student Research"],"url":"report-from-the-rails-trenches","layout":"post","content":"As Annie reports we have begun hacking Prism. I am still surprised by the speed at which we are picking up the skills to build a web application. Our first model is a proof-of-concept, and as Eric Rochester pointed out in session yesterday, we will probably chuck the first model down the road. Part of re-wiring my brain is adapting to this trial and error model. The original model, as we have it now, already seems to me to be quite powerful. Here is what we have so far: I’m having difficulty imagining why we would need to discard it later, but I am starting to believe there is such a thing as Web Development Fate. In this strange deterministic universe you prepare for unknown unknowns as a matter of course. In the meantime, I will be making these models as I learn Rails. The process goes something like this: Read a chapter in the Rails Tutorial, implement something, read a section of RailsGuides, tweak the implementation, rinse-and-repeat. So far, I have not had a pull request accepted into our central GitHub repository, but if everything goes alright, I will have one by the end of this week. I’ll keep you posted."},{"id":"2011-11-04-keeping-it-real-clean","title":"Keeping it Real… Clean","author":"ed-triplett","date":"2011-11-04 19:00:59 -0400","categories":["Grad Student Research"],"url":"keeping-it-real-clean","layout":"post","content":"Now that we have moved forward with our Rails programming for Prism I have been thinking this week about how our group will collectively decide on an overall aesthetic for Prism. I have been scanning the web and taking screenshots of inspiring designs for a few days as Jeremy suggested, and in the process I noticed very few common principles among them. I collected sites that had elaborate splash pages, some with minimalist styles, a hand-drawn quality, photographic backgrounds, sharp vector graphics, muted palates, and bright primary colors. When I step back and look at what I have collected, it is difficult to come up with a common thread between the pages, much less any sense of my own personal taste. I also tend to browse the web completely differently when I flip the “aesthetic judgment” switch in my head. Content fades away and the pages become more abstract. I expect most of us approached our design research with a number of aesthetic filters in mind to help sift through the mass of possible sources of inspiration, with similar disparate results. I am thus brought back to an idea that Bethany initially brought up a few weeks ago. I think it would be helpful for us to focus on a few adjectives that we think should inspire Prism’s aesthetics. Design is rarely expressed successfully by using language alone, but we do have a common vocabulary that we can use to filter the myriad of design options for Prism. For instance, I am very interested in preserving a “tangible” quality for our texts in Prism. On the simplest level, using “tangible” as an instigator for design could mean that it is important to keep images of the original text that Prism users will highlight. Taken further, a “tangible” aesthetic might mean an emphasis on simulated three-dimensionality, a “grainy” quality to flat surfaces, or even a reaction against the hard lines of vector graphics. One medium that I believe possesses an inherently “tangible” aesthetic is letterpress. The presence of the large metal printing press seems to hover behind the image of the completed print. This is especially apparent given the current trend to deepen the impression of the plate into the paper. http://pinterest.com/ I am eager to see how prism might embrace the sharp, “intangible” quality of the vectorized highlighters while also emphasizing a mechanized, physical style in the application’s frame, logos and scanned texts. Another adjective that is often used as a superlative for good design is “clean.” By itself, the adjective is not particularly useful given that it can connote a utilitarian, functional, uncluttered, sharply-defined, clear, high-contrast or even deliberate design. Nonetheless, “clean” is still one of the most common words used to describe web design that “works” and communicates clearly. It is easy to set up “clean” design as an antithesis to more physical, tangible, or “real” imagery, but I hope we are able to avoid this kind of binary distinction. Keeping it real and keeping it clean are not opposing ideas."},{"id":"2011-11-08-designed-to-touch","title":"Designed to touch","author":"brooke-lestock","date":"2011-11-07 19:43:03 -0500","categories":["Grad Student Research"],"url":"designed-to-touch","layout":"post","content":"I enthusiastically agree with Ed’s use of the word “tangible” to describe Prism. I couldn’t have chosen a better word myself. To get the clearest possible definition of the word, I went old school and looked it up in the Merriam-Webster online dictionary where it’s defined as “perceptible by touch.” No surprise there, but the phrasing of the definition did remind me of the importance of touch to perception. From the origins of the ‘Patacritical Demon in those SpecLab transparency exercises, Prism has been all about touch: touching the transparency to the text, touching the highlighter in your hand, touching the highlighter to the transparency, pressing the transparencies together onto the text and deriving some kind of information from the touching or non-touching of highlights. The physical acts of marking up a text and then seeing many markups laid on top of each other are integral to the user’s perception and interpretation of the text in question. Prism’s ultimate goal of “aesthetic provocation” requires the user’s physical interaction with the text, so Prism can’t be successful (can’t “touch” its users) if it loses its tangibility. Like Ed, I’m not exactly sure what “tangible” as a design aesthetic looks like, but as we’ve been asked to look at CSS design galleries for Prism inspiration, I find myself bookmarking the sites that play with the boundaries of the page and implement texture in an interesting way. I also agree with Ed’s emphasis on “keeping it real…clean,” and I would go further to say that “clean” for me means maintaining the primacy of the text on the page, whether it’s a page image or a text transcribed in HTML. While I’m on the subject, Sarah and I briefly chatted in the grad lounge today about tangibility in another sense. There has been a fair amount of discussion as to whether we should use page images or transcribed text and the general consensus we’ve come to in the last few meetings is that we’ll use both. I think having both options opens up some interesting possibilities for interpretation (allowing for an examination of how users mark the same text in each form, for example), but it seems absolutely crucial to the tangibility of Prism to have page images, however complicated that may turn out to be. In my interview for the Praxis Program this summer, I hopped up on my soapbox and boldly claimed, “The book is dead.” Sheath your swords, bibliographers (looking at you, Sarah and Alex), I didn’t quite mean what I said. I meant that paper books will eventually become artifacts, which isn’t a far-fetched claim to make, but the problem with e-readers, in my opinion, is that they aren’t enough like physical texts. They aren’t tangible in the way that readers are comfortable with. Prism obviously isn’t an e-reader, but my point is that translating the tangibility and physicality of the transparency exercise is a design task that is absolutely crucial to its success. We want Prism to make interpretive possibilities “perceptible by touch.”"},{"id":"2011-11-09-building-prism-the-darker-side-of-the-enlightenment-spectrum","title":"Building Prism: The Darker Side of the Enlightenment Spectrum","author":"annie-swafford","date":"2011-11-09 15:16:46 -0500","categories":["Grad Student Research"],"url":"building-prism-the-darker-side-of-the-enlightenment-spectrum","layout":"post","content":"It’s finally happened—we’ve had our first setback. We knew from the start that we would probably have to scratch much of our data model, but we didn’t entirely understand why. Now we do. Apparently our model doesn’t allow for saving a highlighting session; every new visitor’s data would replace the previous entries. Also, in order to enable any sort of interaction with the text (uploading, highlighting, etc.), we need to create a user model. These are only some of the reasons that our original data model needs to be trashed. I’ve always known that trial and error was an important part of coding, and I’ve had ample experience with this from my own digital project. Even so, I’m still surprised by how many mistakes we made. It’s amazing what you can overlook when you’re just starting out on a project and learning what data models are. It seems as though part of the problem was that we began to code our data model before we had done much wireframing, and therefore our model doesn’t accurately reflect the project as it is being designed. We’re planning to wait on rebuilding the model until after the wireframing portion is complete. Although it’s frustrating to start again, I’m hopeful that we’ll be able to produce our second version more quickly having learned more about the process."},{"id":"2011-11-09-the-dirt-on-clean","title":"The Dirt on \"Clean\"","author":"lindsay-oconnor","date":"2011-11-09 15:15:33 -0500","categories":["Grad Student Research"],"url":"the-dirt-on-clean","layout":"post","content":"Last week, when I should have been finishing up a conference paper I gave on Sunday, I instead kept messing with the webpage that Jeremy is teaching us how to design. Coding left me confused and bewildered, but now I realize that it also pretty much left me cold. I did once announce that it was satisfying, but I think only in a “I’m glad I did that” kind of way. In contrast, the process of designing a webpage is fun. I like thinking about design as I go about my life both on the web and otherwise. I like making my apartment look nice in ways I can afford and maintain. I like creating and cooking meals that balance taste, texture, and color. I like bright and bold and playful fashion statements that don’t entirely forsake comfort. These are some of the things I do while procrastinating, while avoiding my academic work because I’m confused or bewildered, so I think designing for Prism will be similarly appealing and therapeutic for me. As for design values that we should adopt, “clean” seems so common a metaphor that it may not actually carry much weight as a guiding principle. Does anyone aim to make things look “dirty” or “messy” when designing something for many different kinds of users? I don’t find it helpful to take anything as a goal or guideline if its opposite is obviously undesirable. But I also don’t know that we need to abandon “clean” entirely— Ed and Brooke have explained a bit about what “clean” means to them, and I think that’s what we should pin down. Just now Brooke, Jeremy, Wayne, and I briefly discussed how different people use “clean” in design terms. It could mean ample white space or big buffers and borders or not too many fonts or all the important stuff near the top of the page. This is the kind of stuff I hope we can start working out pretty soon, and to get that going, I propose “visually logical” as a guiding principle that is (hopefully) at least slightly more specific than “clean.”"},{"id":"2011-11-09-the-hunchback-of-notre-prism","title":"the hunchback of notre-prism","author":"alex-gil","date":"2011-11-09 15:49:14 -0500","categories":["Grad Student Research"],"url":"the-hunchback-of-notre-prism","layout":"post","content":"I’m with Annie . Trial and error has been part of my dissertation for a while, so there is no shattered heart at the prospect of chucking the first model. I pointed out on my previous post that one thing I’m getting tons of praxis on these days is working with the “unknown unknowns” hovering in the air. Throwing the first fork into the recycle bin doesn’t mean it has lost its use, though. Knowing that the first model was bound to be dropped from the beginning anyway, I’ve used it to teach myself Rails. If the past couple of weeks have been marked by a lack of demonstrable results on the front of the stage, I vouch for the intense hustle and bustle on the backstage. I have defeated Rails for Zombies already, have seen many a YouTube video on Rails while doing chores around the house, and I’m half way through the highly recommended Ruby on Rails Tutorial . Even though I’ve mostly worked with the examples in those tutorials, I’ve kept a dirty fork of prism close at hand to prod and tweak and hit with a sledgehammer. Most tutorials give you very doable exercises in a logical order, which makes it hard to screw them up, but also to remember. Having something to destroy and deform, though, can be the best learning tool sometimes. Of course, the world will never know what I’ve done to prism [macabre music plays in the background], but IT, the crooked, hunchbacked prism lurking in the shadows will probably stay there while the Dorian Gray prism goes on to live a youthful life. While I praise and practice open-access and the public life of the mind in general, I write these few words in honour of the pedagogical freedom of the backstage."},{"id":"2011-11-12-cross-posted-it-starts-on-day-one","title":"cross-posted: It Starts on Day One","author":"bethany-nowviskie","date":"2011-11-12 10:13:48 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"cross-posted-it-starts-on-day-one","layout":"post","content":"(This is a brief essay related to the Scholars’ Lab’s Praxis Program, which has been cross-posted from nowviskie.org .) Here’s a modest proposal for reforming higher education in the humanities and creating a generation of knowledge workers prepared not only to teach, research, and communicate in 21st-century modes, but to govern 21st-century institutions. First, kill all the grad-level methods courses. Kill them, that is, to clear room for something more highly evolved – or simply more fruitful – to take their place.  Think: asteroids clobbering dinosaurs.  Choking weeds ripped from vegetable gardens.  The fuzzy little nothings and spindly cultivars in this scenario, squinting cautious eyes or uncurling new leaves into the light, are: those research methodologies and corpora (often but not exclusively gathered under the banner of the “digital humanities”) that address hitherto unanswerable questions about history, the arts, and the human condition; and the new-model scholarly communications platforms we can already recognize as promising replacements to our slow and moribund systems for credentialing, publishing, and archiving humanities scholarship and the cultural record on which it is based. What do these critters need to grow up? The same thing our colleges and universities so desperately need: a generation of faculty and alternative-academic scholar-practitioners who have been trained to work in interdisciplinary contexts and who can not only take advantage of computational approaches to their own research, but who have been instilled with enough of a can-do, maker’s ethos that they feel empowered to build and re-build the systems in which they and future students will operate. Although a small number of extra-curricular experiments (like the Praxis Program ) and curricular interventions (like Michigan State’s Cultural Heritage Informatics Fieldschool ) offer new and concrete models for emulation, there’s little hope for wholesale, bottom-up, grass-roots reform of methodological training in the humanities. With vanishingly few exceptions, required first-year graduate methods courses are dinosaurs and weeds. Some are an abbreviated introduction to journals databases and the mysteries of inter-library loan. Others have little to do with research and production “methodologies” at all, and are instead a crash course in the jargon and en-vogue theories of a given discipline. The intra-institutional level of coordination in developing and teaching these courses, even among closely-allied humanities departments, hovers around zero.  Within single departments, they are catch-as-catch-can, shaped almost wholly by the individual faculty who teach them (often as they themselves were taught a generation or two before) and sometimes vacillating wildly in content from year to year as instructors rotate to make more equitable the “burden” of a course generally construed as service. Is it any wonder they’re a mess? And is it any wonder that we continue to produce graduate students unready to engage with new technologies and opportunities for interdisciplinary and computational work – baffled and frustrated at the conditions of the academic job market and its underpinnings in a dying scholarly publishing industry – and under-prepared for or uneducated about hybrid and non-traditional academic careers? Here comes the asteroid we require. (And in offering a trajectory for it, I want to acknowledge my debt to conversations with participants in the Scholarly Communication Institutes held at UVa Library, with Scholars’ Lab faculty and staff, and with our Graduate Fellows in Digital Humanities and Praxis Program students.) Funding agencies, both private and public – like Mellon, Sloan, and (in the US) the NEH and NSF – should be approached by a respected humanities organization that itself possesses a mandate and a track record of inter-institutional and interdisciplinary collaboration.  I think here of groups like CHCI, the international Consortium of Humanities Centers and Institutes – especially in partnership with centerNet, its digital counterpart – or the American Council of Learned Societies ( ACLS ). The organization should offer, with sufficient funding, to serve as a broker for a prestigious and competitive RFP (request for proposals). The RFP would would be issued to universities with core strengths in the humanities, adequate support for digital scholarship, and a desire – able to be expressed at the institutional level – to create broad-scale curricular change in the way graduate students are inducted into and trained for 21st-century humanities.  Probably no more than 3 or 4 schools would win funding, which would be contingent on this: the planned, top-down, apocalyptic wiping-out – one academic year from delivery of the award – of existing graduate methods courses in (say) four to six core humanities departments; the formation of a small but representative, collaborative, and interdisciplinary team charged with creating the year-long common methods course that will replace them; a commitment by participating academic departments, in the light of the new common course, to re-think the training that they consider to be absolutely unique to their disciplines and to offer an avenue (1-credit classes? discussion groups? new approaches to departmental teaching or to comps and orals requirements?) for students to acquire it; and a rigorous program proposed for assessing and publicizing the successes, failures, and overall impact of the experiment, so that lessons may be learned across institutions and new programs inspired. The common methods course would be required of all incoming graduate students in participating departments.  Grant funding could could support staffing of curriculum design and assessment phases, offer incentives (including course release or professional development) for faculty participation, or pay for teaching assistants. The program would be designed and team-taught by its planning group, which should include faculty from relevant departments, representatives of the offices of deans and provosts, and – importantly – local #alt-ac professionals, trained in the humanities, but working as scholar-practitioners in R&amp;D; or academic support roles in libraries, labs, publishing units, and centers. It should also engage faculty from departments like CS and Architecture, whose students may not participate directly in the program, but who would have important lessons to share about research methods and collaborative practices. As its primary focus, the course must cover current humanities research skills, corpora, and trends – both digital and archival or material. But it should also address issues like: intellectual property and open access; the intersection of scholarship with the public humanities; publishing, preservation, and scholarly communication; funding and material support for research and teaching; interdisciplinary collaboration; matters of credentialing and assessment (peer review, tenure and promotion), faculty self-governance; and the under-interrogated policies that cover and shape the humanities in the modern college and university. This is a tall order – but we can no longer afford to produce humanities PhDs who have only a foggy notion of how universities work, and how they are impacted by external technological and social forces.  The first time a humanities scholar encounters a budget spreadsheet or performs a calculation should not be when he or she becomes department chair. And no new member of the professoriate should feel utterly out of depth in decision-making processes that impact the teaching, research, and service mission of his or her institution.  Likewise, the health of the humanities depends on our production of graduate students who do not simply replicate the faculty of yesteryear, but who are prepared to take uncharted paths in and around the academy, working together to fashion new systems and adapt the ones we treasure to altered conditions. Graduate training in the humanities starts anew every year, on Day One. How, at a moment when we feel so much is at stake, can we allow it to remain so purposeless?"},{"id":"2011-11-15-the-mappy-goodness-that-is-gis-day-in-the-scholars-lab","title":"The Mappy Goodness that is GIS Day in the Scholars' Lab","author":"kelly-johnston","date":"2011-11-15 07:46:16 -0500","categories":["Announcements","Digital Humanities","Geospatial and Temporal","Grad Student Research","Visualization and Data Mining"],"url":"the-mappy-goodness-that-is-gis-day-in-the-scholars-lab","layout":"post","content":"Every November on the Wednesday of Geography Awareness Week the world celebrates GIS Day.  On that day in Charlottesville the geospatial community gathers in the Scholars’ Lab for mappy goodness. And cake. In 2010 we threw open the Scholars’ Lab doors for folks to present geospatial lightning talks.  We were impressed by the breadth of GIS work ongoing across our community. And lots of people came to hear these mappy stories. And for cake. Face it, you love maps.  We invite you to join our band of Virginia Mapheads as we celebrate World GIS day 2011 with a lightning-fast show of the world’s coolest geowork in the Scholars’ Lab on Wednesday, November 16 at 1:30pm. Sadly, I know many of you don’t have the pleasure of working all day every day with maps and geodata, so treat yourself to a once-a-year map fix.  You know you deserve it!  And bring a friend. Again, we have a compelling lightning talk lineup for 2011. Your reward?  Two slices of our soon to be legendary 2011 GIS Day geocake to be revealed on GIS day! 2011 GIS Day Update Over 70 folks enjoyed our 2011 GIS Day celebration in the Scholars’ Lab. Speakers ranged from wily GIS veterans to those who’d recently started using geospatial tools.   Check the speaker list above to see the wide range of topics. The Charlottesville CBS station sent their GIS Day team to cover the event interviewing Eric Johnson, Scholars’ Lab Head of Outreach and Consulting. And we feasted on the already legendary Virginia-shaped geocake decorated with flags marking unusual place names.  All this was followed by delicious hot mulled cider. Thanks to everyone who played a part in making GIS Day 2011 a mappy success! 2012 GIS Day Update The Scholars’ Lab celebrated GIS Day 2012 with lightning talks on spatial topics, one-of-a-kind GIS Day buttons, cartographic cupcakes, and hot mulled cider. We highlighted our recent work with Do It Yourself Aerial Photography using balloons, kites, and copters. Some folks didn’t want GIS Day to end. Thanks to all the friends of the Scholars’ Lab who made GIS Day 2012 a mappy success! And thanks to Ronda Grizzle of the Scholars’ Lab for producing this 2012 GIS Day lightning talk video. http://vimeo.com/56020168"},{"id":"2011-11-16-the-force-of-ux-design","title":"The Force of UX Design","author":"brooke-lestock","date":"2011-11-16 18:30:26 -0500","categories":["Announcements","Grad Student Research"],"url":"the-force-of-ux-design","layout":"post","content":"I’ve had design on my mind for a couple of weeks now, mainly because thinking about how Prism is going to look is so much more pleasant to me than writing code (may Annie and Alex be praised for taking the lead on that). It’s been easy for me to point to websites and say “I like this,” or “I don’t like that,” but Jeremy wisely outlawed the phrase “I like” from our design meetings, forcing us to exercise the same critical muscles we exercise in graduate study; I would get pelted with rotten vegetables and shunned from academia for life if I turned in a paper that said (or even said in a discussion), “I like Virginia Woolf because her books are good.” Now we’re all thinking critically about why certain sites appeal to us and why other sites make us shudder. Right on cue, the Scholars’ Lab hosted a presentation last Thursday by Joe Gilbert  on the Elements of User Experience design, in which I frantically jotted down every word he said because it directly addressed the design questions we’re learning to ask in Praxis. Joe established four elements that comprise UX design: user requirements, design and architecture, usability, and content strategy. We’ve already established our requirements for Prism, on both the user and organizational levels, and after many weeks I think we all finally agree on what we want it to do for us and the user. Now to design it. After last week’s design meeting, we all agreed that we would have some sketches of the main pages of Prism to share, but once I turned to a fresh sheet on paper in my notebook, I couldn’t sketch more than a monitor-shaped rectangle with the word “Prism” in the top left corner. So I flipped to my notes from Joe’s presentation and remembered a phrase he used when he was stressing the importance of visual hierarchy on the page in presenting the user with “obvious calls-to-action.” Jeremy made a similar point in last week’s meeting when he said that a site should follow a path in its interaction with the user: “Entice –&gt; Inform –&gt; Invoke.” It’s all about narrative: get the users’ attention, tell them how to use it, then (and I keep coming back to Bethany’s phrase) “aeshetically provoke” them. We had a really productive conversation in the grad lounge today in which we proposed and discarded some ideas (much like Annie and Alex, I am learning how to kill my [design] darlings), and we made great headway because we kept reminding ourselves  that we need to narrate Prism to the user in order to meet our requirements. We spent the majority of time talking about the first part of the Prism narrative, how to entice our user in the home page and inform them how to use it. Do we put the sandbox and/or demo video on the home page, emphasizing Prism’s interactivity by placing the user right in the midst of it? Or should we have a simple home page with large links to the sandbox, text editor, and visualizations pages? What about our navigation bar (the chapter titles of our narrative, to keep it literary) -  how do we organize a nav bar to offer “obvious calls-to-action”? We raised more questions than we resolved, as usual, but that’s certainly better than settling on design we “like” without asking how and why it contributes to user experience.  In any event, we’re getting somewhere, and I have to thank design Jedis Joe Gilbert and Jeremy Boggs for teaching us how to use the UX force."},{"id":"2011-11-26-giving-thanks","title":"giving thanks","author":"alex-gil","date":"2011-11-26 06:39:22 -0500","categories":["Grad Student Research"],"url":"giving-thanks","layout":"post","content":"I love lists. Here’s one I’m thankful for: The things I’ve learned since I started Prism (including those I learned indirectly): How to negotiate a common charter. How to navigate, edit in and configure Vim. How to write JavaScript to make a reading interface on the web. How to read Ruby. The key principles of programming: a) Assignments; b) Iteration; c) I/O; d) Conditionals; e) Boolean Operations; and d) Data Structures. The difference between an object and a method. How to feed hungry functions. The three parts of a Rails app: a) Views, b) Controller and c) Model. To keep a portfolio of favorite designs. When a constrained vocabulary meets an unbound vocabulary, magic happens. How to test before you code… and watch Henry at the same time. The licenses for software are different than the licenses for other media (I know, I’m slow). How to use the terminal for almost everything (sorry @sramsay, still like my eye-candy). How to Git with my eyes closed. The basics of GIS (thanks, Kelly!) How to run an Omeka-driven classroom. How to run a LAMP test server on a virtual machine. How it all ties together. How greedy I am for more. Bonus: What I will learn next week: How to interact with an API (In my case, Zotero and Juxta will be my first dates)"},{"id":"2011-11-26-whiteboard-wireframing","title":"Whiteboard Wireframing","author":"ed-triplett","date":"2011-11-26 06:00:33 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"whiteboard-wireframing","layout":"post","content":"Over the past few weeks, I feel our program is moving toward my, and others’ comfort zone. We are beginning to wireframe Prism on the whiteboard, so we can each come back after the Thanksgiving break with a few images of each of our main pages. There are new challenges that come from the wireframing process, but unlike some of our other tasks, the group seems more confident when articulating their thoughts about design. The whiteboard work last week got us to make a few serious choices about what the home page will contain, and how the site will be navigated. Without getting too specific, we have come to some consensus that we’d like to keep a constantly visible set of tabs at the top of each page to organize the site. This is not to say that other design schemas will not be offered in the future, but it does help us decide which elements we want visible on the home page.\nOur Tuesday group leaned toward offering a stylized image of a marked up text on the home page to act as a link to a “sandbox” of Prism. Sarah and I both toyed with the idea of a home page with three objects centered on the page to act as links to either the “about” page, the “sandbox” or the “text markup” page. For me, this kind of a home page was inspired by a site like https://github.com/ (after login) with its four immediate navigation options executed through elaborate icons. The tabbed form is strong in terms of being easily navigable, yet tabs themselves can only communicate so much before becoming messy. In addition, once a user has learned how to use the tool and wants to return to mark up texts, it seems strange to immediately click a small tab – that is exactly the same size as other tabs for the “about” page or the sandbox – to navigate away from the home page. I am going to advocate using both large icons to attract attention to the main pages, and tabs to navigate between them once the user has selected a option in the center of the page.\nAs Brooke remarked in her post, it would be nice to create a narrative for how to use the site, beginning with a textual “how-to” and ending with the user marking up our texts/images… or down the road, their own. I agree that it would be helpful to visually direct the user through Prism. I am going to try to mock up some images for each of the icons before our next meeting, but right now I am not sure what form they will take. My only instinct is to combine some abstract element of the Prism logo with a representation of a text/image, a “how-to” filmstrip or some other element in the “use” narrative… although that might need some more thought. I also took a lot away from Joe Gilbert’s talk at the Scholar’s Lab, and I would like to see us all come up with visual examples of the Prism homepage that will call the user to action, yet offer a familiar method of navigation."},{"id":"2011-11-29-testing-and-more-data-modelling","title":"Testing and More Data Modelling","author":"annie-swafford","date":"2011-11-29 08:01:30 -0500","categories":["Grad Student Research"],"url":"testing-and-more-data-modelling","layout":"post","content":"Now that we’ve spent a few weeks focusing on wireframing, we’re back to working on the data model.  We haven’t actually created the models in rails yet, but we have started redesigning it, and it makes much more sense the second time around.  Currently, we’re adding a user model, renaming all the other models, combining the “TextDoc” and “ImgDoc” models into a new model called “PrismObject,” and adding a “Page” model.  We’ll post our next version here when it’s ready. We’ve also been learning about the wonders of testing.  Of course, we’ve all had our own (frustrating) experiences with debugging, but it looks like Test-Driven-Development will help us minimize our errors, or at least catch them earlier.  We’re now supposed to write the test before we write the code, and the test will tell us whether our code passed or failed.  I think it will take some practice to actually write the tests correctly, but I think it will pay off once I get used to it."},{"id":"2011-12-01-projection-lessons-in-maps","title":"Projection Lessons in Maps","author":"chris-gist","date":"2011-12-01 10:41:26 -0500","categories":["Geospatial and Temporal","Visualization and Data Mining"],"url":"projection-lessons-in-maps","layout":"post","content":"For many years, I have used the following map in my presentations. This map is a great example of proportional symbology and is of an interesting subject, especially when juxtaposed with modern oil trading.  Of course, the cartographic style is great too.  I hadn’t much thought of the cartographer or why the map was created until recently.  When I took a closer look, I found the cartographer’s name, Richard Edes Harrison (1901-1994), and that this was one in a series of maps he did for Fortune Magazine mainly in the World War II era.  He was a wonderful cartographer and his style intrigues me.  There is a large amount of information available about Harrison and his maps. A website showing some of Harrison’s work:  http://www.fulltable.com/vts/f/fortune/reh/mn.htm . (Almost all maps in this post are linked from this site). An article in Imago Mundi about Harrison’s work:  http://www.jstor.org/stable/1151400  (JSTOR access required). A book showing much of Harrison’s work for Fortune: Look at the World, The Fortune Atlas for World Strategy . Harrison employed many interesting techniques. Cartograms More information on cartograms Interesting Extent Overviews Series of Small Multiples A Tufte favorite.   More information on small multiples. Comprehensive Data Visualizations Interesting Projections More information on projections The last two maps have very interesting projection lessons embedded in them. Embedded Projection Descriptions How nice is that?  Having a clear, concise description of projection and scale within the map is a great benefit to the reader. In this map, Harrison used comparative scales to show changes in distance across map due to projection distortion. This one is the best.  A clear description with a visual that anyone can understand. Here is simplified version of the “world centrifuged” in a map called On Assignment . A few weeks ago, I came across the following map. What a beautiful projection!  It is know as the butterfly or Cahill projection (Named after the inventor.  Cahill even got a patent for a rubber ball that does the same thing.)  And as the embedded projection description describes, this is a great map for comparing circumnavigation routes because it does not distort distance, area or direction.  Not only that, it gives the “orange peel” figure to further give readers information on how this projection works. I only learned of the Cahill projection after reading this xkcd cartoon . Call for Help If anyone know of other maps with this sort if imaginative visual explanations of projection embedded in maps, please contact me ."},{"id":"2011-12-02-acceptance-testing-for-omeka-plugins","title":"Acceptance Testing for Omeka Plugins","author":"eric-rochester","date":"2011-12-02 11:40:58 -0500","categories":["Research and Development"],"url":"acceptance-testing-for-omeka-plugins","layout":"post","content":"For the month of December, I’m going to be heads-down on NeatlineFeatures ( project page ; Github ). This is an Omeka plugin that lets people associate geo-spatial metadata with Omeka items by drawing on a map. Before I started coding, I wanted to make sure I knew what I was doing, so I wrote a few user stories and passed them around for comment. Part of the value of user stories is that they are expressed in short, natural language statements, but they can also be transitioned into acceptance tests that everyone has had a voice in. (I should mention that I’m really just trying out user stories. They seem like a good idea in theory, but we’ll have to see how it works in practice.) When testing Omeka plugins, I use Omeka’s unit testing framework (PHPUnit) . I don’t see a reason to change that. However, unit testing frameworks generally aren’t a good fit for acceptance tests. They are both too focused and low-level and sometimes don’t have a great way of acting like a browser. A bigger problem is that they shut non-coders out of the loop. Having an English-y way of expressing tests helps to involve everyone working on the project, not just the developers. For the Praxis Program, we’ve been talking about using Cucumber for testing. I thought this would be a good time to try it out. I expected this would be painful. Fortunately, I was wrong. About the System The tests will be driven by Cucumber. Since this is in Ruby, they won’t interact directly with Omeka (there is cuke4php, but I’m not going there). Instead, everything will take place through the browser. Here’s how the system breaks down: Cucumber will read in the tests and execute them. It provides the language that we use to write the tests and the code we use to implement them. Capybara defines a DSL for interacting with the browser. We’ll use this when we define actions or steps for the tests. selenium-webdriver allows Capybara, Cucumber, or any Ruby code to talk to the browser. Selenium is a system for writing tests that run in the browser. It has a Firefox plugin, which we’ll use today for actually doing the tests. Omeka, PHP, and MySQL are all running in a VM, managed by Vagrant . (See this post for how to set up the system.) Rake is used to control the system. Adding Ruby Most of these tools are in Ruby, so the first step is to mix Ruby into the Omeka/PHP plugin project. To manage Ruby, I use RVM, so I added a .rvmrc file to switch to the right version of Ruby and make sure all the right gems are installed: [sourcecode language=”bash”]\nrvm use 1.9.3\nbundle install\n[/sourcecode] The gems are listed in a Gemfile : [sourcecode language=”ruby”]\nsource :rubygems\ngem ‘rake’\ngem ‘cucumber’\ngem ‘capybara’\ngem ‘selenium-webdriver’\n[/sourcecode] That’s it. Now, when I change to the plugin directory, Ruby and the gems I need are available. Setting up Cucumber Cucumber expects a specific directory structure. I created that with these Bash commands: [sourcecode language=”bash”]\nmkdir features\nmkdir features/step_definitions\nmkdir features/support\n[/sourcecode] Cucumber also needs a configuration file in features/support/env.rb : [sourcecode language=”ruby”]\nrequire ‘selenium-webdriver’\nrequire ‘capybara’\nrequire ‘capybara/cucumber’\nrequire ‘capybara/dsl’\nCapybara.app_host = ‘http://features.dev’\nCapybara.run_server = false\nCapybara.default_wait_time = 15\nCapybara.default_driver = :selenium\n[/sourcecode] Adding Features Cucumber groups tests into features . Each feature is in a separate file, and each contains one or more scenarios. For a contrived example, I tested logging into into the Omeka admin console. Feature Files The feature file for this is easy to read and understand, by design. I put this into features/login.feature : [sourcecode language=”ruby”]\nFeature: AdminLogin\n  In order to make changes to the site\n  As the site administrator\n  I want to be able to log into the admin console Scenario: Login\n    Given I visit the admin page\n    And I enter “features” for the “Username”\n    And I enter “features” for the “Password”\n    When I press “Log In”\n    Then I should see a page title of “Omeka Admin:”\n    And I should see a header of “Dashboard”\n[/sourcecode] That’s it. I could have added a scenario for not authenticating correctly or other cases, but I won’t worry about that right now. Step Files Feature files are great for people, but Cucumber/Ruby still doesn’t know what to do with it. To fill that gap, I defined what to do for each step in the scenario in a step file. I put this into features/step_definitions/login_steps.rb : [sourcecode language=”ruby”]\nGiven /^I visit the admin page$/ do\n  visit(‘/admin’)\nend Given /^I enter “([^”] )” for the “([^”] )”$/ do |value, label|\n  fill_in(label, :with =&gt; value)\nend When /^I press “([^”]*)”$/ do |button|\n  click_on(button)\nend Then /^I should see a page title of “([^”]*)”$/ do |page_title|\n  find(:xpath, ‘//title’).has_content?(page_title)\nend Then /^I should see a header of “([^”]*)”$/ do |header|\n  find(:xpath, ‘//h1’).has_content?(header)\nend\n[/sourcecode] (It would be better to group the step definitions into files by task or domain or some other way than one-step-file-per-feature-file, but for this demonstration, this is fine.) Step definitions tell Cucumber what to do for each test. Inside each definition, I used Capybara commands that tell the browser what to do or check the page that the browser’s looking at. These commands use Selenium to actually drive the action, but I don’t have to worry about that. Running Tests We have a couple of steps left to see actual tests being run. First, we could use the cucumber command, but we’ll probably have other things to automate (PHPUnit tests, I’m looking at you), so we’ll go ahead and create a Rakefile that runs the Cucumber tests. This is easy to do, since Cucumber ships with some rake tasks. Put this into Rakefile : [sourcecode language=”ruby”]\nrequire ‘cucumber/rake/task’ task :default =&gt; :cucumber Cucumber::Rake::Task.new do |t|\n  t.cucumber_opts = %w{–format pretty}\nend\n[/sourcecode] Now, as I mentioned before, the Omeka site is running in a Vagrant-managed VM. Start it up: [sourcecode language=”bash”]\nvagrant up\n[/sourcecode] Once the VM’s up, run the tests by just calling rake . You’ll see Firefox start up and close down, and at the end, you should see something like this (you may want to set the video to full screen): Cucumber-Omeka from Eric Rochester on Vimeo ."},{"id":"2011-12-02-design-qa","title":"Design Q&A","author":"brooke-lestock","date":"2011-12-02 05:40:08 -0500","categories":["Grad Student Research"],"url":"design-qa","layout":"post","content":"Today in Praxis: more design. The design team had agreed that we would all bring in some wireframe sketches to chat about collectively and try to agree on a few wireframes to talk about at the group meeting today. We spent some time working on sketches two weeks ago and raised quite a few questions that were readdressed when we met today. For instance, do we want Prism users to be able to access visualizations of texts if they haven’t already marked up that text? In other words, can someone who’s coming to Prism for the first time be able to look at the already-highlighted version of a text (including the one they’re working on)? We were divided on this. Some of us (myself included) were concerned that allowing users to visualize a text that is “open” for markup could influence the way they highlight. Doing so would downplay the interest we have in an individual’s response to the constrained highlighter parameters. Thinking of a classroom scenario: If a student is asked to mark up The Waste Land for Modernism, noise, and water, wouldn’t being able to see her classmates’ highlights affect/tempt her own interpretation? And how much does that matter? We also answered some questions today, I think. Two weeks ago, we were going back and forth on how the home page should look, what should be included on it and how to format that information so that it maintains a clear narrative for how to use Prism. We looked at some other sites for inspiration, and what we liked about For Better for Verse was that the home page puts you directly into the exercise and offers neat little tabs to help you navigate the site.  We wondered if Prism’s home page should be a sandbox, where the user is immediately asked to try Prism out for herself (without saving highlights), but we also want Prism’s homepage to offer some instruction, because using it is a bit more complicated than using For Better for Verse. We also were concerned  that the tabs wouldn’t provide a distinct hierarchy or clear set of instructions for the user. Ed and I were trying to figure out how to clearly offer an instructive narrative on the home page, and we turned to GitHub for inspiration since its home page features four distinct steps (complete with strangely adorable Octocat taking the user through each step). Over the break, Ed did a fantastic job of merging the For Better for Verse tabs with GitHub’s narration-through-icons. Jeremy also drew up a wireframe of the home page that prioritizes instructions and a how-to video while also including prominent links to the sandbox and to the actual text mark-up page (since that’s what it’s all about, after all). In today’s meeting, we’ll collectively talk wireframes, so be on the lookout for more questions and questions answered."},{"id":"2011-12-02-wireframing-and-foundations","title":"Wireframing and Foundations","author":"lindsay-oconnor","date":"2011-12-02 05:40:22 -0500","categories":["Grad Student Research"],"url":"wireframing-and-foundations","layout":"post","content":"Discussing design in the grad lounge continues to raise questions both programmatic and philosophical. Today we revisited the question of whether or not users should be able to see results or visualizations before they highlight a text themselves. Most of us grad students were concerned about this possibility, especially since we have agreed to make the first version of Prism for pedagogical use. We worried that students would either copy what others have marked, or that they would simply be influenced by others’ markings unintentionally. Jeremy and Wayne suggested that that might all be just fine; maybe what we want to find out is how a group interprets as a group, not as a collection of isolated individuals. This raises a major critical or methodological question for us. Are we “croudsourcing interpretation” by collecting many individual interpretations or by creating a space for collaborative interpretation? Do we want many separate interpretations that we can compare and contrast visually, or do we want one interpretation that is the work of many minds all together? I’m surprised we haven’t considered this question as a group before now. If we remain committed to the pedagogical use of the first version of Prism, I do think we should keep the results hidden from students before they have highlighted a text. But if we want to start thinking more broadly, I’m sure there will be use cases in which collaboration would be favored over the collection and collation of individual interpretations. Perhaps we can have it both ways further down the line, with researchers who upload texts given options about what is visible to which users. Our “original” idea of “crowdsourcing interpretation” seems to emphasize difference, but the collaborative version would seem to move toward a singular or unified or at least somehow standardized interpretation of an object. Wayne also pointed out how this issue has major implications for those writing the code, so here again what we thought might be a simple discussion of wireframing turned into a conversation about the methodological approach or assumptions of the tool we’re building. We still need a stronger foundation to hold up the frame."},{"id":"2011-12-05-model-vocabularies","title":"Model vocabularies","author":"alex-gil","date":"2011-12-05 06:50:04 -0500","categories":["Grad Student Research"],"url":"model-vocabularies","layout":"post","content":"Annie drafted a new Model for Prism this week, and we had a chance to tweak it and refine it on Friday. It was our first time programming together. Heck, it was the first time I ever programmed with anyone, period. We eased into the 24-inch monitor with some takeout Chinese. Patiently maneuvering one hand on the keyboard and one on the combination fried rice, we were able to create something close to the following diagram: When we had more or less finished, we posted it on the IRC to discuss with whoever was online. We found Bethany, Eric R. and Jeremy. In the discussion, two suggestions came up which I think are important to keep in mind as Prism moves forward. Originally we had the model now named ‘Prism’ named ‘Experiment.’ The concern there was that the word was not catering to a humanist audience, or at least that part of it that understands itself outside the boundaries of science. In the same spirit, the field now named ‘prompt’ was originally named ‘research_question’. The other concern was over the use of the word ‘Tag’. That was the original name of the model now named ‘Facet’. The concern there was that the word brings with it its own semantic baggage, which might confuse the user. The new choices, ‘Facet’ and ‘Prism’, have the added advantage that they bring the color of Prism into the data models. Something happened in that conversation. I realized that in order to be descriptive with data or objects, I don’t have to be clinical. I had realized the same for prose a while back, but it took this exercise to see what’s good for literature is good for code. Now I am ready to understand the full extent of the claim that code is an expressive language. Even if we will only have a few opportunities to share the language of the model with a general audience (a paper or conference talk here and there), we should code whenever possible with a humanist’s ear."},{"id":"2011-12-13-humanities-in-a-digital-age-symposium-podcast","title":"Humanities in a Digital Age Symposium podcast","author":"ronda-grizzle","date":"2011-12-13 10:42:22 -0500","categories":["Podcasts"],"url":"humanities-in-a-digital-age-symposium-podcast","layout":"post","content":"Institute of the Humanities and Global Cultures: Humanities in a Digital Age Symposium On November 11th, the University’s new Institute of the Humanities and Global Cultures hosted a daylong symposium on “The Humanities in a Digital Age.” The symposium included two panels–one on Access &amp; Ownership and the other on Research &amp; Teaching–and two keynote talks. The first keynote was given by Stephen Ramsay, Associate Professor in the Department of English and Fellow in the Center for Digital Research in the Humanities at the University of Nebraska - Lincoln. The second keynote was given by Dan Cohen, Associate Professor in the Department of History and Director of the Roy Rosenzweig Center for History in New Media at George Mason University. Panel 1: Access and Ownership Jeremy Boggs, Humanities Design Architect, UVa Library Scholars’ Lab Ann Houston, Director of Humanities and Social Sciences, UVa Library [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/DownloadTrackPreview/virginia-public-dz.5154837759.05154837761.11631377447/enclosure.mp3”] Keynote: Stephen Ramsay, “Textual Behavior in the Human Male” [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/DownloadTrackPreview/virginia-public-dz.5154837759.05154837761.11642505413/enclosure.mp3”] Panel 2: Research and Teaching Alison Booth, Professor, Department of English Mitch Green, Horace W. Goldsmith Distinguished Teaching Professor of Philosophy, Department of Philosophy [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/DownloadTrackPreview/virginia-public-dz.5154837759.05154837761.11755983498/enclosure.mp3”] Keynote: Dan Cohen, “Humanities Scholars and the Web: Past, Present, and Future” [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/DownloadTrackPreview/virginia-public-dz.5154837759.05154837761.11643175927/enclosure.mp3”] As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU ."},{"id":"2011-12-13-johannes-kepper-julian-dabbert-mei-or-musical-editions-improved","title":"Johannes Kepper & Julian Dabbert: MEI, or Musical Editions Improved","author":"ronda-grizzle","date":"2011-12-13 10:42:10 -0500","categories":["Podcasts"],"url":"johannes-kepper-julian-dabbert-mei-or-musical-editions-improved","layout":"post","content":"MEI, or Musical Editions Improved On November 4th, the UVa Music Library and the Scholars’ Lab welcomed Dr. Johannes Kepper, Entwicklung/Betreuung Kooperationspartner at the Edirom Project, and Mr. Julian Dabbert, Wissenschaftlicher Mitarbeiter on Project TextGrid at the University of Paderborn. Dr. Kepper and Mr. Debbert discussed the requirements, characteristics and benefits of digital editions based on the Music Encoding Initiative schema, as well as MEI-based applications such as the Edirom toolset and the MerMEId metadata editor. The whole group also discussed the impact of these technologies on scholarly editing in general. As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/DownloadTrackPreview/virginia-public-dz.5154837759.05154837761.11632589066/enclosure.mp3”]"},{"id":"2011-12-13-the-beautiful-truth-about-praxis","title":"The (beautiful) truth about Praxis","author":"sarah-storti","date":"2011-12-13 06:51:19 -0500","categories":["Grad Student Research"],"url":"the-beautiful-truth-about-praxis","layout":"post","content":"I thought that this week I’d give you a behind-the-scenes peek at what we really do in the Praxis Program at the Scholars’ Lab . Brooke and I both wrote posts at the beginning of the semester about how much pressure we felt to appear polished and professional 24/7, but the reality is that 1) this is a learning process and 2) we’re only human. I applied to be part of the Praxis team because I wanted to work on DH with other people (not an opportunity that presents itself every day!) and it really is as great as I thought it would be.  At the risk of making all of you hopelessly envious of our camaraderie, I’m pleased to share below a pictorial flowchart of sorts that illustrates the way things generally happen here at the Praxis Program during Fellows’ “office hours.” First, one of the extremely knowledgeable members of the Praxis team teaches the newbies about something. Here, that something is wireframing. Next, Praxis Fellows try out their own ideas on the new concept, using each other as sounding boards. We achieve various degrees of success depending on a number of factors, including that day’s caffeine intake level. At some point, the knowledgeable Praxis team member usually has to avert some kind of impending crisis: “I’m going to make an intervention, because you need one.” But these interventions are always performed in a very kindly fashion. We appreciate them. (This photo actually captures two such interventions occurring simultaneously… wireframing and programming CAN happen in the same room!) Finally, the focus group (wireframers here again) agrees upon some kind of model to be used for sharing with the larger Praxis team at the weekly Tuesday afternoon meeting. We dissect, discuss, and suggest modifications as a group. Then it’s usually back to the Fellows Office for revisions and a repeat of the process. Disclaimer: the above images are drafts and first-round workings-out of wireframing problems. They are not necessarily representative of where we are now with our wireframing. I don’t think we’re giving away anything too top-secret, and since one of the goals of this program is to be “live and in public” I thought it was about time you got to see something we’ve actually drawn up ourselves. As a final touch to this essay on the joys of group work, I’m appending, for your delectation, a sampling of overheard office hour chat: –Discussing life’s most challenging questions, such as Why We’re Here: Ed: I’m here to break your application with one capital letter. That’s why I’m here. \nWayne: You’re not gonna break MY application with one capital letter.\nEd: You just let me at your application… –And another vignette from today featuring Ed: Ed: YESSSSSSSSSSS!!\nThe rest of us: [quizzical looks]\nEd: I just got Heroku to work on my application. And even better, Wayne was wrong. \n[laughter]\nEd: I’m gonna go tell him. Til next time!"},{"id":"2011-12-24-representative-and-abstract-prism-logos","title":"Representative and Abstract Prism Logos","author":"ed-triplett","date":"2011-12-24 05:16:04 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"representative-and-abstract-prism-logos","layout":"post","content":"The prism Logo now has four prototypes. I spent part of the Thanksgiving holiday a few weeks ago creating ten to fifteen options and presented them to the group at our Tuesday meeting two weeks ago. My initial intention was to focus on creating a cohesive, but abstract shape that could be repeated elsewhere on the site without the full text of the logo. The logo that elicited the most response from the group was a more representative option that I likely spent the least amount of time on. I was initially surprised at the group’s choice, but afterward I realized that this is pretty typical for this kind of work. It can be easy to create an over-wrought logo when you devote a long section of time to it. As you push, pull and minutely alter an initial idea to make it different from a previous example, the result is often an amalgamation of too many ideas. Inversely, I have found that a logo I create quickly “works” best because the quickness of the stroke leads to a corresponding quickness of communication. The group’s decision to pursue a logo that more clearly represented an actual prism gave the logo direction, but it also capped the logo’s level of abstraction. It was my hope however to see how far I could abstract the design without losing its representative quality. At its core, a prism is inherently a narrative object with an implied beginning, a transition, and a result. These three elements had to exist in the logo with a perceivable order. As for most things in a digital medium, the four possible changes could be shape, texture, scale and color. In the end, I could not use all of these elements as part of the transition without the narrative breaking down. The transition was therefore set for scale and color, and the “transition object” was simplified to contrast with the color and scale changes on either side of it. I admit that logos and font choice do not make for the most entertaining reading, especially without visual examples (You readers will have to wait until we have a prototype of the site to see the logo.) That said, I wanted to note that the representative prism “narrative” option ended up completely changing my approach to the font for the prism logo. Initially, I looked exclusively at more stripped-down, modern, sans-serif fonts with the vaguest hint at a meticulous human hand in order to connote a sense of precision for the web application. However, after selecting the representative design of the prism and its narrow beam motif, the width of the sans-serif fonts seemed to dwarf the rest of the logo. I then began to look at fonts with bracketed serifs and long flat bases that invoked the shape of the thin horizontal lines in the rest of the logo. Today’s meeting should offer us an opportunity to get all but the last details decided on for the logo. I am looking forward to hearing everyone’s reaction."},{"id":"2011-12-24-to-scaffold-or-not-to-scaffold","title":"To Scaffold, or Not To Scaffold?","author":"annie-swafford","date":"2011-12-24 05:16:05 -0500","categories":["Grad Student Research"],"url":"to-scaffold-or-not-to-scaffold","layout":"post","content":"Alex and I started trying to build our new data model last Thursday, and we figured that it would go much more smoothly and quickly than the last time we tried.  Like the previous time, we decided to use Rails scaffolding, figuring it would be easier than generating the individual pieces by hand.  However, we ran into some unanticipated road blocks.  For example, we wanted a few different models to have IDs associated with them, so we added them into the model framework.  The scaffolding helpfully added them to our views page as well, so instead of having  IDs generated automatically, they appear on a form for the user to fill out, thus defeating the purpose of the ID field.  After lots of frustration, we decided to try to create a model without scaffolding when we get back from break.  We figure that it will take more time, but will involve less tweaking and will give us the opportunity of better understanding how the component parts of a rails application fit together.  We may decide that it’s too much of a headache and that it would make more sense to use scaffolding and just to delete the unnecessary parts, but in the interests of experimentation, we’ll give it a shot."},{"id":"2012-01-09-generating-html-fixtures-using-zend-omeka-phpunit-and-jasmine","title":"Generating HTML fixtures using Zend, Omeka, PHPUnit, and Jasmine","author":"david-mcclure","date":"2012-01-09 03:49:35 -0500","categories":["Research and Development"],"url":"generating-html-fixtures-using-zend-omeka-phpunit-and-jasmine","layout":"post","content":"One of the reasons that testing JavaScript can be so pesky (and perhaps one of the reasons that so little JavaScript is tested…) is the fact that you have to maintain a library of HTML “fixtures” for the tests to run on. What’s a fixture? Basically, just a little chunk of markup that provides a sandbox environment for a particular test, or a suite of tests. So, if you have a jQuery widget that adds some extra functionality to a form input, your fixture could be as simple as just a single input tag. And indeed, some of the time you can get away with just manually whipping up a chunk of ad-hoc HTML, dropping it directly into your suite, and testing into the sunset: [sourcecode language=”html”] [/sourcecode] This works fine, provided that the HTML your code is working on is relatively simple. In practice, though, most front-end applications that grow beyond a certain critical mass of complexity end up with JavaScript code that makes DOM touches on large, complex markup structures that can’t be so easily replicated independent of the application itself. This issue started to rear its head as Neatline became more and more complex over the course of the last couple months. The Neatline front-end consists of well over 10,000 lines of JavaScript, which works on markup generated by about 20 templates and partials - it would be a frightful headache to have to manually create a library of testing fixtures for the whole application. And even if I took the time to build them all out by hand, I would be committing myself to a labor-intensive, open-ended maintenance task: Every time you make a change to a template, you have to remember to comb through all the files in the fixtures library and replicate the change. Over time - and especially as new developers start working on the project - there’s a high probability that the “real” HTML generated by the live application will start to diverge from your fixtures. My search for a solution led me to this fantastic post from JB Steadman at Pivotal Labs . Basically, he describes a clever method for automatically generating a library of HTML fixtures that uses the server-side test suite as a staging environment that prepares, creates, and saves the markup emitted by the application. That way, your fixtures library can only ever be as old as the last time you ran your back-end test suite, which should be a many-times-daily affair. I was able to implement this pattern in the Omeka/Zend + PHPUnit ecosystem with little difficulty. (Details and code after the jump) Basically, we do this: Create a special controller in your application that exists solely for the purpose of rendering the templates (and combinations of templates) that are required by the JavaScript test suite; Create a series of testing cases that issue requests to each of the actions in the fixtures controller, capture the responses, and write the generated HTML directly into the fixtures library. How does this work in practice? Imagine you have a template called _records.php that looks like this: [sourcecode language=”php”] title; ?> description; ?> [/sourcecode] And when it’s rendered in the application, the final markup looks like this: [sourcecode language=”html”] Record 1 Title Description for record 1. Record 2 Title Description for record 2. [/sourcecode] So, the goal here is to create a controller action that populates the template with mock records objects and renders the markup, which can then be captured and saved by an integration “test” that we’ll write in just a minute (test in quotes, since we’re using PHPUnit not so much as a testing framework, but more just as a mechanism for automating requests). First, add a new controller class called FixturesController, and create an action that mocks any variables that need to get pushed into the template: [sourcecode language=”php”]\nclass YourPlugin_FixturesController extends Omeka_Controller_Action\n{ /**\n * Generate fixture for _records.php.\n *\n * @return void\n */\npublic function recordsAction()\n{\n\n    // Turn off the default Zend layout-discovery functionality.\n    $this-&gt;_helper-&gt;viewRenderer-&gt;setNoRender(true);\n\n    $record1 = (object) array(\n      'title' =&gt; 'Record 1 Title',\n      'description' =&gt; 'A description for record 1.'\n    );\n\n    $record2 = (object) array(\n      'title' =&gt; 'Record 1 Title',\n      'description' =&gt; 'A description for record 1.'\n    );\n\n    $records = array($record1, $record2);\n\n    // Render.\n    echo $this-&gt;view-&gt;partial('public/_records.php', array(\n        'records' =&gt;  $records\n    ));\n\n} }\n[/sourcecode] Basically, we’re just stubbing out two artificial record objects (for simplicity, we add only the attributes that are used in the template) and directly render the template file as a “partial.” Note the call to setNoRender(true) - by default, Zend will try to automagically discover a template file with the same name as the controller action, but we’re just disabling that functionality since we want direct control over which templates get rendered and in what order. Next, add a directory called “fixtures” in the /tests directory, and create a file called “FixtureBuilderTest.php” to house the integration test that will do the work of requesting the new controlled action, capturing the generated markup, and saving the result to the fixtures library. This should look like this: [sourcecode language=”php”]\nclass YourPlugin_FixtureBuilderTest extends YourPlugin_Test_AppTestCase\n{ private static $path_to_fixtures = '../spec/javascripts/fixtures/';\n\n/**\n * Instantiate the helper class, install the plugins, get the database.\n *\n * @return void.\n */\npublic function setUp()\n{\n\n    // Set up the testing environment and plugin.\n    parent::setUp();\n    $this-&gt;setUpPlugin();\n\n}\n\n/**\n * Fixture builder for _records.php.\n *\n * @return void.\n */\npublic function testBuildRecordsMarkup()\n{\n\n    $fixture = fopen(self::$path_to_fixtures . '_records.html', 'w');\n\n    $this-&gt;dispatch('your-plugin/fixtures/records');\n    $response = $this-&gt;getResponse()-&gt;getBody('default');\n\n    fwrite($fixture, $response);\n    fclose($fixture);\n\n} }\n[/sourcecode] Note that you need to specify the location in the project directory structure that you want to save the fixtures to. In this case, I’m saving to the default location used by Jasmine, but you could point to anywhere in the filesystem relative to the AllTests.php runner file in /tests. Make sure that the /fixtures directory is included in the test discoverer in AllTests.php, run phpunit, and your fresh-out-of-the-oven fixture should be saved off and ready for action! All that’s left to do now is load the fixture in your JavaScript test, run your code on the HTML, and start enumerating test cases. We use a testing framework called Jasmine in conjunction with a plugin called jasmine-jquery, which provides an easy way to load fixtures into the tests: [sourcecode language=”javascript”]\n/*\n * Unit tests for the records Javascript.\n */ describe(‘Records’, function() { var recordsContainer;\n\nbeforeEach(function() {\n\n    // Get the records markup.\n    loadFixtures('_records.html');\n\n    // Select the container div.\n    browser = $('#container');\n\n    // Instantiate your code.\n    browser.recordsWidget();\n\n});\n\n// Now, the tests:\n\ndescribe('some class of behavior', function() {\n\n    it('should do X', function() {\n        expect(true).toEqual(true);\n    });\n\n}); });\n[/sourcecode]"},{"id":"2012-01-09-looking-forward-to-prism","title":"Looking forward to Prism","author":"brooke-lestock","date":"2012-01-09 11:22:56 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"looking-forward-to-prism","layout":"post","content":"With the end of the semester and year, and all of the accompanying hullabaloo (to use a polite term for it), I wasn’t able to write my final blog post of the semester, which was going to be a retrospective of my Praxis experience so far. But now it’s the new year and the new semester is imminent, so it seems more appropriate to look ahead - and who wants to look like Janus, anyway? I think it’s brilliant the way that Praxis has been structured, with one training-intensive semester and the next semester spent - I’m assuming - in executing what we’ve been theorizing. That’s not to say that we haven’t done anything yet; we have produced the programming and design foundations for Prism that we will hammer down and build from in the months to come. There have been days in the grad lounge, though, when I’ve felt anxious about how much work there is left to do and how difficult it seems to reach concrete goals when our discussions usually raise more questions than they answer. But even if we spend our entire Tuesday mornings arguing/theorizing about what we think Prism should accomplish, our time isn’t wasted if it means Prism will be (1) a clearly designed tool (2) with a distinguishable thought process that (3) makes a specific intervention. All of our discussions have addressed at least one of those three points without fail, so our mission in the months ahead is to channel our humanities-inspired zeal for theorizing into reaching the goals we set for Prism in September - mainly, that we want to produce a working, 1.0 version of Prism by the end of the academic year. To reach that final, rather intimidating endpoint, I’d like to suggest that the Praxis team begins the new semester by establishing (collaboratively, of course) some real, manageable, short-term goals for the coming weeks. I’m beginning to recover from Winter Break Amnesia (I’m sure our first Praxis meeting tomorrow will quickly bring me back to reality), but I am looking forward to getting back to learning HTML and CSS, to our civilized theoretical arguments in the grad lounge, and to those glorious moments when we (with the help of the gurus) learn how to turn theory into praxis and bring Prism further into the light."},{"id":"2012-01-13-mapping-the-catalogue-of-ships","title":"Mapping the Catalogue of Ships","author":"bethany-nowviskie","date":"2012-01-13 09:02:40 -0500","categories":["Announcements","Digital Humanities","Geospatial and Temporal","Visualization and Data Mining"],"url":"mapping-the-catalogue-of-ships","layout":"post","content":"I’m very pleased to share a guest post by UVa Classics professor Jenny Strauss Clay, describing a new project we’ve undertaken at the Scholars’ Lab.  We’re excited not only at the opportunity to use GIS techniques to test Professor Clay’s theories about the relation of ancient geography to mnemonic devices and poetic form, but also at the possibility that this process might assist in the identification of lost archaeological sites. – Bethany Nowviskie Book Two of the Iliad notoriously contains a list of nearly 190 place names and includes the 29 contingents and that make up the Greek expedition to Troy.  Before launching into an over 250-line catalogue of the leaders of the Greek forces and the number of their ships, Homer appeals to the Muses to aid him in this tour-de-force of memory.  Without their help, he says: I could not recount their numbers nor name them, Not if I had ten tongues and ten mouths, And an unbreakable voice and a brazen chest within, If the Olympian Muses, daughters of aegis-bearing Zeus, would remind me how many came under Ilium. The Catalogue of Ships that follows this invocation can be mapped as an itinerary, or more precisely, three itineraries that traverse most of Greece.  The theoretical basis for the project I am undertaking with the Scholars’ Lab at the University of Virginia Library is already complete. In my recent book, Homer’s Trojan Theater (Cambridge University Press, 2011), I argue that Homer was able to recite the Catalogue by creating a mental journey that used the mnemonic techniques involving loci or places, well known from ancient rhetorical writers.  By envisioning a series of places, Homer could mentally walk – or sail – through Greece and produce a detailed catalogue. Our project will reproduce that journey by showing that the itinerary described follows the natural contours of Greek geography and the patterns of early Greek urban organization. Mapping the Catalogue of Ships involves several steps.  “Least-cost path” GIS analysis by the Scholars Lab is revealing the terrain that must naturally be followed when taking a walking tour of the Greek mainland.  We are creating an interactive map that follows that path.  The Barrington Atlas of the Ancient World (2002) as well as the recent Historischer Atlas der antiken Welt (2007), The Homer Encyclopedia (2011) and the Pleiades Project, a collaborative database for ancient sites, have pinpointed locations for which we have evidence.  We will attempt to link the sites mentioned in Homer with archaeological material and useful bibliographies.  Finally, we hope to do in situ investigations by actually traversing the plotted itinerary at ground level to survey the terrain, and create extensive panoramic photography. Our main goal is to demonstrate that the arrangement of the Catalogue, far from a random list of place names, corresponds to the natural geography of Greece.  In cases where the position of a site is unknown or disputed, we hope that our analysis will provide plausible geographical and literary evidence to help identify its location. Collaborators in this project include Ben Jasnow and Courtney Evans, two of my graduate students who worked with me on the Trojan Theater project and who are assisting with GIS analysis, under the guidance of Chris Gist and Kelly Johnston of the Scholars’ Lab. Wayne Graham and other members of the Scholars’ Lab R&amp;D division are creating a presentational framework for our maps and text, and Jeremy Boggs is our lead designer. Jenny Strauss Clay\nWilliam R. Kenan Jr. Professor of Classics"},{"id":"2012-01-17-praxis-mla-2012-and-timeliness","title":"Praxis, MLA 2012 and timeliness","author":"alex-gil","date":"2012-01-17 10:39:53 -0500","categories":["Grad Student Research"],"url":"praxis-mla-2012-and-timeliness","layout":"post","content":"I’m finally settling back into my C’ville routine. My last stop this winter break was the MLA convention in Seattle. Like many of my colleagues, I also felt that “ the MLA’s heart (like a post-holiday Grinch) grew at least three sizes over the four days of the 2012 conference .” While last year echoed a prominent informer ’s assessment that DH was “the next big thing” with anxiety, this year felt more like “Hey, I like that. How do I do it?” This was especially a good year for those in the business of rethinking the future of graduate methods training (ahem, ahem) and of graduate futures in general . Needless to say, I felt really great about being part of the first cohort of Praxis . Saturday evening I had a chance to catch up with one of my early undergraduate mentors. He had questions. He wanted to know what I knew about the DH world. I’m sure half of his curiosity came out of an earnest desire to hear the tale of my travels. The other half was a shrewd (and responsible) move to build a vocabulary for conversations his department will inevitably have this year with the dean, other departments, the library, etc: Can an isolated DHer work well with limited resources? Do you need a center? How do you get graduate students involved? Our conversation went on for a good three hours and it was very rewarding to offer a candid assessment of the field from where I’m standing. I also realized that where I’m standing is what in battle we would call higher ground . I don’t mean the privilege of hobnobbing with the enormous DH talent we have on grounds. Nothing, of the sort. Although projects are a whole different affair, you could develop decent DH skills and ideas were you connected from Pie Town . I mean the privilege of seeing graduate methods transformation first-hand. I agree with Brooke that there is a continuum that links us to analog models in the department (at least at UVa). But the continuum does eventually lead to new ground. What I’ve seen, of course, has been well recorded here by all the Praxis bloggers. If this is your first time visiting our site and you are interested in the fresh air blowing our way, I encourage you to read more…"},{"id":"2012-01-17-project-management-and-graduate-training","title":"Project Management and Graduate Training","author":"brooke-lestock","date":"2012-01-17 07:41:30 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"project-management-and-graduate-training","layout":"post","content":"As if on cue, right after I posted last week to call for clear, concrete goals for Prism this semester, Bethany began last week’s meeting by asking for a Project Manager. Sarah Storti and I quickly volunteered for the job, probably because we share a love of deadlines, self-imposed or otherwise, and work at similar levels of anxiety without them. Bethany assigned us some weekend reading on Project Management and we convened for “Projectmanageapalooza” yesterday to discuss the material and devise a plan for managing this semester’s hefty workload. The readings were extremely helpful (see the “Intro to Project Management” section of Praxis topics for our short list of most helpful resources), especially Brian Croxall’s  ”12 Basic Principles of Project Management” and Sharon Leon’s “Project Management for Humanists.” Both articles stress the PM’s need to assess the viability/sustainability of a project before it’s begun, the importance of a clear and flexible workplan that is derived collaboratively and realistically, and the PM’s responsibility to manage and encourage frequent communication amongst team members and partners. Both articles also begin with a point that has been made quite frequently, but that has not necessarily been my experience as a graduate student so far: that humanities graduate students are not trained to work collaboratively. While I wholeheartedly agree with Leon, Croxall, and most of the DH community that graduate education must be transformed to_ formally, explicitly_ transmit this kind of training to humanities scholars, for the sake of the individual scholar and the profession as a whole, there are many opportunities to work collaboratively as a graduate student, but they must be sought out and are often extracurricular and small-scale. I recognize that it can be very easy to become the scholar/hermit in a graduate program, especially because programs have not yet adapted to encourage collaborative research in the traditional sense (like a tag-team digital dissertation), but collaboration on the most basic level has been ingrained in my daily experience in UVA’s English department - from discussion in grad seminars that leads to new research, to collaboratively editing papers and personal statements with peers and faculty mentors alike, to extracurricular activities like the Graduate English Students Association and its conference committee (groups that require quite a bit of PM-type skills). This is not the kind of training Leon and Croxall are calling for, but my graduate education (so far) has trained me to seek out opportunities to collaborate with others within the department and outside of it (in my work with Praxis and IATH, for example). I have to stress that I do not disagree with the inadequacy of graduate methodological training; if I did, I wouldn’t be a Praxis Fellow. But I think we can find the basic principles for collaborative research happening already on a very small scale, and it’s up to graduate students to make collaborative research a priority - that is, to find those opportunities, seize them, and ask their programs to support them. Pardon the lengthy post; I know I’m not saying anything new here and I may be totally off-base, but I thought I’d respond with my experience as a nascent scholar and even-more-nascent member of the DH community."},{"id":"2012-01-23-why-i-love-project-management","title":"Why I love project management","author":"sarah-storti","date":"2012-01-23 09:51:32 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"why-i-love-project-management","layout":"post","content":"Last week Brooke and I celebrated our new roles as co-project managers by running our very first Praxis meeting. We had a fairly ambitious agenda, and I must admit that I was a little bit concerned about whether our enthusiastic (debate-loving) group would be able to get through everything we wanted to do, but thanks to Brooke’s no-nonsense attitude, our pre-planned strategy, and the team’s brilliant cooperation, I think we can call our first official act as co-project managers a decided success. The most important product of our meeting was the project workplan timeline the team collectively created. I know that deadline-lovers Brooke and I feel six hundred times better about how the rest of this semester is going to proceed, but I think everybody is happier knowing exactly what needs to happen and when. I would only be exaggerating slightly to say that it was magical to watch the workplan take shape. Despite the fact that we’ve all been meeting weekly for an entire semester, as the project has progressed and the design team and programming team have been working more independently of one another, it has been difficult to see how everything fits together. But when we had to agree on deadlines for project milestones (when will we be able to display a text? when will user accounts be ready? when should the highlighting tool be functional?), designers and programmers had to engage in a dialogue about what each group would need from the other in order to meet these goals. The meeting served as yet another reminder of how absolutely, impossibly lucky I am to be part of this program. As project managers, Brooke and I are ideally positioned to understand how the Praxis Program and our project Prism work as collaborative ventures. It is impossible for every member of the team to keep track of what each subgroup is doing at all times, but it is necessary that project managers do just that. I can’t wait to get my hands dirty! I’m sure I speak for both of us when I say that we are thrilled and honored by the trust our team has placed in us. We won’t let you down! Brooke posted last week about seeking out ways to engage in collaborative work, and I want to point very quickly here to the Digital Humanities Summer Institute . I attended DHSI last summer, and it certainly didn’t disappoint on the collaborative front. Brooke and I are both making the trip to Victoria this summer, and I’m sure she will only find more evidence there of what she’s seen so far as a graduate student in the humanities: seek collaboration, and you shall find it. If you’ve never been but always wondered, I highly recommend that you make this the year you join the (collective) fun!"},{"id":"2012-01-24-building-and-texts","title":"Building and Texts","author":"annie-swafford","date":"2012-01-24 09:03:04 -0500","categories":["Grad Student Research"],"url":"building-and-texts","layout":"post","content":"Alex and I finally have something to show for all our work! We built the document model for Prism that allows us to add new texts to the database!  It even passes the tests we built!  We’re putting the finishing touches on it now, but we should easily make our deadline of this afternoon, so we can successfully pass it on to the design team.  It’s exciting that we’re actually able to put together our rails and ruby knowledge and produce tangible results.  This gives us hope that we will be able to continue to meet our other deadlines and eventually end up with a working project. Our next programming steps include building a user model and incorporating authentication and authorization (probably using CanCan and Devise ruby gems).  This will be more complicated that what we’ve produced so far, but we think that with the help of lots of screencasts and ruby gem documentation, we will persevere. On other prism news, we will officially decide on our texts for Prism today in our meeting!  It’s all starting to take shape."},{"id":"2012-01-25-sci-opening","title":"seeking a Senior Research Specialist for SCI","author":"bethany-nowviskie","date":"2012-01-25 06:19:34 -0500","categories":["Announcements"],"url":"sci-opening","layout":"post","content":"I’m pleased to share a formal job posting for an 18-month research-and-writing position with the Scholarly Communication Institute. The posting is limited to current employees of the Unversity of Virginia for four business days, after which it will open more broadly.*  It’s a short-term position, but an excellent opportunity to work with SCI leadership and representatives of the following groups: the Scholars’ Lab Praxis Program, CHCI, centerNet, PressForward, the Modern Language Association, the Alliance for Networking Visual Culture, the ACLS, CLIR, and more. The timeframe for our decision is very short, so interested applicants should not delay! UPDATE: The position is now open to all qualified candidates. Please note new URL below! Senior Research Specialist, SCI The Scholars’ Lab at the University of Virginia Library seeks a senior research specialist for a full-time, 18-month position with the Scholarly Communication Institute (SCI).  The ideal candidate will have: excellent research, writing, and organizational skills; familiarity with humanities scholarship at the graduate level; and an interest in experimental approaches to digital authoring and publication or the education of emerging scholars and knowledge workers. Some travel is required, and there is a possibility of a work-from-home arrangement for a professional and thoroughly reliable candidate. Reporting to UVa Library’s director of digital research and scholarship, the SCI research specialist will: help design, execute, and analyze a broad-based, anonymous survey of humanities professionals (together with the perceptions of their employers) to examine graduate-level preparation for so-called “ alternative academic ” careers (20%); attend and provide research support for three meetings and regular conference calls on new-model publishing and authoring environments, in order to take notes and help compile proceedings and recommendations (15%); attend and provide research support for two to three meetings centering on the reform of methodological training in the humanities, involving major humanities consortia, professional associations, and funders, to take notes and help compile proceedings and recommendations (15%); assist in research and organizational tasks related to the development and day-to-day operations of the Praxis Program at the University of Virginia Library, including the hosting of a gathering to foster inter-institutional collaboration on similar initiatives (20%); and perform other research and writing tasks as needed by SCI principals (10%). The SCI research specialist will join a vibrant and dedicated community of faculty and staff at the Scholars’ Lab, and as such will be eligible for the self-directed “20% time” that all team members are granted to pursue professional development and their own (often collaborative) R&amp;D; projects.  This is a grant-funded position with a salary of approximately $50k per annum and full benefits as a member of the managerial and professional staff of the University of Virginia. The position begins no later than March 1st, 2012 and extends no later than August 31st, 2013. Experience and Education: Master’s degree or higher in fields related to humanities scholarship or information science. Demonstrated ability as a writer and researcher. Project management experience or experience with survey design and analysis desirable. Please APPLY ONLINE ."},{"id":"2012-01-26-final-prism-wireframes","title":"\"Final\" Prism Wireframes","author":"lindsay-oconnor","date":"2012-01-26 02:22:45 -0500","categories":["Grad Student Research","Research and Development"],"url":"final-prism-wireframes","layout":"post","content":"Despite whatever I might have said in previous blog posts, coding was quite a challenge for me and not something I could see myself devoting sustained attention to, so I was pleased to find some inkling of intuition about graphic design. My days of designing my high school newspaper on antiquated Adobe software suddenly became relevant, and all the time I waste perusing Fab.com and looking for affordable mid-century housewares on Craigslist can now be called “design research.” With lots of help from Jeremy and the internets, a Prism homepage is in development, complete with a header and footer and floated text and working links. Of course we’ll need to do a lot more than that to get this thing looking nearly as good as some of Ed’s designs in Adobe Illustrator, but I finally have some sense of what it will take, and with so many others on the Praxis team interested in design, it all seems possible. Since I’m so new to HTML and CSS, I have no shame about asking for help with the smallest issues (like italicizing text) or with far-reaching errors (the whole page is the body?!), but I also have no shame about celebrating the smallest successes (rounded corners!). In addition to a few meetings with Jeremy, Ed’s great ideas and Illustrator skills and Annie and David’s HTML knowledge have helped me make progress. People are probably going to start sending me “Let me Google that for You” links pretty soon, but at least right now, I’m feeling warm and fuzzy and collaborative. On Monday morning, Brooke, Sarah, Jeremy, Ed, and I worked through the wireframes for Prism one more time. We had some drafts from the end of last semester, but some decisions about how the tool will work have affected how the site will look, so we revisited our wireframes and got them in good enough shape to present to the whole team on Tuesday. This picture shows what we came up with. It doesn’t look like much, but I think we all feel pretty accomplished now that we have a “clean” and simple form to design on."},{"id":"2012-01-26-teasing-the-blogosphere","title":"Teasing the Blogosphere","author":"ed-triplett","date":"2012-01-26 02:21:44 -0500","categories":["Digital Humanities","Grad Student Research","Research and Development"],"url":"teasing-the-blogosphere","layout":"post","content":"Describing visual work in a blog without being able to reveal the images is more than challenging: it’s boring. After we discussed this issue in our most recent Praxis meeting, the group suggested I post a Prism striptease. Linked below is a teasingly cropped and subtly altered image of the logo we decided on last week. The other images show some renders of a 3D model I created to use as inspiration for the site’s color palette. The model is of a “deck prism” which was used to filter light from above the deck of a ship into the cabins below. The refracted colors that emerge when light passes through the prism can be seen in online images, but the digital model offered a lot more control. http://www.flickr.com/photos/28122639@N05/6761716871/\nThe above video shows the visual options offered by the model of the deck prism. While the deck prism is recognizable as an object, its clarity and sharp lines make it a good source of abstract imagery for the site. The model also allowed me to experiment with reflections of the texts we chose as our three highlighting samples. The text was simply applied to a plane directly below the prism in the 3D scene, and the reflections changed on the fly based on the camera position. You can see part of the first line from “The Raven” reflected in one of the images below. Finally, the last image shows the highlighter “palette” that we used for our wireframe. The boxes are mostly just placeholders, but the colors represent some of the options that came out of the deck prism, and I think the categories we chose for Edgar Allan Poe’’s “The Raven” could elicit a thought provoking response from a crowd."},{"id":"2012-02-01-done-is-the-engine-of-more-2","title":"Done is the engine of more.","author":"sarah-storti","date":"2012-01-31 21:22:43 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"done-is-the-engine-of-more-2","layout":"post","content":"(My title is not mine! It is lovingly borrowed from Bre Pettis and Kio Stark’s “Cult of Done Manifesto” ) I love lists almost as much as I love agendas and program management in general. Here is a status update in list form for you, Dear Reader. And Team, please feel free to expand/clarify/correct the following: We have finalized the texts we’re going to be uploading into Prism for first-round users. Today, some of us tested a few of the suggested “highlighting” categories with help from our low-tech friends: transparencies, markers, and photocopies. Ed, Brooke, Annie, and I will be reviewing and revising categories over the next few days, and the entire Praxis team is invited to test our new suggestions this coming week. The development team met their latest milestone goal: authentication and authorization for the Prism site is a go! User accounts! Ding! 3. As Lindsay notes, the wireframes (and thus, for the most part, the user story), are finalized. Design wizzes (plural of wiz?) Ed and Lindsay have been rocking out on the front-end work: Ed continues to wow us with his aesthetic brilliance (see his “striptease” post of Jan 26), while Lindsay works on making the dream a reality via CSS/HTML. We’ve been inspired by the recent trend toward internationalizing Ruby on Rails applications, and have nominated the most Continental of our Fellows, Alex, to the position of Internationalization Expert(-to-be). I’m sure you’ll be hearing more from him about this exciting new development in the near future. Finally, on a more personal note: I continue to enjoy how much time I get to spend with my colleagues while we work toward a common goal. Collaborative ventures certainly pose some challenges that would be non-issues in individual project contexts, but I think we’re all benefiting from learning how to work not only with digital tools but also with one another. The Fellows’ lounge was busy busy busy today–and that’s the way we like it!"},{"id":"2012-02-02-collaborative-mentoring-at-ut-and-uva-co-developing-an-updated-teidisplay-for-omeka","title":"Collaborative mentoring at UT & UVa: co-developing an updated TEIDisplay for Omeka","author":"tanya-clement","date":"2012-02-02 05:43:04 -0500","categories":["Announcements","Digital Humanities"],"url":"collaborative-mentoring-at-ut-and-uva-co-developing-an-updated-teidisplay-for-omeka","layout":"post","content":"In partial answer to Bethany ’s charge in her recent ProfHacker piece “ it starts on day one,” I’m very excited to introduce a cross-institutional effort  between the Scholars’ Lab and the School of Information at UT-Austin to mentor two UT graduate students in the iSchool as they work to develop a DH tool for the DH community. The project will have two corresponding parts based on the background and interest of the students. Zane Schwarzlose, whose background includes extensive experience in developing with PhP and JavaScript will work to enhance TEIDisplay, an Omeka plugin originally written by Ethan Gruber at the Scholars’ Lab, that allows users to upload and display searchable TEI texts within the Omeka environment. Carin Yavorcik, an emerging archivist, will create TEI templates as well as user documentation so that the new tool will be useful not only to the many cultural institutions that Omeka serves but also to instructors who are looking for an environment within which they can teach the integral ways in which a TEI text can function as a cross-platform representation of text. The collaboration makes sense on many levels, but here are two that surface readily: These are complex technologies that function in a complex social and cultural system. We can meet the development needs because we represent institutions with different institutional missions, different (though like-minded) communities, with different resources.  Our students, who will seek jobs in which they work collaboratively in different institutional missions, from the perspective of different (though like-minded) communities, with different resources, must be prepared to meet these challenges within a network of a the wider DH community. If we believe in a basic DH tenet that making is a theoretically framed activity that helps deepen our understanding of our cultural artifacts and our modes of knowledge production, we must instill, as Bethany so aptly articulates, ”a can-do, maker’s ethos” in students who will feel “ empowered to build and re-build  the systems in which they and future students will operate.” To further this cause, we must also instill a second basic DH tenet in our community of scholars, makers, and teachers: we must pool our resources, both technical and academic, and develop our technologies (such as the TEI and Omeka) and mentor our students, together. Both Carin and Zane will blog regularly in this space as the project develops. Onward ho, ya’ll."},{"id":"2012-02-06-an-update-to-teidisplay-for-omeka","title":"An Update to TEIDisplay for Omeka","author":"carin-yavorcik","date":"2012-02-06 06:38:08 -0500","categories":["Digital Humanities"],"url":"an-update-to-teidisplay-for-omeka","layout":"post","content":"This spring, my colleague Zane Schwarzlose and I are working on an update to the TEIDisplay plugin for Omeka, developed by Ethan Gruber at the Scholars’ Lab. While it’s a great tool, it was developed as part of previous versions of Omeka. Even then, it was at times difficult to use, and some TEI elements did not render correctly. We’re hoping to update the plugin and iron out some of those bugs as we progress. My first experience using the plugin was last year for an Introduction to Digital Humanities course at UT-Austin’s School of Information taught by Tanya Clement (also our advisor for this project – she goes into more detail about the project background here ). Dr. Clement asked us to create an Omeka exhibit for a set of correspondences, including images of the original letters as well as TEI documents of marked-up content. It sounds simple enough, but there was a lot of frustrated graduate student Facebook posting going on in the days before the project was due as people ran into unexpected technical difficulties. Overall, there seem to be two main problems with the plugin. For the majority of the documents, we couldn’t get them to format at all – the entire body of text would wind up in one large paragraph in miniscule font at the top of the page. In a few other cases, the document would format enough to be readable, but it wouldn’t pick up the finer details of the TEI tags. For example, &lt;p&gt; tags would result in paragraph breaks, but and elements wouldn’t display– so something like “ sndstorms sandstorms ” would show up as “sndstormssandstorms” in the body of the text. Though this seems like a relatively easy XSLT fix, we imagine other fixes (even possibly this one) may not be so straightforward. Throughout the project, Zane and I will be documenting our work here at the Scholars’ Lab blog. At the end of the semester, we hope to provide not only an update to the plugin, but also user documentation, including use case scenarios and templates. We’re going to be seeking input over the next few weeks on how this plugin would work in an ideal world, and we need your help! Look for a more formal survey soon, but in the meantime, let us know your thoughts in the comments – what are you looking for in a simple, out-of-the-box TEI display tool?"},{"id":"2012-02-06-teaching-prism-how-to-speak-spanish-and-french","title":"teaching prism how to speak spanish and french","author":"alex-gil","date":"2012-02-06 05:48:10 -0500","categories":["Grad Student Research"],"url":"teaching-prism-how-to-speak-spanish-and-french","layout":"post","content":"Well, it looks like me and Annie will go on different development branches for a few weeks. I have been assigned the noble task of teaching Prism how to speak Spanish and French. The goal is to give the tool the largest possible reach on day one. Internationalization has always been one of my hot-button issues for a while, given that I hail from and engage with the global south, where the digital divide and the digital humanities divide is more visible. So far the tool I’m working with is the I18n gem which comes bundled with rails. The tool will make it easy for us to translate the interface. I18n so far seems very straightforward. I am creating YAML documents for each language. The YAML documents assign keys or placeholder names (kind of like variables) to all the strings that need to be translated in the views. Instead of writing strings directly in the views, we will write the placeholders. Depending on the locale that is selected, the placeholders will fetch the appropriate strings from the YAML files. Pretty simple! [crosses fingers nervously]. Besides the ability to abstract strings from their context, I18n also takes care of date and currency formats. Not that we will need the latter, though it’s nice to know we could make it all about the pesos if we wanted to. Adding sample texts in French and Spanish remains a decision for the future, given our time constraints. I’m working as fast as I can to develop the i18n functionality, so that we do have time, but there is no guarantee. Stay tuned!"},{"id":"2012-02-15-all-hail-the-workplan-accountability-and-collaborative-research","title":"All Hail the Workplan!: Accountability and Collaborative Research","author":"brooke-lestock","date":"2012-02-15 08:22:59 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"all-hail-the-workplan-accountability-and-collaborative-research","layout":"post","content":"In our first official meeting as Project Managers, Sarah and I drew a line on the whiteboard that had two distinct endpoints - a start and a finish. The start point was that day’s date, January 17th, and the endpoint would be whatever date we could get the group to agree on for Prism’s launch. After briefing the group in our project management crash course on the elements of a good workplan, I grabbed a marker, Sarah opened a Google doc, and the great experiment began. We worked backwards from the official release of Prism into the world, setting hard dates for the feature freeze and soft launch, and then drew in key points for each aspect of the project (when the highlighting function will be coded, when the pages will be designed, etc.). And so the workplan was born. The process took something like 90 minutes, but it might have been the most productive 90 minutes I’ve experienced in Praxis so far. Though Sarah and I have both freely admitted to our fetishization of deadlines, hard-and-fast deadlines seem to be rare in graduate school. It is not uncommon to ask for, and usually receive, extensions on research papers, even theses (though that involves quite a bit more hoop-jumping), and everyone knows at least one perpetually-dissertating Ph.D. I also don’t know many graduate students who can set rigid deadlines for themselves and adhere to them without fail.  I don’t mean to speak judgmentally of my own ilk, but as a species, humanities graduate students seem to be more comfortable with deadlines of a very flexible nature, and for good reason. Our research is often difficult to demarcate into stages, and hours of work are not easy to estimate because the material we examine is rarely quantitative. I say all this to preface my surprise at the ease in which the Praxis team was collectively able to create and agree on a workplan, as well as at the relief most team members expressed once tasks were clearly assigned to dates on the whiteboard and to people in GitHub, which is what we’ll be using for workflow management. The readings on project management repeatedly emphasized the value of allowing those responsible for certain aspects of the project to set their own deadlines, and so far Sarah and I have been the only ones to push back a deadline we’ve set (for writing the About page prose, no less, which you would think would be easy for English grad students). The team has consistently met their deadlines and each specialized group has met to set even more specific goals for their work. Prism is coming along, at warp speed! I’m beginning to see the efficacy of group workplans outside of Praxis, too, and this coincides with my earlier post about the possibilities for collaborative work available to graduate students who seek them. I am currently one of two representatives for Master’s students on the Graduate English Students Association board, and we’ve organized a thesis work group in response to a request at last semester’s department town hall meeting. I expected the group to be very lax, meeting biweekly to check in and sound off, but the group quickly decided to meet weekly, share work frequently, and set rigid deadlines; our task for this week’s meeting is to devise a thesis syllabus, a workplan for thesis research for the semester. I was not expecting this level of enthusiasm for deadlines and accountability from a room full of second-year MAs -  many of whom are distracted by job-hunting or, like me, the medieval torture chamber that is Ph.D. program admissions -  but even outside of DH projects, it seems that graduate students want  to work collaboratively because being accountable to a group, the joys and shames of having to report your successes and your failures to peers, can be a very powerful motivator."},{"id":"2012-02-19-commentary-on-migrating-an-omeka-site","title":"Commentary on Migrating an Omeka Site","author":"zane-schwarzlose","date":"2012-02-19 13:00:18 -0500","categories":["Digital Humanities","Research and Development"],"url":"commentary-on-migrating-an-omeka-site","layout":"post","content":"This week, for the TEIDisplay project, I migrated an existing Omeka installation to another server for Carin and me to use as our development sandbox. (We could have also locally installed an Omeka collection on our computers, but I wanted to make sure we were always wading through the same river.) I found some good documentation on moving installations of Omeka between servers, but I wanted to add some commentary to these instructions. Institutions might use these instructions and commentary when they wish to upgrade servers or change hosts for their Omeka sites. First, install a new version of Omeka on your new server. Carin and I needed to complete this step so we were sure we were using the latest version of Omeka. The steps for installing Omeka are well documented. I should note that several web hosting companies offer one-click installations of Omeka. This might a preferable option for information professionals lacking technical expertise. Second, export your existing database file. In a moment, we’re going to copy all of your files from your old server to your new server. Intuitively, you might think that the database file storing your collections would be moved along with those files. Unfortunately, that is not the case. Export your database file using these instructions. Please note: these instructions don’t remind you that you need to update the host, user name, password, and database name fields in the database file before importing it into your new server. Make these changes. Third, import your modified database into your new database on the new server. You can import these database commands from phpMyAdmin or the command line. It is important to note that if you don’t change some of the information described above, your tables will still get created. Things will appear to be happening. This is not the case.Your database will not be able to talk to your new server. Fourth, copy all of your files from your previous installation to a hard drive. Get the FTP information for your old server (host name, user, and password.) Use an FTP program to copy these files. Preserve your file structure. Highlight your entire file directory for the site and copy it while preserving the file structure. Moving the files might take some time. (6 hours in my case!) Much of this time is taken by the archival quality image files. Don’t worry. If your FTP program stops working in the middle of the transfer, don’t worry. You might have just disconnected from the Internet for a second. Alternately, some hosting companies throttle your connection if you’re taking up too much bandwidth. Just reconnect to your FTP program and restart the transfer where you started. Fifth, copy the files on your hard drive to your new server. Get the FTP information for your old server (host name, user, and password.) Highlight the entire file directory for the old site (stored on your hard drive) and copy it to the new site. Be sure to overwrite existing files and folders. You now have made a copy of an Omeka installation on a new server. Your user names and passwords will be the same as for the previous installation. Aside from reactivating plug ins and themes, the most prevalent problem I saw in this process were servers write-protecting the database file. This means that the file db.ini in the root directory will remain the default file despite you trying to overwrite it. There are many ways to fix this problem, but you can just delete the default db.ini file from your root directory and then transfer the db.ini from your hard drive."},{"id":"2012-02-21-teaching-coffeescript","title":"Teaching CoffeeScript","author":"wayne-graham","date":"2012-02-21 04:02:47 -0500","categories":["Grad Student Research","Research and Development"],"url":"teaching-coffeescript","layout":"post","content":"As the Prism project has progressed, one of the technologies we kept pushing off teaching was JavaScript. We knew this is one of the core languages that they would need to learn to actually work with the browser, but kept trying to determine the best way to actually introduce JavaScript in a way that would minimize bugs, and not discourage their development efforts. We had been working with the group as a whole introducing them to object oriented thinking with Ruby, but  recognized a huge context switch between writing in Ruby’s (and Rails)  object oriented  programming style to JavaScript’s prototype-based  style. One of the other issues we discussed was how to best teach “good” practices with JavaScript. The language constructs can get you in to a lot of hot water pretty quickly. You may have seen this, but a classic example of this is in the difference in the side Doug Crockford’s JavaScript: The Good Parts and David Flanagan’s JavaScript: The Definitive Guide . This is a bit of an absurd comparison, but it does highlight that there is a big discrepancy in what JavaScript is capable of and what it is actually good at. There are just some things that JavaScript implements in a weird way. A crazy example of this is something like the following: ``` console.log(++[[]][+[]]+[+[]]) ``` If you guessed this expression evaluates to 10, you are correct. There are a variety of other pain points that are common in JavaScript applications; case sensitivity, hard line breaks (they are interpreted as as line-ending semi-colons), extra commas ( IE has fits ), global scope creep (e.g. hoisting ), and issues with binding (e.g. a missing the this modifier). If you have developed much with JavaScript, you have most likely been bitten by several of these issues (hopefully not at the same time). As we started planning the project out, we noticed that @dhh  added support for CoffeeScript in the version of Rails (3.1+) we were targeting for the Praxis program. As part of the Asset Pipeline, Rails will automatically convert CoffeeScript to JavaScript, allowing you to write in a format that is closer to Ruby (with some pesky Python whitespace) that compiles to “good” JavaScript. Quite honestly, I wasn’t sure what to make of CoffeeScript when I first started looking at it. Unlike SASS which immediately changed the way I write CSS, I couldn’t imagine taking nearly a decade’s worth of knowledge for developing JavaScript for this crazy scripting language that just compiles down to what I’ve been writing, and I wasn’t alone with the other developers in the R&amp;D group. So what exactly was the turning point? Well, quite honestly after working with it a bit, I have to admit it has some nice features, but perhaps the biggest draw is the fact that it doesn’t require a big context switch in different modes of development. The other big draw is that the JavaScript that is generated is a better quality ( uses the module pattern ), has splats, comprehensions, takes care of scoping, the fat arrow (=&gt;) can evaluate the ‘ this ’ keyword properly for function binding, and the existential operator . There has been a lot of hype behind this, and Sam Stephenson (37Signals) made a great argument for using the language at the Future of Web Apps in London (2011). Better JS with CoffeeScript - Sam Stephenson (37signals) from Louise Morgan on Vimeo . Eric has walked the group through an  introduction to JavaScript and CoffeeScript, and they seemed receptive to the idea of a language that compiles to JavaScript. So far, nothing has been written for the project in CoffeeScript, but I’m cautiously optimistic that the JavaScript that will drive most of the interactions will be a bit less buggy (and faster to develop) than it would had we targeted JavaScript for this project. If you need even more incentive to take a look at CoffeeScript, be sure to take a look at Gary Bernhardt’s lightening talk from CodeMash 2012 entitled Wat ."},{"id":"2012-02-21-the-models-are-done","title":"The Models are Done!","author":"annie-swafford","date":"2012-02-21 09:23:49 -0500","categories":["Grad Student Research"],"url":"the-models-are-done","layout":"post","content":"Great news! All parts of our data model are now in Rails!  We used the Ruby gem Devise for the user model, and Prism now has user account capabilities (and the links for “sign in,” “sign out,” and “sign up” on the homepage)! The documents are also in the system and each has its own page, and we have also created the Markings, Facets, and Prism models so we’re now set up to start building the highlighting functionality.  All of our tests pass, and apparently 98% of our code has tests written for it (thanks to Wayne for incorporating the Ruby gem Simplecov to give us this information), so we’re in good shape.  After Eric’s helpful Coffee Scripting session, I’ve been watching lots of screencasts to make sure I feel comfortable writing Coffee Script myself. At this point, I know how to convert Javascript to Coffee Script, but I’m not adept at writing it myself from scratch.  I’ve been brainstorming algorithms for the highlighting recently, and I hope to start trying to program them over the next week.  I’ll write about my foray into Coffee Scripting land next week!"},{"id":"2012-02-22-how-do-you-display-tei-documents-online","title":"How do you display TEI documents online?","author":"carin-yavorcik","date":"2012-02-22 10:05:23 -0500","categories":["Digital Humanities"],"url":"how-do-you-display-tei-documents-online","layout":"post","content":"As you can see from Zane’s post a few days ago, we’ve been hard at work on our update to  TEIDisplay here at UT-Austin. While he’s been working to jive the plugin with the newest version of Omeka, I’ve been thinking more about how this tool will be used. While I’ve studied TEI and used it in a few distinct scenarios, I’m still a newcomer to the world of digital humanities. In order to make the best product possible, we need more feedback from our potential users – you! I’ve put up a post over at DH Answers to learn more about what TEI display tools (if any) people are using, and what features are most important. Please join the conversation by giving us your feedback!"},{"id":"2012-02-23-slightly-better-than-brain-dead","title":"Slightly Better than Brain Dead  ","author":"ed-triplett","date":"2012-02-23 08:24:06 -0500","categories":["Digital Humanities","Grad Student Research","Research and Development"],"url":"slightly-better-than-brain-dead","layout":"post","content":"This post was inevitable, but I can’t say I was ready for it. In exactly one week, I will be leaving for Spain on a two-month dissertation research trip. In relation to this blog, this means that my time in the Praxis Program has all but come to a close. As such, this seems like the best time to mercilessly cram my scattered experiences into a box marked “conclusion” and hope it doesn’t explode before I can tell the police that “I was in Barcelona at the time.” I had a moment after the winter break to reflect on how bizarre the Fall semester had been for me. Between my Scholar’s Lab fellowship, the Praxis Program, and my time just 20 vertical feet below the Scholar’s Lab conference room at IATH, my mind was kidnapped by digital humanities. I can think of no semester where I was tossed out of my wheelhouse on such a regular basis. When I returned to my rotating office at Alderman Library after an equally bizarre, warm winter in Fargo, ND my previous schedule resumed… but everything felt different. Two hulking objects loomed out of the fog: Spain and Prism. Predictably, Spain got raucous at night and gave me heartburn thinking about it. Fortunately, I could still join in on the practical attack on Prism with my fellow Praxis-ers during the day while Spain was taking a siesta. Our meetings were held in the same place each Tuesday, but in the second semester, It felt more like a boardroom than a classroom. For everyone in the program, a lot of the intellectual debates that stymied earlier meetings were dropped in favor of getting something done. Real due-dates meant real design and programming assignments. Of course those dates had a slightly different effect on me because I knew I would be gone by Feb. 29th, and I would miss a lot of the process leading up to the release of Prism. Nonetheless, I immediately decided that I needed to at least put one foot back in my wheelhouse if I was going to leave some tangible imprint on Prism. I therefore devoted the last three weeks to creating detailed “full color” wire-frames of Prism’s website using Adobe Illustrator. Like with the logo, I had some missteps and experiments along the way that I would not repeat in the future. At one point - in an ill-conceived attempt to heed the many notes about the site’s color scheme - I dove head-first into “design by committee” by asking the group to offer color-scheme samples. On a related note, Brooke recently reminded me of a saying that my Dad is fond of: “A camel is a horse designed by committee.” At the suggestion of several of the SLab staff, I happily chucked the idea and simply presented a triadic color scheme that had sufficient tonal contrast for the different highlighters, yet would not cause us to imagine a poor kid’s box of crayons. Like most design processes, the five pages I presented to the group yesterday would change a good bit if I had more time. I am reasonably happy with the header, and I believe the rotating backgrounds will be distinctive, but I would love to spend some more time working on the site’s buttons and text boxes. I’d also love to help Lindsay and the others on the design team transform the static illustrator file into a functional page in HTML and CSS. These reservations are true for the entire process. I am disappointed not to be able to see how the data will be visualized while I am gone, and I would have liked to see first hand how Ruby, Rails, CSS, HTML, Coffeescript, and YML fit together. I never even got a chance to sneak some Hungarian erotic poetry after the “en” in Alex’s YML file just to see what he would do with it. Oh well. It is probably better this way because I think Spain is waking from its siesta. I’m still juggling several responsibilities right now, but I feel like one my four tennis balls has been replaced with an anvil stamped “Hecho en España.” Prism is in good hands, and I think my brain is beginning to show 505 errors anyway. As two members of Prism graciously tweeted yesterday, while trying to explain a visualization idea I had, I confidently uttered the phrase “That word will be the size of Yellow!” I think it is time for me to re-introduce both hemispheres of my brain to each other and focus on Spain. Finally, as some of you were around to hear, I am leaving on a high note - yesterday Bethany referred to my “size of yellow” idea as the “slightly less brain-dead” of three possible options! You hear that? Ba-BAM! Incidentally, I am going to try to blog as I travel to 20 different fortresses, monasteries and medieval towns from Catalonia to Extremadura during March and April. The extremely humble URL is here: http://www.edwardtriplett.com/"},{"id":"2012-02-28-hwaet","title":"Hwaet!","author":"eric-johnson","date":"2012-02-28 05:03:43 -0500","categories":["Announcements"],"url":"hwaet","layout":"post","content":"Researchers here at the Scholars’ Lab recently unearthed a manuscript fragment, dating roughly to the 9th or 10 centuries CE, which we here publish for you in rough translation.  It seems to address preparation for some sort of epic conflict. Lo! By the bright-shining beach at Alderman-sea A clarion-call crested the coursing ocean-horses From Beþienni, duty-drover, DH-thane, To defeat the doughty foe, the Day-table, Age-breaker, stress-maker, old-conductor. Constant companions cry-heeded and came, Troop-leaders the two, trusted and able, Gold-bearded and glass-eyed, girded and battle-wise. First the sunset-settled, steel-structured Waenbatte, code-conqueror and calculator-kin. His double, dawn-dweller Aeric, Blue-eyed bridge-builder and feeling-hugger, Schedule-sack on his shoulders not over-full With kinder-kith of the calendar-foe: A mead-room of the mind, moving-chaired and floating-floored, A learning-place for library-kind. Also a seeker-space to supply apt answers To aid the askers of helping-queries. Also the thinking-plan of the tinker-space, Growing to a gregarious maker-hall Subject of singers’ songs and far-flung fame. Also signal fires for SLab-friends, Taken with our teaching-times . . . And here the fragment ends.  But we continue to comb the shelves of SLab Special Collections to see if other pieces of the manuscript might yet be found."},{"id":"2012-03-02-customizing-bash","title":"Customizing Bash","author":"wayne-graham","date":"2012-03-02 06:18:36 -0500","categories":["Research and Development"],"url":"customizing-bash","layout":"post","content":"I spend a lot of time every day looking at a terminal window, and over the last decade I had been tweaking my bash profile to make the terminal act, and look, the way I wanted it to. As a systems administrator in a former life, I had collected a bunch of “useful” scripts that would help me work on a variety of operating systems, from Solaris, to AIX, to SGI,  as well as various flavors of Linux ( CentOS, Fedora, Ubuntu, SUSE, Gentoo, etc.). I had aliases to commands (and my common typos) to log on to servers, control various services in my local development environment, and override common commands I typed all the time (e.g. ss for rails server) in my .bashrc which was symlinked to my .bash_profile due to an OS X quirk. This has resulted in a really long bashrc file (nearly 2000 lines long), and I wanted to take some time to clean things and get rid of a lot of the legacy cruft that was in that file. In my personal workflow, I try to stay in the terminal as much as I can, finding the cognitive shift from a text-based environment to a GUI rather jarring. Imagine that; I connect my MacBook Pro with its two graphics cards to a 24” monitor, yet most of my “work” is done in a terminal (I do use the graphic card for its GPU  regularly though). Since I spend so much time in the terminal, I not only wanted to get rid of the cruft, I wanted to do a bit more to make the environment look as good as this computer performs. I had been intrigued by the oh-my-zsh  approach of bundling some themes and plugins. While I have used Zshell in the past, I never quite found the tab-completion, auto-correction, or the improvements to the scripting language compelling enough to make the move from Bash . However, being able to share a history across sessions and the built-in pager and globbing features almost got me there! Apparently other folks were in the same boat I was in. I ran across the bash-it project a few weeks ago and finally had an opportunity recently to try this out. Basically what this project does is provide a framework for you to build your own themes, plugins, and aliases for your own environment. The installation is straight-forward on OS X (and every Linux box I’ve tried this on): ``` git clone http://github.com/revans/bash-it.git ~/.bash_it\n~/.bash_it/install.sh ``` These two lines clone the bash-it repository to a hidden directory (.bash_it) for your user account (/Users/[your user name]/.bash_it) then launches the installer script. Using hidden directories (.bash_it) hides the directory from Finder, but still gives you access to the directory with the Terminal app. The installation script backs up your current ~/.bash_profile file (if you have made any adjustments to it), then prompts you for the features you want. I answered ‘some’ to most of the questions in the installation script to choose which plugins and aliases you would like to enable. I don’t use emacs or nginx on a regular basis, and use rvm over rbenv, and it turns out the xterm  plugin causes some issues on OS X, so I left those out. Themes The default theme is named Bobby and looks really nice. It uses solarized colors and had one feature I really like: multiline feedback. On one line, I know which Ruby version I have active in RVM, the server I’m on, and where I am on the system. On the second line, I know which git branch I’m on, and if there are uncommitted changes (red x if there are changes, green check if everything is committed). Aliases I had quite a few aliases in my .bash_profile that I had created over the years. There were, however, some nice additional aliases included in the bash-it package to shorten some typing of commands. They included some common mispellings (yes, that was on purpose); most of the git aliases I had been using anyway, some useful aliases for heroku, homebrew (yeah bup), for opening various applications like Firefox, Photoshop, and Chromium, and some nice features for todo-txt . Fonts This isn’t something that the bash-it library deals with, but important in customizing the experience. The default font for the Terminal in OS X is Menlo . It’s a fine system font, but can get a little difficult to read at the distance I sit from my monitor. A really popular font for developers is Inconsolata . I changed my font to Inconsolata 14pt, and it has a very nice look to it. Terminal In Snow Leopard, the Terminal app doesn’t support 256-bit colors. Apple has updated Terminal  in Lion to support this, but I have not yet upgraded to Lion. Quite honestly, the Lion machines I have dealt with have had ‘issues’ getting the tools I use on a regular basis installed, and I have just not had the time to deal with upgrading yet (most of the issues involve issues with XCode’s removal of gcc in favor of llvm . There is a good work around, and I believe the issue has been resolved in recent updates to XCode). There is, however, an awesome Terminal app replacement named  iTerm2 that has a lot of the features I want; 256-bit color support, full-screen mode, ability to split the screens, and hot keys (please don’t make me click when I can type). After installing iTerm2, I can now run the tests for prism  and get all the NyanCat rainbow awesomeness. After updating Terminal, installing the bash-it themes and plugins, and getting a ‘better’ font on my machine (and adding a few aliases back in to the ~/.bash_profile to log on to some servers), I used scp to push these files to the various server environments I work on (replace user and server): ``` scp -r ~/.bash_it user@server:~/ scp ~/.bash_profile user@server:~/.bash_profile ``` Overall, I’ve been happy with the move to this setup. It plays nicely with some of the other cool things I use (e.g.  pianobar, tmux, and vim ). With some cleaver key bindings, and a transparent terminal, I can actually change something on my screen and see the update in the browser without changing programs, Next up? I think I may write either an rsync script that will push any local changes to the various servers I use, or maybe even use the Dropbox client on Linux to symlink these files in, ensuring as soon as I make a change on my local development environment, they will be on the remote systems as well."},{"id":"2012-03-08-scholars-lab-speaker-kathleen-fitzpatrick","title":"Scholars' Lab Speaker: Kathleen Fitzpatrick","author":"ronda-grizzle","date":"2012-03-08 11:04:30 -0500","categories":["Announcements"],"url":"scholars-lab-speaker-kathleen-fitzpatrick","layout":"post","content":"We are pleased to welcome Dr. Kathleen Fitzpatrick to the Scholars’ Lab on Thursday, March 15 at 2:00 p.m. for the second event in our Spring 2012 Speaker Series. What if the academic monograph is a dying form? If scholarly communication is to have a future, it’s clear that it lies online, and yet the most significant obstacles to such a transformation are not technological, but instead social and institutional. How must the academy and the scholars that comprise it change their ways of thinking in order for digital scholarly publishing to become a viable alternative to the university press book? Dr. Fitzpatrick’s talk, entitled “Planned Obsolescence: Publishing, Technology, and the Future of the Academy,” will explore some of those changes and their implications for our lives as scholars and our work within universities. A long time friend of the Scholars’ Lab, Dr. Fitzpatrick is Director of Scholarly Communication of the Modern Language Association, and Professor of Media Studies (on leave), Pomona College. She is author of Planned Obsolescence: Publishing, Technology, and the Future of the Academy, published in 2011 by NYU Press and previously made available for open peer review online ( http://mediacommons.futureofthebook.org/mcpress/plannedobsolescence ), and of The Anxiety of Obsolescence: The American Novel in the Age of Television, published in 2006 by Vanderbilt University Press. She is co-founder of the digital scholarly network MediaCommons ( http://mediacommons.futureofthebook.org ), and has published articles and notes in journals including the Journal of Electronic Publishing, PMLA, Contemporary Literature, and Cinema Journal ."},{"id":"2012-03-12-seeking-praxis-program-fellows","title":"Seeking new Praxis Program fellows!","author":"bethany-nowviskie","date":"2012-03-12 09:06:49 -0400","categories":["Announcements","Grad Student Research"],"url":"seeking-praxis-program-fellows","layout":"post","content":"Last August, we announced a new, pilot initiative for the Scholars’ Lab – the creation of a Praxis Program, which would offer hands-on training to six UVa graduate students in digital humanities project creation, taking them from conception to software and interface design and development, through to deployment, communications, and analysis. Praxis is a unique training program in the international digital humanities community. The most notable thing about it was our plan to involve students as a team – a cohort, who would intern with Scholars’ Lab faculty and staff, blog about their experiences, and work to develop the so-called “softer skills” of collaboration and project management even as they tackled (most for the first time) new programming languages, software tools, and digital methods. The 2011-12 Praxis cohort is in full swing, and – thanks to the generosity of the Andrew W. Mellon Foundation and the support of UVa Library’s Scholarly Communication Institute (SCI) – we are able to undertake a second pilot year of this innovative program, even while we work to explore the creation of an international Praxis Network, involving like-minded initatives that address the changing needs of humanities graduate students in the digital age. (The program itself and its SCI context are are the subject of two recent Inside UVa articles: Praxis Program Gives Future Scholars Crash Course in Digital Humanities Skills and Mellon Grant Extension Boosts Digital Humanities Graduate Training Program .) Six new Praxis students will begin working with us in late August. Each will be awarded $8000 in fellowship funds to offset the commitment of time (generally about 10 hours per week, through May 2013) they will make to living, learning, and building a collaborative digital humanities project in the Scholars’ Lab. All University of Virginia graduate students working in or committed to humanities disciplines are eligible to apply to join the 2012-13 cohort. We particularly encourage applications from women, LGBT students, and people of color, and will be working to put together an interdisciplinary and intellectually diverse team. Luncheon Q&amp;A, Wednesday, March 21st Please join Scholars’ Lab staff and current Praxis Fellows on Wednesday, March 21st for a Q&amp;A session on the program, to be held in the Scholars’ Lab at 12:00 p.m. Lunch will be provided, so RSVP to Ronda Grizzle, Outreach &amp; Training Specialist (rag9b at virginia dot edu) by noon on Monday, March 19. If you’re unable to join us for lunch but have questions about the program, don’t hesitate to be in touch with Eric Johnson, Head of Outreach &amp; Public Services (ej9k at virginia dot edu). Application deadline : Sunday, March 25th The application process is simple: direct an email to Eric Johnson at the address above. Please indicate why you’re interested in the Praxis Program, what you think you will gain from it, and what you feel you would bring to a collaborative digital humanities project. A small committee, consisting of Scholars’ Lab faculty and staff and members of the 2012-13 Praxis cohort, will evaluate expressions of interest and schedule group or individual interviews with finalists."},{"id":"2012-03-12-through-another-prism","title":"through another prism","author":"alex-gil","date":"2012-03-12 17:56:14 -0400","categories":["Grad Student Research","Research and Development"],"url":"through-another-prism","layout":"post","content":"A couple of weeks ago Suzanne Keen and Alison Booth offered a workshop at the Scholars’ Lab . The workshop was an introduction to BESS (Biographical Elements and Structure Schema), “an XML standoff markup schema designed at IATH as part of Professor Booth’s IATH Fellowship to analyze narrative structure.” If you recall from Bethany’s introduction, the original idea for Prism was inspired in part by recent discussions with Alison about her project. The workshop/talk was divided into two parts, each related to our Prism in one way or another. In the first part, we were broken up into four groups, each with a small biography of a woman from Collective Biographies of Women . The biographies were printed on copy paper with each paragraph clearly enumerated. Our task was to work as a team to assign one or more of six basic narratological categories separating the Story from the Narrative to each paragraph. The categories roughly corresponded to how one would linearize a person’s life: a)Before they lived, b) The beginning, c)The middle (career period), d) A climax (some kind of breakthrough), e) The end and, of course, d) The post-life. Me and Bethany ended up in the same team, the Sister Dora team. (Sister Dora, if you haven’t heard about her, was the nurse in whose arms you probably would’ve died in the 19th Century).  We were struck right away by the differences from our prism exercises. Not only were we assigning categories by paragraphs, we had to come to consensus as teams! The team consensus gives the exercise a much different dynamic as you can imagine. The second part of the exercise was closer to our model. Each of us was given four paragraphs in a sheet of paper, a transparency and four markers. Sound familiar? We each could only pick one category out of four. I remember I chose figures of speech . I don’t remember the other three categories, but I do remember being surprised to find only a very small number of figures of speech in the sample text. Again, this exercise was very different than our prism. While we give users access to all categories, in this case users are only allowed one category. After we were all done marking up the texts, Ms. Keen collected the transparencies. At this point she started doing something really neat which I think we are going to emulate in our visualization. She laid one transparency on the projector. She then laid another one on top of the first, ever so slowly. The ‘animation’ effect was very cool to watch, as all the different transparencies started shifting the direction and overall effect of the whole. After the workshop we had time to talk about the goal of the BESS project and how it relates to the transparency and team exercises. BESS consists of a standard set of narratological ‘tags’ for marking up biographies of women. The list Prof. Booth showed us was very long. Teams of students are assigned the task of marking up the texts using these categories. Of course, there will be much disagreement, but the goal is to find those places were different encoders agree as a tool to help the editors decide what the ‘final’ markup should be. This is a world apart from our goals, but does provide another possible use case for the tool we are actually building."},{"id":"2012-03-12-who-says-i-like-right-angles","title":"Who says I like right angles?","author":"lindsay-oconnor","date":"2012-03-12 14:54:45 -0400","categories":["Grad Student Research","Research and Development"],"url":"who-says-i-like-right-angles","layout":"post","content":"Last week, I started tackling what I naively assume to be some CSS issues that “real” web designers might also see as challenges.  The “box model” in CSS allows for lots of “clean” designs, but it discriminates against non-quadrilateral polygons, and against angles other than 90 degrees, for that matter. When a prism is a guiding concept and image for your design, you probably don’t just want a bunch of rectangles, and accordingly, Ed’s designs for the Prism site involve a big “prism” in the background and a number of boxes and links that invoke the prism shape with their borderlines. So, with Ani DiFranco’s voice in my head, I asked the internet what to do. A few online sources for web design advice suggested creating angles and arrow shapes with empty divs with thick borders and no width and height, so I gave that a try today. It seemed strange when I first looked at it, but it turned out to be simple enough. In what appears above, I tinkered with the border size of two empty spans (I used spans instead of divs since they happen to fall inside of list items on this page) until they lined up with the borders on the “highlight” and “visualize” links. I assigned each span a class and styled them like this: .arrow-right { float: left; width: 0; height: 0; border-top: 11px solid transparent; border-bottom: 11px solid transparent; border-left: 22px solid #C0C0C0; } arrow-inset { float: left; width: 0; height: 0; border-top: 11px solid #C0C0C0; border-bottom: 11px solid #C0C0C0; border-left: 22px solid transparent; } Of course, there are a few big limitations here. For one, these spans have such a precise, absolute size that resizing the browser window even the slightest bit will reveal how messy of a solution this really is. And as Jeremy pointed out, the link ends at the edge of its rectangular container, so clicking in the “arrow” that seems attached to it won’t take you anywhere. To solve these problems, Jeremy suggested that I create a background image of an angle for each of the links. It will be a scalable part of the link rather than an object next to it, so the design will look better in different window-sizes and the interface will be more logical and easier to use. Figuring this out will also help me as I work on the large prism shape at the top of the page since it will also need an absolute position behind other elements. I’ve also been working on gradients in CSS, specifically on how best to layer gradients to produce the effects in Ed’s header design.  More on the header next time."},{"id":"2012-03-13-gis-day-2011-lightning-rounds","title":"GIS Day 2011 Lightning Rounds","author":"ronda-grizzle","date":"2012-03-13 11:12:48 -0400","categories":["Podcasts"],"url":"gis-day-2011-lightning-rounds","layout":"post","content":"GIS Day 2011 Lightning Rounds On November 16, 2011, the SLab welcomed a diverse group of speakers presenting lightning round talks as part of our GIS Day celebrations. Eighteen of those speakers agreed to have their talks appear on our podcast, which we’re pleased to present. Speakers: Kelly Johnston, UVa Scholars’ Lab Bill Ferster, UVa SHANTI Simona Babiceanu, UVa Civil &amp; Environmental Engineering Paolo Tovar, Apex Wind Energy Tim Morton, UVa Library Government Documents Randi Lewis, UVa Department of History (and Scholar’s Lab Graduate Fellow!) David McClure, UVa Scholars’ Lab Charles Kromkowski, UVa Department of Politics John Scrivani, Virginia Geographic Information Network Ed Triplett, UVa Architectural History (and Scholars’ Lab Graduate Fellow!) Todd Wascher, Applied Data Consultants, Inc. Kelly Clifton and Larry Buckner, UVa Department of Politics Bill Palmer, UVa Office of the Architect Guoping Huang, UVa Urban and Environmental Planning Wayne Graham, UVa Scholars’ Lab Pam DeGuzman, UVa Nursing Chris Gist, UVa Scholars’ Lab As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.13992627191/enclosure.mp3”]"},{"id":"2012-03-13-lets-get-visual","title":"Let's get visual. ","author":"brooke-lestock","date":"2012-03-13 14:04:10 -0400","categories":["Digital Humanities","Grad Student Research","Research and Development","Visualization and Data Mining"],"url":"lets-get-visual","layout":"post","content":"I am aware of how ridiculous the title of this post is, but I’ll gloss it by saying that visualizations have been a hot button issue in our recent Praxis talks, and in my opinion, they’re by far the “sexiest” element of Prism. After all, the viz page is where the magic happens. That being said, as our deadlines become more and more imminent and ominous, we are constantly running into that pesky thing called reality, and it recently even dared to interfere with our precious visualizations. At our 2/21 meeting, we brainstormed the kinds of visualizations we would like Prism to create and narrowed the list down to four possible types: A heat map, which would be closest to the transparency exercise in that it resembles the layering of many transparencies, but presents some problems, such as how to represent something legible and provocative with muddled colors. Quantitative visualizations, like bar graphs, charts, or tables, which we all agreed would be interesting, but there were some reservations about being that quantitative in our first visualization. We are humanists, after all. A zoomed-out, kind of macro view visualization, and here we used Ben Fry’s “On the Origin of Species: The Preservation of Favoured Traces” as a model. The most compelling aspects of this are its easy navigation of both “distant” and “close” readings, and as Alex mentioned in his post, the possibility for animating the layering of readings so that it resembles the process of compiling transparencies one by one. The last is a kind of deformed reading resembling a word cloud, in which the font size of a word would change based on how often it was selected in a particular color, but the structure of the text would remain intact - that is, the words would not be processed into a cloud shape, but rather stay in the same order on the page. David McClure was the creative mind behind this idea and we collectively agreed that this  visualization was our first choice for Prism. The original idea for the “deformed reading” visualization (we really need to come up with a better name for this…Prism-enstein?) involved layering each deformed reading in each color to represent all of the highlighter colors in one visualization while somehow being offset enough to remain legible and coherent. Then the reality check came, at last week’s meeting. With the deadlines fast approaching, the visualization we chose was just too ambitious. So today we sat down and hashed it out - we revisited the workplan, which we established at the beginning of this semester, and the essential requirements, the goals we set for Prism all the way back in September, to determine what we can realistically accomplish in the time left that would meet our original requirements for a successful project. The group agreed that having Prism produce one visualization is necessary, so we came to a compromise with reality. We opted for a somewhat simpler version of the chosen visualization and discussed how we can gesture towards future possibilities for Prism visualization. I’m hoping our compromise is enough to appease reality for a little while, because when all’s said and done, what’s a prism without its rainbow? Just a clear pyramid…then again, I guess building a pyramid is pretty impressive, too."},{"id":"2012-03-13-narrative-form-and-digital-tools","title":"Narrative Form and Digital Tools","author":"ronda-grizzle","date":"2012-03-13 11:27:22 -0400","categories":["Podcasts"],"url":"narrative-form-and-digital-tools","layout":"post","content":"Suzanne Keen &amp; Alison Booth: Narrative Form and Digital Tools Workshop The Scholars’ Lab welcomed Dr. Suzanne Keen, Thomas H. Broadus Professor and Chair of English at Washington and Lee University, and Dr. Alison Booth, Professor of English at U.Va., on February 24, 2012 for the presentation of their workshop “Narrative Form and Digital Tools.” Workshop Abstract:\n“This workshop suggests that teams of humans can be trained to analyze narrative structure, rhetoric, and other genre conventions using controlled vocabularies—a longstanding dream of narratology. Drs. Keen and Booth will introduce BESS (Biographies Elements and Structure Schema), an XML standoff markup schema designed to analyze narrative structure in short biographies, and will offer examples of its application as a tool for interpreting narratives in large archives, within social networks, and beyond traditional formalism.” As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.14016483491/enclosure.mp3”]"},{"id":"2012-03-19-a-cukeybara-sandwich","title":"a cukeybara sandwich","author":"alex-gil","date":"2012-03-19 10:55:16 -0400","categories":["Grad Student Research"],"url":"a-cukeybara-sandwich","layout":"post","content":"During Spring Break I started working on integration tests for Prism. An integration test is a bit more involved than the regular unit tests we’ve been doing with the RSpec framework so far. An integration test seeks to test the software by imitating a typical user interacting with it. In particular, we wanted to make sure that all the different translations from the i18n process were working. In order to get it right, we needed to emulate a user visiting the homepage in different languages. For Rails development, the tools of choice for this kind of testing are Cucumber and Capybara . Cucumber is a language, a framework and a workflow if I understand it correctly. It is much more human readable than RSpec (even though it borrows some RSpec for some tasks), which makes ideal to share with non-developers in the team. Here, for example, is the .feature file I created to test i18n: Feature: Check language\n  In order to test the selected language is right\n  As a cosmopolitan developer\n  I want to make sure the right words are present in the header\n\n  Scenario: English pages\n    When I visit the \"English\" homepage\n    Then the header should have the words 'Prism is'\n\n  Scenario: French pages\n    When I visit the \"French\" homepage\n    Then the header should have the words 'Prism est'\n\n  Scenario: Spanish pages\n    When I visit the \"Spanish\" homepage\n    Then the header should have the words 'Prism es'\n\n  Scenario: German pages\n    When I visit the \"German\" homepage\n    Then the header should have the words 'Prism ist' Once a feature file has been created, the next step is to ‘translate’ the natural language into ‘step definitions.’ Line by line, we convert the .feature file to Ruby… actually, to Capybara. Capybara is a Ruby gem that helps us imitate the behavior of users visiting a page by sending GET and POST requests if I understand it correctly. Below is our sample .rb ‘step definition’ file translating the original feature into ruby actions. When /^I visit the \"([^\"]*)\" homepage$/ do |lang|\n  if lang == 'French'\n    locale = 'fr'\n  elsif lang == 'English'\n    locale = 'en'\n  elsif lang == 'Spanish'\n    locale = 'es'\n  elsif lang == 'German'\n    locale = 'de'\n  end\n\n  visit('/' + \"?locale=\" + locale)\nend\n\nThen /^the header should have the words ([a-z]*|[A-Z]*)/ do |desc|\n  page.should have_content(desc)\nend Once the feature descriptions and the step definitions are in place, we run the test by simply invoking cucumber from the command line. I am happy to announce that after running the tests above, Prism is in the GREEN. I repeat, Prism is in the GREEN."},{"id":"2012-03-19-wordpress-pharma-hack","title":"WordPress Pharma Hack","author":"wayne-graham","date":"2012-03-19 06:04:42 -0400","categories":["Research and Development"],"url":"wordpress-pharma-hack","layout":"post","content":"A few weeks ago I was walking to my car and was copied on an email from the director of the IT department for the UVa Library: It appears that website redacted has been commandeered…hacked.. Beware if you use windows as there is a malware service running…. Just the kind of email you love to get! By the time I got home (takes me about an hour and a half), there had been a flurry of emails between the company that had a support contract for the site, and various people who were reacting. By the time I actually had a chance to sit down, the site URL had redirected the site to UVa’s home page as a stop-gap measure to not expose the site’s users to malware. I started taking a look at what had happened and apparently the hackers had exploited Wordpress by posting a comment in such a way that Wordpress wrote a file to the filesystem, when then injected code (in base64) everywhere it found the string ‘ &lt;?php ’. Ah yes, pharma hack. It would be brilliant if it weren’t so diabolical. Some clarification on what is actually going on here. There are a few levels of compromise that systems administrators worry about; web defacement and server compromise. The latter is really serious, but outward facing defacement is really really the Internet equivalent of someone tagging your fence. What these hacks are designed to do is abscond with your Internet search results. This is quite a different attack vector than say brute-forcing passwords on a Wordpress site. If you know a little about what you’re doing, this is actually pretty straight forward. In fact, you can script these things pretty easily; this example was written by a hacker over a weekend: With hacks like this, attackers usually have much darker intentions (e.g. there’s a high probability you use the same password for Work/Facebook/Twitter/Google as you do for your Wordpress). Let me take a second here to briefly pause say that I’m not trying to scare anyone in to shutting down their WordPress site. I will, however, say that annoyances like these go along with the maintaining a WordPress site; sooner or later, you will have to deal with with the fact that some script kiddy found an attack vector to leverage your WordPress site for their purposes. WordPress a great tool, lots of people use it, but there are a lot of moving parts in the platform, and there are always ‘issues’ that pop up (issues in PHP itself, plugins, themes, etc.). If you are really concerned about this, you may like to take a look at an alternative like Jekyll ; it’s much harder to inject content in to static files than it is a dynamic framework based on PHP (or Ruby, Python, etc.). Getting back to what was going on with our compromised site, I considered what I saw as a “those damn kids” attack, and not actually commandeering the site . I then set about cleaning up the Wordpress installation to move it to a server environment that I controlled and could make sure that the server had some elements to make this style of attack just a little harder to do. As a former systems administrator who used to actually read the daily security bulletins put out by Windows, the various Linux distributions, my approach to these recovery is a little more dramatic than what various blogs suggest  to clean up. If we’re using the tagging-the-fence metaphor, these posts show you how to paint over the graffiti. Since hackers can be clever little buggers, and I’ve found on at least one occasion that the hackers left something crazy and ‘simple’ to fix to mask what they were really doing (that time it was setting up a store-and-forward server for pornography), I adopted the equivalent of burning down the fence and building a new one when I see this happen. Caveat: I spend almost all day in a terminal, so the code I show to back-up and massage the code is going to use the terminal. If this really isn’t your cup-of-tea, there are generally GUI tools that will do the same (e.g. PHPMyAdmin ) which, if you are on a hosted environment, most likely this tool is there. I just say this in case you are really not comfortable (or have shell access to the server) that you can use the same techniques, but use the tools available. The basic workflow I was using was to get all the data out the MySQL server, and the WordPress theme that was ‘infected’ on to my local machine, download a fresh version of WordPress, import the WordPress data, reinstall any needed plugins, and the theme the site was using. For the particular hack in question, I actually had remote access (on a non-default port) to the MySQL server, so I could actually pull the data directly from the machine to my computer ( as a gist ) mysqldump -h [mysql server] --port=[port number] -u [username] -p [database name] | gzip -c | cat ~/Desktop/`date +%Y-%m-%d-%T`.sql.gz If you aren’t comfortable with a Terminal commands, this can be a little scary. Basically this is a set of chained commands ( Pipes and Redirects ) that dump out the entire database ( mysqldump ), compress the output ( gzip ), and write the compressed output to a file on my Desktop with a current timestamp ( cat ). As an example, my output file with all of the WordPress data was named 2012-02-28:20:40:02.sql.gz. Now to download the latest version of Wordpress and set up a database named hack and inject the data from the remote server in to the newly created database. ``` gunzip ~/Desktop/2012-02-28:20:40:02.sql.gz mkdir -p ~/public_html/hack &amp;&amp; cd ~/public_html/hack curl -O http://wordpress.org/latest.tar.gz tar xvf latest.tar.gz mysqladmin create hacked mysql -u [local mysql root] -p hacked &lt; ~/Desktop/2012-02-28:20:40:02.sql ``` There is a lot here, and worth explaining briefly. These commands Decompress the SQL file from the server that sits on the Desktop; Create a new directory in the user’s public_html directory named hack (creating public_html/hack if it does not already exist); Changes the terminal in to the newly created directory (you can chain multiple commands together with the &amp;&amp; operator) Downloads the latest version of WordPress with cURL ; Creates a new database on you local machine named “hacked” using the mysqladmin command; Connects to the local MySQL server and inserts the contents of the SQL file generated earlier If you are running some other type of system, you may need to change the location of where you put WordPress, but the rest should be pretty much the same. You can now log on to the WordPress system locally (e.g. http://localhost/~wsg4w/hacked/wordpress/wp-admin/) and reinstall your plugins. The one thing that will take a little bit of effort would be cleaning up the theme you are using if  you have made any significant changes to a theme (and not it in a revision control system ), you will need to clean out the hack additions. find ./ -name \"*.php\" -type f |  xargs sed -i 's#&lt;?php /**/ eval(base64_decode(\"aWY.*?&gt;##g' 2&gt;&amp;1\nfind ./ -name \"*.php\" -type f |  xargs sed -i '/./,$!d' 2&gt;&amp;1 These lines use the find command to find PHP files and then use  sed  to remove any of the crazy base64_decode from your files.  If you don’t already have your theme in an SCM, this is a good time. Github is a good option, but if you want a private repository, you can also check out Bitbucket . This will make things easier in the future… After you have everything cleaned up on your local machine, it’s time to push it back up to the server. There are a few ways to go about doing this. If I’m doing this on a regular basis, I typically use rsync, but if it’s a one time deal, I use scp . If you only have FTP access, I highly recommend the lftp utility. ``` cd ~/public_html/ scp -R hacked username@servername:~/path/to/wordpress/install ``` This should put things back on your server to a clean state for your WordPress site. You may want to also make a backup of the file contents occasionally too. This is a script I use to backup servers, modified to look at WordPress: ``` ! /bin/bash cd path/to/wordpress\ntar cvpzf backup.tgz –exclude=/backup.tgz . ``` Postmortem Having a WordPress site compromised can be kind of embarrassing. There are a lot of factors that can go in to the attack vectors that can affect your site. Even if  you regularly update the versions of WordPress, the plugins, etc., you can find yourself in a situation, especially on a shared server, where someone you don’t even know about has left their site open to an attack that compromises your site. There really isn’t a good way to keep this from happening, but what you can do is make it harder for these types of attacks to success. Here are some general guidelines to keeping your WordPress site Cialis free: Keep Wordpress updated; Disable plugins you aren’t using; better yet, delete them off the server; Keep your theme in an SCM; Disable comments (this is one of the most common attack vectors); if you really want them, you can also use IntenseDebates or  Disqus Set your file permissions properly; If this is happening to you more than you’d like, I would also suggest taking a look at some of the static web generators. We use jekyll for the Praxis webiste ; SecondCrack by Maro Arment if you like PHP, and Growl  if you prefer Python."},{"id":"2012-03-20-we-have-highlighting","title":"We Have Highlighting!","author":"annie-swafford","date":"2012-03-20 11:09:09 -0400","categories":["Grad Student Research"],"url":"we-have-highlighting","layout":"post","content":"We have reached an important milestone in Prism development; the highlighting functionality is now complete! A user can now color a given text in accordance with a series of categories and then submit the markings to the database! The user clicks on a category on the right-hand side of the page to select that category, and can then mark up the text.  To mark the text, the user can either click on each word individually or drag over the text and all the words the cursor touches will be marked in that category. We also have an eraser function that lets the user delete any markings! Just click the eraser and then click on the word you want to unhighlight! Building this was quite an involved process; it took some ruby code to make a preprocessor that puts a span around each word and assignes each word a number, lots of coffeescript to control the marking functionality, and some css to control the colors of the markings and where they should appear on the text.  The next step for our talented design team is to figure out which colors we should use and whether we should attempt to layer colors and play with opacity, or whether some markings should appear above the text or below.  For example, should a phrase marked with both blue and red appear as purple, as red with a blue border, or should the words appear as blue with the red either above or below the words?  We’ve currently built it with the latter design in mind, as the image below demonstrates. Our next step from the development side is figuring out how to visualize the data! We know what we want it to look like ultimately, and now we need to build it! I’m going to spend some time trying to make sense of d3.js to see if it will work for our purposes. More next week!"},{"id":"2012-03-22-kathleen-fitzpatrick-planned-obsolescence","title":"Kathleen Fitzpatrick: Planned Obsolescence","author":"ronda-grizzle","date":"2012-03-22 07:19:56 -0400","categories":["Podcasts"],"url":"kathleen-fitzpatrick-planned-obsolescence","layout":"post","content":"Kathleen Fitzpatrick Planned Obsolescence: Publishing, Technology, and the Future of the Academy Dr. Kathleen Fitzpatrick, Director of Scholarly Communication for the Modern Language Association, joined us in the Scholars’ Lab on March 15, 2012 for a talk as part of our spring speaker series. Dr. Siva Vaidhyanathan, Robertson Professor in Media Studies and Chair of the Department of Media Studies at U.Va., served as the initial respondent for this talk. Abstract:\n“What if the academic monograph is a dying form? If scholarly communication is to have a future, it’s clear that it lies online, and yet the most significant obstacles to such a transformation are social and institutional. How must the academy and the scholars that comprise it change their ways of thinking in order for digital scholarly publishing to become a viable alternative? This talk will explore some of those changes and their implications for our lives as scholars and our work within universities.” Dr. Fitzpatrick’s Bio:\nKathleen Fitzpatrick is Director of Scholarly Communication of the Modern Language Association, and Professor of Media Studies (on leave), Pomona College. She is author of Planned Obsolescence: Publishing, Technology, and the Future of the Academy, published in 2011 by NYU Press and previously made available for open peer review online on MediaCommons, and of The Anxiety of Obsolescence: The American Novel in the Age of Television, published in 2006 by Vanderbilt University Press. She is co-founder of the digital scholarly network MediaCommons, and has published articles and notes in journals including the Journal of Electronic Publishing, PMLA, Contemporary Literature, and Cinema Journal . Dr. Vaidhyanathan’s Bio:\nSiva Vaidhyanathan is a cultural historian and media scholar, and is currently the Robertson Professor in Media Studies at the University of Virginia. Vaidhyanathan is a frequent contributor on media and cultural issues in various periodicals including the Chronicle of Higher Education, New York Times Magazine, The Nation, and Salon.com, and he maintains a blog, www.googlizationofeverything.com . He is a frequent contributor to National Public Radio and to MSNBC.COM and has appeared in a segment of “The Daily Show” with Jon Stewart. Vaidhyanathan is a fellow of the New York Institute for the Humanities and the Institute for the Future of the Book. In 2011 he was appointed chair of UVA’s Department of Media Studies. His book The Googlization of Everything — and Why We Should Worry was published by University of California Press in 2011. As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.14243384743/enclosure.mp3”]"},{"id":"2012-03-23-shell-programming-in-haskell-converting-s5-slides-to-pdf","title":"Shell Programming in Haskell: Converting S5 Slides to PDF","author":"eric-rochester","date":"2012-03-23 05:09:09 -0400","categories":["Research and Development"],"url":"shell-programming-in-haskell-converting-s5-slides-to-pdf","layout":"post","content":"Recently, I gave an introduction to Python for Chris’ and Kelly’s GIS Workshop . It was a really great experience, and we had a lot of fun learning about Python and how to use it with ArcGIS. I did my slides for it in Markdown, using S5 . Others around the Scholars’ Lab have used Show-off to compose slide-shows in Markdown, but I wanted something a little simpler, and it had been a while since I’d looked at S5, so I used that instead. Then Kelly asked me for a PDF version of the slideshow. Heh. At first I thought I might have to covert it to Showoff or (worse yet) PowerPoint. But I Googled around and found that converting it wouldn’t be too difficult. The process itself would be simple, and a small shell script would make it even easier. And then my infallible instinct to make any project ten times more interesting (i.e., complicated ) kicked in. I remembered that I’d just read Greg Weber’s post about Shelly, a library to make shell scripting a bit easier in Haskell. I’ve been seriously playing with Haskell for almost a year now, using it for most of my side-projects and for anything that no one else will have to maintain. The thought of using Haskell for shell scripting was intriguing, just because it would be another way for me to wrap my head around this very different computer language. But I was skeptical. At first glance, Haskell doesn’t seem like a good candidate for shell programming. Typically, these scripts are quick, one-off programs, often written in anger, that need to be created quickly and nimbly (dare I say, agily ?). However, Haskell is statically-typed, and its type system is not given to making quick changes. (Well, I’ve found that not to be quite accurate, but it is the perception.) Generally, I think that languages like Haskell are more suited to larger systems, because their power and concision really only become apparent when working with large bodies of code. Whatever my reaction, though, a small script like this, with limited scope, seemed perfect. The Process The process I found to handle the conversion was fairly simple. Get a PNG screenshot of each slide using webkit2png ; Concatenate all of the PNGs into a PDF using the ImageMagick tool convert ; Clean up the PNGs. With that laid out, let’s jump in. Preface First, some book-keeping: I have to let Haskell know that I’m going to use string literals in places that require Data.Text.Text instances: &lt;code class=\"sourceCode haskell\"&gt;&lt;span class=\"ot\"&gt;{-# LANGUAGE OverloadedStrings #-}&lt;/span&gt;&lt;/code&gt; Also, we have to import the Shelly module. &lt;code class=\"sourceCode haskell\"&gt;&lt;span class=\"kw\"&gt;import&lt;/span&gt;           &lt;span class=\"dt\"&gt;Shelly&lt;/span&gt;&lt;/code&gt; And we need some other modules for working with characters, text, and other things. &lt;code class=\"sourceCode haskell\"&gt;&lt;span class=\"kw\"&gt;import&lt;/span&gt;           &lt;span class=\"dt\"&gt;Control.Monad&lt;/span&gt; (forM_)\n&lt;span class=\"kw\"&gt;import&lt;/span&gt; &lt;span class=\"kw\"&gt;qualified&lt;/span&gt; &lt;span class=\"dt\"&gt;Data.Char&lt;/span&gt; &lt;span class=\"kw\"&gt;as&lt;/span&gt; &lt;span class=\"dt\"&gt;C&lt;/span&gt;\n&lt;span class=\"kw\"&gt;import&lt;/span&gt; &lt;span class=\"kw\"&gt;qualified&lt;/span&gt; &lt;span class=\"dt\"&gt;Data.Text.Lazy&lt;/span&gt; &lt;span class=\"kw\"&gt;as&lt;/span&gt; &lt;span class=\"dt\"&gt;T&lt;/span&gt;\n&lt;span class=\"kw\"&gt;import&lt;/span&gt;           &lt;span class=\"dt\"&gt;Filesystem.Path&lt;/span&gt;\n&lt;span class=\"kw\"&gt;import&lt;/span&gt;           &lt;span class=\"dt\"&gt;Prelude&lt;/span&gt; &lt;span class=\"kw\"&gt;hiding&lt;/span&gt; (&lt;span class=\"fu\"&gt;FilePath&lt;/span&gt;)\n&lt;span class=\"kw\"&gt;import&lt;/span&gt;           &lt;span class=\"dt\"&gt;System.Environment&lt;/span&gt;&lt;/code&gt; Converting to PNGs The first step is taking screenshots of each slide. To do that, I used the webkit2png script. For most things, I’m using Python 2.7, but I haven’t bothered installing pyobjc for it. webkit2png uses pyobjc, though, so I have to run that program with Python 2.6, which is the default Python shipped with Mac OS 10.6. I only generate the full-sized screenshot, and I output it to a filename that includes the slide number. In Bash, that would look like this: &lt;code class=\"sourceCode bash\"&gt;python2.6 &lt;span class=\"ot\"&gt;$(&lt;/span&gt;&lt;span class=\"kw\"&gt;which&lt;/span&gt; webkit2png&lt;span class=\"ot\"&gt;)&lt;/span&gt; &lt;span class=\"kw\"&gt;&lt;/span&gt;\n        --fullsize &lt;span class=\"kw\"&gt;&lt;/span&gt;\n        --filename pythongis-000 &lt;span class=\"kw\"&gt;&lt;/span&gt;\n        http://people.virginia.edu/~err8n/pythongis/#slide0&lt;/code&gt; First, let’s create a generic function to run commands in Python 2.6. In Shelly, the convention is to add an underscore to functions that throw away their output: &lt;code class=\"sourceCode haskell\"&gt;python26_ script args &lt;span class=\"fu\"&gt;=&lt;/span&gt; run_ &lt;span class=\"st\"&gt;\"python2.6\"&lt;/span&gt; (script&lt;span class=\"fu\"&gt;:&lt;/span&gt;args)&lt;/code&gt; This is kind of interesting because I wouldn’t abstract this out if I were writing this in Bash, Python, or Ruby. But adding this function felt quite natural in Haskell, which tends to encourage smaller, more generic, yet more focused, functions. Now I’ll build on that to create a command to look for the program webkit2png, and if it finds it, pass it to Python 2.6: &lt;code class=\"sourceCode haskell\"&gt;webkit2png_ filename url &lt;span class=\"fu\"&gt;=&lt;/span&gt; &lt;span class=\"kw\"&gt;do&lt;/span&gt;\n    script &lt;span class=\"ot\"&gt;&lt;-&lt;/span&gt; which &lt;span class=\"st\"&gt;\"webkit2png\"&lt;/span&gt;\n    &lt;span class=\"kw\"&gt;case&lt;/span&gt; script &lt;span class=\"kw\"&gt;of&lt;/span&gt;\n        &lt;span class=\"kw\"&gt;Nothing&lt;/span&gt;      &lt;span class=\"ot\"&gt;-&gt;&lt;/span&gt; echo &lt;span class=\"st\"&gt;\"ERROR: webkit2png not installed.\"&lt;/span&gt;\n        &lt;span class=\"kw\"&gt;Just&lt;/span&gt; script' &lt;span class=\"ot\"&gt;-&gt;&lt;/span&gt; &lt;span class=\"kw\"&gt;do&lt;/span&gt;\n            s &lt;span class=\"ot\"&gt;&lt;-&lt;/span&gt; toTextWarn script'\n            python26_ s [ &lt;span class=\"st\"&gt;\"--fullsize\"&lt;/span&gt;\n                       , &lt;span class=\"st\"&gt;\"--filename\"&lt;/span&gt;, filename\n                       , url\n                        ]&lt;/code&gt; This could be better. For one thing, this command could print an error message if webkit2png isn’t available. If that happens, it should probably also short-circuit the rest of the script. The way to do this in Haskell would be to return a Maybe value, which is what the which function above does. In this case, I know that the program is installed and on the PATH, so I’m being a little sloppy. Converting to PDF The next step is to concatenate all the PNGs into one PDF. I’m using the convert program from ImageMagick to do this. This takes a list of PNG files to convert, the name of the PDF file, and generates the output. &lt;code class=\"sourceCode haskell\"&gt;&lt;span class=\"ot\"&gt;convert ::&lt;/span&gt; &lt;span class=\"fu\"&gt;FilePath&lt;/span&gt; &lt;span class=\"ot\"&gt;-&gt;&lt;/span&gt; [&lt;span class=\"fu\"&gt;FilePath&lt;/span&gt;] &lt;span class=\"ot\"&gt;-&gt;&lt;/span&gt; &lt;span class=\"dt\"&gt;ShIO&lt;/span&gt; ()\nconvert pdf pngs &lt;span class=\"fu\"&gt;=&lt;/span&gt; run_ &lt;span class=\"st\"&gt;\"convert\"&lt;/span&gt; &lt;span class=\"fu\"&gt;=&lt;&lt;&lt;/span&gt; &lt;span class=\"fu\"&gt;mapM&lt;/span&gt; toTextWarn (pngs &lt;span class=\"fu\"&gt;++&lt;/span&gt; [pdf])&lt;/code&gt; Working on Multiple Files Right now, webkit2png_ (the function to download the slides as PNGs) operates on a single slide. But we’ll need to do this for every slide in the show. downloadSlides takes the number of slides and the base URL, and it calls webkit2png_ for each slide. It returns a list of file names for the downloaded PNGs. &lt;code class=\"sourceCode haskell\"&gt;&lt;span class=\"ot\"&gt;downloadSlides ::&lt;/span&gt; &lt;span class=\"dt\"&gt;Int&lt;/span&gt; &lt;span class=\"ot\"&gt;-&gt;&lt;/span&gt; &lt;span class=\"dt\"&gt;String&lt;/span&gt; &lt;span class=\"ot\"&gt;-&gt;&lt;/span&gt; &lt;span class=\"dt\"&gt;ShIO&lt;/span&gt; [&lt;span class=\"fu\"&gt;FilePath&lt;/span&gt;]\ndownloadSlides slideCount baseUrl &lt;span class=\"fu\"&gt;=&lt;/span&gt; &lt;span class=\"kw\"&gt;do&lt;/span&gt;\n    forM_ inputs &lt;span class=\"fu\"&gt;$&lt;/span&gt; (url, file) &lt;span class=\"ot\"&gt;-&gt;&lt;/span&gt; webkit2png_ file url\n    &lt;span class=\"fu\"&gt;return&lt;/span&gt; files'\n    &lt;span class=\"kw\"&gt;where&lt;/span&gt;\n        baseUrl' &lt;span class=\"fu\"&gt;=&lt;/span&gt; T.pack &lt;span class=\"fu\"&gt;$&lt;/span&gt; baseUrl &lt;span class=\"fu\"&gt;++&lt;/span&gt; &lt;span class=\"st\"&gt;\"#slide\"&lt;/span&gt;\n        &lt;span class=\"fu\"&gt;range&lt;/span&gt;    &lt;span class=\"fu\"&gt;=&lt;/span&gt; &lt;span class=\"fu\"&gt;map&lt;/span&gt; (T.pack &lt;span class=\"fu\"&gt;.&lt;/span&gt; &lt;span class=\"fu\"&gt;show&lt;/span&gt;) [&lt;span class=\"dv\"&gt;0&lt;/span&gt;&lt;span class=\"fu\"&gt;..&lt;/span&gt;slideCount]\n        urls     &lt;span class=\"fu\"&gt;=&lt;/span&gt; &lt;span class=\"fu\"&gt;map&lt;/span&gt; (T.append baseUrl') &lt;span class=\"fu\"&gt;range&lt;/span&gt;\n        files    &lt;span class=\"fu\"&gt;=&lt;/span&gt; &lt;span class=\"fu\"&gt;map&lt;/span&gt; (T.append &lt;span class=\"st\"&gt;\"slide-\"&lt;/span&gt;) &lt;span class=\"fu\"&gt;range&lt;/span&gt;\n        files'   &lt;span class=\"fu\"&gt;=&lt;/span&gt; &lt;span class=\"fu\"&gt;map&lt;/span&gt; (fromText &lt;span class=\"fu\"&gt;.&lt;/span&gt; &lt;span class=\"fu\"&gt;flip&lt;/span&gt; T.append &lt;span class=\"st\"&gt;\"-full.png\"&lt;/span&gt;) files\n        inputs   &lt;span class=\"fu\"&gt;=&lt;/span&gt; &lt;span class=\"fu\"&gt;zip&lt;/span&gt; urls files&lt;/code&gt; The only wrinkle here is that the file names that are passed to webkit2png aren’t the ones that are output. Instead, the program appends the size of the image (thumbnail, full, etc.) and the .png extension. Since I want to operate on those files later, I have to create both the file name prefix to pass to webkit2png and the full file name to process later. This is unfortunate and brittle, because if webkit2png ever changes how it names the output files, my script will break. This is also shell-script sloppy in another way. I should really create a temporary directory and download the PNGs there. Maybe someday. Putting it all Together and Getting the Inputs All the pieces are in place. The only things left are to parse the command-line arguments, call downloadSlides and convert, and delete the downloaded PNGs. The main function is the entry-point for the script. It picks three parameters from the command line and tries to make one a Int . If that can’t happen for any reason, it prints the usage message and exits. If the command-line is right, the script continues processing. &lt;code class=\"sourceCode haskell\"&gt;&lt;span class=\"ot\"&gt;main ::&lt;/span&gt; &lt;span class=\"dt\"&gt;IO&lt;/span&gt; ()\nmain &lt;span class=\"fu\"&gt;=&lt;/span&gt; shelly &lt;span class=\"fu\"&gt;$&lt;/span&gt; verbosely &lt;span class=\"fu\"&gt;$&lt;/span&gt; &lt;span class=\"kw\"&gt;do&lt;/span&gt;\n    args &lt;span class=\"ot\"&gt;&lt;-&lt;/span&gt; liftIO &lt;span class=\"fu\"&gt;$&lt;/span&gt; getArgs\n    &lt;span class=\"kw\"&gt;case&lt;/span&gt; args &lt;span class=\"kw\"&gt;of&lt;/span&gt;\n        [slides, url, pdf] &lt;span class=\"fu\"&gt;|&lt;/span&gt; &lt;span class=\"fu\"&gt;all&lt;/span&gt; C.isNumber slides &lt;span class=\"ot\"&gt;-&gt;&lt;/span&gt; &lt;span class=\"kw\"&gt;do&lt;/span&gt;\n            pngs &lt;span class=\"ot\"&gt;&lt;-&lt;/span&gt; downloadSlides (&lt;span class=\"fu\"&gt;read&lt;/span&gt; slides) url\n            convert (fromText &lt;span class=\"fu\"&gt;$&lt;/span&gt; T.pack pdf) pngs\n            echo &lt;span class=\"fu\"&gt;.&lt;/span&gt; T.pack &lt;span class=\"fu\"&gt;$&lt;/span&gt; &lt;span class=\"st\"&gt;\"Wrote PDF to \"&lt;/span&gt; &lt;span class=\"fu\"&gt;++&lt;/span&gt; pdf\n            &lt;span class=\"fu\"&gt;mapM_&lt;/span&gt; rm_f pngs\n        &lt;span class=\"fu\"&gt;otherwise&lt;/span&gt; &lt;span class=\"ot\"&gt;-&gt;&lt;/span&gt; echo usage&lt;/code&gt; This is the usage/help message. &lt;code class=\"sourceCode haskell\"&gt;&lt;span class=\"ot\"&gt;usage ::&lt;/span&gt; &lt;span class=\"dt\"&gt;T.Text&lt;/span&gt;\nusage &lt;span class=\"fu\"&gt;=&lt;/span&gt; &lt;span class=\"st\"&gt;\"&lt;/span&gt;\n&lt;span class=\"st\"&gt;    usage: s5topdf.lhs [slides] [url] [output] n&lt;/span&gt;\n&lt;span class=\"st\"&gt;     n&lt;/span&gt;\n&lt;span class=\"st\"&gt;      slides is the number of slides in the slideshow.n&lt;/span&gt;\n&lt;span class=\"st\"&gt;      url    is the URL to access the slideshow at.n&lt;/span&gt;\n&lt;span class=\"st\"&gt;      output is the filename of the PDF file to create.n\"&lt;/span&gt;&lt;/code&gt; Running To run this script, pass it to runhaskell with the right command-line arguments. For example, here’s a small wrapper script . Conclusion Using Haskell for shell programming hasn’t been bad, but it’s not as fast as shell programming usually is, either. This is still more verbose than the bash, Python, or Ruby versions would be, and it took me (a little) longer to write. (Of course, I was unfamiliar with several of these libraries, and that slowed me down.) However, I needed to do almost no debugging. Once I got the types to line up and runghc to stop complaining, it just worked. There were no bugs hiding in parts that hadn’t run yet. Based on experience with other languages, I’d expected to have to tweak the convert function (the second stage of processing) once I got the webkit2png part working (the first stage). But that wasn’t necessary. After I coaxed the complete script into printing the usage message, everything else worked flawlessly. The bottom line: For very short one-off scripts, this seems like over-kill. For scripts that you expect to grow, Haskell plus Shelly might be more attractive. Second Conclusion One of the things that attracts me to Haskell is it’s history of using literate programming . In fact, I’m using it right now. This post was generated from the script itself. I’ve posted the raw version to a gist, so you can compare them. Using literate Haskell was a success. I really liked being able to interleave extended commentary with the code and to have both be part of the final product. I think it changed the nature of both the script and the post. This might not work as well for larger projects with more lines of code and multiple modules, but for a small script, it was very comfortable. I can see doing this again for descriptions of small algorithms, projects, or demos. Also, having this file double as a script and the post is kind of neat, at least for the moment. &lt;code class=\"sourceCode haskell\"&gt;&lt;span class=\"co\"&gt;-- vim: set filetype=lhaskell:&lt;/span&gt;&lt;/code&gt;"},{"id":"2012-03-23-welcoming-katina-rogers","title":"Welcoming Katina Rogers!","author":"bethany-nowviskie","date":"2012-03-23 06:39:16 -0400","categories":["Announcements","Grad Student Research"],"url":"welcoming-katina-rogers","layout":"post","content":"Today, we’re very happy to announce that Dr. Katina Rogers is to join the Scholarly Communication Institute and Praxis Program teams at the Scholars’ Lab . Katina comes to us from an appointment at the Alfred P. Sloan Foundation, where she has been responsible for a number of operational areas and has contributed to the strategic development of Sloan’s Digital Information Technology program under the direction of Josh Greenberg. Her past work includes positions as Assistant Secretary General to the International Union of Geodesy and Geophysics, and as an instructor in language and literature courses at the University of Colorado, Boulder, where she completed her PhD in Comparative Literature in 2010. In her new role as Senior Research Specialist for the Scholarly Communication Institute, Katina will help to support and share outcomes from a number of conversations SCI is convening: on emerging models for authoring and publication (with the Alliance for Networking Visual Culture, PressForward, and the Modern Language Association’s program in scholarly communication ); and on graduate education reform, with CHCI and centerNet . She will also plan and conduct a broad survey of humanities-trained respondents who self-identify as working in alternative academic careers, to illuminate perceived gaps in graduate-level preparation and help move alt-ac conversations beyond the anecdotal. Finally, she will play a role in the planned expansion of the Praxis Program to a multi-institutional and international effort, geared toward sharing model programs and experiments in humanities methodological training. Katina blogs at Black Ink/White Page, and is @katinalynn on Twitter. She will begin work with us in April and will be based in New York for the duration of this appointment. We’re thrilled to welcome her to SCI and to the Scholars’ Lab family!"},{"id":"2012-03-27-diy-aerial-photography","title":"DIY Aerial Photography","author":"chris-gist","date":"2012-03-27 11:00:24 -0400","categories":["Announcements","Geospatial and Temporal"],"url":"diy-aerial-photography","layout":"post","content":"Here in the Scholars’ Lab, we’ve been interested for some time in having the ability to take aerial photographs on a small scale.  Many great uses exist for such techniques here at UVa.  A landscape architect could use current, high resolution photos of a work site. An environmental science student may wish to see how things have changed along the shoreline over a short period of time.  For these researchers, paying for an aerial survey would be cost prohibitive and unlikely. Flying a kite or balloon with camera attached is relatively inexpensive and can create great results. Hanging cameras from aerial platforms (kites, balloons and pigeons) has been used since the dawn of photography. By combining readily available products and a little ingenuity, just about anyone can fly a camera.  We have taken our inspiration from Grassroots Mapping, an initiative of Public Laboratory, which is a organization dedicated to helping local activists leverage technology. This morning, we made our maiden voyage using the balloon methods described on the Grassroots Mapping website.  With our weather balloon and hacked Canon camera, we headed out in front of the library and made two small flights. ![](http://static.scholarslab.org/wp-content/uploads/2012/03/IMG_84711.jpg) ![](http://static.scholarslab.org/wp-content/uploads/2012/03/IMG_2379.jpg) Clemons Library Terrace ![](http://static.scholarslab.org/wp-content/uploads/2012/03/IMG_2386.jpg) Louis S. on Clemons Terrace After flying over Clemons, we moved to the front of Alderman Library.  The wind eddies were an issue this morning but we got some really nice shots. ![](http://static.scholarslab.org/wp-content/uploads/2012/03/IMG_8472.jpg) Alderman Launch ![](http://static.scholarslab.org/wp-content/uploads/2012/03/IMG_8474.jpg) Balloon Over Harrison-Small ![](http://static.scholarslab.org/wp-content/uploads/2012/03/IMG_2423.jpg) Kelly Holding Balloon Line ![](http://static.scholarslab.org/wp-content/uploads/2012/03/IMG_2447.jpg) Skylights and Sidewalks ![](http://static.scholarslab.org/wp-content/uploads/2012/03/IMG_2453.jpg) Trees and Benches ![](http://static.scholarslab.org/wp-content/uploads/2012/03/IMG_2442.jpg) The Ground Crew In Front of Alderman The folks at the Public Laboratory also created an online georeferencing tool called MapKnitter .  This tool may not be the best but it’s fairly easy to understand and get some fairly decent results.  View our test site here . This is all in our preparation for our DIY Aerials workshops (on April 18 &amp; 19) and THATCamp (April 20 &amp; 21). Guess what else we can do?  We can make 3D models with our shots!  A site called Hypr3D lets you upload images and then calculates the 3D geometry for you.  It’s magic. [iframe width=”700” height=”600” src=”http://www.hypr3d.com/models/4f74a45308c2fa0001000056/embedded_viewer”] http://www.hypr3d.com/models/4f74a45308c2fa0001000056"},{"id":"2012-03-27-examining-tei-displays-across-the-web","title":"Examining TEI Displays Across the Web","author":"carin-yavorcik","date":"2012-03-27 10:00:13 -0400","categories":["Digital Humanities"],"url":"examining-tei-displays-across-the-web","layout":"post","content":"As part of my project with Zane to update the TEIDisplay plugin for Omeka, I have been examining ways that different digital collections present TEI-encoded texts. We hope that by looking at the ways that other display tools function, we’ll be able to gain more insight into what works well and what doesn’t. One of the first sites I looked at was Thomas MacGreevy and George Yeats: A Friendship in Letters, a collection of letters spanning 1922-1965. What first stands out about this collection is the two-paned display. On either side, users can choose from a set of tabs how they want to examine each letter – annotated text, scanned page images, bibliographic information from the letter’s teiHeader, and biographic information about people mentioned in the letter. This setup makes it easier to display necessary information depending on what a user’s goals are. The display of the TEI text itself also includes some nice features. Most letters have scholarly annotations that default to displaying in superscript – readers click on the note indicator and get a pop up box displaying the note, or they can click a button to hide notes entirely and read the letter as is. The encoded texts also display some of the visual features of the original letters, such as noting deletions with a strikethrough and additions with superscript text. The effect is a nice example of how digital editions of scholarly texts can combine different editorial practices such as social or documentary editing, as the MacGreevy Archive editor, Susan Schreibman, notes in her article about the collection, “The Lives of Others: Editing Online Editions of Correspondence,” from Digital Scholarship. The setup of the Victorian Women Writers Project at Indiana University is very different. The focus here is more specifically on the encoded text body of the materials, primarily prose and poetry written by British women of the 19th century. The display is simple but effective. A table of contents runs along the left hand side of the page, giving users a way to navigate through each document. When notes occur in the text, they are displayed at the bottom of each page as footnotes. Additionally, users can download the XML if they so choose. Though page images are not available for most of the materials in the Women Writers Project collection, the system does have the capability to display them, as is evidenced by one of the other Indiana University collections, the Brevier Legislative Reports, which uses the same display tool. If a user would like to examine an image of a page, she can click the “view page” link next to the page number indicator, and a pop-up image appears. However, this image does in some cases cover the encoded text, which makes it a little harder to compare the two. Alternatively, where these page images are available, a user can switch directly to “image mode” and read the page images without the encoded text, or download the whole document as a PDF. I also explored the classics texts at the Perseus Digital Library at Tufts University, which offers a lot of interesting interactive features with the display of its TEI text. Like the Victorian Women Writers site, there is a table of contents on the left-hand side to facilitate navigation. Through a series of tools on the right-hand side of the page, users can choose to display different editions and/or translations of a work, including editions of scholarly annotations. Additionally, place names are automatically extracted from the TEI text and can be searched or visualized in a network or on a map. Users can also jump back and forth between commentary or cross-references to the work, and the site also offers the ability to search within a specific document or discover how words are used throughout a work. There were two main features that really stood out to me when evaluating these projects. One was that for the longer texts, it was really helpful to have the table of contents feature that allowed for easier browsing. The original version of TEIDisplay did have an option to render documents as “segmental” with a similar table of contents – this is definitely a functionality we’d like to keep, so we’ll be experimenting with it as we update the plugin. Another feature I really liked in exploring the projects above was flexibility. It was nice to have the opportunity to customize the view you want to what your needs are – whether it’s hiding scholarly annotations, pulling up page images, or comparing editions. We are currently working on one viewing option that is progressing well – the ability to associate page images with encoded text. Overall, though, it may harder for us to implement something really flexible on the user end out of the box, given that we are not designing for a specific collection with specific needs. Rather, we are trying to make a general tool that works in many situations, which can be easily customized on the designer end to fit different collection needs as necessary. With something so general, it’s impossible to plan for all flexibility options that users may desire. But we hope that with really robust documentation about how everything works in the plugin, the creators of digital collections may be able to get in and tinker “under the hood” to customize the display as necessary."},{"id":"2012-03-27-seeing-the-prism-we-have-visualizations","title":"Seeing the Prism: We Have Visualizations!!","author":"annie-swafford","date":"2012-03-27 09:54:05 -0400","categories":["Grad Student Research","Visualization and Data Mining"],"url":"seeing-the-prism-we-have-visualizations","layout":"post","content":"I am happy to report that we have successfully build visualization capabilities into Prism!  Once users have highlighted the text according to the set categories, the users click on the submit button, which takes them to the visualization page! The users can then click on the categories at the right-hand side of the page to see how people have marked up the text, one category at a time.  The words will all turn the color that corresponds to the category, and the words will increase in size depending on how frequently they have been marked for that category.  Here’s how Jefferson’s “Notes of the State of Virginia” has been marked up for “rhetoric”: Here’s how the same text was marked for “orientalism”: Here’s how the same text was marked for “social Darwinism”: As you can probably guess from the results, I didn’t actually spend time making sure that I’ve highlighted in accordance with the categories; this is merely a proof of concept.\nThis visualization functionality is built with the help of the d3.js library; it’s amazingly powerful for all visualization needs!\nThe design team will now work on making it look even more attractive, but at least we know that we’ll have visualizations to show.\nOur next task is to make sure that only users with accounts can highlight texts and see the resulting visualizations. More info next week!"},{"id":"2012-03-28-day-of-dh","title":"Day of DH","author":"brooke-lestock","date":"2012-03-28 07:45:04 -0400","categories":["Digital Humanities","Grad Student Research","Research and Development"],"url":"day-of-dh","layout":"post","content":"Yesterday, as I’m sure you all know, was Day of DH, and it was my first year participating. I was only able to blog twice because as soon as I sat down to work on some design tasks for Praxis (choosing a font for the Prism site), I had some server troubles that kept me busy “raking,” dropping, migrating, importing, etc., and then it was time for our weekly meeting. In any event, visit my Day of DH page to read about how I got my feet wet in DH and what I was up to yesterday. To add to that, because I didn’t get a chance to update on the Day of DH blog yet, Bethany spoke to the group in our weekly meeting about budgets and grant-writing. She offered some valuable insight into the financial processes that pay for us and our meetings (we should probably watch less cat videos in there), the larger systems that affect budgeting within a university setting, and the small- and large-scale questions a financial manager needs to ask. She also provided us with a plethora of helpful tips on writing grants and devising budgets (like, “Soft money is the devil!”), which cannot be valued enough coming from someone with Bethany’s success rate! Now I need to get back to picking a font. You wouldn’t think it would be this difficult to choose one serif and one sans-serif font, but this Praxis group is chock-full of typography and web design street cred, so I’m feeling the pressure. I’ll end by saying that it was a productive Day of DH for me in the Scholars’ Lab and for the international DH community-at-large, and I’m looking forward to participating in many more!"},{"id":"2012-03-28-die-praxis-programm","title":"Die Praxis-Programm","author":"alex-gil","date":"2012-03-28 05:18:50 -0400","categories":["Grad Student Research"],"url":"die-praxis-programm","layout":"post","content":"[Today was the Day of DH. This post was originally written for that crowd, hence some of the introductory material.] Back at the Scholar’s Lab, the whole gang was hanging out at the graduate lounge waiting for our long-awaited session on grant writing and budgeting. Most of us had never seen a budget sheet until today. As we are wont to do, when the meeting started we went over our progress for the week.  Annie Swafford  had really good news for everyone. Solidly ahead of schedule, she demo’d  Prism’s first visualization . [Cue applause]. Unspoken fears that we would not achieve visualization were laid to rest and a year’s worth of work was validated in the smiles of everyone around our camelot table. We had other minutia to report before moving on to Bethany Nowviskie’s presentation on the cash-money. Jeremy (read my  third post of the day ) and Lindsay reported on their work with the left column of Prism (read my  second post of the day ). I reported on my progress translating the about page to German.  Ja, ich weiß. Geil . I have been working on both the  i18n framework  for our rails application and the translations themselves. This week I will close that branch with the incorporation of links on the footer to allow users to select the language. [![The Praxis crew](http://static.scholarslab.org/wp-content/uploads/2012/03/IMG_1706-300x224.png)](http://www.scholarslab.org/praxis-program/die-praxis-programm/attachment/img_1706/) Doing our Praxis thing Bethany’s presentation was one of the best hours I’ve spent so far this year. It was both full of general advice and very specific detail. Ahead of our times, we were made privy to  the contents of the briefcase . We went over budget spreadsheets, grant applications (join a grant review committee if you haven’t done so), hourly labor calculations, cost sharing, program officers, F&amp;A (Facilities &amp; Administrative Costs), faculty time buyouts, private funders and. public funders, looking for change under the couch, equipment, weathering budget cuts,  und so weiter .  Once our time was up, I was reminded of the many ways in which our  Praxis Program  had succeeded in making me a different kind of professional. [meditation ensued]"},{"id":"2012-03-28-the-end-of-the-beginning","title":"The end of the beginning","author":"sarah-storti","date":"2012-03-28 19:25:45 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"the-end-of-the-beginning","layout":"post","content":"Graduate study in the humanities can be a lonely business. Enter my knight in shining armor, commonly referred to around these parts as the Praxis Program. I think I’ve shared with you all before that I jumped at the call for applicants to the soon-to-become Praxis Program last summer specifically because the invitation promised that the Scholars’ Lab “apprenticeship” would offer ample opportunities for collaboration with not only Scholars’ Lab staff but also with other grad students. Fittingly, I began writing my application essay in the Victoria, BC airport, following my first experience at the ultra-collaborative and congenial DHSI . This summer, along with my gorgeous and talented co-pilot Brooke Lestock, I’ll be presenting the Praxis Program and our crowdsourcing interpretation tool Prism to DHSI 2012 participants during one of the Institute’s colloquium sessions . Funny how things come full circle, isn’t it? Speaking of full circle: last Wednesday the Program held a lunchtime information session for potential 2012-2013 Praxis Fellows. Sitting up at the front of the room with my cohort, flanked by our fearless and supportive SLab team, I couldn’t help but feel ridiculously proud of what we, as a discrete group, have achieved so far this year. Now people want to be us . During the Q&amp;A session, with the help of Bethany and company, we successfully fielded all kinds of questions about how we value the skills we’ve developed and the experiences we’ve shared over the past semester and a half. We were articulate. We sounded like we knew what we were talking about. Because we did know what we were talking about! Finally. Thank you, Praxis Program. This past weekend marked the deadline for submissions to the 2012-13 Praxis Fellowship. Obviously we have to keep details confidential, but it can’t hurt to share that we received what I thought was a shockingly large number of applicants. And the applicants are stellar. It’s going to be terribly difficult to make decisions, but I’m excited about the process (as always!) and can’t express adequately how incredible it feels to know that I’ll have a hand in shaping the next generation of Praxis. I envy the new cohort of Fellows, whoever they may be. To you, future Fellows, I offer the following sage (because I’m on the way out) advice: make the most of your time. I know I’ll miss it. I leave you with my proposal for the 2011-12 Praxis team tshirt design, because I’m too happy about it to keep it to myself (n.b.: 99% of the credit for this goes to Brooke). Jeremy, is there any way we can get a higher-resolution logo? I had to steal this one from the blog. Cheers!"},{"id":"2012-03-29-rambles-of-a-runaway-from-southern-slavery","title":"Rambles of a Runaway from Southern Slavery","author":"chris-gist","date":"2012-03-29 07:06:19 -0400","categories":["Digital Humanities","Geospatial and Temporal"],"url":"rambles-of-a-runaway-from-southern-slavery","layout":"post","content":"Henry Goings was a slave born ca. 1810 on a plantation on the James River between Richmond, VA and Williamsburg, VA.  His interesting history of travel with his owners, escape, and eventual settlement in Canada were chronicled in an until recently unknown book. The UVa Library acquired the book in 2007.  Researches Calvin Schermerhord of Arizona State University, Michael Plunkett, and Edward Gaynor both from UVa Small Special Collections Library were able to find other source material about/from Mr. Goings.  Their research is published in a newly edited version of Mr. Goings’s Rambles of a Runaway from Southern Slavery . The Scholars’ Lab was asked to contribute maps showing the general movement of Mr. Goings and places mentioned in the text.  We gladly obliged and through an iterative process came up with the four following maps. ![](http://static.scholarslab.org/wp-content/uploads/2012/03/Map1-1024x791.jpg) Map 1 - Shows Goings's General Movement ![](http://static.scholarslab.org/wp-content/uploads/2012/03/Map21-1024x791.jpg) Map 2 - Shows Southern Locations with James River Plantations Inset ![](http://static.scholarslab.org/wp-content/uploads/2012/03/Map3-1024x791.jpg) Map 3 - Shows Places From Tennessee to Canada ![](http://static.scholarslab.org/wp-content/uploads/2012/03/Map4-1024x791.jpg) Map 4 - Shows Great Lakes Locations The UVa Press did make some minor changes to the maps (mostly legends and scalebars) for the book.  However, the extents and all the data in the maps are the same as those shown here. I have read the preface and and first chapter.  It is a fascinating story and the footnotes are incredibly informative.  Kudos to the editors!"},{"id":"2012-03-30-welcoming-our-201213-graduate-fellows","title":"Welcoming our 2012/13 Graduate Fellows","author":"eric-johnson","date":"2012-03-30 05:11:08 -0400","categories":["Announcements"],"url":"welcoming-our-201213-graduate-fellows","layout":"post","content":"The Scholars’ Lab is very excited to announce the recipients of the 2012-13 UVA Library Graduate Fellowships in the Digital Humanities : David Flaherty Corcoran Department of History David’s project, part of his dissertation ‘Improving and Enlarging Your Majesty’s Dominions in America:’ The Board of Trade’s Vision for a British Atlantic Empire, 1713-63, will use the entries of the British Board of Trade’s Journal to create a database and maps “that will reveal the Board’s network of correspondents and the widespread places that drew their attention.”  The visualizations will show “precisely when and where the [Board’s] Commissioners directed their energies to gather more knowledge about the colonies and to pursue the implementation of part of their vision of empire,” allowing David “to recreate, literally, the Commissioners’ geographical ‘vision.’” Lydia Rodriguez Department of Anthropology Lydia’s project, part of her dissertation Thinking Gesture: The Dialectics of Language, Gesture, and Thought in Chol Maya, focuses on “the analysis of the gestures that co-occur with temporal utterances” among Chol Maya speakers of Northern Chiapas, Mexico. She intends to create a database of Chol Maya temporal gestures “that will allow me to perform quantitative and qualitative analysis on the frequency and types of gestures co-occurring with temporal utterances” and a website that will allow her to share this data with colleagues working in Maya languages or in gesture research. Annie Swafford Department of English Annie’s project, “Songs of the Victorians,” is a digital scholarly tool facilitating “interdisciplinary discussions of Victorian poetry and corresponding Victorian parlor song musical settings.”  Part of her dissertation, the project will achieve this by “combin[ing] close readings of both Victorian poems and musical settings of them in an interactive environment” based on audio files, visual representations of musical scores, and presentations of poetic texts.  Annie has also held fellowships in the NINES program over the past two years and in the Scholars’ Lab’s Praxis Program in 2011-2012. The Scholars’ Lab will schedule a luncheon in September at which our new Fellows will be introduced and will in turn be given the opportunity to introduce their projects.  We’re delighted to have them as part of the SLab community!"},{"id":"2012-04-03-updating-visualizations-and-users","title":"Updating Visualizations and Users","author":"annie-swafford","date":"2012-04-03 09:30:40 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"updating-visualizations-and-users","layout":"post","content":"We have three new updates for Prism!  First, we slightly changed the html markup for our document files, so our James Joyce excerpt from Portrait of the Artist as a Young Man  is now formatted correctly!  Second, we decided to tweak our visualizations.  Originally, once users select a category for visualizations,  the entire text would turn that color and the words that had been highlighted multiple times for that category would grow in size.  We decided that it looked odd and misleading if the entire text for each category changed color, regardless of whether those words had been selected, so we changed it so that only words that have been highlighted for a category change color. Here’s an example of our new visualization plan (complete with our newly complete Joyce text): Finally, we fixed our user functionality! Now, users need to sign in to their accounts in order to highlight texts or view visualizations! Our next plan is to see if it’s possible to change the cursor to look like a highlighter after users click on a category to markup the text. Stay tuned for more updates!"},{"id":"2012-04-03-wrapping-up-i18n","title":"Wrapping up i18n","author":"alex-gil","date":"2012-04-03 11:07:38 -0400","categories":["Grad Student Research"],"url":"wrapping-up-i18n","layout":"post","content":"Yesterday I was able to wrap up the internationalization of the site. It took us a while to figure out how we wanted the user to choose languages. We have several options: A setting on the browser can decide; users can indicate their preferences in the user account; we can have the location of your IP address decide for you. In the end we opted for the language link approach. No flags in our case, just links with the name. Once the links were made, the next challenge was to make the language choices stick. In order to accomplish this, I had to set all link methods in our rails application attach the ?locale parameter automatically. In practice this meant that every link helper in any of our pages would generate the right language after we had already set the variable for locale at least once. If you didn’t do anything the locale would default to English. Hard links were a completely different matter. Thus far, I was only able to find one hard-link to worry about: the link to the about page on the header section. In order to deal with hard links we had to come up with a little rails hack. We (I had help from Eric Rochester figuring this one out) sent a variable from the actual header tag back into the yml file to have the yml file generate the links in the right language. The code looks something like this: on app/views/layouts/_header.html.erb [html gutter=”false”]\n&lt;p class=\"description\"&gt;&lt;%= t ‘header.description_html’, \n:about_url =&gt; url_for(:controller =&gt; ‘pages’, :action =&gt; \n‘about’) %&gt;&lt;/p&gt;[/html] on the config/locales yml files [html gutter=”false”]\n   description: &gt;\n      Prism es un conjunto de herramientas que sirven \n      para &lt;a href=’%{about_url}’&gt;colectivizar la\n      interpretación &lt;/a&gt;de textos… [/html]"},{"id":"2012-04-04-cross-posted-on-a-definition-of-open-humanities","title":"cross-posted: On a definition of \"open humanities\"","author":"eric-johnson","date":"2012-04-04 12:09:25 -0400","categories":["Digital Humanities"],"url":"cross-posted-on-a-definition-of-open-humanities","layout":"post","content":"Cross-posted from Eric’s blog : In considering  recent definitions of digital humanities, I’m often struck by some common aspects of many of them—the ones that have nothing to do with the digital: ”. . . essentially collaborative . . .” “. . . a commitment to the openness of knowledge.” “ . . . interdisciplinary; by necessity it breaks down boundaries between disciplines . . .” “Making stuff, and using it to collaborate and connect with the public . . .” “. . . genuine interdisciplinarity . . . open communication.” “. . . an inclusive, open community . . .” “. . . using interdisciplinary approaches that may go beyond the comfort level of traditional scholars . . .” They speak to me in large part because I really come from the museum (specifically public history) and library worlds, and many of those celebrated features of DH are fundamental aspects and approaches of those other fields as well—and have been for a long time. The practitioners in those fields? Well, they’re my people. Libraries, museums, the digital humanities, and other public-facing humanities efforts share some or all of the following values: Collaboration Inclusivity Interdisciplinarity Open Access Open Process Open Source Involvement of the public and/or public “communities of passion” I’ve  started referring  to this larger perspective as the  open humanities . It’s a broad term that encompasses those values outlined above, values shared by many libraries, museums, public humanities projects and practitioners of all kinds and standing in opposition to much of the traditional approach of “solo scholar” research and closed publication. If I were to offer a first-pass attempt at capturing these elements in a definition, I might say something like:  the open humanities are those aspects of the humanities aimed at democratizing production and consumption of humanities research .  It makes no judgment on the precise manner or praxis in which this work is done or delivered. And because I work in a digital humanities shop, I wanted to show my take on the relationship between digital humanities and the wider open humanities. The digital humanities are a part of the open humanities to the extent that those same values are held, though of course the purely digital elements (the code, the markup, the hardware) are unique to the digital humanities and live largely outside of OH. That being said, much of DH—the commitment to open source, the collaborative nature of the field, the interdisciplinarity—is open. As a comparison, the public humanities are  virtually entirely open by my definition, so they are included entirely within the circle of OH   almost  entirely open by my definition, but as  Sheila   Brennan   points out  there are still elements even within those institutions that don’t support openness at every turn. (See my original diagram  here .) There is also  some overlap  between public humanities and the digital humanities, as some—but not all—public humanities projects are also digital.  You see how this starts getting a little  overly-Venn-diagrammatical, but you can probably see where I’m going. Those who know me know that I’m no coder—some might say that means I’m not much of a digital humanist. I’d say I’m a proud  open  humanist with one foot solidly in the digital. That counts as DH for me."},{"id":"2012-04-17-an-intro-to-my-work-with-sci","title":"An intro to my work with the Scholarly Communication Institute (SCI)","author":"katina-rogers","date":"2012-04-17 05:52:52 -0400","categories":["Announcements"],"url":"an-intro-to-my-work-with-sci","layout":"post","content":"Hi, readers! It’s such a pleasure to be writing in this space. As you may know, I came onboard as Senior Research Specialist for the Scholarly Communication Institute  just a couple of weeks ago. SCI is housed within the Scholars’ Lab (which means I get to be a part of this stellar group that, if you’re reading here, you already know well), and over the past nine years has convened discussion-based summer institutes on topics related to all aspects of scholarly communication. I’m still green as can be, but the shapes of my projects are becoming clearer, and as I plan them out I’m getting a sense of just how quickly these eighteen months will go. So, what is it exactly that I’ll be doing? At this point I’m thinking of my role as having two main streams. The first stream focuses on understanding and reforming  methodological training in humanities graduate programs, and consists of two related projects: a census and a survey. Through the census, which will become a part of the  #Alt-Academy website, I will seek out humanities-trained scholars who self-identify as working in alt-ac roles; the results will be displayed as a dynamic and searchable directory on #Alt-Academy. It’s important to me that the census cast a wide net in order to get as complete a picture as possible of the constellations of people working in alt-ac positions. The census, useful in its own right, will also be a stepping stone leading towards another goal: working from the census, I will be administering a survey of alt-academics in order to determine opportunities for improved career preparation and refined methodological training in humanities programs. The survey responses, which will be strictly confidential, will help us to move beyond the anecdotal and gain concrete understanding of the skills that advanced humanities programs currently provide relative to the needs that graduates of the programs experience in their careers. Once it’s complete, a report on the survey results will be published by CLIR . The second work stream involves supporting and sharing outcomes from a series of meetings that SCI will be convening on  reforming humanities graduate education and on the continued development of new models of scholarly publishing and authoring . The meetings focusing on graduate education reform will be held in partnership with  CHCI  and  centerNet, and will probe more deeply into questions raised by the results of the survey mentioned above, as well as many other issues surrounding graduate education. The meetings on scholarly publishing and authoring are in partnership with three outstanding projects from the  Alliance for Networking Visual Culture,  PressForward, and the Modern Language Association’s  program in scholarly communication . For each of these meetings (the first of which takes place in early May), I’ll be providing research support and documenting the substance of the conversations. Needless to say, I’m very much looking forward to taking part in both sets of meetings, which include extraordinary people. I’ll post updates here from time to time, so watch for more as the weeks go by! I’ll also continue posting to my personal blog  and my Twitter feed, so if you’re interested in the topics I’ll be working on, you may enjoy keeping an eye on those unofficial channels as well. Finally, I’ll be seeking input for the census and survey over the coming weeks (both in terms of who to include and what questions to ask), so if you have thoughts or questions, please feel free to connect with me directly."},{"id":"2012-04-17-spreading-the-light-prism-development-is-almost-done","title":"Spreading the Light: Prism Development is Almost DONE!","author":"annie-swafford","date":"2012-04-17 06:50:29 -0400","categories":["Grad Student Research"],"url":"spreading-the-light-prism-development-is-almost-done","layout":"post","content":"It’s been a busy few weeks in Prism, which means that we have some exciting updates for highlighting, visualizations, and the sandbox! Our visualizations now display with the correct colors! This change was surprisingly complicated, since it involved adding yet another function in our d3.js algorithm, but it made a huge difference. I also made another important change that will improve the user experience.  On the right-hand side of the page, we have a key that shows which categories correspond to which colors: each category appears to the right of a little colored box.  I wrote javascript that adds a black border around the correct colored box when a user selects a category to make it clearer which category has been selected. We decided to make this change instead of changing the cursor to look like a highlighter. Our final update involves user accounts: originally, users had to log in to access the sandbox (a sample text that users can highlight as a way of practicing before they highlight our three main texts), but I was able to find a way to bypass the login system, so now everyone can use the sandbox without logging in. The next step is to write tests for my javascript with Jasmine, which I will learn how to do today!  Once we have this task completed, I think we’ll just have design work left, since we have all the functionality we want for this version of our tool.  We’ve almost finished building Prism!"},{"id":"2012-04-23-and-then-the-light-bulb-blew","title":"...and then the Herokulypse","author":"alex-gil","date":"2012-04-23 12:21:10 -0400","categories":["Grad Student Research"],"url":"and-then-the-light-bulb-blew","layout":"post","content":"After two and some years hanging around the Scholars’ Lab and earning my badges in the DH community, I finally learned a lesson that should be required learning for all new-comers: plumbing is real. I mean, I was more or less aware of its existence, brief-sightings, a shudder here and there from a ghostly presence. Problem is, I’ve been focusing on the flashy, large, important, big, fancy, loud, loud, loud uses of already-made tools or those tools I dream of, five-million dollars and the-rest-of-your-life tools. You know: The shiny stuff. For the past couple of weeks, I have been working instead on the small stuff that needed to be done to roll Prism into production. Enter the plumbing. What I thought would be a series of small tasks turned out to be a major time vacuum. At issue was getting Heroku to play nice with what we had built in the development branch. The first two weeks, Heroku would not even display our site. A series of ‘Application Error’ messages was all I got. The culprits, in no particular order: the Asset Pipeline, Devise and Jasmine . Eventually, with help from above (i.e. E. Rochester and W. Graham), we got the site running …and then the Herokulypse . Once in a while a bug comes, so uncanny, so daunting, that it makes you want to become a novelist. That was the Herokulypse. I obsessed about it for three days at the expense of my dissertation and everything else, with no results. The great obi-wayne-kenobot finally found the problem. To my relief I was on the right track trying to solve it. I just didn’t figure out the part about disabling page caching on the pages controller . Live and learn, and learn I did: Plumbing is real. I found the lesson timely at a moment when we are debating the obstacles and affordances of coding for digital humanities. The experience with the Herokulypse really brought home for me the idea that code is labor, and that the digital humanities really puts pressure on our notions of leisure, labor and power. I am still working out these issues –issues which all my predecessors seem to have encountered in one way or another– and will be sure to report back to the public when I have more insights. In the meantime, I won’t ask you to be careful of what you wish for. On the contrary, I will encourage you to scurry down the rabbit hole of code, that you may never think yourself superior to anyone who leans on the side of hack over yack."},{"id":"2012-04-29-praxis-through-prisms","title":"Praxis, Through Prisms","author":"bethany-nowviskie","date":"2012-04-29 15:48:10 -0400","categories":["Announcements","Grad Student Research"],"url":"praxis-through-prisms","layout":"post","content":"[Cross-posted from nowviskie.org .] This is just a quick post to share two bits of news about our Praxis Program at the Scholars’ Lab . The first is that I’ve written an op-ed on Praxis and our Fellows’ practicum project for this year’s Digital Campus special issue of the Chronicle of Higher Education . The piece was originally titled “Praxis, Through Prisms” – now “ A Digital Boot Camp for Grad Students in the Humanities .” It’s pay-walled, for now, but I’ll re-publish it in open access format in 30 days. by Chad Hagen for The Chronicle &lt;/figure&gt; Check it out to learn more about the program, get a sneak peek at Prism (launching this Tuesday, which is the second newsflash! congrats, team!) and find out what I see as the great project of humanities computing / digital humanities. Spoiler: it’s “the development of a hermeneutic–a concept and practice of interpretation–parallel to that of the dominant, postwar, theory-driven humanities: a way of performing cultural and aesthetic criticism less through solitary points of view expressed in language, and more in team-based acts of building.” Or, in other words, the kind of thing our amazing grad students and diverse crew of scholar-practitioners are working on at Praxis . Through Prism(s). I’m incredibly proud of the UVa Library staff who have devoted so much energy to teaching and mentoring Praxis Fellows this year (Wayne Graham, Jeremy Boggs, Eric Rochester, David McClure, and Eric Johnson) – and even more proud of our first six Fellows themselves, who have built Prism independently. These are Sarah Storti, Brooke Lestock, Annie Swafford, Lindsay O’Connor, Alex Gil, and Ed Triplett. And in fact, they’ve built Prism from scratch, on time, in public (perhaps the scariest part), with great good humor, and having started with very little practical experience in digital humanities design and development. Lately, I haven’t been able to stop myself from interrupting everything in our weekly Praxis meetings to make exclamations like, “Look at you guys! Look what you can do!” So I hope you’ll stay tuned through this week to the Scholars’ Lab blog, the Praxis site, and to our @PraxisProgram and @ScholarsLab Twitter feeds, for posts on the launch of the Prism beta, an announcement of our 2012-13 Praxis Fellows, and reflections by current Praxis grad students and the rest of the team."},{"id":"2012-04-29-the-last-days-of-development-jasmine-devise-and-visualization-tweaks","title":"The Last Days of Development: Jasmine, Devise, and Visualization Tweaks!","author":"annie-swafford","date":"2012-04-29 12:09:09 -0400","categories":["Grad Student Research"],"url":"the-last-days-of-development-jasmine-devise-and-visualization-tweaks","layout":"post","content":"We’re getting close to deploying, so we’re making all the necessary tweaks to having Prism ready to go! For this past week, that meant writing Jasmine tests, creating error messages for Devise, and tweaking the functionality of the visualization page. Jasmine  is a BDD framework for testing javascript.  It basically does for javascript what RSpec does for Ruby.  It took over a week to get it set up, and we had to also learn to use a few more ruby gems ( jasmine-jquery and   Guard ), but we finally got it to test our javascript.  It’s incredibly useful, not only because it gives an error message when something fails (which is how I learned that our eraser functionality wasn’t working properly), but also because it encouraged me to rewrite some of my javascript.  I discovered that I needed to do some refactoring to make the tests easier, and now the code is much clearer. The next project was tweaking the visualization page: I had built it so that users had to click on the categories to see the visualizations, but the team  pointed out that users would probably rather see a visualization from the start.  I was able to change our code so that users will always see a visualization for the first category of any given text as soon as the page loads. Finally, we needed to include error messages for Devise, our ruby gem that handles user authentication.  Although we had already written a bunch of error and notification messages (ie. “Invalid Password” or “Your password has been emailed to your account”), we hadn’t added the necessary code to make it work.  After adding a partial file and rendering it on the main application page, and then styling it with CSS, I was able to make all our error and notification messages show up!   Now, the design team will make them look attractive and Alex will add internationalization for the messages, and we’ll be all set! It’s been a busy week, but it looks like we’ll be able to make our deadline!"},{"id":"2012-04-30-welcoming-our-2012-13-praxis-fellows","title":"Welcoming our 2012/13 Praxis Fellows","author":"eric-johnson","date":"2012-04-30 10:18:01 -0400","categories":["Announcements","Grad Student Research"],"url":"welcoming-our-2012-13-praxis-fellows","layout":"post","content":"Before things get too hectic here this week with #prismlaunch, we wanted to briefly turn our attention to next year to say how excited we are to announce and welcome the 2012/13 Praxis Fellows to the second year of the program! They are: Cecilia Márquez, History Chris Peck, Music Claire Maiers, Sociology Gwen Nally, Philosophy Shane Lin, History Sophia Gu, English We’re particularly excited about the diversity of backgrounds and experiences of this new cohort.  While they may well end up extending the Prism tool developed by this year’s project participants, they will bring their own unique combination of disciplinary perspective and technical and social know-how to the program and we look forward to seeing where we end up going together.  We can’t wait to get started with them in the fall. As always, Praxis 2012/13 will be a journey shared in the open: follow developments at praxis.scholarslab.org ."},{"id":"2012-05-01-announcing-prism","title":"Announcing Prism!","author":"brooke-lestock","date":"2012-05-01 05:07:44 -0400","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"announcing-prism","layout":"post","content":"We are pleased to announce the official beta release of Prism, a tool for collecting and visualizing crowd-sourced interpretations of texts. In case you are new to this blog, you should know that Prism is the practicum project of the first cohort of Praxis Program Fellows at the University of Virginia Scholars’ Lab. Six of us have been interning with Scholars’ Lab faculty and staff for the 2011-2012 academic year, as part of a pilot project in team-based graduate methodological training. Praxis aims to produce humanities scholars with practical experience in the work that will underlie theoretical advances in the digital age: the formal representation of knowledge, the design of software and user interfaces, and the management of collaborative teams and complex projects. Over the course of the year, we have been meeting weekly to learn what it takes to theorize and to build a DH project, with the ultimate goal of releasing our own–Prism. Prism is an experiment in visualizing many readings of a common set of texts, using concepts shared by its users–“the crowd.”  While the Praxis Program itself makes an intervention in graduate training, Prism is an intervention in the concept of crowd-sourcing, which until now has mostly made fact-checkers and copy editors of the crowd.  One of the fundamental questions behind Prism is: what happens when the crowd is asked to imagine and interpret, rather than merely transcribe? The goal of Prism is not to replace individual interpretations, but to produce aesthetic provocations, that is, collective visualizations that incite and encourage conversation. Now that you know what Prism is, who made it, and why, we invite you to go use it ! We welcome conversation (we are humanities grad students, after all), so feel free to share your thoughts, feedback, or suggestions in the comment field below. If you encounter a bug, please report it on our GitHub page where you can also find our open source code and documentation. And stay tuned for further blog posts from Praxis Program team members this week, reflecting on what we’ve learned and looking forward to what’s next. –Sarah Storti and Brooke Lestock, for the 2011-12 Praxis Team"},{"id":"2012-05-02-prism-and-praxis-reflections","title":"Prism and Praxis Reflections","author":"annie-swafford","date":"2012-05-02 05:31:17 -0400","categories":["Grad Student Research","Visualization and Data Mining"],"url":"prism-and-praxis-reflections","layout":"post","content":"It’s been a whirlwind of a year, but we finally made it! We have a fully functional tool, we’ve fulfilled our goals from our charter, and we can all look back on everything we’ve learned this year and be astonished by how far we’ve come. Although I wasn’t an absolute newbie to the DH and programming world when Praxis began, I’m still amazed by how much more I know now.  When I began, I knew HTML, CSS, and a smattering of javascript, and now I also know Ruby, Rails, coffeescript, much more javascript, and how to write tests in RSPEC and Jasmine.  I’ve also become better at thinking like a programmer; when I started, it took forever to figure out how to do basic programming tasks, like creating loops or methods, but now it feels straightforward.  I’ve also become much better at trouble-shooting.  I remember when we were all having trouble installing everything we needed for Rails and Ruby, I had no idea what the lines I typed into the terminal meant, and I had no idea what to do when it gave me error messages, but now I understand the error message and know how to fix the problem, or know where to look to find the answers, and I feel much more confident working from the command line instead of the GUI.  The most gratifying experience for me as the developer came from building the highlighting and visualization functionality; it was incredibly empowering to go from having no idea of how to split the task into its component parts or how to then reconnect those parts, to understanding it in the abstract, to actually building the working code.  I especially loved learning how d3.js worked and writing one short algorithm that fundamentally changed the appearance of our text. I also had a fantastic time working with the rest of the team.  We all learned to work together, to help each other solve problems and design Prism, and to laugh together when things didn’t quite go according to plan.  It’s been a pleasure and an honor to work alongside them, and I’ll definitely miss the group next year. Although we’ve worked hard and had our stressful times, Praxis has been an incredibly rewarding experience, and I’m grateful to the Scholars’ Lab for giving me the opportunity to be a part of such a fantastic program."},{"id":"2012-05-03-insert-bad-prism-pun-here","title":"[Insert Bad Prism Pun Here]","author":"eric-rochester","date":"2012-05-03 05:05:40 -0400","categories":["Grad Student Research"],"url":"insert-bad-prism-pun-here","layout":"post","content":"[![Praxis Program](http://dayofdh2012.artsrn.ualberta.ca/nowviskie/files/2012/03/IMG_2717.jpg)](http://dayofdh2012.artsrn.ualberta.ca/nowviskie/2012/03/28/afternoon/) Image courtesy Bethany Nowviskie This is a blog post (one of many) celebrating the release of Prism . This project and web site are the outcome of the Praxis Program, an experiment in graduate methodology training that we’ve been conducting at the Scholars’ Lab. Well, Prism is one outcome of the Praxis Program. The most visible, at the moment. But also the least important. For one thing, post-launch can be one of the most stressful phases of a project. Surprise! Everyone hits your shiny new site, and it falls over like a house of cards. Then the world watches as you panic and scramble to put it back together. It’s good for character and a good learning experience. But not much fun. Also, the senior members of this collaboration—the fulltime faculty and staff of the Scholars’ Lab—have always had a different goal for this project: the students themselves. Over the course of the last year, they’ve received learning opportunities that I wish I’d had when I was a graduate student. We’ve tried to equip them to conceive of and plan large, interesting research projects; to evaluate and learn the technologies required to implement those projects; and to administer and manage them from inception to completion. Unfortunately, the most we can do is point out the outlines of the task before them and point them in (hopefully) the right direction. This is a journey they’ll have to travel for many years. But whatever their career paths—whether tenure track faculty, adjunct faculty, alt-ac, or private sector—these skills will be needed. And in this sense, both Praxis and Prism are just getting started."},{"id":"2012-05-03-on-collaboration","title":"On Collaboration","author":"jeremy-boggs","date":"2012-05-03 11:10:59 -0400","categories":["Grad Student Research"],"url":"on-collaboration","layout":"post","content":"Before I started making and sharing stuff, I always thought it was the maker who had power and authority to dispense knowledge. After I started making and sharing stuff, I began to understand that readers and users, not makers, had far more power, to take in that knowledge, critique it, and use it in new ways. At some point, though, if you’re as fortunate as I have been, the lines between makers and users, teachers and students, become so porous that you just have_ collaborators._ It’s kind of frightening to realize how much you don’t know, and once you realize this, it then becomes frightening to make and share something. But I’m starting to understand, more than I ever have, that this is all part of the process of collaboration, that it’s good to feel this way, and that it shouldn’t debilitate us from learning and doing. Collaboration should provide some relief, even confidence, but it should not abdicate us from learning how to do new things. It demands that we pay attention when we don’t know something, and obligates us to look for further clarification. It requires us to respect the skills and perspectives our collaborators bring to the table. It doesn’t mean you have to learn everything. That’s impossible and silly. But it also doesn’t mean you should avoid learning something . Collaboration is yaking and hacking, then yaking and hacking some more, all the way down. I’m not exaggerating when I say I’ve never been more uncomfortable about things, and more excited by that feeling, than I have the last few months working in the Praxis Program . Most of the stuff we covered was brand new to me, too, so I was every bit as much a student during the process. My lack of comfort with many of the technical topics was completely countered by the studio-like atmosphere the Praxis Program team managed to create by simply working together. We’re all collaborating, all the time; if you don’t think you’re collaborating, you’re probably not paying attention. In the spirit of that, the folks in the Praxis program have made a thing and shared it . I hope I speak for everyone when I say, I hope you’ll collaborate with us."},{"id":"2012-05-03-update-diy-aerial-photography","title":"Update: DIY Aerial Photography","author":"chris-gist","date":"2012-05-03 07:09:23 -0400","categories":["Geospatial and Temporal"],"url":"update-diy-aerial-photography","layout":"post","content":"It’s been over a month since our last post re DIY aerial photography. Since then, we hosted two GIS workshops, a THATCamp Virginia session, and our first real “job” flight for an art installation . Our workshops were scheduled for Wednesday morning, April 18 and Thursday afternoon, April 19.  Unfortunately, it started raining hard just before our Wednesday session and we were forced to go to Plan B. We decided to fly the balloon in the mural hall located inside Clark Hall.  The ceiling there is two stories tall.  We did get some interesting images from that “flight.” Although we think we made the point with the flight inside the building, we hoped for much better conditions the next day. On Thursday, the weather was much better but there was more wind than we hoped.  We knew the images wouldn’t be great but decided to fly anyway.  It was quite an event, NBC 29  (news video) and UVa Today  (lots more pictures and story) came.  We did two new things for this flight.  First, John Porter lent us a GPS to put on the balloon rig.  Second, we inadvertently switched the camera to video mode when loading into the balloon rig. The GPS addition turned out to be a great thing.  It gave us a 3D profile of our flight which we converted to KML for viewing in Google Earth.  Please feel free to download it here . The mistakenly-created video is interesting in places but is not good for those among us who may have motion sickness issues. [iframe width=”600” height=”450” src=”http://www.youtube.com/embed/KYilJ3kDLuI”] ![](http://static.scholarslab.org/wp-content/uploads/2012/05/6948719170_bfa9d9dbd7_b.jpg) The Space Stare Next up was the THATCamp Virginia Workshop. We had about ten THATCampers and decided to fly over Nameless Field.  And since our GPS experiment went so well, it now has become a permanent fixture on our flights.  We got a lot of great shots of the tennis courts.  If you look closely, you can see the crew working to prep the surfaces for new paint. We have a KMZ of the flight available here . Next was the art installation being created on UVa Foundation property on 29 North.  A group of students working with Prof. Megan Marlatt are painting the cat shown below in the parking lot.  They are filling in the cat with stencils of a large number of other cats. Here is our documentation of their work. Of course, we have a KML of the whole mission (two flights) and some images and the original conceptual plan.  Please download it here . Our next big balloon event will be teaching some DIY aerial photography at the STEM conference for k-14 educators in June. Oh and by the way, someone asked how we get that large balloon though rather small doorways. [iframe width=”600” height=”450” src=”http://player.vimeo.com/video/41862950” ]"},{"id":"2012-05-04-kenneth-dean-ritual-revolutions","title":"Kenneth Dean: Ritual Revolutions","author":"ronda-grizzle","date":"2012-05-04 07:57:05 -0400","categories":["Podcasts"],"url":"kenneth-dean-ritual-revolutions","layout":"post","content":"Kenneth Dean Ritual Revolutions: Temple Networks Linking South China to Southeast Asia Dr. Kenneth Dean, Lee Chair in Chinese Cultural Studies in the Department of East Asian Studies at McGill University, spoke as part of the Digital Humanities Speaker Series on March 29. The Digital Humanities Speaker Series is co-sponsored by IATH, SHANTI, and the Scholars’ Lab As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.15310479307/enclosure.mp3”]"},{"id":"2012-05-06-future-possibilities-for-prism","title":"Future possibilities for Prism","author":"david-mcclure","date":"2012-05-06 07:11:35 -0400","categories":["Grad Student Research","Research and Development"],"url":"future-possibilities-for-prism","layout":"post","content":"It’s been incredibly exciting to watch Annie, Alex, Lindsay, Brooke, Sarah, and Ed work together over the course of the last two semesters to take Prism from idea to working software . Considering the fact that most of them hadn’t ever written a line of Ruby, Javascript, or CSS when they started last semester, the end result is pretty remarkable. One of the reasons that programming is so invigorating is that software is constantly leaning forward into further elaboration and complexity. Every feature is the precursor of a hundred possible new ones. Code is always the double-delight of what it is and what it could become. Prism currently occupies the place in the evolution of a software project where there’s enough functionality in place to really engage with the shape of the idea, but still enough unfinished that there’s space for broad, exploratory thought about the future direction of things. Annie did a fantastic job implementing a core interface that allows users to apply concept verticals (“highlighters”) to texts. Looking forward, the motivating question is how this capability can be leveraged to produce concrete scholarly outcomes - ideally, new understandings of texts. The first public release seems like an excellent opportunity to start trying to whittle down this question to a set of specific research goals to guide future development. To me, Prism points towards two general lines of inquiry: How can experiments in collaborative markup capture uncommon or dissenting readings? The concept of crowdsourcing - and, really, the social internet in general - has proven highly adept at extracting majority opinions, at taking the pulse of a group of people. What is “liked” by the community of participants? Where is there agreement? Always implicitly contained in the data that yields these insights, though, is information about how individuals and dissenting groups diverge from the majority consensus. Usually, in the context of the consumer web, these oppositions are flattened out into monolithic “like” or “dislike” dichotomies. Tools like Prism, though, capture structurally agnostic and highly granular information about how users react to complex artifacts (texts - the most complex of things). I think it would be fascinating to try to find ways of analyzing the data produced by Prism that would illuminate places where the experimental cohort profoundly disagrees about things. These disagreements could be interesting irritations into criticism. Why the disagreement? What’s the implicit interpretive split that produced the non-consensus? Continuing on the concept of the “experiment.” Prism points at the provocative possibility that literary study could literally take the form of experiments, similar in structure to the “studies” conducted in disciplines like research psychology, sociology, and experimental philosophy. Literary criticism generally asks questions about how texts can be read. The critic conjures highly creative statements of meaning that often stake their value claim on the extent to which they are unexpected, unanticipated, not obvious, or atypical. Prism, meanwhile, provides information about how texts just are read. I think it would be fascinating to take this to the next level and stage formal experiments in which subjects are presented with a text and asked to mark it up with a small number (even just one) of carefully-selected, highly-controlled terms. Done responsibly, and with a healthy aversion to the sugary siren call of Data in a field that’s fundamentally in the business of studying art, I think that this could provide fascinating insights about everything from the concept of Kantian “expertise” in the formation of aesthetic judgments to the questions about how people of different ages, ethnicities, genders, and disciplinary affiliations engage with texts. How does a college freshman read differently than a 6th year English graduate student? How do physicists read differently from philosophers? Either way, I can’t wait to see where it all goes ."},{"id":"2012-05-07-no-famous-last-words","title":"Got Prism?","author":"alex-gil","date":"2012-05-07 10:47:07 -0400","categories":["Grad Student Research"],"url":"no-famous-last-words","layout":"post","content":"Funny thing about collaborative projects in the (digital) humanities: No famous last words. A nun scholar lives and dies by her last words, but around here we’re just part of an incredibox . On the week when we release Prism, I could regale you with all the lessons I learned this year (right about doubled the stock), how wonderful this whole experience has been (I wouldn’t trade your post-doc for my praxis), what exciting times we live in (don’t look down), but after perusing the drafts queued up on the wp-admin, it sounds like those tunes will be sung to perfection by my fellow praxers. Let me instead play a bit. I’m sure it has not escaped your attention that Prism plays a sweet prank on the discursive class. The premise: graduate students collaborate to build a hermeneutic tool while learning much-needed technical and managerial skills their hermeneutic training made invisible. The punchline: Except for the cast of characters witnessing its birth, the tool the students make renders the hermeneut invisible. Prism yanks the out-of-line hero in Aeschylus right back into the incredibox where he came from. Your username is only visible to you. Once you register for the first time, it descends to the MySQL underworld, effectively re-enacting the Eleusinian Mysteries that preceded the birth of Greek tragedy. “You can check out anytime you like/But you can never leave.” Brilliant! BUT, the playful prank is not without use or merit, and we are laughing with you, not at you. Heck, I AM one of you. Yes, Prism abstracts interpretative labor (c.f. Baudrillard on Symbolic Exchange and Death), but it does so without foul. I’m guessing most folks would find the momentary anonymity quite refreshing. Truth be said, we don’t need to doodle our signatures on every wall. (c.f. Derrida… No? You don’t have to). Furthermore,  and as David shows by example on his recent post, many new avenues of research open up with Prism. The tool does not replace, but sits to the left of other hermeneutic approaches. To pick one question of the many new questions that Prism opens up: Given that anonymity changes the way we enact readings; and, given that the ruling ideology in Amerika would have us believe that racial identity is a thing of the past: How does the invisible subject react to symbolic Law? Obviously, that is not the only new question Prism can ask, nor the first to come to mind for most users. We worked hard to make it generic enough that many disciplines could find plug-n-play uses for it. And then again, what else could Prism lead to, but renewed interest in interpretation as a microscopic-practice?  Yes, the lovely texts! Don’t be fooled by the promise of macroscopic understandings of difference. That is just a (mc)lure."},{"id":"2012-05-09-calling-all-alt-academics","title":"Calling All #Alt-Academics!","author":"katina-rogers","date":"2012-05-09 09:35:19 -0400","categories":["Announcements"],"url":"calling-all-alt-academics","layout":"post","content":"I’m happy to announce that a census of alternate academics, the first public-facing component of my work with the Scholarly Communication Institute, is now open to contributions. If you have graduate training in the humanities and work outside of the tenure track, I’d like to warmly invite you to add your information to the growing database. Not #alt-ac? Check out the report  to learn more about who we are and what we do. As I discussed in an earlier post, the census has a dual purpose: First, it will serve the many individuals who are employed in (or considering) alternate academic roles by showing the breadth and depth of career trajectories that can follow graduate work in the humanities. The resulting database may help people to discover others with shared interests, find potential project collaborators, or open up new lines of inquiry. Second, it serves as an important first step towards the survey that SCI will conduct, which aims at better understanding career preparation and #alt-ac employment in relation to humanities graduate programs. I’d like the database to be as broad and truly representative as possible, which means I’ll need help in extending its reach. Please forward the link widely and encourage the #alt-academics you know to contribute–the database becomes more useful as more people join in. This census is part of a suite of new content and features at #Alt-Academy; the announcement is restated below. Please read, contribute, and circulate! We are very happy to announce a new phase of publication at  #Alt-Academy, an open-access online project at MediaCommons. #Alt-Academy was launched last summer with 24 essays by 33 authors, highlighting the role of “alternative” academic professionals in the humanities and related fields. The four projects joining #Alt-Academy today promise to open the publication to an even richer and more diverse set of voices. Please consider contributing to: “Who We Are,” a census of the community, led by Dr. Katina Rogers, who is also (with the Scholarly Communication Institute) conducting a survey of graduate preparation for alternative academic careers:  http://mediacommons.futureofthebook.org/alt-ac/who-we-are “Visible Margin,” a forthcoming regular publication of the site, edited by Drs. Polina Kroik and S. Miller.  Visible Margin will feature creative and critical work by PhDs, graduate students, and alternative academics:  http://mediacommons.futureofthebook.org/alt-ac/visible-margin “Getting There 2,” a second “Getting There” cluster for #Alt-Academy, offering practical pathways, signposts, and advice for people considering alternative academic careers. This cluster will be edited by Dr. Brian Croxall:  http://mediacommons.futureofthebook.org/alt-ac/pieces/cfp-getting-there-2 and “Alt-Ac Goes Entrepreneur,” a new cluster to be edited by Dr. Daveena Tauber, examining the role of entrepreneurialism in academic training, the knowledge economy, and the alternative academic community:  http://mediacommons.futureofthebook.org/alt-ac/pieces/cfp-alt-ac-goes-entrepreneur Alt-Academy also welcomes proposals for further new clusters and features.  For more information, see “How It Works” on our MediaCommons site."},{"id":"2012-05-11-wrapping-it-up-teidisplay-and-collaborative-mentoring-with-utuva","title":"Wrapping it Up: TeiDisplay and Collaborative Mentoring with UT/UVa","author":"carin-yavorcik","date":"2012-05-11 09:51:32 -0400","categories":["Digital Humanities"],"url":"wrapping-it-up-teidisplay-and-collaborative-mentoring-with-utuva","layout":"post","content":"I’m very excited to announce that my colleague Zane and I have completed our work on the TeiDisplay plugin for Omeka! It’s been a little while since I last posted, so if you’d like some background, check out our previous posts on this collaboration between the Scholars’ Lab and the University of Texas School of Information. We have made several changes/additions, though the final product isn’t quite ready for release yet. We fixed a few bugs in the import process from the TEI Header to Omeka’s Dublin Core fields. Additionally, we created a map detailing which fields correspond to which elements. This map also explains how users can customize the import process if the current setup is not optimized for their documents. The main XSLT stylesheet now supports every element in the TEI Lite customization . We also reorganized it, adding headers to correspond with the headers in the TEI Lite documentation where particular elements are discussed, so that those who are familiar with XSLT can find and edit elements easily. At a minimum, in the transformation process each tag is given a, and many have additional styles as well. This way, we guarantee that a defined set of elements will render in the document. Users unfamiliar with XSLT can also use these span classes to customize the style of their documents using CSS. Some of the elements that have been styled for more than just a span class may special requirements in order to render, such as specific attributes that need to be included in the TEI text. We have written a series of formatting tips to help users understand what the XSLT is looking for when it attempts to render the documents. The plugin now has functionality to link TEI text to corresponding page images. This is done using the @facs attribute with the element. Further details on how to use this feature are provided in the documentation. We outlined a number of small changes that a user can make outside of the plugin code itself in order to improve functionality. For example, when displaying longer documents, users might wish to create an anchor that allows readers to jump directly from the top of the page to view the item metadata, which is usually at the bottom. This is done on one of the Omeka PHP documents, so it isn’t officially part of the TeiDisplay package, but we include the appropriate code in our documentation and describe where to put it. We’ve written a number of troubleshooting tips, as well as a more detailed guide to installing the plugin and updating documents. I’m very happy with the changes we made to the plugin, but I think that overall, they were not as extensive as I expected going in. I think the real value added with this project was the extensive documentation we’ve created, as well as a demo site that gives examples of the plugin in action. Armed with this documentation, users will not only be able to better understand how the plugin works with their documents out of the box, but they will also be better able to customize it to fit their needs. Over the coming months the Scholars’ Lab will be conducting more rigorous testing of the plugin as well as making some other additions. Hopefully, the final product will be ready late this summer. Thanks very much to Wayne and Bethany at the Scholars’ Lab, and to our adviser Tanya Clement at UT, for giving us the opportunity to be a part of this project!"},{"id":"2012-05-14-on-complexity","title":"On Complexity","author":"wayne-graham","date":"2012-05-14 12:54:21 -0400","categories":["Grad Student Research"],"url":"on-complexity","layout":"post","content":"When I wrote my first “real” code for a website, things were a lot simpler. I was taking SGML TEI files and running them through a DSSSL generator to create static HTML files. It was pretty straight-forward: I would tag a document, cross my fingers that it would validate, then run a script that would regenerate the entire site. CSS hadn’t been invented yet, so the site layout was handled in a table, and really the only way ‘discover’ the content in the site was to actually read everything that was contained in the ‘site.’ There was a team of five of us attempting to figure all of this, and generally the feature that won was the one we figured out first. Skip forward a decade, and I now use a dizzying array of frameworks, tools, occasional tricks for any given project. As a case in point, look at any of the plugins our group have been developing for Omeka: we write the code in PHP, then use tooling written in Ruby, Java, and nodeJS.  The real question of how to explain exactly what goes on in a modern web-based application is one I, personally, struggled with over the last year. There’s typically a database tier, a presentation tier, business logic, external libraries, and framework idioms and vernacular to explain. Even once you get an OK grasp of how a programming language functions, the jump from making a simple loop to a building a web application that ‘works’ is quite a feat. On top of this, add the softer skills of translating the intent of a feature into actual code and coordinating changes with multiple people working in the same places in the code base, and even what it means to work together in a productive way can be challenging. Just to give you an idea of the complexity of the things swimming around in the various heads of the team that developed Prism, this is a visualization of the library dependencies in the project: With a system this complex, true collaboration is needed, with different people taking on different tasks and responsibility for sections of the code, and working together. You can see in this impact graph that there isn’t just one person making all the changes. You also see experiments that were started (and ended). Not only are experiments waxing and waning, there is also no single contributor who dominates the code base. I feel this is an exceptionally important take away from this last year; no one collaborator on Prism would have been able to execute this project alone. When I reflect on the changes in how web applications are developed since I started in this kind of work, I see no real method to the madness. It was a wild-wild west, and we figured out what we needed to (often making what in retrospect were horrible technical decisions) in order to ship our sites. But things were much simpler: browsers were much simpler, almost everyone had a dial-up connection, and you could really figure out how someone implemented a cool feature (like an ‘under construction’ gif) by looking at the source code. Fast-forward a decade, and there are many more balls to juggle. Viewing the source code for a web application probably won’t get you very far. Along with understanding how software is put together, there are the more frustrating elements of development. Sometimes approaches just don’t work, or they work in one environment, but not another. Dealing with these issues, and being able to adapt or completely rethink a plan of attack, is a skill in itself. Being able to pivot technically, and rethink a given approach (or even better, knowing where that line is) is one of the more difficult skills to learn. As I’ve watched the graduate students in our Praxis Program over the last year, I’ve seen them transform how they think about the problems they encounter, applying the same analytical skills they use in their scholarship in their coding practices. My hope is that the practice in critical thinking and scholarly argument with which they learned to imbue the actual code base of Prism will help them as they go out in to the world as scholars – and will continue to serve them in the future."},{"id":"2012-05-14-spatial-in-the-scholars-lab-spring-2012","title":"Spatial In the Scholars’ Lab: Spring 2012","author":"kelly-johnston","date":"2012-05-14 05:17:47 -0400","categories":["Digital Humanities","Geospatial and Temporal","Grad Student Research"],"url":"spatial-in-the-scholars-lab-spring-2012","layout":"post","content":"With classes over and finals behind us, let’s look back on the Spring 2012 semester with a spatial eye.  Yes, January-May was a very mappy time in the Scholars’ Lab! [![](http://static.scholarslab.org/wp-content/uploads/2012/05/P1010036-766x1024.jpg)](http://www.scholarslab.org/digital-humanities/spatial-in-the-scholars-lab-spring-2012/attachment/olympus-digital-camera/) University of Virginia Library Scholars' Lab Workshops From January through April twice every week my colleague Chris Gist and I welcomed faculty, staff, and students to our free “no experience required” hands-on spatial workshops. The picture below gives you a glimpse behind the curtain at our master workshop list on the left.   In case you missed one, our handouts, datasets, and workshop presentations are all available here: http://www.scholarslab.org/resources/class/Spring2012GIS/  Though our workshops have been popular for many semesters, we shook it up this spring with new sessions based on questions we’ve been asked, trends we’ve spotted, or mappy topics we wanted to get to know better.  And what a treat it was to collaborate with friends and colleagues from the Scholars’ Lab  Research and Development team (thanks Eric Rochester), UVA Facilities Management (thanks Drew MacQueen and Artie McDonough), and the UVA Library (thanks Tim Morton). For all 24 workshops, folks turned out from across disciplines, asked lots of questions, shared their mappy stories, and learned new skills after a hands-on hour.  Later this summer we’ll announce our Fall 2012 workshop series. Classes Like us, students get excited when they see their datasets come alive on a map.  Some of our best experiences working with geospatial resources in the Scholars’ Lab come from collaborations with faculty to develop specialized training for their courses.  American Studies, Architecture, English Criticism, Urban Planning, Environmental Science…we worked with them all this spring.  Check our whiteboard again and on the right side you’ll see some of our Spring 2012 faculty partnerships.  Now we’re meeting with faculty to prepare for Fall 2012. [![](http://static.scholarslab.org/wp-content/uploads/2012/05/amst-1024x702.jpg)](http://www.scholarslab.org/digital-humanities/spatial-in-the-scholars-lab-spring-2012/attachment/amst/) Spring 2012 American Studies 3559 - Mapping Shantytowns Support in the Scholars’ Lab In the Scholars’ Lab, we relish a “GIS Emergency”.   Typically a student visits the lab to work on a project, hits a roadblock, and contacts us for help.  So we drop what we’re doing and spring into action. “How do I use this cool historic map in my GIS project?” “I don’t know exactly where it happened, but I need to show it on a map” “I have a Mac.  I want to run ArcGIS.  Help.” “We’re leaving for Guatemala tomorrow. May I borrow a GPS?” “US Census data from 1950…where do I find it?” “How can I compare solar exposure for my study sites in the French Alps?” Combining drop-in support with email queries, our official tracking system, and pre-arranged meetings gives us a great view of how spatial topics are increasingly visible across academic disciplines.  And it means I have 1,495 emails in my GIS Support folder since Jan 1, 2012. On The Horizon Soon we’ll end the spring semester by hosting Scholars’ Lab grad fellow Ed Triplett for our last Scholars’ Lab mappy event of the Spring .  And we’ll start the summer with Mapping Yourself for Reunions Weekend.   Though our two hard-working GIS students will be gone for the summer, one having graduated and one home to China, we’ll continue our spatial work at James Madison University where we’ll lead two sessions at the 4th National Summit on Geospatial Technologies in K-14 Education .  And our online work continues with contributors and reviewers as we add new content to Spatial Humanities Step By Step, a peer-reviewed series of tutorials and guides to getting things done in teaching and research with spatial tools and resources.  We’ll train a new group of students for summer fieldwork at Mountain Lake Biological Station and keep an eye on the sky as we pursue do-it-yourself remote sensing . Have a very mappy summer!"},{"id":"2012-05-15-praxis-doing-is-thinking","title":"Praxis: Doing is Thinking?","author":"lindsay-oconnor","date":"2012-05-15 06:52:41 -0400","categories":["Grad Student Research","Research and Development"],"url":"praxis-doing-is-thinking","layout":"post","content":"Speaking off the cuff to a group of prospective Praxis Program applicants in March, I found myself explaining how “aesthetic provocation” isn’t the same as argument, that Prism isn’t a tool that produces criticism as much as it is a tool that might prod us to see, read, or critique in new ways. I didn’t know I knew that until I said it in front of 30 people, and that experience might serve as a nice synecdoche for what Praxis is all about. I calmed down about all those big ideas I blogged about in the fall when I started to have design deadlines. I quickly remembered my pre-graduate school ambivalence about being a manager when trying to coordinate design team tasks. I learned CSS tricks when I had things I needed to style in certain ways. Sometimes I worry that I just gave up on all the big ideas in favor of shorter-term, more immediately satisfying web design lessons and goals, that I somehow betrayed my disciplinary yack in favor of less heady but more hands-on hack. But maybe we’ve just been too busy with the latter lately, and these final blogs provide a place for the former now that Prism has been released. I know, as Bethany eloquently explains in her Chronicle article and her blog that Prism and much of DH is focused on creating and enabling new hermeneutics rather than on advancing critique. For that reason, I’m not yet imagining a digital component to my dissertation, but that doesn’t mean I’m not a digital humanist. Praxis has shown me a different, collaborative model of scholarship. Prism as it exists now could use some supporting research and theorization, especially about what kind of hermeneutics our working visualization instantiates or suggests, but it’s still a starting point for many possible readings or arguments to which many people can contribute. I may not be creating a digital dissertation chapter like Annie or a whole digital edition like Alex, but I did manage to learn enough about HTML and CSS this year to style a decent looking website, and I think I can figure out how to style one that looks even better. I could have kept tinkering with Prism for much longer and I know it’s not the most sophisticated or aesthetically pleasing site you’ve ever seen, but I’m still proud to have been part of making it look how it looks. I will continue to seek out opportunities to become a better designer and to work on projects that interest me, and I’ll soon be starting design work with the wonderful UVa English department folks behind redschoolhouse.org. And that means I’ll probably stop into the Scholars’ Lab every so often for design advice from Jeremy and technical help from Wayne and Eric. I know I’ll see Sarah and Annie in the English department, and I hope to run into Ed in line at the dumpling truck. I wish Brooke and Alex all the best next year wherever they find themselves. I have learned so much from and with all you wonderful people, and the technical stuff is the least of it."},{"id":"2012-05-16-the-sidlers-guide-just-smile-and-hit-your-mark","title":"The Sidler's Guide: Just Smile and Hit Your Mark","author":"ed-triplett","date":"2012-05-16 07:08:37 -0400","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"the-sidlers-guide-just-smile-and-hit-your-mark","layout":"post","content":"I can’t help feeling like a Seinfeldian sidler. Prism is up. The code has been written, the pages designed, and the tests passed. Everyone is gathering together getting ready for the post-prism photo-op when out of nowhere some bearded weirdo who smells like tapas and olive oil rides into the frame on a tiny horse just before the flash goes off. “Remember me amigos? I was here when Prism was static!” Honestly it has been very nice to see Prism come together while I was in Spain. I am also proud to see all my Praxis fellows transform into real code monkeys. I had a very quick and grainy glimpse of a functional Prism page when I attended a Praxis meeting about a month ago via webcam, but last week I got the full tour. In a lot of ways I am in a unique position to appreciate how far Prism has come because the software was a squalling infant when I left. Prism is all grown up now, and it is time to send it into the world to socialize with others so it doesn’t end up too “home-schooled.” While my contribution to Prism was almost entirely reserved to designing static templates, I can confidently say I feel very connected to the final product that the group executed. I spent a lot of time in the the Scholars’ Lab during the Fall and early part of the Spring semester, and I hope to be around just as much in the future. Leading up to this Fall, I was far from a rookie when it came to applying graphics solutions to humanities subjects, but until this year, I was entirely at the mercy of a mouse. I am not going to claim I am a huge fan of VIM, but programming is no longer something I try to avoid when I consider how to solve a DH problem. Finally, I’d like to thank the entire Scholar’s Lab staff for their help and patience. I’m sure the first few weeks of Praxis seemed like you adopted too many puppies, but we are house trained now and github is always willing to play fetch with us."},{"id":"2012-05-23-thatcampva-2012-neatline-workshop","title":"THATCampVA 2012: Neatline Workshop","author":"ronda-grizzle","date":"2012-05-23 07:26:02 -0400","categories":["Podcasts"],"url":"thatcampva-2012-neatline-workshop","layout":"post","content":"David McClure &amp; Eric Rochester\nTHATCampVA 2012 Neatline workshop On April 20, 2012, Scholars’ Lab developers David McClure and Eric Rochester gave a talk for interested attendees of THATCAmpVA 2012 introducing  Neatline + Omeka and updating us on the latest developments in the project. Enjoy! As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.15640754273/enclosure.mp3”]"},{"id":"2012-05-29-gff-hankins","title":"Graduate Fellows Forum: Gabriel Hankins","author":"ronda-grizzle","date":"2012-05-29 09:56:55 -0400","categories":["Podcasts"],"url":"gff-hankins","layout":"post","content":"Gabriel Hankins The Map of an Argument: the Spatial Humanities, Modernism, and the League Moment, 1914-1921 On April 11, 2012, Scholars’ Lab Fellow Gabriel Hankins spoke about his work with the Scholars’ Lab during his fellowship in a talk entitled The Map of an Argument: the Spatial Humanities, Modernism, and the League Moment, 1914-1921 . Dr. Jennifer Wicke, Professor, UVa Department of English, acted as respondent for Gabriel’s talk. As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.15515954661/enclosure.mp3”]"},{"id":"2012-05-29-gff-lewis","title":"Graduate Fellows Forum: Randi Lewis","author":"ronda-grizzle","date":"2012-05-29 10:05:58 -0400","categories":["Podcasts"],"url":"gff-lewis","layout":"post","content":"Randi Lewis Mapping the Merchant World: Early American Commerce in Salem, Massachusetts, 1763-1819 On April 26, 2012, Scholars’ Lab Fellow Randi Lewis spoke about her work with the Scholars’ Lab during her fellowship in a talk entitled Mapping the Merchant World: Early American Commerce in Salem, Massachusetts, 1763-1819 . Dr. Max Edelson, Associate Professor, Corcoran Department of History, acted as respondent for Randi’s talk. As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.15636184338/enclosure.mp3”]"},{"id":"2012-05-29-gff-triplett","title":"Graduate Fellows Forum: Edward Triplett","author":"ronda-grizzle","date":"2012-05-29 10:14:57 -0400","categories":["Podcasts"],"url":"gff-triplett","layout":"post","content":"Edward Triplett Military-Religious Orders on the Border with Islam: Mapping the Architectural Progression of the Iberian Reconquest On May 16, 2012, Scholars’ Lab Fellow Edward Triplett spoke about his work with the Scholars’ Lab during his fellowship in a talk entitled Military-Religious Orders on the Border with Islam: Mapping the Architectural Progression of the Iberian Reconquest . Dr. Eric Ramírez-Weaver, Assistant Professor, McIntire Department of Art, acted as respondent for Edward’s talk. As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.15983817764/enclosure.mp3”]"},{"id":"2012-05-31-learning-and-improving","title":"Learning and Improving","author":"eric-rochester","date":"2012-05-31 10:23:08 -0400","categories":["Research and Development"],"url":"learning-and-improving","layout":"post","content":"Mentoring and training have always been a big part of the Scholars’ Lab’s mission. With the Praxis Program, that focus has only intensified over the last year. All this emphasis on growth has me thinking about how I try to improve professionally myself. For the last few years, I’ve been delving into functional programming . This started by working first with Clojure and more recently with Haskell . In 2008 I wrote an extended tutorial introduction to Clojure and natural language processing as a series of blog posts . Although they’re a little out of date, they still see some traffic. Not long ago, Manuel Kiessling commented on the page describing some functions for the stemming algorithm, suggesting an improvement to the m function. This function takes the word currently being stemmed, along with some other data, and counts the number of consonant sequences between the beginning of the word and the end (not counting the initial consonant cluster, if there was one). The description of the algorithm gives these examples: Inputs Consonant Sequence Count tr, ee, tree, y, by 0 trouble, oats, trees, ivy 1 troubles, private, oaten, orrery 2 For reference, here was what I originally wrote: [sourcecode language=”clojure”]\n(defn m\n  “Measures the number of consonant sequences between\n  the start of word and position j. If c is a consonant\n  sequence and v a vowel sequence, and &lt;…&gt; indicates\n  arbitrary presence, -&gt; 0 vc -&gt; 1 vcvc -&gt; 2 vcvcvc -&gt; 3\n    ...\n  \"\n  [stemmer]\n  (let [\n        j (get-index stemmer)\n        count-v (fn [n i]\n                  (cond (&gt; i j) [:return n i]\n                        (vowel? stemmer i) [:break n i]\n                        :else (recur n (inc i))))\n        count-c (fn [n i]\n                  (cond (&gt; i j) [:return n i]\n                        (consonant? stemmer i) [:break n i]\n                        :else (recur n (inc i))))\n        count-cluster (fn [n i]\n                        (let [[stage1 n1 i1] (count-c n i)]\n                          (if (= stage1 :return)\n                            n1\n                            (let [[stage2 n2 i2] (count-v (inc n1) (inc i1))]\n                              (if (= stage2 :return)\n                                n2\n                                (recur n2 (inc i2)))))))\n        [stage n i] (count-v 0 0)\n        ]\n    (if (= stage :return)\n      n\n      (count-cluster n (inc i)))))\n[/sourcecode] Wow. That’s a mess. (For better syntax color highlighting, see this Gist .) Essentially, this is a state machine that kind of trampolines through the characters in a word. count-v walks through a vowel cluster; count-c walks through a consonant cluster; and count-cluster controls the process of alternating between these two functions. The first result passed out of each of these (usually assigned to a variable named stage ) is what needs to happen next: either :return from m with the current count or :break from counting the current sequence (either vowel or consonant). The other two outputs are the character currently being considered and the number of consonant sequences seen so far. Don’t worry if that doesn’t make sense. The code up there is actively obtuse. It’s trying to make you dumber. Manuel suggested merging the three functions together to get this: [sourcecode language=”clojure”]\n(defn m\n  “Measures the number of consonant sequences between\n  the start of word and position j. If c is a consonant\n  sequence and v a vowel sequence, and &lt;…&gt; indicates\n  arbitrary presence, -&gt; 0 vc -&gt; 1 vcvc -&gt; 2 vcvcvc -&gt; 3\n    ...\n  \"\n  ([stemmer]\n    (m stemmer 0 0)) ([stemmer num-c num-cs]\n    (if (not (seq (:word stemmer))) ; Is the word empty? Then we reached the beginning of the stemmer\n      (if (&gt; num-c 1)               ; THEN1: More than 2 consonants in current counting?\n        (inc num-cs)                ; THEN2: we have one more consonant sequence, and we return the number of sequences found plus 1\n        num-cs)                     ; ELSE2: Return the number of sequences found\n      (if (consonant? stemmer)                        ; ELSE1: Is there a consonant at the current index?\n        (recur (pop-word stemmer) (inc num-c) num-cs) ; THEN3: increase the number of currently consecutice consonants, recur\n        (if (&gt; num-c 1)                               ; ELSE3: If not, check if we found more than 1 consecutive consonants\n          (recur (pop-word stemmer) 0 (inc num-cs))   ; THEN4: If yes, we found one more sequence\n          (recur (pop-word stemmer) 0 num-cs))))))    ; ELSE4: If not, then we found only one, and start anew\n[/sourcecode] It’s better, but still not great. (It also doesn’t work, the way it’s presented here, but fixing that wouldn’t be hard, so I’m neither going to make an issue of it nor fix it.) So what’s up? I thought that functional programming was supposed to be concise yet readable. Well, when I wrote the first version, I was still learning to think functionally. I ended up with an imperative algorithm, without mutable state. In other words, I was enjoying the worst of both worlds. I decided to refactor again, this time being more functional. Here’s what I ended up with: [sourcecode language=”clojure”]\n(defn m\n  “Measures the number of consonant sequences between the start of a word an\n  position j. If c is a consonant sequence and v a vowel sequence, and &lt;…&gt;\n  indicates arbitrary presence, -&gt; 0 vc -&gt; 1 vcvc -&gt; 2 vcvcvc -&gt; 3\n  ...\n  \"\n  [stemmer]\n  (let [consonant-group? (fn [ws] (consonant? (first ws)))]\n    (-&gt;&gt; stemmer\n      (iterate pop-word)                    ; a sequence of all the parts of the word, from the whole word to the first letter.\n      (take-while (fn [w] (seq (:word w)))) ; stop when done with the word\n      (partition-by consonant?)             ; break it into vowel/consonant groups\n      (reverse)                             ; reverse so the next step works\n      (drop-while consonant-group?)         ; remove the first (was at the end of the sequence) constant group\n      (filter consonant-group?)             ; filter out vowel groups\n      count)))                              ; finally, count the remaining consonant groups\n[/sourcecode]\n\nBetter (although I already see some places that could be improved). It really doesn't need the comments I added, and if this weren't pedagogical code, I would have left them out.\n\nSo what made the difference?\n\nPrimarily functional programming tries to abstract control flow out into [higher-order functions](http://en.wikipedia.org/wiki/Higher_order_functions) like `map`, `filter`, and `reduce`. (Or in this case, `iterate`, `take-while`, `partition-by`, `drop-while`, and `filter`.) In other words, the code to walk over the items in a sequence (`map`) is separated from what you do with those items, or the code to remove items from a list (`filter`) is separated from the predicate that determines which items to keep. This makes the data flow very clear.\n\nBut what is more interesting about this exercise is reflecting on my own growth as a programmer. As I mentor others, I often want to save them the pain and ugliness I've been through. But I need to remember that the only way to learn to write clear code is to write spaghetti code first and live through the fall-out."},{"id":"2012-06-12-neatline-sneak-peek","title":"Neatline Sneak-Peek","author":"david-mcclure","date":"2012-06-12 08:01:59 -0400","categories":["Geospatial and Temporal","Research and Development"],"url":"neatline-sneak-peek","layout":"post","content":"The R&amp;D; team here at the lab has been quiet over the course of the last couple weeks, but there’s been a flurry of activity under the surface - we’ve been hard at work putting the finishing touches on Neatline, a geotemporal exhibit-building framework that makes it possible to plot archival collections, narratives, texts, and concepts on interactive maps and timelines. Neatline is built as a collection of plugins for Omeka, a digital archive-building framework developed by our partners at the Roy Rosenzweig Center for History and New Media at George Mason University. If you already have an Omeka collection, Neatline provides a deeply integrated, plug-and-play mapping solution that lets you create interpretive views on your archive. If you don’t have an Omeka collection, though (or if it doesn’t make sense represent your material as a collection of archival objects), Neatline can also be used as an effectively standalone application from within the Omeka administrative interface. If you haven’t been following the project, check out the podcast of the workshop that Eric Rochester and I gave at THATCamp Virginia 2012 and read the announcement about our partnership with RRCHNM. So, what kinds of things can you do with Neatline? Here are a few: Create records and plot them on interlinked maps and timelines with complex vector drawings, points, and spans. Set colors, opacities, line thicknesses, point radii, and gradients. Add popup bubbles and define interactions among the map, timeline, and a record-browser viewport, which can display everything from short snippets and captions to long-format interpretive prose. Connect your exhibits with web map services delivered by Geoserver, which makes it possible to create rich displays of historical maps. Drag the viewports around to create custom layouts. Set visibility intervals on a per-item basis, making it possible to create complex time-series animations. Create hierarchical relationships among items, making it possible to curate “batches” of elements in an exhibit that can be manipulated as a group. (Using the Neatline Editions plugin, which is still in alpha and won’t be ready until later in the summer) Create interactive editions of texts by connecting individual paragraphs, sentences, or words with locations on maps and timelines. Watch this space in the first week of July for the full public release with the dedicated website, code, documentation, and a hosted “sandbox” version of the application that will let you register and experiment with creating exhibits before downloading the software. Until then, here are a handful of screenshots from some of the demo exhibits we’re working on:"},{"id":"2012-06-25-diy-aerial-photography-in-a-crowd","title":"DIY Aerial Photography In A Crowd","author":"kelly-johnston","date":"2012-06-25 11:28:58 -0400","categories":["Geospatial and Temporal"],"url":"diy-aerial-photography-in-a-crowd","layout":"post","content":"The Lawn is the center of Mr. Jefferson’s historic Academical Village at the University of Virginia .  We took our Scholars’ Lab DIY aerial photography equipment to the Lawn on Sunday afternoon as part of the University Library’s effort to document the events surrounding President Sullivan’s resignation. Since our last flights, we’ve upgraded our camera to a GoPro HD Hero2 and kept the same balloon aerial photography rig . We’ve learned this kind of aerial photography is like farming, completely dependent on the weather. So we kept one eye on the sky as a strong summer thunderstorm cell loomed .  We’ve dealt with wind before, but keeping the balloon out of the trees while walking through a crowd of over 1000 people with winds ranging 0-4 the Beaufort Scale was our challenge. Calm winds yield a long straight string and a straight down view. Gusty winds cause the camera to swing on its pendulum and we get a beautiful oblique view of lush green summer in Virginia. The crowd gathers before the event.  The Rotunda roof project accounts for the elaborate curved scaffolding. Chris was hoping for calm when he walked toward the front of the crowd.  Can you find him in the image below? [![](http://static.scholarslab.org/wp-content/uploads/2012/06/JoeyTombsRotunda-571x1024.jpg)](http://www.scholarslab.org/geospatial-and-temporal/diy-aerial-photography-in-a-crowd/attachment/joeytombsrotunda/) via @JoeyTombs Views from lower altitudes compress distances and make the crowd appear more dense. While an overhead view shows people tend to stand in remarkably straight lines with impressively uniform spacing. Blue tarps cover West Lawn roofs where chimney repairs are ongoing. A gust of wind gave us a view of more blue tarps covering East Lawn chimneys and our first glimpse of Old Cabell Hall, and the Health Systems complex Existential Threats (and storms and wind and crowds) Don’t Scare Us, We’re Librarians!"},{"id":"2012-07-02-announcing-neatline","title":"Announcing Neatline!","author":"bethany-nowviskie","date":"2012-07-02 11:01:04 -0400","categories":["Announcements","Geospatial and Temporal"],"url":"announcing-neatline","layout":"post","content":"What do you get when you cross archives and artifacts with timelines, modern and historical maps, and an appreciation for the interpretive aims of humanities scholarship? Today, the Scholars’ Lab is proud to announce the launch of Neatline, our set of Omeka plugins for hand-crafted geo-temporal visualization and interpretation. You can head right over to http://neatline.org/ to download the 1.0 software, see sample exhibits or play in the sandbox, and read more about the project, including news and history . Neatline is a geotemporal exhibit-builder that allows you to create beautiful, complex maps and narrative sequences from collections of archives and artifacts, and to connect your maps and narratives with timelines that are more-than-usually sensitive to ambiguity and nuance. In other words, Neatline lets you make hand-crafted, interactive stories as interpretive expressions of an archival or cultural heritage collection. Every Neatline exhibit can be your own close reading of a humanities collection – expressed in the visual vernacular.  Ours is a small-data approach in a “big data” world. Stay tuned to the Scholars’ Lab blog and to Neatline.org for a series of posts and screencasts to be shared over the course of the next two weeks. We’ll be providing support for this open-source software on the Omeka forums and dev list ."},{"id":"2012-07-10-announcing-a-new-sci-study-on-alternative-academic-career-paths","title":"Announcing a new SCI study on alternative academic career paths","author":"katina-rogers","date":"2012-07-10 06:02:46 -0400","categories":["Announcements"],"url":"announcing-a-new-sci-study-on-alternative-academic-career-paths","layout":"post","content":"I’m pleased to announce that the  Scholarly Communication Institute  is conducting  a study on career preparation in humanities graduate programs . As part of this study, we have launched two confidential surveys: the  first is for people on alternative academic career paths  (that is, people with graduate training in the humanities and allied fields working beyond the professoriate); the  second survey is for their employers .  The surveys will be open until October 1, 2012. Humanities scholars  come from a wide array of backgrounds and embark on a variety of careers  in areas like libraries, museums, archives, higher education and humanities administration, publishing, research and technology, and more. SCI anticipates that data collected during the study will contribute to a  deeper understanding of the diversity of career paths  we pursue after our graduate studies, while also highlighting  opportunities to better prepare students for a range of careers  beyond the tenure track. The surveys  complement the   public database  that we recently created as a way to clarify the breadth of the field, and to foster community among a diverse group. If your work represents the diversity of the broad #alt-ac community, it’s not too late to  tell us about yourself! The surveys and directory are being administered as part of the Scholarly Communication Institute’s  current phase of work  – which includes a close concentration on graduate education reform (largely in the North American context) and the preparation of future knowledge workers, educators, and cultural heritage and scholarly communications professionals. The survey results will help us to make curriculum recommendations so that graduate programs may better serve future students, and anonymized or summarized data will be made available at  #alt-academy  at a later date. Please  contact me  if you’d like to know more. Complete the main #alt-ac survey ** Complete the employer survey ** Add your information to the database ** View the database report ** Other ways to get involved"},{"id":"2012-07-17-neatline-and-the-framework-challenge","title":"Neatline and the framework challenge","author":"david-mcclure","date":"2012-07-16 22:55:19 -0400","categories":["Geospatial and Temporal"],"url":"neatline-and-the-framework-challenge","layout":"post","content":"With the first public release of Neatline out the door, I’ve had the chance to take a short break from programming and think back over the nine-month development cycle that led up to the launch. In retrospect, I think that a lot of the exciting challenges we ran up against - the big, difficult questions about what to program, as opposed to how to program - emerged from tensions that are inherent in the task of creating frameworks as opposed to conventional applications. What’s a framework? As an experiment, I’ll define the term broadly to mean applications that make it possible to create things, as opposed to applications that make it possible to accomplish tasks . Frameworks are generative in a way that normal applications are not. Instead of controlling systems, crunching numbers, automating processes, boosting efficiency, or providing entertainment, frameworks are set apart by the fact that the allow the user to spawn off new things that are independent of the software itself. Microsoft Word is used to create documents ; Wordpress is used to create blogs and blog posts ; Drupal is used to create websites ; Ruby on Rails is used to build web applications ; Illustrator is used to create vector graphics ; Maya is used to create 3d models and animations . Omeka and Neatline fit straightforwardly into this definition. Omeka is used to build online digital collections; Neatline, a framework-within-a-framework built on top of Omeka, is used to create interactive maps and timelines. In each case, the final unit of analysis is some sort of discrete, addressable thing that is generated with the assistance of the software. It can be viewed, visited, or printed. Frameworks empower users to create things that would be difficult or completely impossible to create without the assistance of the software. The paradox, though, is that frameworks have to simultaneously constrict the user’s agency in the act of expanding it. Barring some kind of mythological ur-framework that would allow for direct, unmediated, and unbounded realization of thought (Prospero’s book of magic), all frameworks, whether implicitly or explicitly, have to define a range of final outputs that will be “supported” by the software. In practice, this means paring down the supported outputs to a vanishingly small subset of the original possibility space. Frameworks are defined as much by what they disallow as by what they allow. For the developer, deciding on the “range” of the framework is a difficult and sometimes agonizing process because it involves a fundamental tradeoff between power and accessibility - and, by extension, the size of the potential audience. As a framework becomes more powerful and allows a wider range of possible outputs, it also becomes more complex and locks out users who aren’t willing to invest the effort to become proficient with the tool. As a framework becomes more narrow and focused, a larger number of people will be able and willing to use it, but the diversity of the final outputs drops, and the tool becomes suitable for a much smaller range of use cases. It’s a zero-sum game. Over the course of the last couple months, I’ve realized that this opposition between power and ease-of-use provides an interesting vocabulary for defining Neatline and situating it in the ecosystem of existing geospatial tools. Up until now, it seems to me that existing frameworks have clustered around the two ends of the power / ease-of-use spectrum. Consumer web applications like the Google mapmaker allow the user to drop pins and annotate them with short captions. This is delightfully easy, but all of the end-products look the same, and the tool doesn’t really provide a critical mass of flexibility and the opportunity for real intellectual ownership that’s required for serious scholarly use. Meanwhile, at the other end of the spectrum, desktop GIS applications like ArcMap provide an incredibly powerful and feature-rich platforms for analyzing geospatial data and creating visualizations. For projects that have access to custom software development, programming libraries like OpenLayers, Leaflet, PolyMaps, and Timeglider provide flexible, highly-customizable toolkits for creating interactive maps and timeline - but only at the level of code. There’s been an underpopulated zone in the middle the spectrum, though - not many spatio-temporal tools have tried to more evenly balance power and accessibility. Neatline tries to land in a “goldilocks” zone between the two poles. It tries to be simple enough out-of-the-box that it can be used by a large majority of scholars and students who do not have programming experience or advanced GIS expertise, but still complex enough to allow for significant diversity in the structure and style of the final output. This means, of course, that Neatline could be more powerful and could be easier to use. My argument, though, is that it couldn’t both - at least, not without tripping over itself and breaking apart into incoherence. Instead of choosing one pole at the expense of the other, we decided to make a studious attempt to balance the two. This is difficult to do - perhaps more difficult than committing wholesale to one or the other, which can often have the effect of locking in a cascading series of almost automatic design decisions leading towards a more singular objective. Building “middle-ground” frameworks requires a constant (re-)calibration of the feature set over the course of the development process, a sort of gyroscopic vigilance to keep the software perched in the tricky zone between flexibility and accessibility. Like all real challenges, though, this one was also fantastically exciting to tackle. Now that Neatline is out in the wild, I can’t wait to see what people create with it."},{"id":"2012-07-20-parent-child-relationships-in-neatline","title":"Parent-child relationships in Neatline","author":"david-mcclure","date":"2012-07-20 01:19:18 -0400","categories":["Geospatial and Temporal"],"url":"parent-child-relationships-in-neatline","layout":"post","content":"One of the most powerful features in Neatline, our newly-released Omeka -based tool for geo-temporal interpretation of humanities collections, is the ability to create parent-child relationships between records in an exhibit. Any record can be the parent of any other record, and there are no limits on the depth of the nesting - a parent record can itself have a parent record, and so on and so forth. This relationship is established from the child to the parent. To set a parent record, click into the “Relations” fieldset and use the dropdown to select a record: What’s the point of this? When you set a parent record, the child automatically inherits all of the styling and visibility settings of the parent . In a nutshell, this makes it possible to create “batches” of records that share a common set of styles and phase in and out of visibility in unison. For example, imagine that you need to split the records in your exhibit into two a “blue” category and a “red” category. Instead of combing through each record and typing in the exact same lineup of styles for all the records in each of the categories, you can just create two abstract “template” records that contain the style defaults for each group and associate each of the content records with one of the templates. With six records, three blue and three red, that would look like this: Blue 1, Blue 2, and Blue 3 are children of [Blue Parent], and Red 1, Red 2, and Red 3 are children of [Red Parent]: And in the final exhibit, the colors are rendered correctly without ever having to set a single style on Blue 1, Blue 2, Blue 3, Red 1, Red 2, or Red 3: Powered by Neatline View fullscreen This also works for the record visibility settings that control the date range on the timeline during which the a record is visible - the “Start Visible Date” and “End Visible Date” fields in the “Temporal” fieldset. In the example above, say that you want the blue points to be visible from 1900-1960, and the red points from 1940-2000. Set these visibility intervals on the parent records, and the child records will phase in and out of visibility in unison: Powered by Neatline View fullscreen What if you need to selectively override the defaults, though? What if you want a record to inherit most style and visibility settings from upstream in the inheritance chain, but you want to adjust one or two settings to differentiate the record? For example, imagine you want one of the blue points to be yellow - but you still want it to phase in and out of view with the other blue points. Instead of having to break away the record and re-set all of the settings just in order to make the color change, you can just directly change the color setting on the record, and all of the other unchanged settings will continue to inherit from upwards in the chain: Powered by Neatline View fullscreen Neatline always tries to find record-specific value first, meaning that an inherited value can always be clobbered by a locally-set value (think of it as !important in CSS). If Neatline doesn’t find a record-specific value, it starts to traverse up the inheritance chain to the parent record(s), and stops when it finds a record-specific value on one of the parents. If none of the parents have a value for the attribute in question, then Neatline falls back on the exhibit default values, which can be configured in the “Map Settings” dropdown tab. Parent records in action How does this work with real content? The Battle of Chancellorsville demo exhibit makes heavy use of parent records. This is a complex exhibit with a lot of moving parts. There are three separate base maps, one for each of the three days of the battle - May 2, May 3, and May 4, 1863. Each of the maps has a large collection of spatial annotations and numbered waypoints that are relevant to just one of the maps - as the map switches out in response to the position of the timeline, the corresponding set of annotations and waypoints needs to phase into view at the same time. Meanwhile, the spatial vectors can be broken down into categories that should share similar styles - the Union and Confederate lines should all share the same shades of blue and red. You could just go through and directly set the correct colors and visibility dates on each individual record. This is labor-intensive, though, and it tends to lock you into design decisions that you make at the beginning of the process – if you change your mind down the road and want to adjust the Union blue, you’d have to work through 50-odd records and update them individually. Parent records make it possible to formalize the conceptual relationships and manipulate the groupings in bulk – once the correct inheritance chain is set up, you can set the style and visibility settings a single time at the top of the stack and the settings will cascade downwards to all of the children. For this exhibit, the inheritance structure looks like this: ` May 2, 1863 (visible: May 2 - May 3)\n— [may 2 condeferate lines] (color: #b52f2f)\n——– May annotation 1 (visible: May 2 - May 3; color: #b52f2f)\n——– May annotation 2 (“ “)\n——– May annotation 3 (“ “)\n——– (…)\n— [may 2 union lines] (color: #093696)\n——– May annotation 4 (visible” May 2 - May 3; color: #093696)\n——– May annotation 5 (“ “)\n——– May annotation 6 (“ “)\n——– (…) May 3, 1863 (visible: May 3 - May 4)\n— [may 3 condeferate lines] (color: #b52f2f)\n——– May annotation 7 (visible: May 3 - May 4; color: #b52f2f)\n——– May annotation 8 (“ “)\n——– May annotation 9 (“ “)\n——– (…)\n— [may 3 union lines] (color: #093696)\n—— May annotation 10 (visible: May 3 - May 4; color: #093696)\n—— May annotation 11 (“ “)\n—— May annotation 12 (“ “)\n—— (…) May 4, 1863 (visible: May 4 - May 5)\n— [may 4 condeferate lines] (color: #b52f2f)\n——– May annotation 13 (visible: May 4 - May 5; color: #b52f2f)\n——– May annotation 14 (“ “)\n——– May annotation 15 (“ “)\n——– (…)\n— [may 4 union lines] (color: #093696)\n——– May annotation 16 (visible: May 4 - May 5; color: #093696)\n——– May annotation 17 (“ “)\n——– May annotation 18 (“ “)\n——– (…) ` The top-level visibility dates are set on the three records that house the base maps. Under each of the three map records, abstract style records define the colors and opacities for the Union and Confederate lines. The actual content records then inherit from these records, receiving both the top-level visibility parameters on the map records and the styles on the abstract records. Now, there is some duplication of content here - the colors for Union blue and Confederate red have to be set separately in each of the three sets of abstract styling records. This is because all of the styles/visibilities on record can only be a part of a single inheritance chain, making it necessary to “split” each of the three chains under the top-level map records. Originally, I toyed around with the idea of making it possible to create “style-specific” inheritance chains - so, for example, a record could inherit its fill color from one record, its line width from another, its visibility from another, etc. In the end, though, this would have required a large amount of added UI overhead, and the same results can be achieved with a minimal amount of extra work with the technique used here."},{"id":"2012-07-27-diy-aerial-photography-and-edgar-allan-poe","title":"DIY Aerial Photography and Edgar Allan Poe","author":"kelly-johnston","date":"2012-07-27 05:21:16 -0400","categories":["Geospatial and Temporal"],"url":"diy-aerial-photography-and-edgar-allan-poe","layout":"post","content":"Earlier this year Professor Megan Marlatt from the University of Virginia McIntire Department of Art began work with her students to create a jumbo outdoor mural titled “Hello Pluto, Good-bye Kitty” based on Edgar Allan Poe’s short story “ The Black Cat ”.   The mural design covered a large suburban parking lot. From street level, viewers see small cats painted on the parking lot. But when we gain the larger view by lofting a camera attached to a helium balloon several hundred feet into the air we get our first views of the entire work in progress. “‘Hello Pluto, Good-bye Kitty” was off to a fine start but much work was still to be done. When Professor Marlatt and her students completed their work a few weeks later we returned with our aerial photography rig to document the finished product. [![](http://static.scholarslab.org/wp-content/uploads/2012/07/GOPR1623cropped-1024x730.jpg)](http://www.scholarslab.org/geospatial-and-temporal/diy-aerial-photography-and-edgar-allan-poe/attachment/dcim100gopro-12/) \"Hello Pluto, Good-bye Kitty\" This project illustrates some of the benefits of do it yourself aerial photography.  With a small investment of equipment and time we collected high resolution imagery to document change over time for a discrete study area. We see applications for these techniques across many academic disciplines.  So we’re working now to fine-tune our approaches while we collaborate with faculty and students. Contact us in the Scholars’ Lab to chat about how your work can benefit from do it yourself aerial photography."},{"id":"2012-07-31-translating-neatline","title":"Translating Neatline","author":"jeremy-boggs","date":"2012-07-31 11:19:27 -0400","categories":["Research and Development"],"url":"translating-neatline","layout":"post","content":"If you’re fluent in English and another language, and would love to help with the Neatline project, please consider contributing a translation for our Neatline plugins! We’re using a service called Transifex to manage translation work. To get started, just sign up for a free account on Transifex (or log in using your Twitter or Facebook account), then check out our  Neatline project page . If you already see a language listed, just click on it, then request to join that language team to begin contributing. If the language you’d like to contribute doesn’t appear on the list, just click the link near the top of the languages list to “Request a new team” and we’ll add that team to the list, and add you to it. Once all that is set up, you can begin adding translations to any (or all!) of our plugins. Once you’re added to a language team, you can click on that language, and see a list of the resources (i.e., Neatline plugins) we have available to translate. Clicking on any of the resources should bring up a modal window with various options for translating the resource, including a button to “Translate Online”. From there, you’ll be presented with a page that has the English word or phrase on one side, and a text area to contribute a translation on the other. [![](http://static.scholarslab.org/wp-content/uploads/2012/07/Screen-shot-2012-07-31-at-3.18.19-PM.png)](http://www.scholarslab.org/slab-code/translating-neatline/attachment/screen-shot-2012-07-31-at-3-18-19-pm/) Translation interface on Transifex.net Translating static word strings—phrases like “Save your exhibit”, for example—are pretty straight-forward. Occasionally, though, you will run across a string that includes text like %s or %$1s. This text is a placeholder for dynamic content, generated by the Neatline plugin when used in Omeka. To translate strings with these placeholders, simply translate the other words in the string, and move the placeholder text wherever it would make sense for your translation. For example, the string “The timeline “%s” was successfully added!” is translated into Spanish as “Se agregó la secuencia “%s”!” The Omeka team also manages their translations through Transifex, and they have some nice  documentation on contributing translations  using the platform; if you have questions, try checking their page. Additionally, Transifex also has some nice documentation for many features of their service, including contributing translations . As translations are completed, we’ll add them to each plugin’s  languages  directory before each release. So if you’d like to see Neatline in a different language, please consider contributing a translation. We appreciate your help in giving Neatline a truly global reach!"},{"id":"2012-08-03-data-visualizations-learning-d3-js","title":"Data visualizations: Learning d3.js","author":"katina-rogers","date":"2012-08-03 08:35:30 -0400","categories":["Visualization and Data Mining"],"url":"data-visualizations-learning-d3-js","layout":"post","content":"[cross-posted at katinarogers.com ] The SCI study on humanities graduate programs and career preparation is humming along, and while survey responses come in, I’ve been working on determining how best to translate the data into meaningful graphics. After a lot of experimenting, I think the winner is d3.js . Short for for Data-Driven Documents, D3 is Michael Bostock’s creation; a quick glance at his gallery shows the kinds of beautiful and complex visualizations it’s capable of. It’s a low-level tool, though, which means that learning to use it even in a rudimentary way has already involved picking up some html, css, and javascript along the way. It’s a lot to chew on, but I think I’m starting to turn a corner as a blurry whirl of concepts, terms, and commands are slowly resolving themselves into some clarity. While I don’t have anything that cool that to show yet, I’m excited that I do have a little something. Here’s the fruit of everything I’ve learned so far:\n[iframe height=”200” width=”690” src=”http://katinalynn.github.com/dataviz/demo_randomCircles”] It might not look like much, but you guys, I drew those shapes with CODE! (And a lot of help. Mostly I drew them with a lot of help, actually.) In my earlier post on text mining, I also included some images made from data – but in that case, the graphics you see in the post are nothing more than links back to the tool itself, which does all the work. The image here is different. Here’s what’s so great about it: The shapes are determined by data in the code. In this case, the size of the circles is determined by a dataset of randomly generated numbers, because that’s what’s specified in the code. Don’t believe me? Reload the page – the shapes change! I actually understand the code. I can play with it, change things, and not break it. I made this particular image by starting with samples from  Scott Murray’s great tutorials, then building on what I learned, combining elements from various lessons to create a new graphic. I learned a ton trying to get the image to display in this post; turns out that it’s not as simple as uploading an image or linking to a page, or even pasting the code into the post. I now have a much better sense of what does and doesn’t work in WordPress; how iframes work; how to really use an FTP client; how to create and work with GitHub repositories; and how to publish GitHub Pages so that you can actually see the rendered images, not the source code. I have been learning tech skills in scattered bits and pieces, and this is the first time some of the threads have come together instead of constantly branching off in new directions. I feel like I’m starting to understand how to actually make something on the web. (There must be something in the air in the Scholars’ Lab that makes it impossible to resist the desire to make things.) It feels empowering – I was incredibly giddy when I finally got the images to display in the draft post – but also really humbling, like I’m trying to tie my shoes while wearing mittens. Right now I’m just excited to be doing something that I can point to and look at. It feels a little like magic. I’m also excited about D3 itself, which @thisisaaronland recommended when I told him about my project. In recent weeks I’ve been exploring all manner of data visualization tools, and while I knew that any of them would require that I learn new skills (or else settle for paltry Excel charts), I was becoming overwhelmed by the options. Everything I looked at seemed to need a different language – python, R, etc. – and I was already feeling like I had started too many new things without becoming proficient in any of them. D3 is quite powerful, and learning it should help me to start applying some of what I’ve learned while also pushing me into new terrain. It’s also what the Praxis team used for the visualisations in Prism, so all signs point to it being a great visualization tool that’s worth taking the time to learn."},{"id":"2012-08-09-timr-optimizing-web-requests","title":"timr: Optimizing Web Requests","author":"eric-rochester","date":"2012-08-09 05:38:20 -0400","categories":["Research and Development"],"url":"timr-optimizing-web-requests","layout":"post","content":"One of the fundamental tensions in programming is balancing the program’s requirements for time (programmer time and running time) against its space requirements (disk space and memory space). Optimizing one of these costs—i.e., looking for ways to shift that balance, usually to have the program run faster—is a common task. Recently, I’ve needed to speed up requests on a couple of different websites I’ve worked on: Neatline and a small, personal work-in-progress I call What is DH? . Of course, optimizing programs too early can turn your program into an unreadable mess and waste your time. ( The Wikipedia page on Program Optmization has a good overview of the issues and trade-offs.) The rule is: don’t optimize . But if you must do it, do it right. That’s where this post comes in. Lather, Rinse, Repeat A typical work flow when optimizing a program goes something like this: Measure how long it takes or now much memory it takes right now. Don’t skip this. If it’s good enough, stop ; otherwise, keep going. Change something. GOTO 1 . That seems simple enough, but it’s really quite complicated. For example, in a web app, many things slow down requests. One slow database query. Too many database queries. Pulling in too much unused data from the database. One intensive computation. A bunch of small computations. I’m leaving out maybe one or two things, but you get the idea. The timings are also complicated by a number of factors: The interpreter needs to allocate a bunch of memory (instead of using pre-allocated memory), which is relatively slow. The interpreter executing your program could decide to take out the garbage during the run, effectively tying up your program. Your computer/OS may suddenly decide that it has to do some intensive computation right now, ’cause, well, you know, computers are helpful like that. A bunch of small tasks may start up, creating a smaller, but still noticeable, performance hit. You have no control over any of this, and they will all throw off the timings. Generally, I’ve learned to take a number of measurements (3–5, say), and take the lowest . Not the average. The lowest will be the time of the processing, with the least about of other things interfering. You’re Wrong! There’s one complication I haven’t mentioned yet. The biggest problem with optimizing code is this. Your intuitions about what is so slow are wrong . Maybe not always, but often enough that you shouldn’t trust them. Or to put it another way: Bottlenecks occur in surprising places, so don’t try to second guess and put in a speed hack until you have proven that’s where the bottleneck is. — Rob Pike (And to be fair, the tool I’m getting ready to describe, timr, doesn’t help you identify which part of your code is taking so much time, but it will tell you whether what you changed helped or not. Finding bottlenecks will be the subject of another blog post.) My Kingdom for Some Data Because you’re going to be wrong, optimization is largely a data-driven task. What data? Multiple timings for each small change you make. You probably only want to look at a summary each group of timings, however. The return value of each web request. Whatever you changes you make, you probably don’t want this to change. Data is just another word for lots of bookkeeping, which is another way of saying boring and error-prone . Software developers hate boring and error-prone, and I’m no exception. As I was working on optimizing an AJAX call in Neatline, I created a small script to help me keep track of the data I was accumulating. I call this timr (because leaving out vowels is always a good idea). Installing Timr requires Python, and if you have Python and Pip, you can install it with: [sourcecode language=”bash”]\npip install timr\n[/sourcecode] Using Timr is a command-line tool, and once it’s installed, using it is pretty straight-forward. Configuration Files The easiest way to use it is to gather all the command-line arguments for a project into an ad hoc configuration file. For example, save this as fetch.conf . It will time a POST request with my name, and it will send the output to fetch-output.csv : [sourcecode autolinks=”false”]\n–method\nPOST\n–url\nhttp://whatever.com/resource/\n–header\nAccept: application/json\n–data\nfirst_name=Eric\n–data\nsurname=Rochester\n–output\nfetch-output.csv\n[/sourcecode] NB: Remove the extra lines around the URL. For some reason, WordPress adds those in, but they shouldn’t be there and will cause an error if they’re included. These values won’t change between runs, so this provides consistency and documentation. Fetch Now, call timr fetch with the arguments from the configuration file, plus the message that you want attached to the timing group (in this case, initial timings). [sourcecode language=”bash”]\ntimr fetch @fetch.conf -m “initial timings”\n[/sourcecode] This executes the POST request multiple times (4 times, by default) and write the resulting times out to a CSV file. Report Looking at the raw output isn’t that helpful, however. Instead, you want to summarize and aggregate the timing sessions. Most of the time, I just dump the aggregate data out to the screen: [sourcecode language=”bash”]\ntimr report –input=fetch-output.csv\n[/sourcecode] But sometimes I want a pretty chart or graph. Timr doesn’t do visualizations, but you can send the CSV to a file. This way you could pull it into Excel or something that does do visualizations. [sourcecode language=”bash”]\ntimr report –input=fetch-output.csv –output=report-output.csv\n[/sourcecode] That’s really all there is to it. E.G. For example, let’s see how fast a Google search for timr is over a couple of sessions. First, we’ll create a configuration file named google.conf : [sourcecode autolinks=”false”]\n–method\nGET\n–url\nhttp://www.google.com\n–data\nq=timr\n–output\ngoogle-timr.csv\n[/sourcecode] Now run it a couple of times: [sourcecode autolinks=”false”]\ntimr fetch @google.conf -m “initial search”\ntimr fetch @google.conf -m “another session”\n[/sourcecode] This doesn’t actually pull up the search results. Instead, it goes to the page that looks like it should have results, but only has the search suggestion drop-down at the top of the page. I’m not going to worry about that right now. After all, trying to optimize Google search results isn’t very useful unless you work at Google. Let’s see what this outputs: [sourcecode]\n2012-08-07 10:13:08.871731,03a227c0-e09a-11e1-ad5b-c82a1417b0e9,initial search,02a0f14a92b4e0070ee17275f1d78c3a7db1ba68,955,0.141083955765\n2012-08-07 10:13:08.871731,03a227c0-e09a-11e1-ad5b-c82a1417b0e9,initial search,02a0f14a92b4e0070ee17275f1d78c3a7db1ba68,955,0.0433859825134\n2012-08-07 10:13:08.871731,03a227c0-e09a-11e1-ad5b-c82a1417b0e9,initial search,02a0f14a92b4e0070ee17275f1d78c3a7db1ba68,955,0.0436539649963\n2012-08-07 10:13:08.871731,03a227c0-e09a-11e1-ad5b-c82a1417b0e9,initial search,02a0f14a92b4e0070ee17275f1d78c3a7db1ba68,955,0.044303894043\n2012-08-07 10:14:03.237169,240e6f5c-e09a-11e1-962c-c82a1417b0e9,another session,02a0f14a92b4e0070ee17275f1d78c3a7db1ba68,955,0.389742851257\n2012-08-07 10:14:03.237169,240e6f5c-e09a-11e1-962c-c82a1417b0e9,another session,02a0f14a92b4e0070ee17275f1d78c3a7db1ba68,955,0.0447700023651\n2012-08-07 10:14:03.237169,240e6f5c-e09a-11e1-962c-c82a1417b0e9,another session,02a0f14a92b4e0070ee17275f1d78c3a7db1ba68,955,0.0436999797821\n2012-08-07 10:14:03.237169,240e6f5c-e09a-11e1-962c-c82a1417b0e9,another session,02a0f14a92b4e0070ee17275f1d78c3a7db1ba68,955,0.0441081523895\n[/sourcecode] The fields here are a timestamp for the run, a unique identifier hash for the session, the session message, a SHA hash of the results, the number of bytes returned, and the elapsed seconds for the request. Now let’s generate the report, dumping it to a file: [sourcecode language=”bash”]\ntimr report –input=google-timr.csv –output=google-report.csv\n[/sourcecode] And this outputs: [sourcecode]\n03a227c0-e09a-11e1-ad5b-c82a1417b0e9,initial search,0.0433859825134,0.141083955765,0.0681069493294,0.0486528640897\n240e6f5c-e09a-11e1-962c-c82a1417b0e9,another session,0.0436999797821,0.389742851257,0.130580246448,0.172775632452\n[/sourcecode] The fields here are the session identifier, the session message, and some summary statistics on the timings: 1. minimum time,\n\n\n2. maximum time,\n\n\n3. mean time, and\n\n\n4. standard deviation. From this we can see several things: The minimum times are very close (0.0433 and 0.434). There’s a lot more variance in the maximum times (0.141 and 0.390). This could be caused by network latency or other issues and doesn’t accurately reflect the time it took Google to process the query. But looking at the output from the timr fetch calls, the first request takes the longest, and that could be because the Python VM is warming up. The added time of the first request throws off the mean and standard deviation, so they’re not that useful either. More Information and Feedback For more information about timr, see the readme . Timr is a very new tool, and there are lots of missing features or even bugs. If you have a feature request or encounter a problem, please create a new Github issue . For example, I could imagine that having the option to throw out the longest or first timing when generating the report would be helpful. What do you think?"},{"id":"2012-08-20-using-neatline-with-historical-maps-georeferencing","title":"Using Neatline with historical maps :: Part 1 - Georeferencing","author":"david-mcclure","date":"2012-08-20 05:24:20 -0400","categories":["Geospatial and Temporal"],"url":"using-neatline-with-historical-maps-georeferencing","layout":"post","content":"[Cross-posted from dclure.org and neatline.org ] Out of the box, Neatline (our recently-released framework for building geotemporal exhibits) can be used to create geo-temporal exhibits based on “modern-geography” base-layers - OpenStreetMap, Google satellite and street maps, and a collection of beautiful, stylized layers from Stamen Design . For historical and literary projects, though, one of Neatline’s most powerful features is its deep integration with Geoserver, an open-source geospatial server that can pipe georeferenced historical maps directly into Neatline exhibits. For some examples of this, check out these four demo exhibits built on Civil War battle maps by Jedediah Hotchkiss. Geoserver is a pretty complex piece of software, and the process of assigning geographic coordinates to static image files (called “georeferencing” or “georectifying”) can be a bit tricky at first. This is the first post in a three-part series that will walk through the entire process of rectifying a historical map using ArcMap, post-processing the image, uploading it to Geoserver, and importing the final web map service into a Neatline exhibit. Georectification To start, all you need is a static image file that can be positioned in some way or another on top of a real-geography base layer. Usually, this is a map of some sort, but it could also be aerial photography, or, in more experimental and interpretive use-cases, it could even be a totally non-geographic image that would gain some kind of meaning from being situated in a geospatial context (for example, see the georeferenced manuscript pages in the “ My Dear Little Nelly ” exhibit). Since the final map will be presented in an interactive environment that lets the user zoom in and out at will, it’s best to try to find a high-resolution version of the image you want to work with, which will make it possible to zoom further in before the image starts to noticeably pixelate. That said, the images don’t need to be excessively large - as Kelly Johnston (one of the GIS specialists in the Scholars’ Lab) pointed out, extremely high-fidelity images (~10,000 pixels in height or width) often don’t really provide that much more value than somewhat smaller images, and can have the effect of choking up Geoserver and slowing down the speed with which the map is rendered in the final Neatline exhibit. For historical and literary use cases, I’ve found that images with dimensions in the 3000-5000 pixel range provide a good balance of resolution and speed. In this tutorial, I’ll be working with map #124 in the Hotchkiss Map Collection at the Library of Congress (see the full list of maps here ). To get the static image file, go to the view page for the map and right click on the “Download JPEG2000 image” link at the bottom of the screen and click “Save Link As…” With the image in hand, let’s fire up ArcMap and get the environment set up: Add a base map by clicking on File &gt; Add Data &gt; Add Basemap . The base map is the real-geography foundation, the “true” map against which the image will be referenced. Select one of the nine options and click “Add.” This is largely just a matter of preference. For for maps with a lot of human geography (roads, railroads, cities), I like the “Bing Maps Road” layer, and for maps with natural geography (rivers, mountains, coastlines) I like the “USA Topo Maps” layer. After you’ve added a base map, a listing the layer will appear in the “Table of Contents” column on the left, which lists out all of the assets available in the environment. You can toggle layers on and off by clicking the checkbox next to the layer title. Add the static image that you want rectify by clicking on File &gt; Add Data &gt; Add Data . Navigate to the location of the image, select it, and click “Add.” (Note: If the folder containing the image is not already available in the dropdown menu to the right of “Look in,” you may have to “connect” to the folder by clicking on the folder icon with the black “+” symbol in the toolbar to the right. Select the folder, click “OK,” and the folder should become available in the main dropdown menu.) If you get a popup asking if you want to generate pyramids, click “No,” and if you get an alert labeled “Unknown Spatial Reference,” click “OK” (ArcMap is just reacting to the fact that the image doesn’t have existing geo-coordinates). Enable the Georeferencing toolbar by clicking Customize &gt; Toolbars &gt; Georeferencing . The toolbar will appear at the top of the screen, and can be merged into the main top bar by dragging it upwards in the direction of the main navigation controls. Move to the rough location of the image that’s being rectified by using the navigation controls at the left of the top toolbar to zoom the base map to the approximate location and bounds of the historical map. In this example, since the image I’m working with shows the town of Fredericksburg and the course of the Rappahannock southeast of the town, I’ll center the viewport a bit below and left of Fredericksburg, maybe zoomed back a bit to show the whole area that will be covered by the image. Show the static image by clicking on Georeferencing &gt; Fit To Display . This just plasters the map directly on top of the base layer, using the bounds of the current viewport (set in the first step) to determine the position and scale of the image. Basically, this is just setting a crude, starting starting set of geo-coordinates that can be refined by laying down point associations. Now, the actual rectification. All this entails is creating a series of associations (at least two, as many as ~15-20) between points on the static image and points on the real-geography base layer. As you add points, ArcMap will automatically pan, rotate, scale, and ultimately “warp” the image to match the underlying base layer. Lay a positioning point : I like to start by picking the most obvious, central, easy-to-find point on the historical map. In this case, I’ll use the position at which the Richmond Fredericksburg Railroad crosses over the west bank Rappahannock. To lay the first point, click on the “Add Control Points” button in the Georeferencing toolbar and click at the exact position on the historical map that you want to use as the starting point. Then, without clicking down on the map viewport again, move the cursor over to the “Table of Contents” pane and check off the historical map, leaving just the base layer visible. Then, click on the location on the base layer that corresponds to the original location on the historical map. Once you’ve clicked for a second time, the dotted line between the two clicks will disappear. Display the historical map again by checking the box next to its title in the “Table of Contents.” The image will now be anchored onto the base layer around the location of the first point association. Lay a scaling and rotation point : Next, pick another easily-mappable point on the historical map, this time ideally near the edges of the image, or at least some significant distance from the first point. Follow the same steps of clicking on the historical map, hiding the historical map, clicking on the corresponding location on the base layer, and then re-enabling the historical map to see the effect. At this point, you already have a minimally rectified image - the second point will both scale the image down to roughly correct proportions and rotate the image to the correct orientation. From this point forward, adding more points will make the rectification increasingly accurate and granular by “warping” the image, like a sheet of rubber, to fit the lattice of points as accurately as possible. How many points is enough? Really, it depends on the accuracy of the map and objectives of the Neatline exhibit. In this case, Hotchkiss’ map is already quite accurate, and the just first two points do a pretty good job of orienting the map and showing how it fits into the larger geography of the region. For literary and historical projects that don’t gain anything from extreme precision, a handful of points (2-5) is often sufficient. When a higher level of precision is required, though, or when the historial map is significantly inaccurate (as is the case for older maps), more points (10-20) can be necessary. It’s not an exact science - just lay points until it looks right. As you work (especially in cases where you’re laying down a lot points) experiment with different “transformation” algorithms by clicking Georeferencing &gt; Transformations and selecting one of the five options (1st Order Polynomial, 2nd Order Polynomial, etc). Behind the scenes, these algorithms represent different computational approaches to “fitting” the image based on the set of control points - some of the transformations will leave the image roughly polygonal, whereas others will dramatically “warp” the shape of the image to make it conform more accurately to the point associations. Depending on the type of image you’re working with and its accuracy relative to the base layer, different transformations will produce more or less pleasing results. For now, I’ll just leave it at 1st Order Polynomial. Once you’re done laying points, save off the image as a georeferenced .tiff file by clicking Georeferencing &gt; Rectify . As desired, change the filename and target directory, and click “Save.” Links ArcGIS georeferencing documentation Quantum GIS georeferencing tutorial (open-source alternative to ArcMap) Georeferencing - making historic maps spatial [Cross-posted with dclure.org ]"},{"id":"2012-08-23-using-neatline-with-historical-maps-part-2-transparency","title":"Using Neatline with historical maps :: Part 2 - Transparency","author":"david-mcclure","date":"2012-08-23 05:33:08 -0400","categories":["Geospatial and Temporal"],"url":"using-neatline-with-historical-maps-part-2-transparency","layout":"post","content":"Update 8/27/12: After posting this last week, a comment by KaCeBe led me to go back and look for a way to get Geoserver to render transparent borders without having to manually add an alpha channel to the file. Although I still can’t find way to make Geoserver do it automatically, I did find this thread on the OSGeo.org forums in which user bovermyer finds a solution that’s much faster than the Photoshop workflow described in this post. With gdal installed (see below), open up the terminal and run this command: gdalwarp -srcnodata 0 -dstalpha file1.tif file2.tif …where file1.tif is the name of the original file generated by ArcMap and file2.tif is the name of the new, transparency-added copy of the file1.tif generated by gdal. Then (re)build the geotiff with this command: gdal_translate -of GTiff -a_srs EPSG:4326 file2.tif file2_rebuilt.tif …which we’ve found is necessary to avoid errors during the Geoserver upload process. At this point, file2_rebuilt.tif is ready to be loaded into Geoserver and brought into a Neatline exhibit. Much faster than pointing-and-clicking in Photoshop! [Cross-posted with dclure.org and neatline.org ] This is part 2 of a 3-post tutorial that walks through process of georeferencing a historical map and using it in Geoserver and Neatline. Check out part 1, which covers rectification in ArcMap. In the first part of this series, we brought a static image into ArcMap and converted it onto a georeferenced .tif file. In this article, we’ll post-process the image in Photoshop to get it ready to be loaded into Geoserver. The problem: Black borders around the image If you open up the newly-generated .tif file in a regular image editing program, you’ll see that ArcMap added in regions of black around the actual map to make it fill the rectangular aspect ratio of the file. This happens almost every time, since the process of rectification usually involves rotating the image away from its original orientation. In the context of a Neatline exhibit, this is problematic because the black borders will completely occlude the real-geography base layer (or underlying historical maps) immediately surrounding the image. Fortunately, former Scholars’ Lab GIS assistant Dave Richardson figured out how to strip out the borders in Photoshop by converting them into transparencies. This step is a bit of a nuisance, but we’ve found that it dramatically improves the final appearance of the map. Here’s how to do it: Go to the directory that the file was originally saved to. You’ll notice that ArcMap actually generated four files - the .tif, along with a .tfw, tif.aux.xml, and .tif.ovr. Leave all the files in place, since we’ll need them at the end of the process to rebuild the geospatial header information after we post-process the image. Open up the main .tif file in Photoshop. In Photoshop, right click on the starting background layer and click “Layer from Background.” This will delete the locked background and replace it with a regular layer with the same content. Use the “Magic Wand Tool” to select each of the borders by holding down the shift key and clicking inside the black areas. A dotten line will snap to the edges of the borders. If the wand tool is selecting parts of the actual map image, drop down the “Tolerance” setting to 1, which will limit the selection to the exact color value of the clicked location on the image. Once the borders are selected, press the delete key to clear out the selection. At this point, the image should be surrounded by the default, checkered background graphic. Add an alpha channel to the image by clicking on the “Channels” tab on the top toolbar of the layers window (If the “Channels” tab isn’t available by default, activate it by clicking Window &gt; Channels ). Click the dropdown icon at the right of the toolbar, and click “New Channel.” Check the “Masked Areas” radio button, and set the color to be pure black with 0% opacity. Click “OK” to create the channel. Now, activate the Magic Wand Tool again and select each of the checkered, transparent areas around the image (the regions that were originally filled with the black borders). Then, invert the selection by clicking on Select &gt; Inverse . At this point, the selection should exactly frame the map itself (the portion of the image that should not be transparent). Back over in the Channels tab, click on the listing for the Alpha channel that was created in step 4 and hide the RBG channels by clicking the visibility checkbox next to the top-level RGB listing. This will cause the image to go totally black, with the selection of the map region still active on top of the alpha channel. Activate the Paint Bucket Tool and set the foreground color to pure white (If you don’t see the icon for the paint bucket in the Tools column, click and hold the icon for the “Gradient” tool and a drop-down select will appear with a listing for the Paint Bucket). Then apply the paint bucket on the selected area on the Alpha channel, creating a white area over the region occupied by the map. Make sure that both the Alpha channel and all of the RGB color channels are marked as visible in the Channels window. Then go to File &gt; Save As . So as not to override the name of the original file, change the name to something like [original filename]_processed . Uncheck “Layers,” check “As a Copy” and “Alpha Channels,” and click “Save.” On the “Tiff Options” dialog box, leave “Save Image Pyramid” and “Save Transparency” unchecked and make sure “Discard Layers and Save a Copy” is checked. Now, we have a second version of the .tiff file with an Alpha channel that converts the black borders into transparent regions. The problem, though, is that the process of re-saving the file strips out the critical geospatial information in the original .tiff - we’ll have to insert this data back into the processed file before it can be used in Geoserver and Neatline. Rebuilding the geotiff We’ll take care of this using a utility called gdal, a powerful command line library that can do a wide variety of transformations on geospatial files. Head over to gdal.org for full documentation on how to install the command line utilities. On Mac OSX, using the homebrew package manager, it should be as easy as brew install gdal . If you’re on Windows, a binary distribution of the tool can be found here . With gdal installed, fire up the terminal and change into the directory with the original .tif, the processed .tif, and the three *.tfw files. First, create a copy the original .tfw file with a name that matches the processed .tif file that was created in step 8 above. So, if the original .tif was called hotchkiss.tif, and the processed file was saved as hotchkiss_processed.tif, copy hotchkiss.tfw as hotchkiss_processed.tfw (this can be done with cp hotchkiss.tfw hotchkiss_processed.tfw ). The file names have to match in order for gdal to know where to pull information about the coordinate projection when we rebuild the header. Now, still assuming we’re working with files named hotchkiss_processed.tif and hotchkiss_processed.tfw, rebuild the header with this command: gdal_translate -of GTiff -a_srs EPSG:4326 hotchkiss_processed.tif hotchkiss_processed_rebuilt.tif . ( Note : It doesn’t actually matter what you call the derivative files at the various steps of the process. All that matters is that the .tfw file matches the name of the processed .tif file.) This will create a new file called hotchkiss_processed_rebuilt.tif that contains the transparency channel and the reconstructed geospatial information. At this point, the file is ready to be uploaded to Geoserver and brought into a Neatline exhibit."},{"id":"2012-08-29-using-neatline-with-historical-maps-geoserver","title":"Using Neatline with historical maps :: Part 3 - GeoServer","author":"david-mcclure","date":"2012-08-29 05:55:07 -0400","categories":["Geospatial and Temporal"],"url":"using-neatline-with-historical-maps-geoserver","layout":"post","content":"Note This is specifically for Omeka/Neatline 1.x. If you are using Omeka/Neatline 2.x, you can upload your maps to Geoserver with Option 2 below. Follow Editing Record Imagery for working with the WMS layers. [Cross-posted with dclure.org and neatline.org ] This is part 3 of a 3-post tutorial that walks through process of georeferencing a historical map and using it in GeoServer and Neatline. In part 1 of this series, we used ArcMap to convert a static image into a georeferenced .tiff file. In part 2, we post-processed the file with gdal to remove the black borders around the image. In this article, we’ll load the .tiff file into GeoServer and import the final web map service into a Neatline exhibit. Generating the web map service on GeoServer There are two ways to upload the .tiff file to GeoServer - the entire process can be performed through the Omeka interface using the Neatline Maps plugin, or the file can be uploaded directly onto the machine running GeoServer and the service created by way of the GeoServer administrative interface. The first option is easier, but there’s a fundamental restriction that makes it unworkable in certain situations - since Neatline Maps has to upload the .tif file through Omeka before it can create the map service via the GeoServer API, it’s impossible to upload files through Neatline Maps that are larger than the file upload limit set by the upload_max_filesize and post_max_size settings in the php.ini file on your server. Depending on the hosting environment, these values can be set to anywhere from 2-20 megabytes by default. If you have access to the php.ini file, you can bump up the limit, but beyond a certain point it probably makes more sense just to upload the file directly to the server running GeoServer and create the web services manually using the GeoServer administrative interface. Since high-resolution .tiff files can weigh in a hundreds of megabytes or even gigabytes, this is often a more controlled and reliable approach, especially in cases where you’re working with multiple files at once. Regardless of how the file is uploaded, the final process of importing the map service into Omeka and Neatline works the same way. Option 1: Upload through Neatline Maps If your file is small enough to be uploaded through Omeka, the Neatline Maps plugin provides plug-and-play connectivity with GeoServer: With Neatline Maps installed, click on the “Neatline Maps” tab in the top toolbar of the Omeka administrative interface and click on “Create Server.” Fill in the URL, Username, and Password for your GeoServer. In the Name section, enter a plaintext identifier for the server (used for content management in Omeka) and use the Workspace field to specify the workspace on the GeoServer installation that will house the new stores and layers. Click “Save” to create the server record.( Note : If you want to upload files to more than one installation of GeoServer, you can create as many server records as you want. At any given point, though, only one of the record can be marked as the “Active” server - this the server that the plugin will use to handle new .tif uploads). Create an item to associate the web map service with (or edit an existing item). In the Item add/edit form, click on the “Files” tab, click on “Choose File,” and select the .tiff file as you would for a regular file upload. When you save the item, Neatline Maps will automatically detect that you’re trying to upload a georeferenced .tif file and create a corresponding web map service by way of the GeoServer API. Once you’ve saved the file, if you go back into the Item edit form and click on the “Web Map Service” tab, you’ll notice that “WMS Address” and “Layers” fields have been automatically updated to point to the new web map service. On the show page for the item, the map will be displayed in a small, interactive widget below the default metadata fields. Option 2: Upload directly to GeoServer First, upload the file to the server running GeoServer with scp or another file transfer protocol. It’s usually a good idea to get the file out of the /tmp directory, but it doesn’t matter beyond that - GeoServer can read the entire file system. We’ve gotten into the habit of putting the source .tiff files in /var/geotiff . In the GeoServer administrative interface, click on “Stores” in the left column and then click “Add new Store.” On the next screen, click GeoTIFF under the “Raster Data Sources” heading. Select a workspace for the store and enter a name. Under “Connection Parameters,” click the “Browse..” link, and use the pop-up window to navigate to the file. Click “Save” to create the store. Next, we have to publish the store as a public-facing layer. On the next screen, click the “Publish” link. Now, the tricky part. We have to manually tell GeoServer to deliver the layer using a coordinate projection system that Neatline can use to layer the map on top of the real-geography base layers in OpenLayers. Scroll down to the “Coordinate Reference Systems” heading and enter EPSG:900913 into the “Declared SRS” field. Under “SRS handling,” select “Force declared.” Under the “Bounding Boxes” heading, click both the “Compute from data” and “Compute from native bound” links. Now, with the layer created, we can associate the new web map service with an item in your Omeka collection by manually filling in the two fields in the “Web Map Services” tab: Go back the Omeka administrative interface and find the item that you want to associate the map with (or just create a new item). Open up the edit form for the item. Click the “Web Map Services” tab. Fill in the the top-level WMS address for the GeoServer installation (this always ends with /wms, and might look something like localhost:8080/GeoServer/wms ) and enter the list of comma-delimited layers that you want to be associated with the item. For example, if you have a workspace called “hotchkiss” with layers “chancellorsville” and “fredericksburg,” you could enter: hotchkiss:chancellorsville,hotchkiss:fredericksburg . Save the item. Use the map in a Neatline exhibit The two methods both have the end result of filling in the two fields in the “Web Map Services” tab. The only difference is in whether the .tif file is uploaded through Omeka or directly into GeoServer. Once an item is linked to a web map service, Neatline automatically detects the map and loads it into an exhibit when the item is activated on the map. With the item queried into the editing environment for an exhibit, just check the middle of the three checkboxes next to the listing for the item in the content management panel: …and the WMS layer will appear on the map:"},{"id":"2012-09-06-bulk-editing-in-vim","title":"Bulk Editing in Vim","author":"eric-rochester","date":"2012-09-06 04:57:25 -0400","categories":["Research and Development"],"url":"bulk-editing-in-vim","layout":"post","content":"I regularly have to perform a short sequence of small, regular edits on a collection of files. If you work with computers long enough, that’s something everyone has to do. Often I reach for a scripting language. But other times the edit is so small that even sed seems like overkill. Or maybe the edits are just the wrong kind of complexity to capture easily with code. My fingers may be able to make the changes quickly and repetitively, but when I try to break down how a script would do it, I get a headache. Over the years, I’ve been confronted with problems like this often enough that I’ve developed a well-tested approach using Vim . It’s become one of those tools that I don’t really think much about: I just use it from time to time, and it makes my life easier. But not long ago, Jeremy mentioned that he had a small change to make to a series of files in NeatlineMaps . Usually, he switches to TextMate for tasks like this, but he agreed that to let me show him how I would handle this in Vim. Heh. Whenever I try to explain to someone how to do something in Vim, I invariably sound like, Then hit escape, 4h, 0, now type whatever. It’s kind of funny, but it’s not a lot of fun, either for me or for the person I’m shouting keystrokes at. Hopefully, this will make a better blog post. Here’s what we did: The Problem Jeremy had tried to add some Vim mode lines to some PHP files. These are comments at the top of a file for setting options in Vim. Currently, they look like this : [sourcecode language=”php”]\n/* vim: set expandtab tabstop=4 shiftwidth=4 softtabstop=4: */\n[/sourcecode] But they weren’t working. It turned out that what should have been a colon near the end of the line was actually a semicolon, and once that was fixed, the settings worked fine. That was all right. But he needed to make that change on almost every file in NeatlineMaps. The Solution The process I showed him has four parts. Let’s break them down. Part One: :args First, we have to load the files to process. When you open Vim from the command-line and pass in files there, the file names are stored in the argument list . You can access the argument list inside Vim—either to see what files are in it or to set the files it contains—using the :args command: [sourcecode language=”bash”]\n:args */ .php\n[/sourcecode] This searches for all the files in the NeatlineMaps directory and subdirectories that have a .php extension. These files are loaded into the argument list. What’s nice about the argument list is how easily you can navigate over it using a few simple commands: :rewind Moves to the beginning of the list. :next Moves to the next file in the list. :Next Moves to the previous file in the list. :previous Also moves to the previous file in the list. :last Moves to the last file in the list. All these can also be abbreviated. So for example, you can use :n and :N to move forward and backward. Have a :n mapped to control-n, so navigating forward is especially easy. Part Two: q With the first file loaded into the buffer, now we make the change that we want to make on all files and record the keystrokes into a buffer. For this we chose the t buffer. There’s no reason for that particular letter: It was just the first one I thought of: [sourcecode language=”bash”]\nqt\n[/sourcecode] Now the bottom of the Vim screen should say recording . At this point, we go ahead and make the edit. Part Three: :s/../../e What I had Jeremy do was slightly more complicated and precise, but basically, I had him do this: [sourcecode language=”bash”]\n:%s/softtabstop=4;/softtabstop=4:/e\n[/sourcecode] This looks over the whole file ( % ) and performs a search-and-replace ( s ). It searches for the string softtabstop=4; and replaces it with the same string, except it used a colon ( softtabstop=4: ). The e at the end just means that it should ignore errors and keep chugging. That way, if a file does not have a modline (and not all did), it would keep going. Once we’ve made the change, let’s save it and move to the next file. [sourcecode language=”bash”]\n:wn\n[/sourcecode] This combines the _w_rite command and the _n_ext command (from above). That’s all we need to do for each file. Now hit q to stop recording: [sourcecode language=”bash”]\nq\n[/sourcecode] You can replay that now by pressing @t . Jeremy and I did that a few times to make sure it was doing what we wanted and wasn’t chewing up the files and spitting the pieces back in our faces. Part Four: n @ Once you’re sure that everything’s safe, change the rest of the files. Most commands in Vim can take a numerical prefix, which tells Vim how many times to perform the command. For example, j moves down one line, and 10j moves down 10 lines. In this case, tell it to play the recorded keystrokes 100 times: [sourcecode language=”bash”]\n100@t\n[/sourcecode] And Vim goes to work. It will stop on the first error, which will happen when :next reached the last file in the argument list and isn’t able to move any further. Solved Well, looking back, this particular problem would have been perfect for sed . But sometimes that requires looking at documentation. And that’s it. It seems more complicated than it actually is, and once you’ve been through it a few times, you can do it very quickly. Vim’s ability to record and replay keystrokes, combined with its commands to navigate in and across files, make an incredibly powerful combination. To show how easy this process is, here is a screencast of me walking through the problem outlined above on NeatlineMaps code. You may want to click through to a larger version, more readable version of the video. Bulk Editing Vim from Eric Rochester on Vimeo ."},{"id":"2012-09-06-omeka-neatline-metadata-survey","title":"Omeka + Neatline Metadata Survey","author":"ronda-grizzle","date":"2012-09-06 11:19:50 -0400","categories":["Announcements","Geospatial and Temporal"],"url":"omeka-neatline-metadata-survey","layout":"post","content":"As part of our collaboration on Omeka + Neatline, the Scholars’ Lab and Omeka development teams are seeking your assistance to help make our projects more useful across many scholarly disciplines–including beyond the humanities and cultural heritage fields in which they originated. We’ve developed a short survey asking questions about new data types that Omeka+Neatline could display and metadata formats that it might import and describe. While Omeka + Neatline can handle many metadata standards and formats familiar to humanities scholars, archivists, and museum professionals–such as Encoded Archival Description schema (EAD), Text Encoding Initiative schema (TEI), Dublin Core (DC) and Visual Resources Association Core (VRA Core)–it is also possible to import simple CSV files or any flat XML format, with potential to handle other formats more applicable to your field’s standards. We’d like to hear about other specific standards and formats that could make Omeka + Neatline more helpful to your research and scholarship. Thank you for taking a few minutes to answer our survey! Please click here to take the survey"},{"id":"2012-09-10-geocoding-for-neatline-part-i","title":"Geocoding for Neatline - Part I","author":"wayne-graham","date":"2012-09-10 05:18:03 -0400","categories":["Digital Humanities","Research and Development"],"url":"geocoding-for-neatline-part-i","layout":"post","content":"Recently I was asked if there was a way to import place names in connection with lat/lon points . Twitter’s character limitation does’t provide an adequate format to respond, and this technique can be quite useful outside of Neatline too, so I thought I would dive in a bit and explain a method to can get prepare place names for use in Omeka and beyond. This will be a two part series where I cover the basics of geocoding locations in a CSV file, then move to using these points with the NeatlineFeatures and Neatline plugins for Omeka . Geocoding Geocoding is a method of deriving geographic descriptions (a latitude/longitude point or series of points) from  geographic data like an address (or portion of an address) or place name. The more granular the geographic data, the more accurate the location information. For example, Charlottesville, Virginia is far more accurate than Virginia, and the center point that a geocoder would return would use different points to represent this data. As you can see, the red marker is the center point for geocoding ‘ Virginia,’ where the blue marker codes the center of ‘ Charlottesville, Virginia .’ [iframe src=”http://jsfiddle.net/wsgrah/rbHhj/embedded/result,js,html,css/” allowfullscreen=”allowfullscreen” frameborder=”0”] There are some complexities, as data can be a bit ambiguous (e.g. W Jefferson St. vs Jefferson St.), but clever engineers have been working on issues like that and (generally) do a good job figuring out what is intended from partial address information. It is also worth noting that different services have different algorithms for calculating the result from a partial address (center of a county, city, state, country, etc.), so be sure to check your results to make sure they’re what you are expecing! Web Services With the growth of different mapping services over the last several years, getting access to geographic information has become pretty straight forward thanks to a lot of really smart engineers working on the problem of providing location-based information. Most likely you’ve consumed these services (even if you didn’t realize) and you’ve probably heard of some of the big companies that provide APIs to their Geocoding services like Google, Yahoo!, and Microsoft . These are great resources, and provide well documented access to their data, but do place some restrictions on the use of the data retrieved from their system. Generally these restrictions are on the commercial re-use of the information, but do read and understand the Terms of Service to make sure your intended use is in compliance with the Terms of Service for the geocoding service you use for  your project. While these companies provide access to their information, I did want to take a moment to  highlight some other services that are not backed by large corporate entities. Geocoder.us is a service that has good coverage if you’re data is in the US ( Geocoder.ca provides a service for Canadian addresses) and allows up to 50,000 requests per day. USC’s GIS Research Laboratory provides a service for up to 2,500 addresses per day, with very few restrictions on its use. The last one I’ll mention (and there are many more of these services), Geonames.org has world-wide coverage, with more than 8 million place names. This service is particularly useful for things you don’t have a street address for (e.g. Eiffel Tower ). First Steps I’ll start out by saying it’s not necessary for you to do any coding whatsoever to do geocoding. However, this is a good programming exercise, allowing you to work with a number of technologies. If you have no interest in writing custom code for this, you can check out Google’s Fusion Tables to do something similar to what I will explain here. The examples in this exercise will be written in Ruby, but the concept can be ported to just about any language ( node, Python, PHP, Haskell, jQuery, …). If you don’t have Ruby (use a 1.9 version) installed on your computer, you will need that ( Windows, OS X, Linux ). I’ll also be using the Geocoder gem as well as the built-in Ruby libraries for working with spreadsheets (specifically CSV, comma-separated values). Project Setup The basic approach for this project will be to create a project directory, install the necessary libraries, create a spreadsheet of the addresses we want to look up, write a short program that will retrieve the latitude/longitude points (and other information) we need, then write those back this information to a new spreadsheet we can use later on. Assuming you have Ruby (and rubygems ) properly installed, all you will need to get going is set up a new project space. A few simple commands will get this project going in your terminal : mkdir -p ~/projects/geocode\ngem install geocoder\ntouch ~/projects/geocode/locations.csv\ntouch ~/projects/geocode/geocoder.rb\ncd ~/projects/geocode/ This set of commands commands created a new directory ( ~/projects/geocode ) for the project if it does not exist, installed the geocoder library, and created a couple of empty files to store our locations and our program logic. Lastly, we change the current directory ( cd ~/projects/geocode for the terminal to be the project directory containing the files. The Datasource In order to explore this technique, we need some data. A really easy format for working with text is the CSV format, or comma separated values. For the purposes of this exercise, paste the following in to a file named locations.csv . Address,City,State\n\"1600 Pennsylvania Ave\",Washington,DC\n\"931 Thomas Jefferson Parkway\",Charlottesville,VA\n\"Eiffel Tower\" The CSV format is pretty straight forward; you literally separate the values (columns) with commas. The one gotcha is that if you have something with a comma in it, you will need to escape the comma with a backslash (e.g. “Charlottesville, VA” as a single field). Reading the File Now with a little bit of data, we can turn our attention to reading the values out of the CSV file with an actual program. This is done with the Ruby CSV class, which provides methods to read and manipulate the contents of CSV data. Our program for this exercise will go in  the geocoder.rb file. With your favorite text editor, open the geocoder.rb file and add the following: ```\nrequire ‘csv’ LOCATIONS = ‘./locations.csv’ CSV.foreach(LOCATIONS, :headers =&gt; true, :header_converters =&gt; :symbol) do |line|\n  p line[:address]\n  p line[:city]\n  p line[:state]\nend\n``` This code reads the CSV file, converts each header value in to a symbol (for easy reference) then steps over each line in the file and prints the address, city, and state fields on a new line in the terminal. You can run this program in the terminal by executing the Ruby script with ruby geocoder.rb . When you do, you should see output along these lines: ○ → ruby geocoder.rb\n\"1600 Pensylvania Ave\"\n\"Washington\"\n\"DC\"\n\"931 Thomas Jefferson Parkway\"\n\"Charlottesville\"\n\"VA\" Adding Geocoding Now that we can read the data, we can change (or refactor) the program to use the geocoder gem and write logic to look up location information. At this point, we need to include the Geocoder library, concatenate the address fields together, then retrieve the location information. ```\nrequire ‘csv’\nrequire ‘geocoder’\nLOCATIONS = ‘./locations.csv’ CSV.foreach(LOCATIONS, :headers =&gt; true, :header_converters =&gt; :symbol) do |line|\n  address_string = “#{line[:address]}, #{line[:city]}, #{line[:state]}”\n  result = Geocoder.search(address_string)\n  p result\nend ``` Now when you run the program ( ruby geocoder.rb ), you’ll see that there is a lot more information returned from the geocoding web service. To make this a bit more useful for our purposes here, we can use the latitude and longitude convenience methods to display the latitude and longitude coordinates. ``` require ‘csv’\nrequire ‘geocoder’ LOCATIONS = ‘./locations.csv’ CSV.foreach(LOCATIONS, :headers =&gt; true, :header_converters =&gt; :symbol) do |line|\n  address_string = “#{line[:address]}, #{line[:city]}, #{line[:state]}”\n  result = Geocoder.search(address_string).first\n  lat = result.latitude\n  lon = result.longitude puts “#{lat}, #{lon}”\nend ``` Now when you run the program, you should see that the program prints the latitude and longitude for the address line. ○ → ruby geocoder.rb\n38.8976777, -77.03651700000002\n38.0054041, -78.4563433\n48.858278, 2.294254 This is great, but what we really want to do is write this back to a CSV file in a format that we can use in Omeka. The Neatline plugins currently use a format called WKT format to describe geographic information, so we need to get our results in this format using the WKT ** “Point”** data definition. If you can remember any of your middle school algebra, a point is a set of coordinates (typically X and Y). The Earth’s X axis is longitude, and the Y axis latitude, so we just use this format and wrap the coordinates with “ POINT() : ```\nrequire ‘csv’\nrequire ‘geocoder’ LOCATIONS = ‘./locations.csv’ puts “address,city,state,point,lat,lon” CSV.foreach(LOCATIONS, :headers =&gt;\n true, :header_converters =&gt;\n :symbol) do |line|\n    address_string = “#{line[:address]}, #{line[:city]}, #{line[:state]}”\n    result = Geocoder.search(address_string).first\n    lat = result.latitude\n    lon = result.longitude point = \"POINT(#{lon} #{lat})\"\n\nputs \"#{address_string}, #{point}, #{lat}, #{lon}\" end\n``` Notice I removed the comma in the POINT value ; this is important as this is specified in the WKT format. Now with a shell trick ( not this kind ), we can run the program and generate a file that contains the new data. The trick is actually the I/O redirection command ( &lt; ) which can takes the output of one command and redirects it to a file. I generally prefer (when possible) to generate new files when dealing with massaging data. I’ve just deleted too many files accidentally in code. ruby geocoder.rb &gt; geocoded.csv This command will run the file, but redirect the content that was being shown on the screen in to a file named ‘ geocoded.csv .’ Coordinate Systems Did you know that there were a lot of different ways to actually define latitude and longitude? Before taking this job I had only really seen coordinates expressed in either in the degrees, minutes, seconds format (e.g.  38° 1′ 48″ N, 78° 28′ 44″ W) or it’s decimal equivalent (e.g. 38.03, -78.478889). Turns out there are a lot different ways to actually describe these coordinates because our maps are flat, and our planet is not. If the world was actually a real sphere, this wouldn’t be a difficult problem, but the Earth’s shape actually what is referred to as an oblate spheroid, which makes getting precise locations on from the curved Earth to a flat map problematic. How you deal with the conversion of points on the spheroid (Earth) to a map is a projection. This conversion can introduce distortion, like the maps I remember in school growing up where Greenland is larger than Africa.  Projections are chosen according to the purpose of the map to preserve qualities like shape, area, distance, or direction. The European Petroleum Survey Group (EPSG) maintains a database of all the myriad projections and datums. After a while, you start to know the more regularly used projections, and the results we got back from the Geocoder gem are in a projection for the WGS 84 coded EPSG:4326 .  ( Note: for a nice piece on projections, check out Projection Lessons in Maps .) But what does this have to do with the coordinates? In Neatline we are using a different coordinate system to make some of the conversions in using Google base maps a bit easier. Basically we need to take the decimal degrees and covert them to meters. For example, the coordinates of the White House, this: POINT(-77.03651700000002 38.8976777) Which uses a spherical interpretation of the globe becomes the following when converted to meters. POINT(-8575665.843733624 4707025.360473459) These are the same points, just described differently. So how can we handle this in the code? There are services that you can go out and use to re-project your data, but this one is pretty straight forward with a little trigonometry (didn’t think you’d read that today, did you). With a little mathematical hand-waving, I wrote the following method to convert degrees to meters: ```\ndef degrees_to_meters(lon, lat)\n    half_circumference = 20037508.34\n    x = lon * half_circumference / 180\n    y = Math.log(Math.tan((90 + lat) * Math::PI / 360)) / (Math::PI / 180) y = y * half_circumference / 180\n\nreturn [x, y] end ``` The 20037508.34 the the above code is half the circumference of the earth in meters (I looked it up), and there are some math tricks to account for a generalized flattening of the earth. Now we can call this function and calculate the projected coordinates to use in a Neatline exhibit. ```\nrequire ‘csv’\nrequire ‘geocoder’ LOCATIONS = ‘./locations.csv’ def degrees_to_meters(lon, lat)\n    half_circumference = 20037508.34\n    x = lon * half_circumference / 180\n    y = Math.log(Math.tan((90 + lat) * Math::PI / 360)) / (Math::PI / 180) y = y * half_circumference / 180\n\nreturn [x, y] end puts ‘address,city,state,point,lat,lon’\nCSV.foreach(LOCATIONS, :headers =&gt; true, :header_converters =&gt; :symbol) do |line|\n    address_string = “#{line[:address]}, #{line[:city]}, #{line[:state]}”\n    result = Geocoder.search(address_string).first lat = result.latitude\nlon = result.longitude\n\n#point = \"POINT(#{lon} #{lat})\"\nprojected = degrees_to_meters(lon, lat)\npoint = \"POINT(#{projected[0]} #{projected[1]})\"\n\nputs \"#{address_string}, #{point}, #{lat}, #{lon}\" end ``` When you run the program as described above, you will get results similar to this: address,city,state,point,lat,lon\n1600 Pennsylvania Ave, Washington, DC, POINT(-8575665.843733624 4707025.360473459), 4707025.360473459, -8575665.843733624\n931 Thomas Jefferson Parkway, Charlottesville, VA, POINT(-8733720.184442518 4580189.258447956), 4580189.258447956, -8733720.184442518\nEiffel Tower,,, POINT(255395.18699487977 6250848.2584100235), 6250848.2584100235, 255395.18699487977 Now you can regenerate your CSV file of the properly formatted location information from the terminal: ruby geocoder.rb &gt; geocoded.csv If you’ve already created a geocoded.csv file, the above command will overwrite the file. If you’re wanting to append similar content to the same file, you can use the &gt;&gt; operator which will add the output to the end of the file. Summary In this post I covered the basics of using geocoding services through a programming API, as well as reading CSV files, and redirecting output to a new file. I waved my hands with a little magic (some math, some programming, some Unix commands), but there are a lot of techniques here you can use in a variety of scenarious. In my next post, I will cover how to automate populating this information in an Omeka instance, automating the population of this information in new items, then them in a Neatline exhibit."},{"id":"2012-09-11-introductions","title":"Introductions","author":"claire-maiers","date":"2012-09-11 07:04:00 -0400","categories":["Grad Student Research"],"url":"introductions","layout":"post","content":"Hello All, In this introductory post I am going to tell you a little bit about myself, my research interests, and the sources of my enthusiasm for Praxis and Prism . So, let’s start with the basics:  I am currently a third year graduate student at UVa’s Department of Sociology.  Though, as a sociologist, I am trained to think scientifically about the social landscape, I also have a strong connection to the humanities.  I have a B.A. in music (with a focus on vocal performance) and a M.A. in musicology (focus on research and scholarship).  Given my background, it might not be surprising that I specialize in cultural sociology.  This means a lot more than an interest in applying social theory to culture or the arts.  It means that I am interested in the way that culture shapes our thinking, constrains or enables action and agency, and structures our experience of the world.  More specifically, I am interested in the way that the available scripts (you might also say logics or frameworks) within a culture structure our way of making meaning and making truth claims. My interest in the Digital Humanities really began when I attended the interest meeting about Praxis last spring.  Hearing the 2011-2012 team talk about Prism quickly got me excited about the research possibilities of the Digital Humanities.  Where most data sets available to sociologists provide demographic information and perhaps basic information about opinions or attitudes, a tool like Prism would allow us to get a better handle on how people think through and make meaning from texts–something that is very difficult to get at through traditional survey methods.  Although Prism clearly has other potential (I am also enthusiastic about the way it could be integrated into the classroom), I am hoping that the coming year will prepare me to incorporate some of the innovations of the Digital Humanities into my own research."},{"id":"2012-09-11-the-impossible-proposal","title":"The Impossible Proposal","author":"chris-peck","date":"2012-09-11 07:05:34 -0400","categories":["Grad Student Research"],"url":"the-impossible-proposal","layout":"post","content":"For my first post as a Praxis Fellow I’d like to share an exercise that a mentor of mine often used to kick-off courses in interdisciplinary collaboration : The Impossible Proposal. At the first meeting of a class of engineers, artists, musicians, dancers, writers, etc. tasked with creating a tangible project by the end of the semester, our professor would ask us to come up with a proposal for an impossible project—not just difficult but totally and utterly  impossible . Few of us succeeded on the first try. The point here is not simply inspirational . The practice of generating impossible ideas is like weight training for your brainstorming muscles. And on the path to the impossible are many possibilities that could otherwise be overlooked. In those classes we managed to do a number of things that would have seemed impossible (or at least unwise). We floated a large light-responsive sound installation on the pond next to the music building. (My proudest distinction as a freshman music student was to achieve the rank of ‘first-chair’ radio-controlled boat operator.) I’m pretty sure that project started off as a failed response to the Impossible Proposal assignment—a ‘non-impossible’ idea. The Impossible Proposal is now a reflex for me in the early stages of a project. I can’t help it, even when I start to notice that more pragmatically-oriented collaborators are losing patience. At the first meeting of the Praxis Fellows last week I proposed that Prism should be able to record the eye movements of readers. But—just like the floating installation—it turns out this proposal is far from impossible. After the meeting Wayne informed me that UVa employs such technology in usability testing for websites, and he even seemed prepared to dive into the technical details of how this could be incorporated into our project… So at today’s meeting I’ll do better. Maybe something involving time travel ?"},{"id":"2012-09-12-geocoding-for-neatline-part-ii","title":"Geocoding for Neatline - Part II","author":"wayne-graham","date":"2012-09-11 21:00:07 -0400","categories":["Digital Humanities","Research and Development"],"url":"geocoding-for-neatline-part-ii","layout":"post","content":"In my last post ( Geocoding for Neatline - Part I ), I covered how to programmatically geocode a set of addresses and generate a CSV file for use in Neatline. In this post, I’ll go over how to actually post this information in Omeka and make it available for use in your Neatline exhibit. Requirements As in the previous post, I’ll be making use of Ruby here, but I’ll be making use of a different gem ( Mechanize ) to handle interacting with an Omeka server. This is what you’ll need to get going: A running Omeka instance with the Neatline, and NeatlineFeatures plugins installed. Ruby A text editor (e.g. vim, Sublime Text 2, notepad++ ) CSV data from the last exercise The terminal The Technique With a prepared geocoded CSV file, we can start dealing with how to actually get this data in to Omeka. If you’ve done an Omeka project in the past, you may be familiar with the CSVImport plugin, and this may be a first impulse to use. Unfortunately, because of some technical reasons I won’t get in to here, this won’t work. However, as a developer, this simply becomes a constraint for a different system. This is where the Mechanize gem comes to the rescue, allowing us to automate filling out the Omeka forms for our items. The first step here is to install the library with the gem command in the terminal: gem install mechanize The basic idea in using Mechanize, which allows us to write a set of automated steps, is to take the file we just generated in the previous post, read all the information, then fill out the Omeka form and save the newly created item. In a new script (e.g. populate.rb ), we require the libraries we’ll be using: ```\nrequire ‘rubygems’\nrequire ‘mechanize’\nrequire ‘csv’ code to process CSV points ``` In Mechanize you can define a user agent (a web browser), and it’s a good practice to define the user agent as a browser that you don’t use on a daily basis to avoid any caching or username/password issues. For me, I set this to “Mac Safari” (you can use this on Windows too), but you can choose from any of the user agent aliases Mechanize provides. ```\nagent = Mechanize.new {|a|\n  a.user_agent_alias = ‘Mac Safari’\n} code to fill out Omeka forms ``` Now we just need to mechanize how to log on to Omeka. I’m doing everything locally, so you will need to fix the path to the Omeka admin area as needed: ```\nagent.get(‘http://localhost/omeka/admin/’) do |page|\n  omeka_page = page.form_with(:action =&gt; ‘/omeka/admin/users/login’]) do |form|\n    form.username = ‘your user name’\n    form.password = ‘your password’\n  end.submit # read CSV file\nend\n``` If you ran this code right now, it wouldn’t actually do anything visually, but this bit of code finds the form on the admin page that contains the login information, then sets the username and password on the form, and submits it, effectively authorizing you to do other things with Omeka in the context of the program. Next, we want to read the CSV file ( geocoded.csv ) that we generated in the previous post to read the data. This is done in the same way before: CSV.foreach('./geocoded.csv', :headers =&gt; true, :header_converters =&gt; :symbol) do |line|\n    # add logic to fill out form\nend This should look familiar. The code just reads the CSV file, converts the headers to symbols, and steps through each line. For each line (row) in the CSV file, we want to tell Mechanize to click on the ‘Items’ link (the Items tab) then the ‘Add Items Link’ to get to the form to fill out, which will look like the following: ```\n# click on items\nitem_page = agent.click(omeka_page.link_with(:text =&gt; %r/Items/))\n# click on add items\nadd_item_page = agent.click(item_page.link_with(:text =&gt; %r/Add an Item/)) Add the item to the form ``` This code tells Mechanize to click on the link with the text of Items with a regular expression . In this case, the regular expression isn’t necessary, but is useful when you need to do a partial match on a link (or some other component) that is on a page, and something I do by default. Now the program is at the new item form, and it’s time to set data on the form from the CSV file. This is slightly trickier because of the way in which Omeka names its form fields. There are two areas we want to populate, the Title field and the elements to actually create the point for NeatlineFeatures. If you look at the source code of the Item Add page, you’ll notice that the Title field actually has a name of ‘ Elements[50][0][text][/text],’ which looks scary, but the program knows what to do with it. I also know that the Coverage field in Omeka is ‘ Elements[38][0] ’ with a field name after it ( [text][/text], [wkt], [zoom], [mapon], …). We can populate this information with data from our spreadsheet now: [gist id=3307210 file=add_item_snippet.rb] This bit actually fills out the form, turning the map component on, setting a zoom level of 10 for the map, then setting a center point to focus the map. If you’ve been following along, your script should now look like this: [gist id=3307210 file=populate.rb] After running this script, when you log on to Omeka, you should see newly created items with their locations populated and a pretty map that you can use in Neatline. Note: these items were not set to be ‘public’, but you can easily add this to the script, and can be your homework. Assuming you’ve read the documentation on creating Neatline exhibits, you can now simply add these items and have them placed spatially by clicking on the Map icon in the Neatline editor. Summary This set of posts shows you how to automate geocoding of your data and integrating it with a Neatline exhibit. These automated methods can save you a lot of time doing data entry, but you should keep in mind that a really great Neatline exhibit really requires you getting in to the exhibit and using your imagination to tell an interactive story. How your users interact with your exhibit depends not only on the quality of your data, but the time you spend working on the interactions."},{"id":"2012-09-12-grad-fellows-forum-introducing-the-2012-2013-fellows","title":"Grad Fellows Forum: Introducing the 2012-2013 Fellows","author":"ronda-grizzle","date":"2012-09-12 12:36:28 -0400","categories":["Podcasts"],"url":"grad-fellows-forum-introducing-the-2012-2013-fellows","layout":"post","content":"Graduate Fellows Forum Introducing the 2012 - 2013 Graduate Fellows &amp; Praxis Fellows On September 5, 2012, the Scholars’ Lab kicked off the Fall semester with a Graduate Fellows Forum welcoming our new Graduate Fellows and Praxis Fellows and getting an update on the work developing Prism done by our 2011-2012 Praxis cohort. Praxis Fellows: Cecilia Márquez, History Chris Peck, Music Claire Maiers, Sociology Gwen Nally, Philosophy Shane Lin, History Brandon Walsh, English Graduate Fellows: David Flaherty, History Lydia Rodriguez, Anthropology Annie Swafford, English As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.18488681804/enclosure.mp3”]"},{"id":"2012-09-13-binsh","title":"#!/bin/sh","author":"shane-lin","date":"2012-09-13 13:40:45 -0400","categories":["Grad Student Research"],"url":"binsh","layout":"post","content":"(My apologies; Brandon beat me to the Hello World joke.) My name is Shane Lin. I am one of the new Praxis cohort. This is my second year as a PhD student in the history department. My background is a bit different from that of most of my peers: I studied computer science and history at the University of Texas and subsequently worked for four years as a software engineer for a local Seattle bookstore. A foolhardy sense of romanticism caused me to abandon my career and turn instead to the Academy.  Now, I’m focused on the history of information, with a particular interest in cryptography and the Internet. I became interested in the Praxis Program because, although I have a fairly strong technical background, I didn’t actually know much about digital humanities. I feel like the type of research that I’m drawn to suffers the opposite problem to many other historical fields: instead of suffering a dearth of documents, I’m inundated by a titanic flood. It’s intimidating, to say the least, to try to use something like a Usenet newsgroup or a mailing list archive with tens of thousands of participants as a source. Getting a better grasp on digital humanities techniques and approaches will no doubt be an invaluable experience. I’m looking forward to dusting off the skills of my former (and now hopefully future) life and especially to foisting my own technical prejudices and ideological biases on my fellows. “No, no - over here where you typed in ‘Perl’ - you really meant ‘Python’”"},{"id":"2012-09-13-hello-world","title":"Hello World","author":"brandon-walsh","date":"2012-09-13 06:17:28 -0400","categories":["Grad Student Research"],"url":"hello-world","layout":"post","content":"For some time now I have led a double life as a musician and book lover. As a third year PhD in the English department at UVA, I work primarily on twentieth-century fiction in relation to music and sound. These interests drew me to Praxis in the first place: writing about sound is incredibly difficult in a print medium where the reader can’t hear what I describe. I am very excited to be a part of the Praxis team this year, where as a DH novitiate I hope to learn how technology can help make those two fields work together more easily. The Scholars’ Lab team seems happy to have us as well, welcoming the new team with open arms and inscrutable computer science jokes. Last year’s Praxis team reached out to us many months ago from their charter : Preparations for future cohorts In order to allow the next Praxis Program team to start work right away on a project, we will make suggestions for that project before our tenure is over. How nice to be thought of! But I think this excerpt shows how our situation is fundamentally different from theirs: last year’s team had to deal only with the future, but our team also has to deal with the past. It would be easy for us to feel anxiety as latecomers to the Praxis party, so I think it’s important that our own charter reflect the ways in which our work will talk back to last year’s team. As I look through last year’s blog posts, I’m struck by the problem of knowledge we have in store for us. Implicit in the suggestions offered to us are all the ideas that last year’s team discarded, thought better of, and revised. Those are what I really want to see! The archive of blog posts can only give a skeletal sense of the past: there is no replacement for sitting in that chair all last year. So beyond the question of how we deal with their great suggestions, I am struck by a more basic dilemma: what sort of dialogue will we have with last year’s group? I imagine that the cheerful team in the Scholars Lab will welcome repeat conversations as opportunities to rethink and retool, but I also believe that we can benefit from the experience of those who came before. I hope that we can continue the conversation with both those members of last year’s team that are still on grounds and those that have moved on to wonderful jobs across the country. Even so, it is also important for us to recognize that, try as we might, we can never know everything about the project’s history to date. Our team will work better in the long run if we welcome the unknown and greet it just as enthusiastically as it welcomes us."},{"id":"2012-09-13-my-first-praxis-post-for-lack-of-a-better-title","title":"My First Praxis Post (for lack of a better title)","author":"cecilia-márquez","date":"2012-09-13 07:39:31 -0400","categories":["Grad Student Research"],"url":"my-first-praxis-post-for-lack-of-a-better-title","layout":"post","content":"Internet introductions are by far the weirdest introductions because you have no idea who is reading what you’re writing.  For all I know people I went to high-school with will find me here blogging about the Digital Humanities, or a future employer, more likely it will be my mother and grandmother.  So here goes internet…don’t let me down…and hi Mom! I’m Cecilia Márquez.  I’m have a BA from Swarthmore College in Black Studies and Gender and Sexuality Studies.  Now I’m a second year PhD student in the History Department here at UVA.  My work is mostly focused on African American History, Labor History and Latino History. My interest in Praxis came as a shock to most people who know me.  I have a serious aversion to most technology and an even bigger aversion to being confused.  So a program that focused on embracing and challenging my ignorance of technology felt a little like a leap into the belly of the beast.  I am happy to report that after three meetings I have not felt the desire to run for the door or throw my computer at the wall. At our first Praxis Team meeting–aside from me laughing nervously at all of the programming jokes I didn’t understand–we talked about our charter.  The conversation made me think more about how my values influenced my interest in the Digital Humanities. I believe in the importance of working collectively.  Although I often stumble doing this effectively I believe that working in community will always be better than the solitary experience of the library basement.  On a related point I have to laugh all the time, especially when I’m working, and especially when what I’m working on is incredibly hard.  I always want to bring a sense of humor to the work I do and so far the Praxis team has not come up short on good humor.  I’m committed to the democratization of knowledge–both historical knowledge and technological knowledge.  This is obviously central to the work of the Digital Humanities. The next time I blog I will have gotten through HTML/CSS training… hopefully me and my computer emerge unscathed."},{"id":"2012-09-13-praxis-2-0","title":"Praxis 2.0","author":"gwen-nally","date":"2012-09-13 06:14:20 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"praxis-2-0","layout":"post","content":"The 2012-2013 Praxis team has assembled! We’ve moved into our lounge (who knew Alderman Library could be so comfortable?) and started to think about chartering this year’s team project. More on the charter and the project in the weeks to come… Although we’ve just met, if I had to describe my fellows in a single word, I’d probably say that we are a “diverse” team. We hail from across the humanities–from sociology to history, music, english, and philosophy. (I am a sixth year graduate student in the philosophy department, interested primarily in topics in ancient philosophy, applied ethics, and aesthetics. My dissertation, The Ascent to Beauty: the Epistemology of Goodness in the Middle Dialogues, proposes a unified theory of knowledge in the ascent passages from this period in Plato’s work. My research tackles questions like “Why is goodness the ‘highest’ form?” and “Why does Plato identify goodness with beauty?”) The fact that we Praxis fellows have such different backgrounds seems to be what really distinguishes us. Digital humanities is known for its interdisciplinarity; even so, most DH projects seem to be undertaken with specific individuals filling certain predetermined roles. A visualization project might, for example, consist of one or more researchers, usually from a single or neighboring disciplines, a project manager, digital technologists, and designers. Our team seems really unique in that, in addition to a team of skilled digital researchers, technologists, and designers, we hail from five different humanities disciplines. What’s more, unlike other DH teams we were not chosen to address any particular research question. Over the next several weeks, we’ll have the rare opportunity to design and undertake a project of our own devising. We might pick up where last year’s team left off or, who knows, we might enter into completely uncharted territory? Whatever we decide, this year’s project will, I hope, showcase the successes, failures, and inner workings of a truly interdisciplinary team."},{"id":"2012-09-17-moonlighting-with-the-praxis-crew","title":"Moonlighting with the Praxis Crew","author":"katina-rogers","date":"2012-09-17 06:28:07 -0400","categories":["Announcements","Grad Student Research"],"url":"moonlighting-with-the-praxis-crew","layout":"post","content":"One great perk of my role with the Scholarly Communication Institute is that I have the opportunity not only to learn about how the Praxis Program functions, but also to learn alongside the team members, particularly in this second year. The semester has barely begun, and already the collective wheels are turning as the new group begins to develop its own charter, to brainstorm new directions for Prism, and to learn the fundamental skills they’ll need as they extend, modify, and rethink it. I’m excited to take part from the start of the academic year. When I joined SCI in April 2012, the first Praxis cohort was nearing the completion of their year-long project. I watched from my Brooklyn perch as they untangled the last knots in functionality, design, deadlines, and communications as they prepared to launch Prism. They had already learned and assimilated the skills that the project required, and I was impressed by how fluidly the group worked together to navigate the complex hurdles of polishing and releasing their work. As the second cohort begins its year together, I have the opportunity to observe from day one. Sitting in on the group’s weekly meetings (as a disembodied Skype presence), my goals are double: First, consistent with SCI’s intention to work with a handful of unique but allied programs in the development of a Praxis Network, I’ll be watching to see how the program functions, which elements of it are particular to the Scholars’ Lab ecosystem, and which might be useful to other programs wishing to develop similar initiatives. Second, and more selfishly, I’ll be joining as an eager student. In the past few months I’ve worked at learning some of the skills required to create the kinds of data visualizations I want to use to report on SCI’s study of alternative academic careers, and also to gain more control over my website. Giddy over minor successes with Vim, GitHub, HTML/CSS, and D3.js, I’ve tasted the Kool-Aid, and am excited to learn more. Since I’m not physically in the Scholars’ Lab space, I’m not only looking forward to the technical instruction of the SLab gurus, but also (even more) to the collective nature of the process as we all go a bit out of our comfort zones, embracing the challenges and failures that are a part of learning new skills."},{"id":"2012-09-18-code-spelunking-with-ctags-and-vim","title":"Code Spelunking with Ctags and Vim","author":"eric-rochester","date":"2012-09-18 06:05:54 -0400","categories":["Research and Development"],"url":"code-spelunking-with-ctags-and-vim","layout":"post","content":"When I’m programming, I spend a lot of time code spelunking ( use the source, Luke! ). A lot of times, the best documentation for a system is the source code for it. Knowing how to get to important places in your code and in others’ code makes a huge difference in how productive you are and even in what you can figure out how to do. And the most important location for any method, function, class, variable, or whatsit is where it’s defined. Usually there’s documentation near. Sometimes there is information about parameters. It’s all very useful. For finding those places, CTags is indispensable. What is ctags ? CTags is program that finds the lines of code where things are defined. It knows 41 languages . (And if your favorite language isn’t on the list, you can probably find another program that generates compatible tags files for it.) You run it occasionally, and it indexes your code and stores it in a file named tags . Your editor reads this file and helps you jump through your code. Installing ctags The first step in using ctags is to install it. Windows The ctags page has a link to a ZIP file with a binary of ctags compiled for Windows. Download this and unzip it somewhere on your %PATH% . Linux If you’re using Linux, all major distributions have a package for ctags . See your documentation for details. Mac I’ve saved the Mac installation for last, because it’s the most complicated. You weren’t expecting that, were you? Mac OS X comes with a program named ctags . Unfortunately, it is not, in fact, Exuberant CTags, and it’s far more limited. You can get the correct version of ctags using Homebrew : [sourcecode language=”bash”]\n&gt; brew install ctags\n[/sourcecode] Now you have to make sure that your shell finds the right version: [sourcecode language=”bash”]\n&gt; which ctags\n/usr/bin/\n[/sourcecode] Well, that’s not right. Let’s rearrange our $PATH . [sourcecode language=”bash”]\n&gt; PATH=/usr/local/bin:$PATH\n&gt; which ctags\n/usr/local/bin\n[/sourcecode] That’s better. You probably want to put that into your ~/.bash_profile file to make sure you find the right ctags in the future also. Using ctags in a Text Editor Now that ctags is installed, let’s put it to work. First, open up a command line or terminal or whatever you call it and change into the main directory of your project. Right now, I’m working on NeatlineFeatures, so I’ll use that for this example: [sourcecode language=”bash”]\n&gt; cd ~/omeka/plugins/NeatlineFeatures/\n[/sourcecode] Now, let’s run ctags over the code base. We want to to walk through the entire directory tree. But here’s the catch: we really want it to walk over the entire Omeka directory, including the Features plugin: [sourcecode language=”bash”]\n&gt; ctags -R ../..\n[/sourcecode] This runs ctags over everything in the Omeka directory ( ../.. ) and all subdirectories ( -R ). This will take a while, and you’ll get some warnings about JavaScript files. Don’t worry about them. CTags has a few problems parsing JavaScript, but that won’t stop it from indexing the other files. Let’s see what we have: [sourcecode language=”bash”]\n&gt; ls -lh tags\n-rw-r–r–  1 err8n  staff    23M Sep 14 16:00 tags\n&gt; wc -l tags\n   75917 tags\n[/sourcecode] Wow! More than 76,000 lines and 23MB. That’s a lot of indexing. But then, Omeka’s a large codebase. What’s in the file? Let’s not worry about that right now. It’s plain text, and there is some metadata and lines detailing identifiers and files and line numbers. It’s actually a little scary, and we don’t have to worry about that anyway. (If you’re really curious about the file format, look at the format page .) The tags file can be used by a bunch of different text editors . In fact, there’s more than is on that list. Here are links to integrating tags into some popular editors: OpenCTags : An add-on for using tags with Crimson Editor, EditPlus, UltraEdit, and Notepad++. Emacs : Comes with etags, so it supports tags out of the box. CTags bundle : A bundle for using tags in TextMate . Sublime Text 2 and CTags : An add-on for using tags with Sublime Text 2 . I use Vim, and it comes with support for tags files built in. The rest of this post shows about how to use tags from Vim. Using with Vim First let’s start Vim from the directory containing the tags file. If you want to use gVim or MacVim, this may be different (hint: use the :cd command). I’ll just use the command line to start MacVim on one of the models. [sourcecode language=”bash”]\n&gt; mvim models/NeatlineFeatureTable.php\n[/sourcecode] The Tag Stack The main point of all this, of course, are the powerful navigation commands. Let’s see what they are. Vim maintains a stack of locations where we’ve been. This stack starts out empty. Navigating Forward When you jump to a tag, your current location is added to the stack. As you jump ahead, more locations are added to the stack. Here’s how to jump forward and add locations to the stack. First say I want to look at the code for Omeka_Db_Table . I just move down to where it’s mentioned in the code and hit Control-] . And I’m there. I can open up the class and look in it. But say this doesn’t tell me what I want. I really want to look at Zend_Db_Table . I don’t see it, so I can’t use Control-] . Instead, from command mode, I type out :tag Zend_Db_Table . And I’m there. But it’s empty. I need to jump to Zend_Db_Table_Abstract . I just move my cursor down there and hit Control-] . Say I want to know how fetchAll is defined. I move down there and open it up. But how else is it defined? I put my cursor on fetchAll and hit Control-] . At the bottom of the Vim window, it says tag 1 of 11 or more. Interesting. How do I get to them? In normal mode, I just use the command :tselect . Now Vim displays a list of everywhere that fetchAll is defined. I can select the number for which one I want, and Vim moves me there. Examining the Tag Stack Now I’ve jumped several times, and I’m a little confused about where I am. How do I find myself again? In normal mode, use the :tags command. Vim will print out a list of which tags you’ve jumped to and where it is. Navigating Backward At this point, I want to move back to where I was. To do that, I just hit Control-t multiple times. Each time I do, it pops one position off the stack and moves be back to the previous tag location. Jumping into a new window Of course, it would be nice to be able to see what I’m working on and the tag too. Yes, I can be demanding. Fortunately, Vim can oblige me. Unfortunately, it’s not as convenient as Control-], but it’s not bad. (And creating a normal-mode map for this isn’t difficult.) In normal mode, just give the command :stjump [identifier] . This splits the window, and in the new split, it jumps to the tag. If there’s more than one definition for the tag, it prompts you for which you want, just like :tselect does. Tag Navigation Cheat Sheet So here’s what we’ve learned today: `:tag [identifier]` Jump to the identifier. `:tags` List the tag stack. `Control-]` Jump to the tag under the cursor. `:tselect` Select which tag location to go to for the current tag. `Control-t` Jump back from the current tag. `:stjump [identifier]` Jump to the identifier in a new split window. What does it look like? CTags in Vim from Scholars’ Lab on Vimeo . With bonus content! Next Steps I’ve just presented the basics. Here’s some more about using tags in Vim. Learning More The Vim documentation for tags lists all of the many commands Vim has for working with tags. Tagbar Tagbar is a Vim plugin that shows the structure of your code for the file you’re in. It opens a side panel and displays the classes, methods, and other identifiers defined in the current file. Running Automatically Finally, Tim Pope has an excellent blog post on how to integrate Git, CTags, and Vim. He also explains how to set up your git repositories to run ctags automatically whenever you commit. It also makes it easy to customize how you run ctags for each project. I highly recommend this system. It makes ctags even more awesome than it already is. So let us know, what’s your favorite development productivity or code navigation tool?"},{"id":"2012-09-20-on-not-knowing","title":"On Not Knowing","author":"cecilia-márquez","date":"2012-09-20 11:26:34 -0400","categories":["Grad Student Research"],"url":"on-not-knowing","layout":"post","content":"I made it through HTML/CSS and miraculously I still have a computer and most my sanity. These weeks of learning HTML/CSS have happened to coincide with my first weeks of being a Teaching Assistant. Having these experiences together has been invaluable for a few reasons. It has forced me to be a student again. While I am still in graduate courses as a student, it has been many years since I was in a course where my knowledge of the subject was so limited and my teachers knowledge so extensive. I’ve already gained a lot of insight into how to calmly and politely point out to a student that what they are doing is completely off base. This is a gift the Praxis team has given to me. If has reacquainted me with the feeling of “not knowing.” In graduate school it is rare that I find myself completely out of my depth. I take courses in my field with professors who study the same things I study. In Praxis I have had to embrace confusion, work through frustration, and learn to ask (what feel like) stupid questions, over and over. This is a gift the Praxis team has given to my students. Reconnecting with feelings of “not knowing” have helped me act more compassionately with my students and embrace my role as an educator to help them work through their frustration and confusion. To watch all these fun emotions manifest you can follow the development of my very first website here: http://cmarque1.github.com/ Right now it is mostly a mess but in time I’m hoping for masterpiece status."},{"id":"2012-09-21-failure","title":"Failure","author":"brandon-walsh","date":"2012-09-21 12:07:47 -0400","categories":["Grad Student Research"],"url":"failure","layout":"post","content":"In middle school I built a website about the seven wonders of the ancient world. Nothing fancy – just images and some links – and I never published it. Building a personal website over the past few days as per Jeremy’s request feels a bit like coming full circle. My HTML skills remain prepubescent at best, and my barebones site keeps the spirit of GeoCities in the early 1990’s alive and well. Check it out. http://bmw9t.github.com . I admit to some hesitation in posting this fledgling site for all to see: so much more could be done to bring it into the twenty-first century. But I keep coming back to Bethany’s encouragement last week to fail in public, advice with which I am in love. My graduate training thus far has emphasized polish and perfection, for clear and obvious reasons. But even at this early stage, my work with Praxis feels more electric knowing that mistakes are welcome and that failure is viewed as a space of experimentation and elaboration rather than embarrassment. I cannot imagine any sort of collaborative activity (teaching included) that would not benefit from a healthy injection of interpersonal risk. We can’t really work together until we know each other, and that depth of knowledge only comes from admitting that we don’t have all the answers and that we don’t always succeed. Opening yourself up to such failures and recognizing their importance as part of any process is a necessary step towards collaboration that is more honest and certainly more human. A promise for you out there in the ether: by the end of the year I will have turned failing into an art form. I’m sure in May I will look back on this early site with disdain and an eye to incorporating all sorts of tech wizardry. Maybe I’ll add a GIF. For now, I’m off to break the Internet."},{"id":"2012-09-21-praxis-the-innovator","title":"Praxis: The Innovator","author":"claire-maiers","date":"2012-09-21 06:43:52 -0400","categories":["Grad Student Research"],"url":"praxis-the-innovator","layout":"post","content":"In addition to getting a crash course in html and css, we’ve spent our time in the fellows’ lounge this week actually putting some prose together for our charter.  One of the great things about this process has been the way in which working on the charter has actually provided a platform for us to get to know each other and to begin to have a number of other important conversations.  Will we continue with Prism?  Who is Prism for: academics, researchers, teachers, the entire web-surfing public?  Is it possible to address the needs of all the disciplines we collectively represent in this project? This process has also raised another concern for me that I only began to articulate yesterday.   One of our motivations for working on our own version of the charter was to convey our own ethos in the text.  As Gwen pointed out during one of our first meetings, we don’t want to create problems by foreseeing them.  If the charter can be seen as a self-fulfilling prophecy—an attempt to articulate the desired goals and experiences for the next year that will motivate us to meet those goals—we didn’t want to convey an expectation of problems and conflicts.  So far, I think we’ve been successful in this regard.  But it has left me wondering what else we are importing into Praxis without notice or intent. To my mind, the Digital Humanities represent a possibly radical corner of the academic landscape.  Here—as the word “interdisciplinarity” instructs us—we are not supposed to adhere to the confines of our disciplines.  What other “rules” are we meant to challenge? In pondering that question, my sociological mind turns to cultural theory.  One of the things that sociologists often talk about is the way that culture instructs us not only in what we should do, but in how we should do it.  We take an immense number of things for granted, assuming that other options do not exist.  The cultural instructions for how to do something in one area of life often spill over into another.  Something meant to be innovation becomes mundane. These cultural instructions that we rely on are difficult to notice.  Thus, we might import the rules and processes of bureaucracy, for example, into our own project without realizing it.   This was one of my concerns this week when we discussed potentially including a clause for conflict resolution (requiring a neutral third party) into our charter.  Not only did this seem to contradict the ethos we wanted to convey, but it imposed a seemingly overly bureaucratic construct into what has the potential to be a new kind of scholarly work environment and project.  In another example, there have also been concerns (from myself included) about how our project will fit the requirements of respective disciplines and be accepted as legitimate scholarship.  While we certainly benefit from structure and clearly articulated goals, why must we rely on the taken-for-granted goals and rules of bureaucratic educational institutions? I want to encourage us to be on the lookout for these taken-for-granted assumptions.  I want Praxis to be something that breaks academic molds–something innovative.  Doing this requires both attentiveness and a willingness to have our own assumptions and habits challenged."},{"id":"2012-09-24-good-practice","title":"Good Practice","author":"gwen-nally","date":"2012-09-24 13:02:54 -0400","categories":["Grad Student Research"],"url":"good-practice","layout":"post","content":"This week we’ve made major strides towards adopting a charter. It’s interesting to note (as Claire does in her post ) that charters are often somewhat pessimistic, anticipating the problems of working in a group and setting out certain rules for managing these “inevitable” conflicts. We’ve decided to try for a slightly more positive document, one that focuses on our goals and group ethos . While it might sound a bit cliché to say that we’ve decided to “stay positive”, there is something to the idea that writing conflict management procedures into the charter might somehow cause us to identify ourselves as a group destined for conflict. Oscar Wilde, in his essay ‘On the Decay of the Art of Lying’, tells a story about a young woman, who is so influenced by a fictional character (one that appears in a French serial) that, when the character starts to make destructive decisions, running away with an “inferior” man, the woman feels compelled to follow. Wilde points out that the young woman’s identity is so tightly bound up in the fate of the character that she cannot extricate herself from making the same bad choices. At least where identity is concerned, we are all susceptible this kind of self-fulfilling influence. This is all just to say that, if we begin to label ourselves as a group in need of conflict management strategies, we might force ourselves to become a group that actually needs them. Our attitude, at least this far, has been to think instead about certain good practices (Eupraxia?) that capture the spirit of the program, the people that brought us together, and all the good stuff that DH has to offer. The list reads something like this: Be nice. Be professional (but not too professional). Be respectful. Listen. Reach for consensus. Enjoy the process. Fail in public. Reflect. Retool. Have potlucks."},{"id":"2012-09-24-omeka-capistrano-recipes","title":"Omeka Capistrano Recipes","author":"wayne-graham","date":"2012-09-24 09:31:57 -0400","categories":["Research and Development"],"url":"omeka-capistrano-recipes","layout":"post","content":"The Scholars’ Lab has been working a lot with Omeka over the last several years, and in that time I’ve accumulated a bunch of different installations of Omeka. On neatline.org alone, there are four different Omeka containers running. If I were a glutton for punishment, I would manage these by setting up a new space on the target server, downloading Omeka, going out and grabbing the individual plugins, and setting everything up by hand. There are a few downfalls in this, upgrades are a pain, you can forget what you did from one instance to the next, and it’s horribly inefficient. And, if you’ve ever spent more than a few minutes with me, you’ll know how much I despise inefficiency… We use capistrano to automate deployments for our faculty projects, so that was the tool I was using. Basically I have a directory on my computer named deployments that contain sub-directories for all the projects I am responsible for deploying. In it are the capistrano scripts I use to manage software deployments on those servers. The actual code lives in various git repositories, and these directories literally just have capistrano scripts for deploying software in them. The basic layout looks like this: ```\n$ ls -l | awk ‘{print $9}’ Gemfile\nGemfile.lock\nfalmouth.lib.virginia.edu\nhenshaw.neatline.org\nhotchkiss.neatline.org\nlovecraft.neatline.org\nneatline.org\nsandbox.neatline.org \n``` When I need to deploy a new release, I just go in to that  project’s directory and use cap deploy and not worry about much. Most of those directories had copy-n-pasted code from each other, with the main difference in them being the name of the application (e.g. henshaw, hotchkiss, lovecraft, etc.). With that level of copy-n-pasting, things are sure to go south eventually, so over the weekend I abstracted these tasks out in to a gem and pushed the omeka-recipes gem up to rubygems. There are some useful tasks (like backing up your database and tailing your log files), as well as helping walk through the setup of a new instance. Basically you’ll need ssh access to the remote server, know the path to deploy the software to, and a MySQL admin password. The cap setup task will generate the db.ini file you’ll need on the remote server; no need to learn vim, nano, or emacs! There is some documentation, and there’s a bit more work to do getting this ready for Omeka 2.0 (namely the shift in the naming of the ‘archives’ directory), but if you use Omeka at all, check it out. As always, you can leave an issue on the issue tracker, and I’d love pull requests."},{"id":"2012-09-26-ignoring-your-first-child","title":"Ignoring your First Child","author":"jeremy-boggs","date":"2012-09-26 06:00:37 -0400","categories":["Research and Development"],"url":"ignoring-your-first-child","layout":"post","content":"And no, I’m not talking about human children. I’m talking about CSS selectors! I usually have instances in my web designs where I would like to apply some styles to all the elements of a particular type  except the first one. For example, when displaying a navigation list, I like to add a light border in between each list item, but don’t want to add a border to the first one. There are a bunch of ways to do this, depending on which browsers you want to support. The most straightforward way is to just select your element normally in one declaration, then select the first child of that element using the :first-child pseudo-class, like so: ```\n/* Select all list items. */\nli {\n    border-top: 1px dotted #ccc;\n} /* Select the list item that’s the first child. */\nli:first-child {\n    border-top:none;\n}\n``` This CSS will make sure each list item is separated by a border, and takes away the top border from the first list item, like so: Of course, you’ve got that extra selector to basically take away the border for the first child. That’s fine, too, but there are ways of writing this declaration with one selector to apply styles to elements except the first one. When the :not pseudo-class first came out, it was love at first sight for me. It’s a handy little selector, allowing you to select anything except elements that met some condition. It is, as the W3C documentation says, a negation class. So, using the :not pseudo-class, we could rewrite our CSS to look like this: ``` /* Select all list items except the first child. */\nli:not(:first-child) {\n    border-top: 1px dotted #ccc;\n}\n``` Nice and simple, and only requires writing one CSS declaration instead of two. And you can see, the results are the same: I’ve used this approach in a lot of designs and felt pretty happy with for the most part. One problem, however, is that the :not pseudo-class is not supported by IE8 and earlier. Until recently, to get around that, I’d load a JavaScript polyfill called Selectivizr to basically add support for CSS3 pseudo-classes and a few other selectors to IE 8 and earlier. (More information about Selectivizr, including the features it adds and documentation on how to implement it, on their website .) Only recently did I realize there’s a better way, one supported by IE8 and IE7 that doesn’t require the JavaScript polyfill. That way involves using the the adjacent sibling selector, which is just a + sign, to select only the list items that are adjacent siblings of another list item. Using the adjacent sibling selector, our CSS would look like this: ``` /* Select any list item that is an\n   adjacent sibling of another list item. */\nli + li {\n    border-top: 1px dotted #ccc;\n} ``` This selector doesn’t style the first list item, because it doesn’t have a sibling before it. And again, the results are the same: I like this approach now because it keeps the CSS minimal (no need to undo the style with a second selector, which I like to avoid) and it’s supported all the way back to IE7. Of course, there are plenty of instances where CSS3 selectors like :not are the only way to accomplish what you’d like, and using those selectors and adding support for them with something like Selectivizr is a perfectly fine approach. (I do this for a lot of projects.) But it’s better to make sure there are other ways you can write your selectors that are backwards compatible as much as possible."},{"id":"2012-09-28-living-in-the-future","title":"Living in the Future","author":"chris-peck","date":"2012-09-28 05:28:19 -0400","categories":["Grad Student Research"],"url":"living-in-the-future","layout":"post","content":"Okay, so it can’t make tea, but this MakerBot gizmo is pretty dang cool. And there’s a K-cup machine on the other side of the office, so I guess we can make do with a replicator that only does plastic. That’s right, The Scholars Lab has a MakerBot Replicator. Several of us new Praxis Fellows have been giggling with delight over the shear magic of the thing. Jeremy helped me print a 3-tone whistle design we found on thingiverse, and it really works! Here’s a recording of me blowing into the it, slowed down by a factor of 8 so you can really get the full effect of the three tones. Watching a 3-d model become actual plastic right before your eyes is of course pretty cool. But my glee about the Replicator is not mere idiot glee. I plan to put this thing to use. I’ve been composing music for small mass-produced instruments for a while now, including a series of performances involving plastic soprano recorders . (Yes, like the ones you played in grade school. Okay, so maybe some idiot glee is still at play here.) These instruments have a number of advantages over those conventionally chosen by composers of concert music. For starters, they cost about $2.49. That means that on a modest budget I was able to purchase identical instruments for a group of 40 or so performers, many of whom were self-described “non-musicians.” Part of the concept of that project was to create an ensemble that included dancers, visual artists, and other sorts of people with diverse backgrounds in addition to a few “trained” players. I’ve continued to use small, cheap instruments such as harmonicas in a number of collaborations with choreographers. Just a few weeks ago I ordered a large box of plastic whistles online, in part to discover any subtle variation between them in terms of pitch, sound quality, or response. Such variation could be compositionally useful. When I learned in our first-day tour of the Scholars Lab that a similar plastic whistle could be “printed” from a digital model, I had a mild flash of inspiration:  why rely on mass-produced small plastic instruments? I could design my own. A quick search of thingiverse.com and I’ve already found an  ocarina and even a  recorder . The next step is to learn something about 3-D modeling so I can start modifying theses designs. What will be the effect of subtle variations in sizes of various parts of the mouthpiece, placement of finger holes, etc? My first “original” design will probably be a simple tuned set of whistles made by printing off a number of them at different scales. Someone has already made a double-sized variation on the standard whistle used as a Replicator test print, which of course should sound an octave lower than the normal size. With other factors I should be able to build an entire scale and end up with something like an irritainment handbell choir. I’m also starting to fantasize about designing a physics of music class where the final project would be an instrument design for the Replicator. Do you think we can make a bugle or a small-scale trombone? How about a shawm or a simple clarinet? This takes me back to 8th grade, when I built a flute out of PVC pipe for a science fair project. I guess I’ve been waiting for the MakerBot to appear in my life for quite some time. Jeremy says that he and Wayne have been talking about how to put together a lab so that more UVa folks can access this technology, and I hope that comes to fruition. I’ve been showing off my new 3-tone whistle to friends in the Music Department, and many of them already have ideas for projects. Max  had independently been thinking of using desktop fabrication to build boxes for electronics projects. (If you’ve ever tried to cut a slot for a slide potentiometer in a Radio Shack project box with a Dremel then you understand how exciting this is!) And he just sent me an article about a new low-cost 3-D printer that uses stereolithography instead of heated plastic like the MakerBot, meaning much finer resolution. With developments like this on the horizon its exciting time to start experimenting. As I said, my fellow fellows have been excited about the Replicator too. Fellow Gwen has been printing jewelry . But my question for you, internet, is what (if anything) does this have to do with Prism ? Collective interpretation of tchotchkes anyone?"},{"id":"2012-10-01-casing-your-text","title":"Casing your Text","author":"jeremy-boggs","date":"2012-10-01 11:06:47 -0400","categories":["Research and Development"],"url":"casing-your-text","layout":"post","content":"One of the first things I remember doing as a graduate assistant was editing a few dozen HTML files to change all of the headings, which someone had typed in upper case, to use title case. This was not too much fun. It was even more painful to realize later that you can easily change the letter case of any text by using a handy CSS property called text-transform . The text-transform property has been around since CSS2.1, and has wide support in modern browsers. It allows you to change the letter case of any element’s text that you select. It has five possible values: capitalize, uppercase, lowercase, none, and inherit . The two values I most often use are uppercase and lowercase . capitalize will capitalize all the words in a string of text, which can be handy, but can also capitalize words that don’t need to be capitalized (and that, of course, changes depending on which style guide you want to use ). uppercase will set everything in a text string to upper case, while lowercase, will, you guessed it, set everything in a text to lower case. Let’s have a look. To use the upper case for an element, we’ll use text-transform: uppercase; in our style sheet: ``` h2 {\n    text-transform: uppercase;\n} ``` This will upper-case all h2 elements on your page, like so: To use lower case for an element, we’ll use text-transform: lowercase; in our style sheet: ``` h2 {\n    text-transform: lowercase;\n} ``` This will make all h2 elements on your page lower-cased, like so: Of course, it’s totally fine to type your text directly in a specific case, whether upper or lower, but I would recommend doing so only if its semantically meaningful to the text itself, and not as a stylistic measure. If you are using a specific case for presentation, save yourself a lot of trouble and just write out the text using normal capitalization. You can then set its style using text-transform in your CSS. That way, if you have to change it to something else later, you’ll only need to edit one line of CSS—not hundreds in different HTML pages."},{"id":"2012-10-01-marking-and-explanation-in-prism-2","title":"Marking and Explanation in Prism","author":"brandon-walsh","date":"2012-10-01 11:08:17 -0400","categories":["Grad Student Research"],"url":"marking-and-explanation-in-prism-2","layout":"post","content":"My first experience with Prism last spring brought something of an existential crisis along with it, when I was asked to mark my beloved A Portrait of the Artist as a Young Man in terms of realism/modernism: He was baby tuckoo. The moocow came down the road where Betty Byrne lived: she sold lemon platt. O, the wild rose blossoms On the little green place. Considering Portrait as a modernist novel was nothing new, but being asked to consider individual words in that context was altogether unsettling. Words like “tuckoo” and “moocow” are easy to mark as modernist, but “baby” is perfectly reasonable within a realist text; it only begins to feel experimental as a part of the larger phrase “baby tuckoo.” And how do we account for the two lines of verse, where the words themselves are not particularly experimental but the juxtaposition between prose and poetry creates just the sort of genre bending you would expect from modernism? I think this very brief analysis tells us something very important about how we make meaning: interpretive choices depend not just on the words in the text but on perceived relationships among different textual groupings within the text as a whole. Part of the perceived value of a crowdsourcing tool like Prism comes from its ability to generate conversations about these interpretive decisions. I wonder, though, if we can include these conservations in the tool itself. The obvious response to this perceived need would be to prompt users to justify each interpretive decision they make, but this seems undesirable. I expect that the interface would become unwieldy very quickly if we asked users to explain every marking that they make, and I imagine that the defensive posture this would engender in users could be inimical to the nature of the tool’s ethos. Perhaps we could consider an opt-in approach, where we offer users the option to select a particular marking and offer a short explanation after marking and just before saving the highlights. Then these terms could be footnoted in some way for visualization. Perhaps we could even run some sort of thesaurus program to link similar explanations across different marked passages. I don’t mean to imply that this is a flaw in the Prism’s design: I think one of the more mind expanding aspects of the tool is how it lays bare the process by which word transforms into meaning. I hope we can find a way to front-end this strength in the user interface."},{"id":"2012-10-02-fun-with-prism","title":"Fun with Prism","author":"gwen-nally","date":"2012-10-02 06:44:24 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"fun-with-prism","layout":"post","content":"Prism could be a tool that we use for scholarly entertainment (like  Old Weather ). It could also be an extremely powerful tool for research–provided that we make the controls fluid enough. Earlier this week, Claire and I dreamed up a rather elaborate interface that would showcase Prism’s playful qualities, in order to generate “subjects” interested in participating in the research side of things. While brainstorming, we started casually referring to all the possible projects housed on Prism as “rooms”, each with its own “door” through which users would enter. Among other adjustments that we imagined, we thought a lot about customization, allowing a project leader to make a number of crucial decisions before launching the room. Who would be allowed to mark up the text? And how would a reader be presented with the request? Perhaps the most useful innovation we discussed was the option to remove the fixed “categories” from the front-end experience, so as to provide for freer responses. But then what? Would readers be able to provide their own tags and categories? Would it be desirable to run these tags through some kind of linguistic software in order to generate more general data? Might all of this be left up to the person launching the project? This conversation led to a shift in my thinking about how different disciplines might approach Prism–I even started to imagine uses in my own field. In the past, I’ve been a bit skeptical of crowd-sourcing in that very few questions in philosophy have traditionally been answered by appealing to surveys or statistical analysis (with the exception of some pretty cool stuff in experimental philosophy ). But thinking about these rooms, with fluid controls and the possibility of limiting the number of readers, got me thinking: Prism could also be a powerful tool for experts. I even got a little giddy when I realized that this could be a tool for, say, presenting what the very best living readers of Ancient Greek might have to say about certain portions of Plato’s Republic . (Big fun, right?!) This type of “expert-sourcing” could provide students with an excellent commentary on a given text, and might also serve to define new areas for research. An interpretive void–what the experts don’t mark up–could be just as useful as what they do mark, drawing our attention to passages that have received less scholarly attention. In keeping with this idea, I’ve also started to think about the possibility of showcasing finished or closed projects. People could visit certain rooms to see the ways that certain groups have marked important texts. I can imagine all sorts of fun rooms:  Classical musicians respond to a piece of Noise Music, Research participants in placebo-controlled trials hash out the informed consent documents, Followers of Kabbalah interpret the  Torah, Fourth-graders comment on the US Constitution…"},{"id":"2012-10-02-images-in-prism","title":"Images in Prism","author":"cecilia-márquez","date":"2012-10-02 06:46:11 -0400","categories":["Grad Student Research"],"url":"images-in-prism","layout":"post","content":"One thing we have thought about in recent weeks is the role of Prism in our goals for the semester.  As part of this brainstorming I wanted to share some of my thoughts about a potential future for Prism. What I was most interested in is how we can use images with Prism.  Images call for a different type of literacy that is potentially more accessible and inviting.  Specifically I am interested in looking at historical photographs as texts to interpret through crowdsourcing.  For example, this photo is an image used in a colleague’s work . This image, used by the Southern Rural Action Inc., was taken in Georgia in the 1970s.  However, little of this information is self-evident in the photo.  What I would be interested in finding out is what viewers, both those educated in Southern African American history, and those viewers who may only know Southern African American history through popular culture.  Offering both these groups a forum to analyze these photos will serve a few goals.  First, it can illuminate the role of Southern African Americans in the imaginary of those who examine the image through Prism.  Second, the call for audiences to bring an analytical eye to bear will hopefully expose the constructed nature of photographs.  Hopefully it will show how photographs are deployed for political ends and are not neutral snapshots of a moment in time. What I imagine is the viewer being introduced to an un-annotated photograph where they can respond to questions like: Who is in this photo? Who are the subjects in this photo? Where is this photo taken? What is happening in this photo? Who is the audience of this photo? After completing their own analysis the viewer can go on to see the bibliographic information of the photo (the who, what, when, where) and additionally the analysis of a specialist on the topic.  This layer of annotation could include a more discursive analysis of the image that can expose layers of meaning in the photo. This is just one idea and I look forward to seeing what is possible!"},{"id":"2012-10-03-hot-off-the-presses-the-solrsearch-plugin","title":"Hot off the Presses: the SolrSearch Plugin","author":"eric-rochester","date":"2012-10-03 07:13:45 -0400","categories":["Announcements"],"url":"hot-off-the-presses-the-solrsearch-plugin","layout":"post","content":"The Scholars’ Lab is pleased to announce the first release of the SolrSearch Omeka plugin. SolrSearch allows you to replace Omeka’s default search with Solr . Solr is a standard, popular, open source, fast text search engine server. It handles hit highlighting, date math, numeric aggregation functions (mean, max, etc.), indexing for 33 languages, replication, and many, many more things. It’s used by whitehouse.gov, Instagram, AT&amp;T;’s yp.com, Ticketmaster, and Netflix, to name a few (see the list of Public websites using Solr ). It does require running Solr as a separate server process (although possibly on the same machine), so it does require more resources–both personnel and technical–but it’s often worth the investment. Search Pages and Exhibits SolrSearch now indexes Simple Pages and Exhibits. Performance Did I mention that Solr is fast? It’s been optimized for high-traffic sites, and it can easily handle much more data than MySQL full text search can. Scalability And because it’s been engineered for large, high-traffic sites, Solr can handle more data, faster than MySQL. This especially becomes an issue when you have collections with a large number of items or items with a lot of data attached to each. Configuration The SolrSearch plugin in highly configurable. You can decide which fields to search, which can be used for facets, and how to label them. Facets Facets slice up your items and allow users to navigate through those slices. For example, The Falmouth Project used an early version of the SolrSearch plugin to give users not only free-text search, but also to allow users to browse the buildings it records by neighborhood, date, and use. You can find the download on the SolrSearch plugin page . The code is hosted on the SolrSearch github page . If you have any feedback about the plugin, find any bugs, or want to suggest features, head over to the issues page . And if you have questions, feel free to post in the Omeka forums . As always, we look forward to seeing how you’ll use this."},{"id":"2012-10-08-hot-off-the-presses-2-bagit-plugin","title":"Hot off the Presses 2: BagIt Plugin","author":"eric-rochester","date":"2012-10-08 07:41:46 -0400","categories":["Announcements"],"url":"hot-off-the-presses-2-bagit-plugin","layout":"post","content":"Photo by kittybabylove Continuing our roll-out of Omeka plugins we’ve been working on here at the Scholars’ Lab, I’m pleased to announce the BagIt plugin for Omeka . BagIt is a specification by the Library of Congress for creating containers of files with metadata. However, the files don’t actually have to be in the container. There is a fetch.txt file, which lists URLs for content to add to the container when you take everything out of it. The first part of this release is the BagIt PHP library . This is a generic PHP library for working with BagIt files. We announced an earlier version of this here, but we’ve updated it and fixed some bugs. If you’re using it, you may want to grab the latest copy of it. The second part is the BagIt Omeka plugin . This is built upon the BagIt library and provides an easy-to-use user interface for it. You can create a bag from a set of Omeka files. You can ingest bags into the Omeka Dropbox plugin, and from there you can attach them to items. This plugin does have a couple of requirements. Both the library and the plugin require the Archive_Tar PHP library, and the plugin depends on the Dropbox plugin . You can find the download on the BagIt plugin page . The code is hosted on the BagItPHP github page and the BagItPlugin github page . If you have any feedback about the library or the plugin, find any bugs, or want to suggest a feature, visit the issues page . And if you have questions, feel free to post in the Omeka forums ."},{"id":"2012-10-10-hot-off-the-presses-3-fedoraconnector-plugin","title":"Hot off the Presses 3: FedoraConnector Plugin","author":"eric-rochester","date":"2012-10-10 07:58:19 -0400","categories":["Announcements"],"url":"hot-off-the-presses-3-fedoraconnector-plugin","layout":"post","content":"Photo by Swing Candy For part three of our release parade, we’re showcasing the 1.0.0 release of the FedoraConnector plugin for Omeka . Fedora Commons is a digital repository management system. It’s used by libraries to manage and scale their online repository and digital assets and collections. As such, it’s often used by larger institutions; however, this isn’t aimed at those organizations. Instead, it’s meant to be used by people who wish to pull information from institutions that use Fedora Commons. The FedoraConnector plugin doesn’t help you discover resources in a repository. But once you have the PID for something that you’d like to include in an Omeka site, you can use that to pull in the metadata for that item, as well as any images or other content streams associated with it. There is a little magic involved. You have to know the incantation to reach your Fedora server and you have to know how to get the PIDs for the items you’re interested in. But David McClure has worked hard to hammer out an easy, fluid workflow for getting data from Fedora into Omeka. And once you know these things, this plugin will allow you to pull content into your Omeka site that you otherwise wouldn’t have access to: things you’ve found in your library catalogue and wished you could include, but didn’t know how. You can download the plugin on the FedoraConnector plugin page . The code is on the github repository . If you have feedback, complaints, or feature requests, visit the issues page . And if you have any questions, feel free to post to the Omeka forums ."},{"id":"2012-10-17-the-direction-of-prism","title":"The Direction of Prism","author":"brandon-walsh","date":"2012-10-17 07:27:23 -0400","categories":["Grad Student Research"],"url":"the-direction-of-prism","layout":"post","content":"This week, the team has been throwing around a number of ideas as to how we can further develop Prism. I keep falling back on spatial metaphors to categorize the changes currently in play. I threw the following model onto the SLab white board, wherein I propose two types of changes: vertical and horizontal. First, the vertical axis, those changes which work within the tool’s existing functionality. Some changes deepen a function already in place. The suggestions offered by last year’s team for other visualizations would fall into this category. A new visualization can offer a more dynamic interaction with a set of markings, but users are still engaging with the same dataset. The fundamental function of the tool would remain the same. These sorts of vertical changes offer a deeper, more robust mode of interaction with the operations that the tool can already carry out. I oppose these to horizontal changes, which more fundamentally alter the number and types of things that a tool can do. We have talked a lot about carrying Prism into other media: music, images, and video. We have also talked about freeing up the controls on Prism, making it possible for users to upload their own texts or to hide the categories prior to marking. These sorts of modifications change the face of Prism more drastically, resulting in different data sets and new possibilities for the users. These horizontal changes seem to carry the highest risk. As we discussed at our meeting last week, we risk diluting Prism’s identity if we try to add in too many features: a tool that can do anything might just be a tool that can do nothing effectively. The SLab crew also pointed out how some of our broadening changes might carry us into territory that has already been covered by other developers and other tools. At the same time, though, some of suggestions for horizontal growth are incredibly exciting. Taking Prism into other media, freeing up the controls – these changes could help Prism reach a broader audience by teaching it new tricks. I think we should take them seriously. Of course, this horizontal/vertical distinction does not quite hold up under close scrutiny. I think it would be a mistake to think of depth-oriented changes as being somehow narrower than broader changes. A new visualization could fundamentally alter your relation to the data in such a way that it feels as though the functionality has altered entirely. No modification slots entirely into one or the other category, and the spatial metaphors might muddy the waters unnecessarily. All the same, I find the distinction helpful for mapping out the morphing shape of the Prism of the future. So do we build depth, or do we broaden our base? Do we dig into the features already present, or do we add new ones? At what point does Prism cease to be Prism? What would Prism look like if we graphed it?"},{"id":"2012-10-17-the-whiteboard","title":"The Whiteboard","author":"gwen-nally","date":"2012-10-17 07:26:05 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"the-whiteboard","layout":"post","content":"This is what the whiteboard in the grad lounge looks like this week. We’ve been dreaming pretty big. Here are a few of the questions that we’ve been mulling over: 1. What will Prism look like? What sort of interface would allow users to upload their own materials and browse other ongoing (or completed) projects? (This is where Claire and I started thinking about different “rooms”. ) 2. What kinds of things might we leave to the control of users?  Would users be able to upload different kinds of “texts”, like images and music? Could users hide or display different categories to guide interpretation? How might we make for freer responses? Can freer responses be turned into usable data? Would there be some value to closing projects? Or should all projects be kept open to the public? 3. How might we visualize whatever data we collect?  4.  Do we want to add more features or make the features that already exist more robust?  5. What should we do next? It seems like the next big hurdle for us will be to carefully weigh our ambitions against our limited time-frame. While brainstorming, Brandon suggested that we’re going to need to be careful to take on manageable goals. One way to do this might be (#4) to focus our efforts on making the existing features more robust. This is an attractive idea. We could begin by trying to realize the sorts of visualizations that last year’s cohort dreamed up. Although we’re not really sure how to answer these questions, we may be a bit closer than we were. In yesterday’s meeting, I finally realized something that had been lost on me: Prism serves a unique role in the DH community. It is the first tool of its kind in that it crowd-sources user interpretations. There have been many crowd-sourcing tools, but most (if not all?) have treated the user as a means of collecting or transcribing large amounts of data.  So whatever we decide to do, I hope that we are careful not to loose sight of this innovation–capturing user interpretation as usable data is, in a sense, what makes Prism tick."},{"id":"2012-10-18-digital-humanities-growing-pains","title":"Digital Humanities Growing Pains","author":"cecilia-márquez","date":"2012-10-18 10:05:18 -0400","categories":["Grad Student Research"],"url":"digital-humanities-growing-pains","layout":"post","content":"This has been a tough week for me and the Digital Humanities. We are all grappling seriously with what we want to do this year with Prism or with some other project entirely. This has led to some really amazing, but at times tense and frustrating, conversations amongst the group. Part of what is blocking me from being able to move forward is trying assess the use of Prism. Is it a pedagogical tool? is it a tool for researchers? is it a tool for entertainment? Who benefits from the production of these crowdsource interpretations? In my mind there are some obvious benefits as a historian to having many sets of eyes on a particular primary source: meanings I may have missed become apparent or alternative readings emerge. But that is a fairly utilitarian goal for Prism, to benefit me as an academic. Additionally, the interdisciplinarity of our group forecloses that as a viable goal. What serves me as a historian doesn’t serve my colleagues in Music or Sociology. If the goal is to use it as a pedagogical tool then we will certainly have to figure out how to set some controls on who can comment on a page to ensure only a class will comment. But even then, as a pedagogical tool, does this project really serve the “crowd.” I understand the goal is to harness the “interpretive energy ” of groups of people, my concern is once that energy is harnessed what do we do with that information? I get stuck in a cycle of being skeptical of “crowdsourcing” because it anonymizes and mechanizes human creativity but also at the same time finding great value in some of the projects for myself as an academic. However, this personal benefit feels self-serving and I worry it is not committed to a democratization of knowledge, which brings me back to the “crowd.” I would love answers to any or all of these questions. These are just some of the things I’m pondering. In other news…we ratified our charter today and you can look forward to reading it in the next few days!"},{"id":"2012-10-19-a-project-for-prism","title":"A Project for Prism","author":"gwen-nally","date":"2012-10-19 02:48:34 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"a-project-for-prism","layout":"post","content":"In the last meeting we played the transparency game: Everyone highlights a transparency on top of a text according to a set of categories. At the end of the exercise, all the transparencies are stacked together. What does this show? In theory, a number of interesting results emerge.  The game could show where there is consensus. It could also show conflicting interpretations or unexplored parts of a text. (This is very exciting stuff. I can imagine using Prism in a classroom to generate debates or in a research setting to mark those regions of a text that are in need of more attention.) In practice, though, our transparency game generated some pretty confusing results. When we stacked the transparencies together, nothing really jumped out. It was visual chaos. Did something go wrong? Had we not thought about the texts carefully enough? Was our sample size too small? Were our categories misleading? Should we have been more specific? Did we break it? Maybe the transparency game is just supposed to generate more questions? We also noticed a number of somewhat troubling features of the game. The biggest issue was that some people tend to mark a text heavily. Others tend to mark only a few lines. The result: Those who mark less seem to have less of an impact in the aggregate visualization. Is this a problem? Should we be assigning some weight to the markings? In all the confusion, a few of us noticed that we began to unstack the transparencies, spreading them out on the table. Although this didn’t lead to any clearer results, we noticed that it was a natural impulse to unstack them. This points, I think, to an interesting tension that is built into the very fabric of Prism, a tension between individual interpretation–something very familiar to academics–and the “aggregate interpretation” that Prism generates. The meeting ended with an assignment to design a project for Prism. … Here is my project: Overview: The text I have in mind is Plato’s Phaedrus. The research question would be to use crowd-sourcing to determine the perceived truth-value of Socrates’ statements. There is a long literature about whether and to what extent we ought to take Socrates’ statements literally. Many of the dialogues suggest that there are certain types of argumentative practices–lying, deception, misleading, trickery, confusion tactics–that are inappropriate for the philosopher to adopt insofar as he is concerned with the pursuit of truth. But there is also a long history of viewing Socrates as a trickster, a dissembler, and even a liar. This project would ask readers to decide: Is there evidence to suggest that Socrates does, at times, mislead his interlocutors? And, more importantly, does this occur even in the midst of otherwise serious philosophical investigations? The categories: A) Serious discourse B) Ironic discourse C) Humorous discourse and D) Misleading discourse. A big question for me is how to frame these categories. Should people be able to interpret the categories freely? Or should I say more? I’m inclined to say more, insofar as I’m interested in particular features of a very old text. Situating the research question in the scholarship might help to shape people’s readings in a productive way and avoid anachronism (Shane’s right!). The audience: People who are comfortable with Plato’s dialogues, or more generally, with ancient Greek culture. Explaining the literature might make this a project better suited to a mass audience. Some modifications: 1) A place to explain the research question. 2) A place to explain the categories. 3) A longer text. I would want people to engage with and mark the entire dialogue. Prism is, at the moment, set up very much like the transparency game. Each text is no more than a page long. But this precludes the ability to delve into themes, structures and other features of a work that require a more synoptic approach."},{"id":"2012-10-19-neatline-omeka-theme-name-contest","title":"Neatline Omeka Theme Name Contest","author":"jeremy-boggs","date":"2012-10-19 05:56:51 -0400","categories":["Announcements"],"url":"neatline-omeka-theme-name-contest","layout":"post","content":"Yesterday I tweeted asking for name suggestions for an Omeka theme based on the design of Neatline.org . We’ve already gotten a few great responses, but we’ve decided to kick it up a notch. We’ve got a few Neatline t-shirts. They’re nice t-shirts, as demonstrated by our friendly Scholars’ Lab Waynebot: [gallery size=”large” link=”file” columns=”1”] Between now and next Thursday morning, say 9AM Charlottesville time (east coast US), leave us a name suggestion through Twitter (using the #neatlinetheme hash tag) or in the comments on this post. The Neatline team will look them over, pick one, and the winner will get a lovely Neatline t-shirt, some Scholars’ Lab stickers, and a credit line in the theme’s code. The person who mentions a name first will get credit for it. No multiple winners, we don’t have that many tshirts! The name could be a play on “Neatline” or something having to do with functionality of our lovely suite of Neatline plugins. Or it could be something totally random. Common decency, of course, is most appreciated. We’ll announce a winner next Friday morning. So send us a theme name!"},{"id":"2012-10-19-to-crowdsource-or-not-to-crowdsource","title":"To Crowdsource or Not To Crowdsource?","author":"claire-maiers","date":"2012-10-19 02:49:39 -0400","categories":["Grad Student Research"],"url":"to-crowdsource-or-not-to-crowdsource","layout":"post","content":"Sneaking its way into many of our conversations of the last month and half has been a debate over the value of crowdsourcing.  Should we do it?  Is it useful?  My original intention with this post was to offer a defense of crowdsourcing as a valuable endeavor for academia.  While I still think that, ultimately, I am supportive of crowdsourcing or something similar, the fact that it has taken me two weeks of stops and starts to write this post speaks to my own struggles, doubts, and uncertainty about our project as a crowdsourcing project.  As I understand them, some of the concerns about crowdsourcing from our team are as follows: Does crowdsourcing (used for interpretive purposes) create any kind of useful data, knowledge, or insights for academic purposes? Does crowdsourcing treat people like cogs in a system, resulting in their dehumanization? Assuming that we want our project to be relevant beyond academic walls, can a DH crowdsourcing project actually reach beyond those walls?  Are non-academics interested?  Will people from other walks of life even have the chance to be exposed to such a project? I think these are legitimate concerns which we need to address.  However, I am going to argue that we should, nevertheless, endeavor to incorporate some sort of crowdsourcing aspect into our project.  Without roaming too far away from the central issues, I will try to explain why: One of my primary concerns with academic pursuits is a failure to consider the implication of our profession, research, and practices for the society beyond our own institutional borders.  This might be a surprising criticism of a world that often (though not always) hails the insights of feminism and postmodernism, which invite an acknowledgment of subjective meanings and encourage a self-reflective awareness during research and writing.  However, while this approach may be used within the confines a research project, I often the lament the degree to which we fail to ask important self-reflective questions about research and academia in general.  What is the role of a university?  Of a library?  How do our practices matter in the world beyond the ivory tower?  How might our scholarship influence policy, definitions of truth, or identity?   What are the lines of communication and influence between our universities and the rest of society?   Is our scholarship relevant to someone aside from other academics?   As privileged members of the some 30% of the U.S. population who graduate from college and the even smaller enclave who make their living in academic institutions, I feel we are obligated to ask these questions. Part of what attracted me to the Praxis program was the use of crowdsourcing, which I saw as an opportunity to engage in these big questions by bridging the space between the ivory tower and the world beyond.   Not only could a crowdsourcing tool have research potential, it could also help to make us aware of the world outside our own institutions and to (hopefully) keep an eye toward that world as we pursue our own scholarship.   I am still excited about this potential despite our concerns over crowdsourcing.  So here are my thoughts on the three questions listed above: Will a crowdsourcing project produce useful information for academic pursuits?  Perhaps not in direct manner.  But, it could clue us into important and relevant questions which we can then address in our work, ensuring a certain degree of relevancy between our work and the nonacademic world. Does crowdsourcing turn people into dehumanized cogs?  This is such an important question as we endeavor to be responsible and reflective scholars!  The way in which crowdsourcing has been used in the past to harness the energy of the masses makes this a legitimate concern.  However, I think our team is in agreement that we are interested in a project that centers on interpretation rather than using “the crowd” to accomplish a particular project.  Perhaps this implies that we need a new term for this approach—maybe we are not really doing crowdsourcing so much as suggesting that through a collaborative interpretive project we could get a sense of the pulse of a community.   Does this still result in dehumanization of individuals?  I am uncertain—this is definitely a concern which deserves more conversation. Can a DH project really reach beyond the walls of the academy?  Could it provide a line of communication between professional scholars and others?   I think that the right kind of project with an inviting, playful interface and an approach that is presented in non-specialist language could, in fact, do just that.  However, I agree with some of my team members, that actually reaching beyond the academic community would be challenging.  Reaching beyond the 30% of the population which is college educated might be even more unlikely.  But I am still excited by the potential to do these things, and don’t think that the difficulties they present should prevent us from undertaking them. I’m sure there are other concerns when it comes to building a tool that depends on crowd participation.   I feel as if I have barely tipped the iceberg on this issue, so comment away.  Should we crowdsource?   What are the benefits?  The pitfalls?  How does this decision relate to the larger mission of our work and the work of academic institutions in general?"},{"id":"2012-10-22-prism-project-proposal","title":"Prism Project Proposal","author":"cecilia-márquez","date":"2012-10-22 07:17:20 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"prism-project-proposal","layout":"post","content":"Full disclosure: this list of questions came from a prompt by  David McClure . This also grows out of a blog post I wrote earlier about my interest in utilizing images in Prism. What’s the overarching intellectual goal? Analyze what visual cues in photographs trigger regional distinctions for their viewers.  Using photographs from the 1950s-1960s I am curious what images signal the “South” or the “North” in viewers of the photographs.  Specifically I am interested in the relationship between “blackness” and “Southernness” in these images. What text/image/file? Which tags? I have three images that I would be interested in processing through Prism.  They each portray some form of racial control or a response to that control.  As you can see, some fit easily into our memory of Jim Crow, a man at a segregated water fountain where separate is clearly not equal.  However, I am also interested in including photographs that frustrate our national memory of Jim Crow.  For example, in what region would a Jim Crow sign that refuses “Spanish” or “Mexicans” reside?  Finally, an image like the one below where there are no obvious signs of Jim Crow.  I’m curious what region people will choose in a photograph devoid of obvious regional signifiers. In terms of tags I guess I would have a tag that says: Region and like on Facebook you could select a portion of the image and indicate what region you think that it is indicative of and why.  For the photos that include people, I would  have a tag like “subjects” where people can select the person and include a response about who they think this person is, what they are experiencing, why they are being photographed etc.  A last tag would not rest in the physical photograph itself but rather a space for readers to indicate why they thought the photograph was taken, what the political goals were etc.  I guess images don’t fit easily into the highlight model of Prism but I think there could be some fascinating visualizations that come out of this! Who would participate? In what context? I want participation to be as open as possible.  My preference would be to maintain some demographic data of who is participating (ex. age, race, gender, location).  I could imagine, for example, splitting the responses up by region, considering what Northerners consider the South and what Southerners consider the South.  I am not interested in a closed environment setting, rather I think the more people across different regions, education status, ages, genders etc. responded the more interesting the outcomes could be. How does it converse (if at all) with existing disciplinary lines of inquiry? There are important historiographical implications for this project, specifically in thinking about the relationship between the cultural production that emerges out of segregation and the subsequent memory of Jim Crow.  The specific interest in the Jim Crow signage as text grows out Elizabeth Abel’s  recent book Signs of these Times: the Visual Politics of Jim Crow, in which she examines the layers of discursive meaning in varied Jim Crow segregation signs.  Grace Hale’s work, Making Whiteness,  also informs the roots of this project.  She suggests that the rise of modern consumer culture created spaces of conflict in which the “culture of segregation” was formed.  It is intriguing to consider how these spaces of consumption memorialized in photography and subsequently in the national imaginary?  There are obviously many other scholars whose work this intersects with, those are perhaps for a later post."},{"id":"2012-10-23-prism-proposal-against-anonymity","title":"Prism Proposal: Against Anonymity","author":"brandon-walsh","date":"2012-10-23 09:14:53 -0400","categories":["Grad Student Research"],"url":"prism-proposal-against-anonymity","layout":"post","content":"Jeremy asked us to offer a Prism exercise that would take the tool in a new direction. Rather than expanding the tool’s parameters, I would like to think about how we can get a more nuanced understanding of the information it already collects. In what follows I offer a vaporware exercise that we will play in person, suggest some thoughts on the disciplinary and pedagogical questions it might answer, and I conclude with some suggestions for adapting the exercise for digital use. The idea: rather than marking multiple categories, all participants mark a single category. Each person has his or her own unique marking color. The text: The specific text is not important for this particular idea, but for the sake of the exercise I will use “The Weary Blues” by Langston Hughes The marking: Other(s) The research/pedagogical issues at stake: The exercise was designed with pedagogy in mind. In the final visualization, the color constellation tells us who marked what in a clearer way: red for Brandon, green for Cecilia, etc. This is absolutely vital if we are going to use the tool for pedagogical purposes. Rather than submerging the voices into a bundle of data with no way back, this approach allows us to see the overlap and disjunctures between different interpretations more clearly. In the vaporware version, we can’t really do this without pulling the transparencies apart again to see them individually, which destroys the collective image. By restoring clarity to the individual voice, we can more easily open up a conversation with students. What did a particular student mark? Why? How does this relate to the aggregate? How does a particular voice interact with the crowd? These discussions of individual vs crowd work particularly well for this textual example: the poem obsesses over the relation between speaker and musician, poetry and tradition, individual and community, etc. Just as scholars would discuss how the lines blur between individuals and groups of people in the poem, the marking activity can open a meta-discussion about the nature of our own marking activity as an intellectual endeavor – a collaborative one nonetheless comprised of individual gestures. There is also a fairly selfish short-term application to this: it simplifies things. As Gwen mentioned, throwing together eight transparencies all marked for three different things comes across as a big glob of data. Maybe the above approach can declutter things for our short-term training as Prism initiates. In this vaporware experiment, a mark means one thing only—the color tells you who did it. One category might not mean fewer markings, but it would at least be easier for us to process with fewer different systems of meaning on the overhead. I imagine that this (hopefully) simplified exercise is something that we need to just get a sense of what sorts of conversations, interpretations, and gains can come out of Prism at all. In a similar manner, the current Prism visualization only deals with one marking at a time. You sort by marker, and you don’t see all the markings at once. The above approach is designed for the vaporware transparency game, so it obviously only works with small groups of people. This makes me wonder if it is even a viable option for working with a “crowd.” It seems very useful for pedagogy, but would such a thing even be feasible with a class of 20 students? I don’t think so. You’d have to get a really big packet of markers and a very nuanced sense of the color pallet. So this is really an idea that needs the technology in order to function. Here is a variation on the idea behind this exercise that would work for the online tool. Users mark as usual and go to visualize. Below or next to the list of markers, we have a dropdown menu with a list of the users. Mousing over a particular username causes their markings to appear in the text, allowing you to compare their markings with the crowd’s aggregate. How these individual markers are visualized in relation to the mass is up for debate. One idea to implement this would be to underline each word that the user marks: a really big word not underlined means that the user did not mark it, but a large portion of the crowd did. A small word underlined means that the user marked it, but the crowd did not. This is not a perfect system, so I’m open to ideas. And this doesn’t even begin to broach the question of how the sorts of conversations I imagine coming out of Prism can possibly emerge when users are not all in a single room together. But I do think this larger question of retaining the individual user’s gesture and their markings is an important one – a step towards retaining their voice and even their humanity, in just the sense that Claire references."},{"id":"2012-10-24-a-practical-prism-pedagogy-proposal","title":"A Practical Prism Pedagogy Proposal  ","author":"shane-lin","date":"2012-10-24 11:00:07 -0400","categories":["Grad Student Research"],"url":"a-practical-prism-pedagogy-proposal","layout":"post","content":"Everyone’s been writing up proposals of new ways to use Prism and new functionality that can be implemented. One of the really exciting ideas (for me at least) that we’ve been tossing around collectively has been the idea of linking arguments to supporting evidence. In speaking with my fellow history department TAs, we’ve noticed that many of our students are doing poorly at uncovering how history literature connects these two things and some are not even sufficiently aware of what constitute evidence. Prism, used for pedagogy, could force a direct and individual engagement with these concepts. Instead of marking up documents along predetermined themes or to arbitrarily choose new ones, students could be asked to mark particular passages as arguments.Then, they can mark up passages within the document that act as support for the argument. Other students can choose to contribute additional evidence markup for an existing argument or to contribute their own. Instructors can monitor similar theses and marge them dynamically in order to prevent duplication. In the end, students can compare what they’ve highlighted as arguments and evidence against what others have chosen. The resulting aggregate set of arguments and evidence can also readily serve as review materials. This scenario diverges from the current implementation of Prism in a few ways. Central to this excercise is the ability to more nimbly compare one’s own selections versus that of the group aggregate, but also with other individual selections. Some mechanism to  To help gamify Prism, we can offer statistics on the distance from the mean and the closest and furthest members from a user’s selections. This data could then be used to group people with discordant views together for in-class discussion. Analysis of these relationships would also be helpful to tease out connections between arguments and between evidence. For example, highlights might help uncover which arguments are tightly connected or else have little to do with each other because of a lack of overlap in evidence. Different types of evidence might also be marked differently, perhaps based on predetermined categories - by its primary or secondary nature, its position in different historiography schools, its type, or the class or gender of its author. Analysis could then show which arguments are most strongly supported by evidence of each type."},{"id":"2012-10-24-not-joking","title":"Not Joking","author":"chris-peck","date":"2012-10-24 11:24:54 -0400","categories":["Grad Student Research"],"url":"not-joking","layout":"post","content":"Conventional wisdom holds that nothing is quite as un-funny as a joke explained. I was reminded of this last week when I felt compelled to explain to a class of 18-22-year-old music theory students why d-minor is the saddest of all keys . But could it also be the case that the funniest joke results from the over-explanation of a joke which was never actually very funny to begin with? To test this hypothesis, I propose the following social-annotation-with-transparencies game: The text is the score to the fourth movement of Haydn’s Op.33 No. 2 quartet—known as “The Joke.” We’ll listen to a recording two or three times. Each participant has an identical loop of string that they use to enclose the area on the score that they consider most humorous. Shapes of string loops are traced on transparencies and overlaid."},{"id":"2012-10-24-plastic-debacles","title":"Plastic Debacles","author":"jeremy-boggs","date":"2012-10-24 07:00:07 -0400","categories":["Research and Development"],"url":"plastic-debacles","layout":"post","content":"The Scholars’ Lab has a Makerbot Replicator with dual extruders, and it’s become a Praxis Program favorite. We’ve printed musical instruments, bracelets, animals, puzzles, and even a topographic map. If you get into 3D printing, chances are pretty high you’re gonna have some failed prints. Things happen in the course of tinkering with models and print settings. Temperature fluctuations, drafts around the printer, the level of the build plate, the speed of the print, all kinds of stuff can affect the outcome. Debugging this stuff is a learning experience itself, and I find myself learning a little more each time a print gets messed up. So here’s a gallery of our plastic debacles. Lovely, lovely debacles: [gallery link=”file” ids=”6572,6571,6569,6567,6566,6565,6564,6563,6562”]"},{"id":"2012-10-24-reading-socially","title":"Reading Socially","author":"katina-rogers","date":"2012-10-24 12:52:20 -0400","categories":["Grad Student Research"],"url":"reading-socially","layout":"post","content":"As the Praxis team has been discussing the values and drawbacks of anonymity (or pseudonymity) in crowd-sourced interpretation, I’ve been thinking about what it means to read socially – more specifically, what we rely on from other readers, and what we provide to other readers. Like most everyone, I spend a significant portion of my time reading, in many different formats – from work-related research in the form of journal articles, news, blog posts, and popular media, to the professional/personal world of Twitter, to purely personal novels and side projects. The network of people around me has a strong effect on my reading patterns. I pick up book based on recommendations of friends with similar taste; I click countless links a day because someone I know or find interesting has mentioned them. Unfiltered, the volume would be simply overwhelming, so I rely on a variety of cues to decide where to direct my attention. Part of this filtering process has to do with what I’m thinking about or looking for. Another factor is the expertise or particular qualifications of the person recommending something (depending on the context, “qualifications” can mean anything from general brilliance, to specific topic knowledge, to an ability to spot something funny). So, the background of the person influences my desire to read something, and also affects how I read it. These pointers are incredibly useful. At the same time, some social reading features – I’m thinking particularly of the Kindle’s “Popular Highlights” – drive me completely crazy. Why do some aspects of social reading help me to read more effectively, while others distract me? A lot of what matters seems to be the identity and role of the person influencing my reading, and what that cues me to look for in the text. The anonymity of Kindle’s Popular Highlights renders the annotation a meaningless distraction – much like picking up an overly-highlighted used book. On the other hand, annotations from a respected colleague can add a great deal of depth to my own reading experience. Prism’s current iteration isn’t meant to guide the reader as she reads – but rather, to aggregate the reading experiences of many users to bring new meaning to the text. What would it look like if Prism could not only capture readers’ interpretations, but also enable readers to be guided toward different or deeper readings based on the interpretations of others? The team has been talking about how different ways of sorting the kinds of markings that people make might make participation more meaningful, and I’m looking forward to seeing where te discussion leads."},{"id":"2012-10-25-crowdsourcing-for-profit-and-pleasure","title":"Crowdsourcing for Profit and Pleasure","author":"shane-lin","date":"2012-10-25 05:27:14 -0400","categories":["Grad Student Research"],"url":"crowdsourcing-for-profit-and-pleasure","layout":"post","content":"This post is in response to Claire’s thoughtful writeup on Crowdsourcing, which I think raises and tries to answer some absolutely salient questions. Originally, I think the intent was to wrote a simple “con” piece, but since Claire - and Brandon in reply - have taken such nuanced and sophisticated positions, I suppose that I’ll have to do the same lest I appear a rube (don’t be mean, Cecilia). I’ll switch up the order to keep things interesting. Does crowdsourcing turn people into dehumanized cogs?  We need to first break down what crowdsourcing means. Crowd, used here, is like one of those weasel-words that my students use when they don’t want to get into specifics - like “the people”, or (occasionally) “the rabble”. Crowdsourcing is the delegation of tasks to an arbitrary collection of individuals. Typically, this is done to harness the quality which they, as a faceless wall of flesh, share in common: being humans instead of computers. The problems where being humans are useful are therefore generally those that are impossible (maybe in one of the two Turing senses) or unfeasible for computers to do. Definitionally, if we care about who people are, it’s not crowdsourcing. It’s just, you know, sourcing. Crowdsourcing strips away the individual, as a feature. Does that mean that it dehumanizes? I guess that depends. In the Mario Salvio New Left sense, I guess it would. But clearly, in social science disciplines where the individual is not so important as the aggregate study of humanity, this is not the case at all. Which is why, I suppose, crowdsourcing platforms like Amazon Mechanical Turk have been widely used for experiments in these fields. Will a crowdsourcing project produce useful information for academic pursuits? I don’t know. But I think that trying to approach this question agnostic to discipline is  a mistake. Interdisciplinary is a laudable goal, but in the end, it’s hard to deny that there are substantial differences not just in approach but in purpose that divide the various humanities. Crowdsourcing is a tool; it makes as much sense to consider its worth to “humanities” as determining the value of a cyclotron to “science”. In my own field of history, as my fellows have heard endlessly, I feel that crowdsourcing is not very useful for research because of the inescapable fact that obtaining any kind of data from  ”the crowd” happens in the present and not in the past. This is the kind of inescapable statement as  ”cyclotrons aren’t useful for biology because its interactions are not on a macro scale.” Clearly, these may not be concerns for disciplines that aren’t history or biology. But that’s the point, I suppose. Can a DH crowdsourcing project really reach beyond the walls of the academy? Do we mean that the crowd is outside of the ivory tower or that the users are? For the former, I think that there are certainly many interesting crowdsourced transcription projects (Bentham, Old Weather), that have found success and wide appeal. One really interesting academic (though not humanities) use of crowdsourcing that’s gotten alot of attention of late is the protein-folding game FoldIt ( http://fold.it/portal/ ). FoldIt, and I guess Ender’s Game, really illuminate the power of gamification in attracting an active and broad audience for such esoteric subjects as viral pathology and intersteller genocide. Cecilia and I actually had a brief discussion returning personal statistics in Prism, like calculating a particular user’s distance from the mean or the ability to show shortest-distance and farthest-distance users. That’s one step toward, if not gamification, then individualized feedback. Of course, this discussion naturally led to talk of using this metric for online dating. “Hey baby, I see that we both highlighted the same sentence in The Raven…”"},{"id":"2012-10-25-prism-proposal-cultural-bundles","title":"Prism Proposal: Cultural Bundles","author":"claire-maiers","date":"2012-10-25 05:08:38 -0400","categories":["Grad Student Research"],"url":"prism-proposal-cultural-bundles","layout":"post","content":"Prelude to a Proposal: As both Cecelia and Gwen have indicated, our conversations from the last week have been marked by a lot of confusion, tension, and doubt about our project and goals.  In an attempt to bring some clarity back, we are each posting a potential project for Prism this week.  As I sat down to write about one such project, I began by reading through project proposals of some of my fellow team members (or   Praxisers ?).  Given the frustration of the past week, I was surprised to discover that there is a notable amount of agreement over our concerns about Prism and possible directions that our project could take.  As I walk you through my proposal, I will touch on what I see as some emerging themes. Apprehensions about a Prism Proposal:  I am extremely interested in how people make meaning from a text.  However, I worry that deploying Prism in its present form (or a form that only allows for the highlighting of certain categories) might not allow for data collection that captures the process of meaning making.  Like Gwen, I share a concern about the interpretation of text without a consideration of the text as a whole.  In order to get at the polysemic nature of a text, we need to know more than how individuals interpret specific words or phrases.  We need to know what message the reader pulls out of the text as a whole.  In order to apply a more structural approach and to understand how the internal relationships within a text generate meaning, we would need to be able to mark more than categories.  We would also need to indicate how various parts of the text relate to each other.  Despite these concerns, I offer a proposal which could be conducted within the current Prism premise by including a few additions to the processes of analysis and visualization.  Though the particular text is not important, I have included a small excerpt from a wedding ceremony as a test case. A Question for Prism: In cultural studies, we often talk about the way cultural references come bundled together.  We can think of this in two ways.  In the first, the e lective affinity between two theoretically distinct worldviews causes the two to become intertwined (for example, scholars often discuss the way in which Protestantism and capitalism have a shared ethic of work, allowing for an easy expansion of capitalism in the protestant world).   However, bundles also exist within the minds of individuals.  People can hold associations in their heads between beliefs and concepts which are not necessarily connected in practice.  For example, when someone reads the word “capitalism,” it may also signal concepts such as “the West,” “democracy,” “freedom,” or “exploitation.”  In another example, the word “love,” may bring concepts such as “marriage,” “commitment,” “fate,” or “betrayal” to mind.  One use for Prism would be to assist scholars in identifying these bundles and figuring out how they frame the interpretive process. Two-Stepping: In order to do this, I think there would need to be a two-step process.  Like other Praxisers, I am concerned about imposing my own categories upon the interpretive process.  So, I suggest that specific Prism projects would benefit from a more open stage.  However, as our transparency activity from last week demonstrated, completely free form interpretation failed to produce anything sensible in the aggregate.  To solve this problem, I would introduce some constraints, asking users to highlight phrases and label them with a single word of their choosing.  Either a researcher or a semantic linguistic program would then work to develop a limited number of categories relevant to the text.  In stage two, the same text would be posted for markup again with the user-generated categories.  This process mimics a well-established interpretive processed used in the social sciences (called Grounded Theory Coding : see Charmaz and Strauss and Glazer ).   However, a tool like Prism allows for an innovative modification: the initial categories to be developed by users (those who actually make meaning), rather than the researcher. Analysis and Visuals: As indicated by several other Praxisers, I think it would be crucial to keep each user’s interpretation autonomous.  Although I think some sort of aggregate interpretation is important, it would also be necessary to be able to sort interpretations according to users.  In order to see how cultural bundles might influence meaning, I would want to be able to answer two key questions.  What is the actual content of the cultural bundles that influence the interpretation of this text?  Can a user’s interpretation of a particular phrase indicate how that user will interpret subsequent phrases?  Answering these questions would require that I can sort the aggregate interpretation to show only certain users.  For example, I might want to see a visualization of all the users’ interpretations who marked a particular phrase with a particular category.   Did all those users who indicate that “one true love” is about fate mark the text in similar ways?  Do they tend to make the same interpretation of the phrase “lawfully wedded?”  Are there patterns that let indicate the specific content of cultural bundles related to love and marriage? Wedding Vows: I, (Bride’s Name), take you, (Groom’s Name),\nto be my  lawfully wedded husband,\nsecure in the knowledge that you will be\nmy constant friend,\nmy faithful partner in life,\nand my one true love."},{"id":"2012-10-26-neatline-omeka-theme-name-winners","title":"Neatline Omeka Theme Name Winners!","author":"jeremy-boggs","date":"2012-10-26 09:28:20 -0400","categories":["Announcements"],"url":"neatline-omeka-theme-name-winners","layout":"post","content":"After extensive deliberations at the Scholars’ Lab, we’re pleased to announce that we have two winners of the Neatline Omeka Theme Name Contest:Amanda Visconti and the theme name “Astrolabe,” and Franky Abbott with the theme name “Neatscape.” We’ll be getting in touch with Amanda and Franky separately, to get those beautiful Neatline tshirts their way. This means we’ll develop and release two Omeka themes. (I’ll have my work cut out for me, but it’s wonderful work.) We’ll make those themes compatible with the upcoming 2.0 version of Omeka, so look for those themes shortly after Omeka 2.0 is released!"},{"id":"2012-10-30-kindle-prism-pdf-prism","title":"Kindle Prism? PDF Prism?","author":"chris-peck","date":"2012-10-30 07:26:31 -0400","categories":["Grad Student Research"],"url":"kindle-prism-pdf-prism","layout":"post","content":"It turns out that Amazon already does some (very basic) analysis of crowdsourced interpretation. They publish several lists of most popular highlights  from Kindle readers. Apparently this group of readers really thinks the first sentence of Pride and Prejudice is significant for one reason or another. But far more find something highlight-worthy about this line from Catching Fire (The Second Book of The Hunger Games): Because sometimes things happen to people and they’re not equipped to deal with them. This is a bit like Prism with tens of thousands if not millions of users (the quote above was highlighted by 17784 users at this time of this post) and no constraints on the meaning of highlights—no “categories.” The parallel was noted at least in passing by last year’s team . Kindle has come up a number of times with the current team so far, and most recently in Katina’s post this week about the social aspects of reading. Perhaps it’s time to revisit the comparison? It would be interesting to try out some of the Prism visualization ideas on such a large pool of annotations. We could also experiment with computational linguistics techniques to make sense out of free text comments attached to highlights by users (it’s been proposed that Prism could use free text input too instead of fixed categories). But from what I can tell there’s no API that would allow us to work with the data.  The closest I’ve found is a tool developed to scrape a single user’s highlights from the web . But one user wouldn’t do us much good. Access to the entire pool of public highlights is what would make this really interesting. Another way we could think about overlap with Prism is in the reader software itself. How would this project change if we considered developing Prism as a plugin for existing software already in use by futuristic readers  such as ourselves? Is there functionality in these apps/platforms that could benefit Prism? Adobe Reader, for instance, has a wider palette of annotation tools (not just highlighting but free text comments, ovals, translucent boxes, arrows, etc.) that could lead to different user experiences and also different visualizations. Proposal: next week let’s try some annotation games with the Praxis team, but using PDFs instead of transparencies. E-mail them to me and I can separate the highlights and overlay them in Illustrator in a few minutes…  An automated tool to merge highlights on PDFs would (I think) be very doable, and could even incorporate some Prismy visualizations."},{"id":"2012-10-30-social-interpretation-repertoire","title":"Social Interpretation Repertoire","author":"chris-peck","date":"2012-10-30 07:24:29 -0400","categories":["Grad Student Research"],"url":"social-interpretation-repertoire","layout":"post","content":"Can approaches from (experimental) music/sound to social interpretation of text shed any light on our thinking about Prism? Let’s get the Praxis Band together and find out: 1. Significant Rhythm Hocket Text: a poem, perhaps a poem that has some repetition, text as well as a recorded reading. Additional Materials: a sound playback device with headphones and a musical instrument for each participant. Instrument can be small percussion/found object/otherwise improvised or homemade. Instructions: Mark the one word (or syllable) you find most significant in the poem. Mark each occurrence of that word. Group listens to recorded reading of poem simultaneously (all press play on your iPods at the same time). Devise a sound of appropriate length on your instrument. Make this sound (always the same sound) each time you hear your word. 2. Inverse Rally Text:  a political speech. Materials:  Sound recording equipment and sound playback device with speaker for each participant. (Or a multichannel playback system with one channel per participant.) Instructions:  each participant rehearses the speech and records their own performance. Recordings are assembled and played back simultaneously on a loop through a multi-channel sound installation. Correlation between sound file data (or sonograms? normalized in some fashion?) is visualized. 3. Laugh-o-meter Text:  A recorded text that’s meant to get laughs. Perhaps a comedy record? Additional Materials:  Sound recording equipment. A situation in which group can be recorded together as well as separately. Playback with headphones for each participant. Instructions: Participants’ vocal reactions are recorded while listening to comedy. Ideally they are recorded with a high quality microphone in a quiet space (like a recording studio) so that subtle subvocal reactions are recorded as well as laughing proper. Group is also recorded while listening to the text together for comparison. Sound recordings are correlated with the text and visualized. And, of course, we compile the separately recorded laugh tracks, mix them, and listen to them without the original comedy."},{"id":"2012-11-01-neatline-release-omnibus-edition","title":"Neatline Release Omnibus Edition","author":"eric-rochester","date":"2012-11-01 07:32:18 -0400","categories":["Announcements"],"url":"neatline-release-omnibus-edition","layout":"post","content":"For the next and final round of the Omeka plugin release parade, I’m pleased to announce minor or patch releases for all Neatline plugins. Neatline is an Omeka plugin that helps you tell stories in time and space from your Omeka collection. For more information, see our original announcement or the Neatline site . For now, these are maintenance releases that patch up bugs and improve performance. Stay tuned over the course of the next couple months, though, for news about some exciting new directions for Neatline. We’re hard at work on a new round of development that’s going to migrate Neatline over to Omeka 2.0, make it a lot easier to use Neatline in a multi-user classroom environment, and make it possible to connect paragraphs, sentences, and words in TEI texts to specific locations in Neatline exhibits. Here’s some information about the bug-fix releases. Neatline Summary The main changes here are bug fixes. We also now store the GIS feature data as [KML](http://en.wikipedia.org/wiki/Kml) instead of [Well-Known Text](http://en.wikipedia.org/wiki/Well-known_text). Version 1.1.1 Commit Details Plug-in Page On Github Issues NeatlineMaps Summary The main changes here are bug fixes and adding internationalization support. Version 1.0.1 Commit Details Plug-in Page On Github Issues NeatlineTime Summary The main changes here are extra configuration options, bug fixes, and internationalization support. Version 1.1.0 Commit Details Plug-in Page On Github Issues NeatlineFeatures Summary The main changes here are bug fixes, [KML](http://en.wikipedia.org/wiki/Kml) data, and internationalization support. Version 1.1.0 Commit Details Plug-in Page On Github Issues For all of these, you can download the plugins from their plugin pages and provide feature suggestions or report problems on their issues pages. For general questions and help, feel free to ask on the Omeka forums ."},{"id":"2012-11-05-outside-the-pipeline-from-anecdote-to-data","title":"Outside the Pipeline: From Anecdote to Data","author":"katina-rogers","date":"2012-11-05 05:24:20 -0500","categories":["Announcements"],"url":"outside-the-pipeline-from-anecdote-to-data","layout":"post","content":"_I gave the following presentation at SCI’s recent meeting on rethinking graduate education . It was the first time I’ve publicly discussed results from the study on career preparation in humanities graduate programs that I’ve written about previously . I was honored to discuss the topic with our extremely knowledgeable group of participants, and the thoughtful questions and comments that the talk generated will inform my thinking as I work toward a more formal report and analysis. I would welcome additional comments and questions. A PDF of the presentation is also available, and has been cross-posted to SCI’s website and my personal site ._ I’m thrilled to have the opportunity to present some of the early findings from the Scholarly Communication Institute’s recent study on perceptions of career preparation in humanities graduate programs. The impetus for this study came from recommendations made at SCI’s ninth summer meeting in 2011, where rethinking graduate education emerged as one of the critical priorities for the current humanities landscape. The study complements the series of meetings SCI is hosting this year and next, of which this meeting is the first. The primary goal of the study is to move from anecdote to data in the conversation about alternative academic careers and career preparation, in hopes of providing a body of data that can help support programs wishing to modify their graduate curricula.   We finished collecting data at the beginning of October, so the analysis is not complete, but already raises some provocative questions. We were very pleased with the number of responses that we received. At the same time, the response rate also highlighted an important discrepancy. The study included two surveys. The primary survey targeted people with advanced humanities degrees who self-identify as working in alternative academic careers. (A somewhat loose definition, to be sure – I’ll discuss our methodology in a moment.) A second, shorter survey targeted employers that oversee one or more employees with advanced humanities degrees. We set an initial goal of 200 responses on the main survey, and 100 on the employer survey. We were blown away by the responses to the main survey, which totaled nearly 800 when we closed it, for almost four times our goal. The employer survey, however, attracted far fewer responses, totaling around 80. This is a significant finding in itself, as it shows a pronounced disconnect between the motivations of job seekers compared to employers. Any recommendations SCI makes must keep this discrepancy in mind. I’d like to jump straight into some of our findings, many of which will not come as a surprise – but again, the goal was to get numbers to back up the general sense that many of us have about these questions. First, a large majority of students enter graduate school expecting to pursue careers as professors – a total of 74%. What is perhaps more interesting is their level of confidence: of that 74%, 80% report feeling fairly certain or completely certain that this was the career they would pursue. These expectations are not at all aligned with the current realities of the academic job market. What this signals to me is that we are failing at bringing informed students into the system. This raises a few questions: –First, whose responsibility is it to help incoming students understand their postgraduate options? This is a bit outside of the scope of this particular meeting, but it becomes our concern when students enter without the knowledge that they need.\n–Second, what is the role of faculty and advisors relative to uninformed students?\n–Third, and more speculative, should post-graduate planning play into admissions in any way? Is there an ethical responsibility here, especially considered in conjunction with increasing student debt? Put differently, should departments be admitting students – particularly if funding is limited or unavailable – if they do not understand their post-graduate options? Deepening the problem, students report receiving little or no preparation for careers outside the professoriate, even though we’re at a moment when the need for information about a variety of careers is most acute. Only 18% reported feeling satisfied or very satisfied with the preparation they received for alternative academic careers. The responses are rooted in perception, so there may be resources available that students are not taking advantage of – but whatever the reason, the bottom line is that students do not feel that they are being adequately prepared. Again, this probably comes as no surprise, but we have significant room for improvement. This raises additional questions: –Are faculty adequately prepared to provide the kinds of advice that students need?\n–Should they be?\n–If it’s not the faculty’s role, then whose role is it? Perhaps alumni or third-party service providers could fill the gap; if so, what are the trade-offs of outsourcing these kinds of preparatory roles to organizations or individuals outside of departments? Along with questions that asked people to choose from pre-selected options, we also included a number of open-ended questions. The survey tapped into what can be an intensely emotional topic, and the wide range of responses we received suggests that people felt comfortable being candid. The variety of emotions expressed in open-ended responses varied from optimistic and happy, to bitter and resentful. Many people report feeling betrayed. Below is another sampling of the kinds of responses we received in the open-ended questions. We’ll be doing more systematic analysis of these responses in the weeks ahead, but for the time being, this will give you an idea of the kinds of reactions and reflections that our respondents provided. We received a wide range of practical suggestions, too, such as offering more one-off workshops; including short credit or non-credit courses; and connecting students with alumni working in varied positions. It’s worth noting that while many were skeptical about even the possibility of creating a meaningful cultural change, they emphasized that for sustainable change to occur at all, it is important that it comes from within existing structures if it is to be perceived as valid. One thing seems clear: the persistent myth that there is nothing but a single academic job market available to graduates is damaging, and extricating graduate education from the expectation of tenure-track employment has the potential to benefit students, institutions, and the health of the humanities more broadly. However, as long as norms are reinforced within departments – by faculty and students both – it will be difficult for any change to be effective. Low tenure-track employment rates are not a new problem, but as the responses on the previous slides show, departments are not succeeding at providing accurate and realistic information to their students. Perceived reputational risk is a significant roadblock to increased transparency regarding post-graduate career paths. A common refrain among the respondents was a call to collect and publicize employment data. However, departments have little incentive to collect this information, and even less motivation to make it public, especially if they think it makes them appear unfavorable relative to peer departments. Are departments the only groups who can collect this kind of information? If they are, how can we change the incentives such that departments find it advantageous to publicize all types of employment among their graduates? Turning now to the employer survey: while we haven’t yet done rigorous analysis on the qualitative responses, I did want to give a taste of the kinds of things employers indicate they would like to see . First, some specific skills come up frequently, including project management, the ability to work with and manage people, collaboration, and written and verbal communication with a variety of audiences. In addition, employers mention a number of broader aptitudes, like a commitment to public engagement, general work experience outside of academia, and an ability to adjust to the culture of different types of workplaces. Many employers placed high value on these employees’ understanding of academic structures and environments, but in order to serve as the valuable cultural translators that they could be, employees with graduate training also need to be sensitive to the ways in which academic environments differ from other workplace cultures. For those that graduate without limited (or no) outside work experience, the gap can be very challenging to bridge. While I’ve only scratched the surface of the study’s findings, I’d like to shift gears in order to highlight what we hope will be the first of many concrete actions to come of this study: the development of the Praxis Network. This is a network of several existing programs that are focusing on innovative models of methodological training, along the lines of the Praxis Program at the University of Virginia. We anticipate that many programs that want to make changes will want to look to existing models for guidance, and by highlighting a handful of differently-inflected programs, we can bring together some patterns and commonalities among them, while also underscoring the unique idiosyncrasies of each – the strong individuals providing leadership; the particularities of an institution; the available infrastructure; the funding model; and so on. In addition to sharing information with the public, we hope that the network will enable increased possibilities for communication and collaboration among the participants of each program. In addition to the Praxis Program, we are currently working with Ethan Watrall at the Cultural Heritage Informatics Initiative ; Matt Gold with the CUNY Digital Humanities Initiative ; Claire Warwick and Melissa Terras at UCL’s Centre for Digital Humanities ; and Bill Pannapacker with the Hope College Mellon Scholars program. The participating programs may fluctuate somewhat, but the intention is to keep the network small – more as a showcase than a comprehensive directory. I’d like to circle back briefly to discuss our methods. First, the study had two main phases : so far, I’ve focused on the second, confidential phase. The first phase was public. In order to scope out the terrain of individuals to include in the survey, the first phase involved creating a public database where people comfortable enough to publicly identify as “alt-ac” practitioners could add their names to form a loose community of peers. We built the database within the framework of the #alt-academy project in order to leverage the energy of existing conversations. “Alt-ac” may not be a perfect moniker, but it has created a space to talk about careers that are not quite what most people envision as academic careers (within the professoriate), but that are not completely outside the academic sphere, either. Many people have found it to be an incredibly useful umbrella term, and have used it to talk about the kinds of intellectually stimulating careers that can be found outside the professoriate. We were pleased with the initial turnout, and found that people were engaging more deeply than expected. It’s worth noting that even though the database has been open for a much longer period of time than the survey was (and it remains open to new entries now), far fewer people participated in the public space than in the confidential survey space. To me, this suggests that there is still a sense of discomfort – and even shame – about having pursued a job outside the traditional pipeline. Once we launched the surveys themselves, we used this public group as an initial body of potential respondents. Because we were working with an unknown population, our subsequent distribution focused on “opt-in” strategies—social media, word of mouth, listervs, and traditional media coverage. While this method has definite weaknesses, we hoped to learn something not only from the content of the responses, but from the number and type of respondents. One reason this study was important because even though the topic is deeply connected with other persistent issues in higher education, there were significant gaps in the data available from previous studies. One of the most closely aligned studies was completed just recently by the Council on Graduate Schools and the Educational Testing Service. The study, Pathways Through Graduate Schools and Into Careers, examines current and former graduate students’ career expectations, their awareness of career opportunities, and the actual career paths they pursued. However, the study focused on a broad range of disciplines, which means that it could not go into much depth on concerns that are particular to the humanities. Further, the data is unpublished, so at least at this point, it is not possible to disaggregate the humanities respondents from the STEM respondents. A 1996 study by Maresi Nerad and Joseph Cerny at the University of Washington, PhDs Ten Years Later, focused on similarly relevant questions. However, the only humanities discipline represented in their data was English. In addition, the study excluded people who left their programs before completing the degree, and because so many people exit their programs if they decide not to pursue an academic career, we wanted to include their perspectives. Of course, the data is now more than 15 years old, and the cohort graduated more than 25 years ago, making it overdue for an updated look. Finally, we will build off of information collected by broad surveys like the Survey of Earned Doctorates, which provides useful baseline demographics but limited depth. The data from the study does have some significant limitations: We had limited data to use as a foundation or control, as I just mentioned; we were working with a population with fuzzy boundaries; and we relied on self-identification and self-reporting. For all of these reasons, the results should be considered more exploratory than definitive, and the respondents cannot be considered a representative sample. We see this survey as an important initial step, and we hope others will build on it. We still have a good deal of work ahead of us, and because we want this survey to be maximally useful to our partners in humanities centers and digital humanities centers, as well as the broader humanities ecosystem, I’d welcome input in a number of areas. In the weeks and months ahead, we’ll be engaging in deeper analysis of the data; we’ll be conducting follow-up interviews by phone and email with a number of participants who indicated their willingness to do so; and we’ll be continuing the conversation in more depth at subsequent SCI meetings. We’ll eventually be making recommendations based on the data analysis. Finally, we’ll be publishing a final report in partnership with the Council on Library and Information Resources, and we’ll also publish the data so that others can work with it. As the discussion on rethinking methodological training continues, I hope you’ll keep this study in mind and share your thoughts with me on how it can best serve its audiences. Thank you! References Council of Graduate Schools. Pathways Through Graduate School and Into Careers. 2012. &lt; http://pathwaysreport.org/ &gt; “Doctorate Recipients from U.S. Universities: 2010.” Based on data from the Survey of \nEarned Doctorates. National Science Foundation, June 2012. &lt; http://www.nsf.gov/statistics/sed/ &gt; Nerad, Maresi, and Joseph Cerny. “PhDs: Ten Years Later.” Center for Innovation and Research in Graduate Education, University of Washington; 1996. &lt; http://depts.washington.edu/cirgeweb/c/research/phd-career-path-surveys/phds-ten-years-later/ &gt; Nerad, Maresi, and Joseph Cerny. “From Rumors to Facts: Career Outcomes of English PhDs.” ADE bulletin 32.7 (1999): 11. &lt; http://www.mla.org/bulletin_124043 &gt; Photo Credits Slide 1: “ Pipeline ” by stigwaage \nSlide 7: “ Pencils ” by Elle *\nSlide 11: “ He Didn’t ‘Mind the Gap’ ” by Scott Smith\nSlide 12: “ Fenced In Part 2 ” by gomattolson"},{"id":"2012-11-06-teaching-git","title":"Teaching Git","author":"eric-rochester","date":"2012-11-06 08:38:22 -0500","categories":["Grad Student Research"],"url":"teaching-git","layout":"post","content":"In Praxis, we just finished covering Git . Everyone seemed to catch on pretty well, so I thought I’d write a bit about my thought process as I was planning the sessions. There were a few principles I tried to keep in mind: ![](http://farm1.staticflickr.com/10/11250506_58dee63095_m.jpg)Photo by [The Rocketeer](http://www.flickr.com/photos/kt/11250506/), [CC BY-NC-ND 2.0](http://creativecommons.org/licenses/by-nc-nd/2.0/) Repeat ourselves. Rather than work on something new, we repeated Jeremy’s lessons on HTML and CSS, except that where he went into detail on HTML and skipped over Git, we went into detail on Git and skipped over HTML. This meant that—although the project we were working on was a web page—everyone copied-and-pasted the exact same content for every page and commit. I posted links to raw HTML pages that everyone could copy into their text editors. We then examined the changes and committed them in Git. This meant that the task we were performing was a little familiar so no one had to think about it. Instead they could focus on Git and the new concepts we were encountering there. Repeat ourselves. I tend to get a bit abstract when I’m explaining things, but I was careful to keep things concrete. I stopped to explain constantly. However, I didn’t expect that the explanations would make sense until everyone had gone through them in practice several times. At one point, I even told everyone that I wasn’t going to ask if they had questions, because I knew that they did. And I wasn’t going to answer them. Enlightenment would come through use. (I should mention that this is why I’m not actually posting the tutorial itself: the guided practice was the tutorial.) Repeat ourselves. In different media. I explained the task, and we did them. I explained the concepts. I also kept two diagrams: one of the working area, staging area, and committed repository and one of the commit log tree. As we talked and I explained what was happening, I kept updating those. These diagrams also made good discussion points to make sure everyone was keeping up. It’s an onion, all the way down. Git sees the world as a series of concentric circles, and my explanation followed that. First we worked only in the working directory. Then we moved a file into staging. Then into the repository itself. For this we needed a limited number of git commands: status, diff, add, commit, and log . Then we introduced branching, so we used checkout, branch, and merge . For the next session, I introduced the remote repositories and push . Keep a Cheatsheet on Hand. I printed out a cheatsheet for everyone and passed them out in the first session. My central theory for all of this was that we learn technical things not by explanation, but by practice. However, we often need to have someone hold our hands and guide the way while we’re learning. This seemed to work pretty well in these sessions. I’d be interested to hear from those in those Praxis sessions. Let me know what worked and what didn’t. And I’d like to hear from everyone else. Are there Git tutorials that you like."},{"id":"2012-11-07-forking-fetching-pushing-pulling","title":"Forking, Fetching, Pushing, Pulling","author":"jeremy-boggs","date":"2012-11-07 09:45:34 -0500","categories":["Grad Student Research","Research and Development"],"url":"forking-fetching-pushing-pulling","layout":"post","content":"Even though a lot of open source projects encourage folks to look at the code and modify it, they don’t just let anyone add anything back to the original project. Projects usually have one or several people with direct commit access, who don’t need permission to do commits. This doesn’t mean you can’t contribute to the project; you’ll just need to get your own copy of the code, make changes there, and then send them back to the original project for review. Contributing to an open source project can be a lot of fun, and Github makes that process pretty easy. Still, there are a few steps to keep in mind to make sure your workflow is sound and your contributions have better chances of getting accepted. To follow along, you’ll need an account on Github, and you’ll need to have git installed on your machine. This post won’t go into detail about using git, but if you’re not familiar with it, check out Eric’s post on  Teaching Git  and the  git resources we’ve collected for the Praxis Program . For this post, I’m going to use Omeka as our example, since I’ve been sending a few pull requests their way as I’ve been developing themes for their upcoming 2.0 release. But the process I describe can easily be applied to many other projects. (In fact, Scholars’ Lab would love for anyone to fork any of our repositories and send stuff back.) Forking [![](http://static.scholarslab.org/wp-content/uploads/2012/11/Screen-shot-2012-11-07-at-2.25.08-PM.png)](http://www.scholarslab.org/dh-developer/forking-fetching-pushing-pulling/attachment/screen-shot-2012-11-07-at-2-25-08-pm/) Dashboard for my fork of Omeka, indicating that it has been forked from omeka/Omeka. Just to be clear, forking is a Github thing, not a git thing. When you fork a repo on Github, you’re essentially making a copy of a repo at a particular point in time to your own account on Github. For this example, I’ll refer to the fork of the Omeka repo  on my own account. These are two separate repositories. I don’t have direct commit access on ‘omeka/Omeka’, but I do on my own, ‘clioweb/Omeka’: Omeka repo - http://github.com/omeka/Omeka My repo - http://github.com/clioweb/Omeka The dashboard for you fork will indicate that it was forked from somewhere. [![](http://static.scholarslab.org/wp-content/uploads/2012/11/Screen-shot-2012-11-07-at-2.10.41-PM.png)](http://www.scholarslab.org/dh-developer/forking-fetching-pushing-pulling/attachment/screen-shot-2012-11-07-at-2-10-41-pm/) The fork button Github’s own help pages include a nice run-down on how to fork a repo, but to summarize: To fork any project on Github, you’ll can just click that big “Fork” button near the top right of the project’s Github page. Github will then ask you where you want to fork it, you choose which answer, and Github will do the rest. It shouldn’t take too long to fork, but that depends on the size of the original repo. Once you have your own fork, you can clone your own repo to your computer to start working on it. In your command line interface of choice, here’s the command to do that on your own machine: git clone git@github.com:clioweb/Omeka.git Now you can make changes and do commits however you like, without any fear of breaking the original repo! As I’ll explain later, though, we won’t just make commits however we like with our forked project. We’ll want to follow some conventions to make our work more productive to send patches back to the original project. Fetching When you fork a project, you get a copy of it at a specific moment in time. There aren’t any built-in ways of automatically getting updates from the original repo after you forked it. The developers on the original repo are (hopefully!) going to keep developing on the project, and you’ll want to get those updates regularly. You’ll need to fetch those updates through git, and to do that you’ll need to add another remote that points back to the original repo. To add this, you’ll need to be in your clone’s directory, then use the git remote command to add the original repo as a remote: git remote add upstream https://github.com/omeka/Omeka.git This creates a remote named upstream to our remotes list, and points to the origina Omeka repository. You could name this something other than “upstream” if you prefer. After adding this upstream remote, you should have not one, but two remotes. When you clone your fork, that creates the first remote, origin, that points to your fork on Github and that you can push back to. The second one, upstream, points to the original repo you forked. This is a pretty good thing to keep in mind. When you clone a repo, you’re making a copy locally, but it also make sure there’s a way to send stuff back to the repo you cloned. Now you can get any updates from the original project, merge them back into your own repository, then push those back to your fork. Making sure you’re on your local master branch, you can do either of these: git fetch upstream\ngit merge upstream/master -or- git pull upstream master The latter is just a shortcut for the former. Now that you have your local master branch updated with changes from the original repo, we can start working our own commits! Pushing In git,  “branches are cheap and easy.”  So the best thing to do when adding a new feature or fixing a bug in a git repo is to do that work on a separate “topic” branch. That is, a branch whose changes encompass, roughly, a single idea or topic: adds a single feature, or fixes a specific bug. This allows you to work on different features/bugs simultaneously and separately. You can keep updating a specific feature after you’ve submitted a pull request for it, which is handy if in the course of discussing the pull request other developers suggest changes to it. While you can add multiple features to a single topic branch, you might run into a situation where you don’t like that feature anymore or, more commonly, the developers on the original repo only want one of the features and not both. Similarly for bug fixes. The granularity of this depends, of course, on the nature of the feature or bug you’re addressing. To make a branch, you’d use: git checkout -b my-topic-branch Here you would replace ‘my-topic-branch’ with whatever you’d want the name of your topic branch to be. You’ll want to name your topic branch something brief but descriptive, mainly to help you remember what that branch is about. For example, I recently submitted a pull request that added an options parameter to Omeka’s JavaScript queueing functions. I named that topic branch queue-js-options . Once you make your topic branch, you can now start editing code. Any commits you make will only be on this branch, too, so you can always check out the master branch while working on a feature to do some other work if you want. As an aside, commit messages should provide enough description for someone to understand the nature of the changes. I’ve yet to run into a commit message that was too descriptive, but I’ve seen plenty of commit messages that weren’t descriptive enough, so I try to err on the side of too much information. When I do a commit in git, I’ll usually leave off the message flag in the command so I can write the message in a separate window. The first line of the commit message should be a short summary. The rest of the commit message can be as descriptive as necessary, and if you use Markdown syntax in your message, Github will format these message nicely. For example, here’s my commit message for the queue-js-options feature: [![](http://static.scholarslab.org/wp-content/uploads/2012/11/Screen-shot-2012-11-07-at-2.41.23-PM.png)](https://github.com/clioweb/Omeka/commit/2db757ac00a54be26c72a84c008692db3e5a595e) Screenshot of my queue-js-options commit message in Github Once you have all your commits for a feature or bug fix ready, now you can push your topic branch up to your fork on Github to share it with others: git push -u origin my-topic-branch The -u flag will create a new branch to your fork on Github called ‘my-topic-branch’. After you do this, you continue to make changes on that branch, you can push them up to your Github fork without the -u flag. Pulling Now that you’ve got your topic branch on your Github fork, its time to send a pull request back to the original project. Github can automatically tell if you’ve added a topic branch to your forked repo, and will display a “Pull Request” button on your fork’s dashboard. Click that button, and Github will display a form confirming the pull request. It’ll fill out the details using your commit messages (another good reason to be descriptive in your commit messages!), but allows you to modify that text. Modifying the pull request details won’t change your commit messages. Once you submit, Github will create an issue on the original repository, with tabs to see the individual commits and the changes in the files: [![](http://static.scholarslab.org/wp-content/uploads/2012/11/Screen-shot-2012-11-07-at-2.38.43-PM.png)](https://github.com/omeka/Omeka/pull/363) Page for my pull request to Omeka Developers on the original repository can comment on the request, see the code attached to it, and approve or reject the request. (Check out the current pull requests on Omeka to see this in action.) If they do request changes, you can just update your topic branch locally, make your commits, and push them back up (this time, omitting the -u flag, since that topic branch now exists on Github.) In the meantime, you can go back to your local copy, create a new topic branch, and work on something else! Summary Here’s a quick rundown to get set up: Fork the original repo Clone your fork to your computer Add the upstream remote pointing to the original repo Here’s a quick rundown to send a feature or bugfix back to the original project: Checkout the master branch. Pull from the upstream remote to merge updates. Create a topic branch for your feature or bug fix. Make your commits, with good commit messages. Push your topic branch back to your fork on Github Send a pull request back to the original repo. Converse with developers, make updates to your topic branch if necessary, and keep pushing back to your fork. Repeat for a new feature or bug fix. Hope this helps!"},{"id":"2012-11-14-poster-abstract-and-code-camp","title":"Poster Abstract and Code Camp","author":"brandon-walsh","date":"2012-11-14 10:09:01 -0500","categories":["Grad Student Research"],"url":"poster-abstract-and-code-camp","layout":"post","content":"It’s been a tad quiet on the blog front over the past couple weeks. Here is what the Praxis squad has been up to. It’s been a great week for collaborative writing. We put together a poster abstract for DH 2013 that crystallized a lot of our thoughts on Prism and crowdsourcing thus far. Perhaps we will see some of our loyal readers there? The poster abstract that we put together discusses crowdsourcing as a continuum from more mechanical interaction to forms that maintain more of a sense of the autonomous individual. We concluded by conceiving of a few different types of interfaces for Prism and the ways in which they might affect the nature of the crowdsourcing process. Much of the abstract was put together with multiple people writing different parts of the document at the same time, which I thought was an interesting enactment of the sorts of questions and issues that we have been working through.  It was great to see each person’s piece combined into the whole.  Special shout-out to Gwen for taking the lead on the abstract coordination/writing and to Jeremy for his expert wisdom. Part of the reason for radio silence has been a distinct shift in our day to day work. After finishing the poster abstract, Wayne and company started hitting us hard and fast with code boot camp. So far we’ve dipped into command line, git, and ruby on rails. Things are going pretty well so far - it feels a little like taking a giant cyber-multivitamin as we prepare to put all of our plans for Prism into action in the next few months. When asked to explain git and version control, I like to do so by comparing it to time travel without the risk of creating a universe collapsing paradox . For next week we are moving through the first ten exercises of Learn Ruby the Hard Way . The idea is that you type out letter for letter a series of code, trouble shoot if it doesn’t run, and then reflect back on what you wrote. This seems a bit unusual - I would expect a more traditional format to teach you the concept first and then enact the code yourself. The LRTHW model works great for my personality type: I always have a hard time with training of any sort. Invariably in any new employee training, my attention immediately starts to wander as soon as my superior starts to explain something to me, and I always wind up learning by doing. So this reversed pedagogical model works well for me: I do first and then think back on the process. I can imagine it being very frustrating, though, if you are the kind of student that dutifully pays attention to the lesson on view. It’s an interesting model, and I wonder if it could be adapted for the humanities. What might that entail? Maybe I could have my students type out an essay paragraph word for word and then reflect on it after the fact. Would the model still work, or is there something about humanities thinking that requires that the thinking/learning precede the doing?"},{"id":"2012-11-16-ruby-cat-poem","title":"Ruby Cat Poem","author":"chris-peck","date":"2012-11-16 04:12:59 -0500","categories":["Grad Student Research"],"url":"ruby-cat-poem","layout":"post","content":"AA                  line\ntabbed mmmmmmmmmmmmm\nnononono                         mmmmmmmmmmmmm m doof\ncat\nCat a in Catnip  Grass I     llllllllllllllllllll split a               tsil do     II         Fishies                     I IIIIIIII —Learn Ruby The Hard Way, Ex. 10, modified to create cat poem ( source )"},{"id":"2012-11-19-trial-by-fire","title":"Trial by Fire","author":"claire-maiers","date":"2012-11-19 09:48:12 -0500","categories":["Grad Student Research"],"url":"trial-by-fire","layout":"post","content":"After several weeks of dreaming big and working through some conceptual difficulties, we Praxis fellows have returned to the concrete task of learning to code and program.   Currently, we are wandering our way through the world of Ruby on Rails .   It has been a while since I have had the opportunity to learn something completely new, and I’m finding the task both disorienting and rewarding.  Given that some of you out there might be experimenting with your own training programs, I thought I would take some time to discuss the approach to teaching and learning that we have been using. Generally, I am not a fan of trial-by-fire method of teaching.  I was the kind of kid who learned to swim by practicing strokes in the grass, not by jumping head first in the water, figuring it would turn out all right.  I like to proceed methodically, slowly building up a familiarity with vocabulary, concepts, and skills.  Theory before application. Given my preferred learning style, I was perfectly content when Wayne began the first lesson on Ruby, providing basic concepts, definitions, and reasons for choosing Ruby over other options.  But 20 minutes later, I was feeling pretty lost.   It wasn’t so much that the concepts or vocabulary were difficult—it was that I didn’t know what language we were speaking.  I had no grammar to help provide context, and I had no real world experience upon which I could map these new concepts.  So, even though, for example, definitions of the difference between a “language” and a “framework” were provided, those definitions were largely meaningless. But luckily, the wise staff here at the scholars lab (read: Wayne) knew that we needed to pair our theory with practice in order to really make sense of Ruby.  We were sent to Learn Ruby the Hard Way to complete a series of exercises.   This ebook is arranged in the trial-by-fire manner.   Rather than explaining Ruby and following it up with practice, the user is instructed to copy code and then run the program to see what happens.  No explanations of commands are provided, just questions for the user to answer.    To my surprise, this was an incredibly effective way to learn.  I had to examine the commands and the resulting printed screen to figure out how the commands worked for myself.  At the same time, I was building muscle memory for frequently used commands and processes.   My hope is that learning to program in this way will help me to learn at a deeper level—really retaining the skill rather that “cramming” the information only to forget it later.  However, this learning method has its downfalls: there were several places where the exercises themselves do not clarify extensively enough how a command works or how it is distinct from another command.  Luckily, we have the Scholars Lab staff to fill in these holes, and after a subsequent session with them, I am feeling pretty comfortable (for now). So, what have I learned about learning:  I am finding a new appreciation for the trail-by-fire method.  You learn differently and have to think deeper when the answers are not simply given to you.  It is no longer about memorization or referencing a guide, but about experiential knowledge.   At the same time, I find there is value talking theoretically and having face-to-face conversation with experts and teachers, especially when paired hands-on experience."},{"id":"2012-11-20-onward-and-upward","title":"Onward and Upward","author":"cecilia-márquez","date":"2012-11-20 10:58:21 -0500","categories":["Grad Student Research"],"url":"onward-and-upward","layout":"post","content":"This week we wrapped up git and got started on Ruby. I’m starting to build the muscle memory with git. Although they are mostly simple tasks I can make a change in an html document, stage those changes, and then commit them to a repository in github . This feels like major progress given how I was feeling about git the first weeks. I’m also building up some basic skills with Terminal. I can quickly figure out where in my computer I am, move through directories, build directories and make new documents. Also, the first half of “ Ruby the Hard Way ” went pretty smoothly. I can do basic arithmetic and am reconnecting with my former love of math. Overall I am feeling much more confident with my computer which is a nice change. For example, this past week my computer was headed toward a major meltdown. I spent a lot of time on a separate computer figuring out what could be the problem, accessed my computer infrastructure, identified the problem and then–in a shocking turn of events–FIXED IT! Certainly this is not something we learned in Praxis but one of my major goals coming into this program was feeling more comfortable using digital tools and I already feel much more confident and open to making mistakes and trying again!"},{"id":"2012-11-27-learning-ruby-again","title":"Learning Ruby (again)","author":"shane-lin","date":"2012-11-27 18:22:07 -0500","categories":["Grad Student Research"],"url":"learning-ruby-again","layout":"post","content":"Things are going a bit better than the last time I tried to pick up Ruby. Part of it is just the fact that I’m not learning it this time for work, but kind-of on the side. But I think a big part of it is just getting my feet wet with just the Ruby language rather than Rails. There’s also the fact that two years ago, my main language was Java, but now I’ve switched back to  doing more Python hacking and so have been more comfortable right off the bat with things like dynamic typing and not worrying about things under the hood (to the extent that a Java programmer worries about things under the hood). On the other hand, some things are already starting to annoy.  Things don’t seem to be as bad as with Perl yet, but the ugly head of TMTOWTDI is already evident. It’s an irreconcilable difference for a dyed-in-the-wool “there’s one way to do it” guy like me.  I long for the velvet fascism of strict indentation. Not to mention that the sheer impudence of Ruby’s syntactic sugar is more than a bit galling; I mean, my god, have some decency. On the other other hand, Ruby does a better job of breaking some of my Java conventions. My Python designs tend to rely heavily on protocols and more hierarchical inheritance; with Ruby, I’ve been having some fun with mixins. I’m also looking forward to working with Ruby’s reportedly more robust closure support. On the balance, I feel like it’s a language that I can actually get to enjoy. But this tiny sapling of optimism is overwhelmed by the staggering volcanic firestorm of apprehension at the rough beast of Rails, slouching inexorably toward us."},{"id":"2012-11-27-your-digital-life-in-140-characters","title":"Your Digital Life in 140 Characters","author":"brandon-walsh","date":"2012-11-27 18:24:30 -0500","categories":["Grad Student Research"],"url":"your-digital-life-in-140-characters","layout":"post","content":"I just recently hopped on Twitter for the first time as part of a conference through UVA’s Institute for the Humanities and Global Cultures . It was a bit rough for the first five or six tweets as I worked out the kinks. Here are some thoughts that came out of the experience. I am hooked on tweeting conference presentations. I was a bit worried that tweeting would be too distracting during a talk, but I actually found that it made me a better listener. I was forced to listen hard to find discernible takeaways, which helped me to digest the information being thrown at me. Also, it gave me a way to occupy my wandering attention in a productive way. Tweeting also seems like a good way to extend the life of the conference after the proceedings are complete. On that same note, the SLab crew has had a few conversations about how Twitter interacts with intellectual property. I felt obligated to assign many tweets I made to a particular person, assuming that I was giving credit where it was due. If our labor is intellectual, I wanted to do my best to make sure that I didn’t obscure the architect of a particular idea. I wonder, though, if conference attendees would rather not be held permanently accountable for comments they make during a question and answer session. Is Twitter in this form a form of publishing? Do you need someone’s permission to post their thoughts in the Twitterverse? I also find Twitter interesting for the amount of access it gives you to people and groups that you might not otherwise be able reach. Tweeting can put you in touch with so many people that would be otherwise inaccessible. It’s really quite incredible - I’ve heard several stories now of people who got jobs or made industry contacts that were completely unexpected simply by shooting some messages out into the ether. I don’t have an personal experience like this just yet, but I will keep you posted. A question: what is tweeting etiquette - more generally but also in the context of tweeting conference presentations? How many tweets are appropriate? I was averaging five or six tweets per talk, but I noticed that other people tweeting the conference were doing far fewer. I could not help but feel that my loyal followers (all four of you) were being bombarded by my messages about global cultures. It remains to be seen how much I will tweet in my everyday life, but I think I will use it for conferences from now on. Follow me - @walshbr"},{"id":"2012-11-28-mountain-lion-and-rvm","title":"Mountain Lion and RVM","author":"wayne-graham","date":"2012-11-28 06:32:30 -0500","categories":["Research and Development"],"url":"mountain-lion-and-rvm","layout":"post","content":"I recently upgraded my computer to use the latest version of OS X (Mountain Lion) and I ran in to a problem with the rvm package manager. Basically I would get to the point of actually compiling the version of Ruby, and get this nasty error: Error running 'env CFLAGS=-I/Users/wsg4w/.rvm/usr/include LDFLAGS=-L/Users/wsg4w/.rvm/usr/lib ./configure --enable-shared --disable-install-doc --prefix=/Users/wsg4w/.rvm/rubies/ruby-1.9.3-p327', please read /Users/wsg4w/.rvm/log/ruby-1.9.3-p327/configure.log After reading through the log file, I noticed that the make utility wasn’t installed on the system. What? I installed the full version of XCode, as well as the apple-gcc42 packages installed. Turns out Apple doesn’t include these essential command-line utilities in the default installation of XCode anymore and you have to install these from within in XCode. Basically you need to get in to the XCode Preferences and click on the Downloads tab and install the Command Line Tools . After this, I had to reinstall all rubies I had installed: rvm reinstall all --force After waiting a while for all the ruby versions I had on my computer to recompile, everything started working again and I could successfully compile code again!"},{"id":"2012-11-28-populating-mysql-tables-with-node-js","title":"Populating MySQL tables with Node.js","author":"david-mcclure","date":"2012-11-28 09:31:51 -0500","categories":["Research and Development"],"url":"populating-mysql-tables-with-node-js","layout":"post","content":"[Cross-posted from dclure.org ] Over the course of the last week or so, I’ve been working on implementing “as-needed” spatial geometry loading for Neatline - the map queries for new data in real-time as the user pans and zooms on the map, just loading the geometries that fall inside the bounding box of the current viewport. Using the spatial analysis functions baked into MySQL, this makes it possible to build out exhibits with many hundreds of thousands of spatial records, provided that the content is organized (in terms of spatial distribution and min/max zoom thresholds) so that no more than a couple hundred records of visible at any given point. I needed a way to build out a really big exhibit to run the new code through its paces. Originally, mostly just because I was lazy to write the SQL, I had been generating testing data using a temporary development controller that called out to a helper functions that actually created the exhibits / records. These actions were invoked by Rake tasks that just spawned off GET requests to the controller actions. This works fine for relatively small data sets, but once I started trying to insert more than about 10,000 rows the loop ran for so long that the request timed out and the process died (at least, I think this was the problem). And, either way, this is just generally slow (all in PHP) and clunky (litters up the codebase). Instead, I decided to write a couple of little standalone scripts that would programmatically build out a big SQL insert and run it directly on the database. In the past, I might have done this with Python, but I remembered how difficult it was to get the Python &lt;-&gt; MySQL bindings working in the past and decided to try in with Node. This turns out to be easy and performant. The basic gist, using the standard node-mysql package: It’s inefficient to run a separate INSERT query for each row; better to clump them together into a single, massive query, which can be accomplished by stacking up a bunch of parentheticals after the VALUES : This can build out a 500,000-record exhibit in about 10 seconds: Full code here ."},{"id":"2012-11-29-fizz-buzz","title":"Fizz Buzz","author":"gwen-nally","date":"2012-11-29 16:53:47 -0500","categories":["Grad Student Research"],"url":"fizz-buzz","layout":"post","content":"Wayne asked us newbies to solve the Fizz Buzz problem for homework.  Here is my solution. I got stuck at the start, where I couldn’t remember how to make anything print–let alone all the numbers between 1 and 100. I also got stuck on how and where to introduce the iterative step. I find the way computers “count” to be quite confusing. Then, I wrote a program that ran indefinitely because I had the iterative step in the wrong spot. It hadn’t occurred to me that computers read programs in order. This exercise took me back to learning how to solve word problems in grade school.  The most difficult part is to figure out what is being asked and which tools are appropriate."},{"id":"2012-11-29-holy-crap","title":"Holy crap","author":"shane-lin","date":"2012-11-29 16:52:17 -0500","categories":["Grad Student Research"],"url":"holy-crap","layout":"post","content":"You can dynamically add methods to built-in classes in Ruby without re-instantiating them?"},{"id":"2012-11-30-literals","title":"Literals","author":"shane-lin","date":"2012-11-30 06:22:27 -0500","categories":["Grad Student Research"],"url":"literals","layout":"post","content":"It turns out that Fixnums are special and are represented as “immediate values”, which from what I understand is just Ruby for “literals”. This kind of lets the air out of the whole “everything in Ruby is an Object” when they are really no such thing."},{"id":"2012-12-04-make-it-work","title":"Make it Work!","author":"cecilia-márquez","date":"2012-12-04 06:46:40 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"make-it-work","layout":"post","content":"Tomorrow, in an ongoing effort to teach us how to use Ruby, we are embarking on the adventure that is “Pair Programming.”  We are going to create a “Jotto” game, courtesy of Eric by breaking it into discrete classes and having each pair work on a different class.  The goal is to have one person as the “driver” (typing all the code) and one person as the “navigator” (reading from the ruby documentation).  We can now move from the theoretical training that we’ve mostly been doing up until now and get some “on the job” training. I’m excited to work collaboratively with some of my Praxis cohort.  Given how solitary and isolating graduate school work can be, I’m looking forward to a project that is based on working collectively.  The natural parallel to draw for tomorrow (thank you David McClure) is Project Runway.  In fact once David said it it seemed impossible to not see the parallels.   A group of good looking young people struggling to break into a career that is almost impossible to get into, time deadlines will likely drive us to get into some comedic fights and then reconciliations, and we have a group of calm white guys walking around the office telling us to “make it work!”  Fortunately the competitive edge that often drives Project Runway contestants to tears is not looming over our heads.  This also means we don’t have to work behind each others backs and form alliances wherein we only share code with certain members of the group.  No, I think instead this will be the best of Project Runway, the collaborative spirit that generates new ideas and capitalizes on the productivity of working across lines of difference (either disciplinary or in terms of expertise).  If all of that falls through, maybe we’ll at least get a good work out in the process… **\n**"},{"id":"2012-12-04-speaker-series-dr-guoping-huang","title":"Speaker Series: Dr. Guoping Huang","author":"ronda-grizzle","date":"2012-12-04 05:04:52 -0500","categories":["Podcasts"],"url":"speaker-series-dr-guoping-huang","layout":"post","content":"Speaker Series: Dr. Guoping Huang Geographic Information System (GIS) &amp; the Humanities On September 25, 2012, Dr. Guoping Huang, Assistant Professor, Department of Urban and Environmental Planning at UVa spoke in the Scholars’ Lab on GIS and the Humanities, discussing several digital humanities projects, including the Digital Atlas of Roman and Medieval Civilizations (DARMC) project, the WorldMap project, and the Chinese Historical GIS project, to showcase how GIS can help humanists explore new grounds for interdisciplinary research. As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.19050142938/enclosure.mp3”]"},{"id":"2012-12-06-music-theory-in-ruby","title":"Music Theory in Ruby","author":"chris-peck","date":"2012-12-06 06:42:37 -0500","categories":["Grad Student Research"],"url":"music-theory-in-ruby","layout":"post","content":"Learning (and playing) with Ruby these past few weeks I’ve been looking for ways to solve modest, day-to-day Humanities problems. Digital Humanities, after all, doesn’t just have to be about big questions like crowdsourcing, right? Here’s something that’s been making me very happy this week: automated generation of randomized music theory drills. I’m currently teaching an introductory theory course, and especially at the end of the semester the students are hungering for extra drills to prepare for their final exam. Wouldn’t it be great if I had a magic machine that would just pump out an endless number of these worksheets? (click for the answer key) Well…now I do! This worksheet was generated with a Ruby script (here’s the source ) writing music notation in LilyPond . I also noticed that working out an algorithm for interval identification to generate the key helped me figure out how to better explain this task to the students. Next up? An entire generated exam! (I’ll be a hero to future generations of instructors to this course.) I have some ideas about applications to more serious scholarly and artistic endeavors as well, but more on that later…"},{"id":"2012-12-10-digital-humanities-speaker-series-dr-w-gardner-campbell","title":"Digital Humanities Speaker Series: Dr. W. Gardner Campbell","author":"ronda-grizzle","date":"2012-12-10 08:05:22 -0500","categories":["Podcasts"],"url":"digital-humanities-speaker-series-dr-w-gardner-campbell","layout":"post","content":"Digital Humanities Speaker Series: Dr. W. Gardner Campbell HD.EDU: Learning in a Digital Age On October 23, 2012, Dr. W. Gardner Campbell, Associate Professor of English and  Director of the Professional Development and Innovative Initiatives at Virginia Tech spoke on the future of learning and the changes wrought by technology in academica. The Digital Humanities Speaker Series is co-sponsored by SHANTI, IATH, and the Scholars’ Lab. As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.20125708191/enclosure.mp3”]"},{"id":"2012-12-10-fizzing-buzzing","title":"Fizzing, Buzzing","author":"brandon-walsh","date":"2012-12-10 05:56:33 -0500","categories":["Grad Student Research"],"url":"fizzing-buzzing","layout":"post","content":"Since Gwen  just posted her solution to the Fizz Buzz homework assignment,  I thought that I would throw mine up here as well. Here is my solution . It’s pretty similar to Gwen’s take on the problem. I just switched the order of a couple things and used a couple shortcuts. I also apparently have a penchant for extra parentheses to help organize things. I do feel like I perhaps missed the point of the exercise, given that it was tacked onto Wayne’s discussion of object-oriented programming and classes. Perhaps we were meant to solve this problem using that approach? In any case, there is no one way to make breakfast; I’ll take my approach for now, until I have a better grasp of object-oriented programming. I’ll use what I know."},{"id":"2012-12-10-grading-in-ruby","title":"Grading in Ruby","author":"brandon-walsh","date":"2012-12-10 05:57:34 -0500","categories":["Grad Student Research"],"url":"grading-in-ruby","layout":"post","content":"Chris recently posted his very exciting experiment that uses Ruby to create music theory worksheets for his students. Inspired by this, I have been playing around on Ruby with much more modest aims: I wanted to use Ruby to do my grading for me. I always do my grading with an Excel spreadsheet and a series of formulas. I am not particularly skilled at that interface, though, and it usually takes me a while to get it to work right. I thought I might as well try to do the same thing in Ruby. Admittedly, this idea also came from Chris: he has been claiming that such a thing was a very real possibility for weeks now. Here is the source code . It’s a fairly rudimentary interface at this point, but it does seem to work. It allows you to compute a student’s final grade in a course where assignment types are weighted differently. I hope to keep tinkering with this over time. Some things that I would want to include in later versions: 1) Clean up the code. I don’t feel very comfortable with classes at the moment, so I bend over backwards to accommodate that lack. I think the first step to making this more dynamic and workable will be to class it up. Right now the code only runs straight down, start to finish. Organizing the code in a more dynamic way will allow the program to be more workable to a variety of circumstances, which leads into my second point. 2) Make the interface more flexible for a variety of types of course arrangements . Right now, it’s really only set up to work for a certain type of class with a few types of assignments. I have it set up so that you input the number of different types of assignments you have, ex: paper one, paper two, midterm, and final. But there isn’t really a way to account for multiple smaller assignments that make up a larger chunk of grading, like individual quizzes as part of a larger whole. Right now you would have to average the quiz grades ahead of time and then enter the composite grade in as one lump “quizzes” category. That also means that, if you want to drop the lowest quiz grade, you have to do that on your own. Also, the program completes its run after completing the grade for one student. My priority right now was making it work for my purposes - a class with less than twenty students and a limited number of assignments. It seems to work for that. Take a shot at breaking it."},{"id":"2012-12-12-scholarly-communication-brown-bag-series-speaker-brian-nosek","title":"Scholarly Communication Brown Bag Series Speaker: Brian Nosek","author":"ronda-grizzle","date":"2012-12-12 05:08:34 -0500","categories":["Podcasts"],"url":"scholarly-communication-brown-bag-series-speaker-brian-nosek","layout":"post","content":"Scholarly Communication Brown Bag Series Speaker: Dr. Brian Nosek Scientific Utopia: A Radical View On November 29, 2012, Dr. Brian Nosek, Associate Professor in the Department of Psychology at the University of Virginia, spoke at the Library’s scholarly communication brown bag lunch on the changing landscape and future of scholarly communication in the sciences. Summary:\nHow can existing scientific communication practices be improved to increase efficiency in the accumulation of knowledge, and improve the alignment between daily practices and the values of the academic community? Brian will outline some present and possible futures of scientific communication– from relatively mundane to borderline nutball—and describe his vision for a new utopia for scholarly communication. Articles by the Speaker:\nNosek, B. A., &amp; Bar-Anan, Y. (2012). Scientific Utopia: I. Opening Scientific Communication. Psychological Inquiry, 23(3), 217–243. Nosek, B. A., Spies, J. R., &amp; Motyl, M. (2012). Scientific Utopia II. Restructuring Incentives and Practices to Promote Truth Over Publishability. Perspectives on Psychological Science, 7(6), 615–631. As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.20590199748/enclosure.mp3”]"},{"id":"2012-12-12-scholars-lab-speaker-series-mills-kelly","title":"Scholars' Lab Speaker Series: Mills Kelly","author":"ronda-grizzle","date":"2012-12-12 05:09:16 -0500","categories":["Podcasts"],"url":"scholars-lab-speaker-series-mills-kelly","layout":"post","content":"Scholars’ Lab Speaker Series: Dr. Mills Kelly Pedagogy of Disruption: What Happens When You Teach Students to Lie? On October 25, 2012, Dr. Mills Kelly, Director of the Global Affairs Program in the Department of History and Art History at George Mason University, spoke on how student learning might be transformed if traditional modes of instruction were turned on their head. In his talk, Kelly, the professor of GMU’s “Lying About the Past” course, explores the up and downsides of disruptive approaches to teaching and learning. As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.20577302454/enclosure.mp3”]"},{"id":"2012-12-13-ruby-grading-2-0","title":"Ruby Grading 2.0","author":"brandon-walsh","date":"2012-12-13 06:33:40 -0500","categories":["Grad Student Research"],"url":"ruby-grading-2-0","layout":"post","content":"I just recently posted my experiment with making a Ruby program that you can use for doing your own grading . I have since made several improvements upon the first draft of the code, so I present to you Ruby Grading 2.0. The changes: 1) I converted the code to incorporate classes, which was a huge learning process for me that involved  lots of troubleshooting. I think I am starting to get the hang of classes and understand their importance. The conversion process was nightmarish, but they allow you to be a lot more flexible when trying to include new tasks. Plus, I learned a lot from the ordeal. 2) I made it so that you can now input your entire roster into the grading program and calculate all your final grades there. You only input the syllabus parameters once, at the beginning. The program saves these and then applies them to all the students on your list. At the end it will spit out a nice little list of students’ names and their final percentage grades. 3) I also tweaked several small things as I went to account for differing input situations. Before the program couldn’t account for certain types of input. For example, if you typed in “three” and not “3” when prompted for the number of assignments, the program would break. I still am not quite sure how best to deal with those sorts of situations; I couldn’t really find a satisfactory test for whether or not input was convertible to an integer or not, so I think I invented one that seems to work (see lines 136-141 of the code). My test: [gist id=4252685] I cast the input string to an integer. If the integer is greater than one the code works. If not, something has gone wrong (who assigns fewer than one assignments), and it will ask for the input as an integer. It works, but it feels like a particularly dirty way of doing things. Next up, three things if I should continue: 1) Make it so that quizzes can be dynamically inputted into the program interface, with the possibility to drop the lowest one if necessary. 2) Consider making a version of the program that reads out of other files so that you don’t have to do all your grading in one session. After all, we keep our gradebooks as we go, so this information is all typed somewhere else. It seems like the biggest time save would be to have it open a file, examine its contents, and spit out a calculated final grade for you. 3) Maybe include a method that will convert the percentages to letter grades."},{"id":"2012-12-14-7067","title":"Now at ProfHacker: “Turning Up the Volume on Graduate Education Reform”","author":"katina-rogers","date":"2012-12-14 03:24:33 -0500","categories":["Announcements","Grad Student Research"],"url":"7067","layout":"post","content":"The last couple of weeks have seen a great deal of news and conversation about graduate education reform. I have a lot to say about it (unsurprisingly!); you can find my take on it over at ProfHacker . The piece includes some discussion of SCI’s latest work, the Praxis Program, and the budding Praxis Network, so I hope you’ll take a look! I’m also happy to note that I’ll be talking more about all of this at the upcoming MLA Convention in Boston—if you’re interested the topic, consider attending this roundtable on Rebooting Graduate Training . There will be ample time for discussion at the session, so come ready with questions and ideas."},{"id":"2012-12-20-scholars-lab-speaker-series-jeremy-dibbell","title":"Scholars' Lab Speaker Series: Jeremy Dibbell","author":"ronda-grizzle","date":"2012-12-20 09:13:20 -0500","categories":["Podcasts"],"url":"scholars-lab-speaker-series-jeremy-dibbell","layout":"post","content":"Scholars’ Lab Speaker Series: Jeremy Dibbell The Libraries of Early America Project: Bringing Historical Libraries to Life with LibraryThing On December 4, 2012, Jeremy Dibbell, Rare Books and Social Media Librarian for LibraryThing, discussed the Libraries of Early America Project . Summary:\nThe Libraries of Early America project is an effort to digitize and make widely available the library collections of American readers from the early colonial period through 1825. Using the online book-cataloging site LibraryThing.com, scholars and volunteers from institutions around the country have begun the process of creating an extensive online database of early American libraries. Current subjects include Thomas Jefferson, John Adams, Benjamin Franklin, Lady Jean Skipwith, James and Mary Murray, and other early American readers (some well-known, others obscure). The Libraries of Early America collections through LibraryThing allow users to quickly and easily make comparisons between libraries (what books did John Adams and Benjamin Franklin have in common, for example, or what books were most commonly shared among all the Signers of the Declaration of Independence?), and to search collections which may not exist today in physical form or which are spread across multiple institutions and private collections. Further, LibraryThing’s capabilities allow significant data about each book to be added to the record where known: transcriptions of marginalia, information about acquisition of the title, the binding, correspondence about a given book, or even a ink to a digital scan of the volumes (as with the John Adams collection at the Boston Public Library). So far, data on more than 1,400 early American libraries has been added, with more information constantly being collected and included. I’ll discuss the origins of the project, sources and methods, and future plans and enhancements, focusing on some of the new things we’ll be doing in the future, including a fascinating look at libraries confiscated from Massachusetts loyalists during the American Revolution. Speaker Bio:\nJeremy Dibbell is the Librarian for Rare Books and Social Media at LibraryThing. He received his B.A. from Union College and M.A./M.L.S. degrees in History and Library Science from Simmons College. In the summers, he can generally be found at the University of Virginia’s Rare Book School, assisting with the school’s weeklong courses. Along with the Libraries of Early America project, Jeremy’s at work on a history of books and printing in Bermuda, writes regular columns for “Fine Books &amp; Collections” magazine, and blogs about books and reading at PhiloBiblos . He can be found on Twitter at @JBD1 . As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.20622506184/enclosure.mp3”]"},{"id":"2013-01-06-rebooting-graduate-training-an-mla-roundtable","title":"Rebooting Graduate Training: An MLA Roundtable","author":"katina-rogers","date":"2013-01-06 12:31:56 -0500","categories":["Announcements","Grad Student Research"],"url":"rebooting-graduate-training-an-mla-roundtable","layout":"post","content":"Cross-posted at my personal website. I gave the following talk at the 2013 MLA Convention in Boston as part of an excellent roundtable organized by Paul Fyfe, who has also collected a number of resources in a Zotero library . The wide-ranging presentations sparked many thoughtful questions that I hope will lead to continued discussion about the ways that graduate training could be modified for the good of students, the discipline, and the public. Some of the slides are taken from my earlier presentation on SCI’s survey on career paths for humanities PhDs (a full report of which will be available later this year). I’d like to take a step back from the question of how to reboot graduate training, and think instead about why we need significant change. Lately, some long-standing issues in higher ed, such as employment rates for PhD holders, seem to be receiving renewed attention that will hopefully set the stage for broad-based action. Some of these recent developments and articles include: The report on the 2011 Survey of Earned Doctorates, which presented the grim fact that 43% of doctoral recipients have no job or postdoctoral plan upon receiving their degree; A proposal at Stanford, designed by past MLA President Russell Berman and other faculty members, to dramatically reduce time to degree and to reform many aspects of humanities education; and Current MLA President Michael Bérubé’s talk at the Council of Graduate Schools’ annual meeting, in which he discussed the critical importance of reforming graduate training. Note: I’ve written about the items above in more detail in a recent ProfHacker post . The post includes a broader range of links to other write-ups on these topics. There are numerous other examples of ongoing work of this nature, such as the MLA’s Task Force on Doctoral Study in Modern Language and Literature, which hosted an excellent discussion on the topic at the convention. These conversations help to amplify current work being done by a number of individuals and organizations, at the Scholarly Communication Institute and elsewhere. My work with SCI has been deeply informed by the essays at #Alt-Academy, for instance, an open-access publication of Media Commons that explores the many issues related to the sometimes tense relationship between scholarship and professional directions. Of course, I’m also indebted to the extraordinary work and people of the Scholars’ Lab, which has housed SCI for the past few years. Over the past several months, SCI has embarked on a study of career preparation among humanities scholars working in alternative academic careers, and we are also developing a network of innovative humanities programs to highlight possibilities for reform (more on that in a moment). The goal of SCI’s study was to move from anecdote to data in the conversation about alternative academic careers. All of us know stories of the victories and challenges of pursuing an intellectually stimulating career beyond the tenure track, but there was little data to back up the narratives. With that in mind, the study had two main phases. The first phase, which was public and exploratory,  involved creating a public database where people comfortable enough to publicly identify as “alt-ac” practitioners could add themselves to a loose community of peers. The second phase of the study consisted of two confidential surveys. We built the study within the framework of the #Alt-Academy project in part to leverage the energy of existing conversations. While “alt-ac” isn’t a crisply defined term, many people have found it to be an incredibly useful umbrella under which they could gather to talk about the kinds of satisfying careers that can be found outside the professoriate. Trends among the nearly 800 responses we received to the main survey reveal a strong misalignment between the expectations of graduate students and the realities that they face upon completing their program. As may be expected, a large majority of students enter graduate school expecting to pursue careers as professors—a total of 74%. What is perhaps more interesting is their level of confidence: of that 74%, 80% report feeling fairly certain or completely certain that this was the career they would pursue. Keep in mind that the survey respondents are all working outside the tenure track. These expectations are not at all aligned with the current realities of the academic job market, as we know, and the urgency of finding a remedy to the lack of transparency is compounded by the rising amounts of student debt that burden so many graduates. Deepening the problem, students report receiving little or no preparation for careers outside the professoriate, even though we’re at a moment when the need for information about a variety of careers is most acute. Only 18% reported feeling satisfied or very satisfied with the preparation they received for alternative academic careers. The responses are rooted in perception, so there may be resources available that students are not taking advantage of—but whatever the reason, they do not feel that they are being adequately prepared. Again, this probably comes as no surprise, but it does reveal that we have significant room for improvement. To give you a small taste of perspectives on the other side of the employment equation, here are some of the kinds of things employers indicate they would like to see: Some specific skills come up frequently, including project management, the ability to work with and manage people, and written and verbal communication with a variety of audiences. Beyond that, employers mention a number of broader aptitudes, like a commitment to public engagement and an ability to adjust to the culture of different types of workplaces. The good news is that all of the elements that employers seek would also be hugely beneficial for those grads that do go on to become professors. By rethinking their curricula in such a way that students gain experience in things like collaborative project development and public engagement, departments would be strengthening their students regardless of the path they choose to take. UVa’s Praxis Program is designed as one of several new initiatives that help to assess needs and opportunities, develop and articulate new models, and foster the growth of collaborative networks among relevant institutions and individuals. To help showcase strong models of reform, SCI is now developing the Praxis Network : a network of several existing programs that have developed innovative models of methodological training along the lines of the Praxis Program at UVa. We anticipate that many programs that want to make changes will want to look to existing models for guidance, and by highlighting a handful of differently-inflected programs, we can bring together some patterns among them, while also underscoring the unique idiosyncrasies of each. In addition to sharing information with the public, we hope that the network will enable increased possibilities for communication and collaboration among the participants of each program. One thing seems clear: the persistent myth that there’s nothing but a single academic job market available to graduates is damaging, and extricating graduate education from the expectation of tenure-track employment has the potential to benefit students, institutions, and the health of the humanities more broadly. However, as long as norms are reinforced within departments—by faculty and students both—it will be difficult for any change to be effective. Low tenure-track employment rates are not a new problem, but as the survey responses show, departments by and large are not succeeding at providing accurate and realistic information to their students. For change to be possible, it’s essential that institutional norms and measures of prestige shift in favor of highlighting successful outcomes across a broader spectrum of possibilities. SCI hopes that our current work will help begin to rise the tide of transparency and innovation."},{"id":"2013-01-15-speaker-series-meg-stewart","title":"Meg Stewart to talk about the Fulbright Scholar program, Thursday, Jan 17","author":"ronda-grizzle","date":"2013-01-15 06:02:18 -0500","categories":["Announcements","Digital Humanities","Geospatial and Temporal"],"url":"speaker-series-meg-stewart","layout":"post","content":"A Fulbright Scholar Talks About Participatory GIS, the Caribbean, Google Earth and How a Fulbright Could Be in Your Future Thursday, January 17\n2:00 - 3:00pm\nAlderman Library, Room 421 Meg Stewart Academic Technology Consultant and Fulbright Ambassador A Fulbright Scholar in 2009-10 to the University of the West Indies in Barbados, Meg Stewart will talk about her experiences as a geospatial technologist in the Caribbean. Meg will also talk about the Fulbright Scholar program and encourage you to apply for a grant. _Speaker Bio:\n_Meg Stewart is an academic technology professional working with professors and students to successfully integrate technology into teaching and learning. With a master’s degree in geology from the University of Nevada, Las Vegas, she worked in environmental consulting for a five years, and then went into higher education. Starting out as a GIS (geographic information systems) consultant in the earth science and geography department at Vassar College, she helped faculty members teach with GIS software. Stewart has written several papers on the topic of teaching with technology in higher education. Stewart received a Fulbright Scholar award in 2009-10 to the Centre for Resource Management and Environmental Studies at the University of the West Indies in Barbados. While at the UWI, she assisted with teaching a GIS class, gave lectures on teaching with Google Docs, tablet PCs, and other technologies in education, and went with students and two faculty members on a field course to Belize. A website detailing Ms. Stewart’s Fulbright experience can be seen at: http://www.cies.org/ambassadors/mstewart . Fulbright Ambassador Program: The Fulbright Ambassador Program trains and utilizes a select group of Fulbright Scholar alumni to serve as representatives for the Fulbright Scholar Program at campus workshops and academic conferences across the United States. Ambassadors have been selected from the full spectrum of U.S. academic disciplines, higher education institutions, and geographic regions and serve as official representatives of the Fulbright Scholar program at the events for which they are selected."},{"id":"2013-01-16-but-i-dont-like-programming-gender-and-our-division-of-labor","title":"...but I don't like programming: gender and our division of labor","author":"claire-maiers","date":"2013-01-16 07:06:34 -0500","categories":["Grad Student Research"],"url":"but-i-dont-like-programming-gender-and-our-division-of-labor","layout":"post","content":"As last semester wound down, Cecilia and I  committed ourselves to honing our skills with Ruby.  During our first study session, we found ourselves talking about gender issues and the emerging role of each member within the Praxis team.  It is looking increasingly like the men will be more involved with programming, while the women of our group will focus on user interfaces, linking with social media, and management of the project.   During the last few meetings of our Ruby Boot Camp with Wayne, I’ve been aware of this emerging division, noticing that the women (perhaps mostly Cecilia and myself) have a tendency to ask more questions and to lay bare our lack of programming skills by joking about it or blatantly declaring our confusion.  In contrast, the men in our group are able to engage in two-way dialogue with Wayne in a vocabulary that is still largely foreign to me. Despite my sense of unease and disappointment in this stereotypically gendered division, I had been comforting myself by insisting that such a division of labor within Praxis was a result of coincidence.  In general, the men in our group came to Praxis with some previous experience in programming, while the women did not.  Both Cecilia and I are still in coursework and teaching, which meant that we had different scheduling difficulties from some of the others.  As Ruby boot camp amped up at the end of the semester, so too did the demands from coursework.  As a result, both Cecilia and I struggled to find time for our Ruby homework.  And finally, I don’t find myself drawn to programming.  While it is incredibly satisfying to solve a puzzle and get the program to complete some task, it is not the kind of work I would like to do all day long. Some of these circumstances actually are merely coincidences, but most are themselves structured results.   There are structured reasons why men often have more programming experience than women.    There are even structured reasons whyI have no great desire to be a programmer.  Gendered structures and practices work on us both externally and internally, shaping our desires, personalities, and goals.  (For more on these issues, check out this article on Forbes and one from the Chronicle of Higher Ed .) So, the question is, what do we do now?  From a practical sense, it seems reasonable to let those who already know how to program to do their job.  The rest of us will find other ways to contribute.  This is certainly the most practical and efficient way to complete the set of ambitious goals that we are developing for the coming semester.   But—-as some might recall, I wrote a blog post earlier this semester suggesting that Praxis had the potential to challenge norms within academia.  Shouldn’t we also try to undo gender stereotypes and stop the perpetuation of gendered structures?   Is there a way to make use of the skills we have brought to this team (gendered and otherwise) without perpetuating such norms?  My hope is to start a conversation about this issue.  I’m looking forward to your thoughts."},{"id":"2013-01-25-spring-2013-gis-workshops","title":"Spring 2013 GIS Workshops","author":"chris-gist","date":"2013-01-25 10:53:15 -0500","categories":["Geospatial and Temporal"],"url":"spring-2013-gis-workshops","layout":"post","content":"Every semester Kelly Johnston and I teach a workshop series around specific topics in GIS.   Typically, we stick to the basics for fall but branch out and mix it up a little by teaching new topics in spring.  Our sessions are one hour long and generally designed to be hands-on and don’t require prior knowledge to participate.  Preregistration is not required. All sessions are free and open to the UVa and larger Charlottesville community. Please find a PDF of the below schedule here . Acquiring and Using US Census Data in GIS Wednesday, February 6\n4:00 – 5:00pm\nCampbell Hall, Room 105 Session repeats on\nThursday, February 7\n3:00 – 4:00pm\nAlderman Library, Room 421 (Electronic Classroom) The United States Census has made big changes in their surveys and in the online tools to find and use US\nCensus datasets. Join us for a hands-on session introducing the newly redesigned American Factfinder online\ntool for discovery and access to free data from the US Census. No experience working with US Census data\nor geographic information systems is required. Defining Watersheds with Digital Elevation Data Wednesday, February 13\n4:00 – 5:00pm\nCampbell Hall, Room 105 Session repeats on\nThursday, February 14\n3:00 – 4:00pm\nAlderman Library, Room 421 (Electronic Classroom) Want to know the extent of any watershed? This session will teach you the process of delineating any\nwatershed in ArcGIS using elevation data. Using Neatline Wednesday, February 20\n4:00 – 5:00pm\nCampbell Hall, Room 105 Session repeats on\nThursday, February 21\n3:00 – 4:00pm Alderman Library, Room 421 (Electronic Classroom)\nNeatline is a set of plugins for Omeka developed by the Scholars’ Lab. With this tool, anyone can create\nbeautiful, complex maps and narrative sequences from collections of archives and artifacts, and to connect\nmaps and narratives with timelines that are more-than-usually sensitive to ambiguity and nuance. See\nhttp://neatline.org/ for more information. Making Cartograms Wednesday, February 27\n4:00 – 5:00pm\nCampbell Hall, Room 105 Session repeats on\nThursday, February 28\n3:00 – 4:00pm\nAlderman Library, Room 421 (Electronic Classroom) A cartogram is a thematic map that uses area to represent something other than area. Imagine a map where\ncountry area represents population, or cancer rates. You will learn how to send a powerful message with this\nthematic technique. Introduction to GDAL Wednesday, March 6\n4:00 – 5:00pm\nCampbell Hall, Room 105 Session repeats on\nThursday, March 7\n3:00 – 4:00pm\nAlderman Library, Room 421 (Electronic Classroom) The Geospatial Data Abstraction Library is an open source utility library for raster geospatial data formats. As\na library, it presents a large number of utilities to the calling application for all supported formats. It also\ncomes with a variety of useful command line utilities for data translation and processing. We will focus on the\ncommand line utilities. Do It Yourself Aerials Wednesday, March 20\n4:00 – 5:00pm\nCampbell Hall, Room 105 Session repeats on\nThursday, March 21\n3:00 – 4:00pm\nAlderman Library, Room 421 (Electronic Classroom) We have three aerial platforms, balloon, kite and hexcopter. Come get an update from us and find out how to\ndo your own aerial photography. Introduction to Quantum GIS Wednesday, March 27\n4:00 – 5:00pm\nCampbell Hall, Room 105 Session repeats on\nThursday, March 28\n3:00 – 4:00pm\nAlderman Library, Room 421 (Electronic Classroom) Quantum GIS (QGIS) is an open source, multi-platform GIS. While not nearly as powerful as ArcGIS, the\n80/20 rule applies. Probably 80% of the things most users want to do with GIS can be done with QGIS. The\nsession will introduce the interface and participants will make some nice maps. Learn more about QGIS at\nhttp://www.qgis.org. Advanced Techniques with Quantum GIS Wednesday, April 3\n4:00 – 5:00pm\nCampbell Hall, Room 105 Session repeats on\nThursday, April 4\n3:00 – 4:00pm\nAlderman Library, Room 421 (Electronic Classroom) One of QGIS’s strengths is its ability to pull in various streaming open standard data services. We will pull\nsome data in from a remote location and do some spatial analysis."},{"id":"2013-01-29-gendering-praxis","title":"Gendering Praxis","author":"cecilia-márquez","date":"2013-01-29 06:31:52 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"gendering-praxis","layout":"post","content":"As we embark on the next semester of the Praxis Program we have begun to think about our “roles” in the Prism project.  This has led to a surprising rush of gender feelings.  Parts of these feelings feel too second-wave feministy to be appropriate in this day and age, but regardless I find myself frustrated with myself for fitting into certain gendered tropes.  We have six people in our Praxis cohort, three are men and three are women and two of these men have previous experience in the coding/programming world.  Thus as we assign jobs I find myself gravitating towards design, project management, outreach and community engagement.  While the rest of the positions have yet to be assigned, its hard not to feel a division of labor emerging that is highly gendered. What makes this all the more frustrating is that my hesitation to take on “coding” responsibilities grows out of the realization last semester that I am not that interested in learning programming languages.  I’m happy that I have learned HTML/CSS and have a basic understanding of Ruby on Rails, my computer proficiency has expanded a thousand fold, and I will be certainly implementing digital tools in my dissertation.  However, I may not necessarily want to do the backend work necessary to make those tools manifest.  It is hard to let go of my deeply feminist goal of mastering coding languages and democratizing this knowledge for underrepresented groups in the digital world (women, people of color, queer people etc.).  This is not to say I have somehow given up on learning Ruby or other aspects of programming, but it is clear to me that I do not have the passion or drive to ever truly master these tools. An additional challenge that my gender has presented me with is my frustration at not knowing and needing to constantly seek help.  Even though the Scholars Lab staff could not be more amazing, kind, and understanding as they explain what is likely obvious to them, I feel that my constant confusion and steep learning curve plays into gendered assumptions about women and the digital world. Unfortunately right now I have more questions than I do answers about gender in the digital humanities.  How do we make digital humanities spaces that are truly feminist and anti-racist that recognize the historical and structural inequalities each person brings to the space?  How can we as people from underrepresented groups work through feelings of insecurity or “out of placeness” in the digital world?  How do we implement progressive pedagogical approaches to the digital humanities that resist teacher/student dynamics that disempower students? I know that there are already groups doing this work and I am excited to spend more time engaging with their pedagogical approach (groups like BlackGirlsCode and TransformDH ).  In the meantime I’m curious what other people think…"},{"id":"2013-02-01-looking-forward","title":"Looking Forward","author":"brandon-walsh","date":"2013-02-01 06:52:33 -0500","categories":["Grad Student Research"],"url":"looking-forward","layout":"post","content":"As we move into the second half of our tenure on the Praxis squad, we are feeling the pressure to start actually making something. To that end, the SLab crew asked us to make a list of our priorities for Prism . Hopefully this will help us make a to do list in the coming weeks. The following was the list that seemed to come out of yesterday’s meeting, though it’s possible that other people could have gotten different impressions from the conversation. These seem to be our priorities, in no particular order except roughly that in which they were brought up: Types of texts - we’re interested in opening up the Prism framework to deal with pictures, music, etc. Getting a crowd - we want to get a large group of people working with Prism, however that might be arranged. Visualizations - we’re interested in expanding Prism’s visualizations to allow you to see the individual set of markings in relation to the crowd and to let you see one marking category’s visualization beside another. User uploads - we want to make it possible for users/instructors to upload a text for marking themselves, which we see as essential for classroom use. Complicating the sense of how we highlight - there has been discussion of complicating how the marking system works, allowing for a limit to the number of markings or a type of marking economy that is distributed across your selections to encourage different types of engagement with the data. Design makeover - we want to remake the design of the site with an eye to usability. I think my own priorities lie in making it possible for users to upload their own texts for marking and in possibly adding more visualizations. If forced to choose between the two, I would spend the time on making the tool capable of handling user uploads. As quickly became clear, any one of these goals spins off into dozens of other questions. What do we mean by “upload a text”? Will those texts be public? Private? How will we navigate copyright and fair use? In the coming days we’re aiming to get a sense of our collective ordering of the list."},{"id":"2013-02-06-neatline-one-million-records","title":"Neatline Feature Preview - 1,000,000 records in a single exhibit","author":"david-mcclure","date":"2013-02-06 04:37:30 -0500","categories":["Research and Development"],"url":"neatline-one-million-records","layout":"post","content":"[Cross-posted with dclure.org ] ;tldr - The upcoming version of Neatline makes it possible to build huge interactive maps with as many as 1,000,000 records in a single exhibit. It also introduces a new set of tools to search, filter, and organize geospatial data at that scale. Watch the screencast: One of the biggest limitations of the first version of Neatline was the relatively small amount of data that could loaded into any individual exhibit. Since the entire collection of records was loaded in a single batch on page-load, exhibits were effectively constrained by the capabilities of the browser Javascript environment. Beyond a certain point (a couple hundred records), the front-end application would get loaded down with too much data, and performance would start to suffer. In a certain sense, this constraint reflected the theoretical priorities of the first version of the project - small data over large data, hand-crafted exhibit-building over algorithmic visualization. But it also locks out a pretty large set of projects that need to be built on top of medium-to-large spatial data sets. In the upcoming version 1.2 release of the software (which also migrates the codebase to work with the newly-released Omeka 2.0 ) we’ve reworked the server-side codebase to make it possible to work with really large collections of data - as many as 1,000,000 records in a single Neatline exhibit. Three basic changes were needed to make this possible: Spatial data needed to loaded “on-demand” in the browser. When the viewport is focused on San Francisco, the map doesn’t need to load data for New York. Huge performance gains can be had by loading data “as-needed” - the new version of Neatline uses the MySQL spatial extensions to dynamically query the collection when the user moves or zooms the map, and just loads the specific subset of records that fall inside the current viewport. As long as the exhibit creator sensibly manages the content to ensure that no more than a couple hundred records are visible at any given point (which isn’t actually much of a limitation - anything more tends to become bad information design), this means that the size of Neatline exhibits is effectively bounded only by the capabilities of the underlying MySQL database. The editor needed more advanced content management tools to work with large collections of records. In the first version of Neatline, all the records in an exhibit were stacked up vertically in the editing panel. If the map can display 1,000,000 records, though, the editor needs more advanced tooling to effectively manage content at that scale. Neatline 1.2 adds full-text search, URL-addressable pagination, and a “spatial” search feature that makes use of the map as a mechanism to query and filter the collection of records. There needed to be an easy way to make batch updates on large sets of records in an exhibit. Imagine you’re mapping election returns from the the 2012 presidential election and have 20,000 points on neighborhoods that voted democratic. If you decide you want to change the shade of blue you’re using for the dots, there has to be an easy way of updating all 20,000 records at once, instead of manually updating each of the records individually. In version 1.2, we’ve made it possible to assign arbitrary tags to Neatline records, and then use a CSS-like styling language - inspired by projects like Cascadenik - to define portable stylesheets that make it easy to apply bulk updates to records with a given tag or set of tags. These are big changes, and we’re really excited about the new possibilities that open up with this level of scalability. At the same time, all development carries an opportunity cost - working on features A and B means you’re not working on features C and D. Generally, Neatline is on a trajectory towards becoming a much more focused piece of software that hones in on a lean, extensible toolset for building interactive maps. We’re taking a hard look at features that don’t support that core competency. In the coming weeks, we’ll release an alpha version of the new codebase and solicit feedback from users to figure out what works and what doesn’t. What’s essential? What’s expendable? What assumptions are we making that nobody else is making?"},{"id":"2013-02-11-art-in-the-scholars-lab","title":"Art in the Scholars' Lab","author":"becca-peters","date":"2013-02-11 06:29:34 -0500","categories":["Announcements","Research and Development"],"url":"art-in-the-scholars-lab","layout":"post","content":"The first time I walked into the Scholars’ Lab, I was amazed by the space.  With the high ceilings, black-and-white tile floors, and the bold red wall, it looked like something out of a magazine, not a computer lab at UVA! I’ve been at UVA for a long, long time and I’ve worked in some nice, if a little traditional, spaces.  But the Scholars’ Lab is the first place I’ve worked that has been so modern and energetic.  The décor is a direct result of the purpose of the space: energy, synergy, and creativity.  So it was no surprise to me that Bethany aspired to have the art in the Scholars’ Lab as dynamic as the rest of the environment.  In the spring of 2008, when I was new to the Scholars’ Lab and Bethany had been the director for less than a year, we set out to change our walls from hosting a set of functional, framed posters to a rotating art show. [![](http://static.scholarslab.org/wp-content/uploads/2013/01/Erin-Chilton-Art1-247x300.jpg)](http://www.scholarslab.org/slab-events/art-in-the-scholars-lab/attachment/erin-chilton-art-2/) Art from Erin Chilton's show. In the late spring of 2008, Erin Chilton, a 4th year student with an exhibit of beautiful, realistic paintings, shared her art in our space.  The following fall, our own Digital Humanities Graduate Fellow, Jean Bauer, contributed several lovely color photographs .  They were beautiful in their simplicity.  We kept them up throughout the school year. We hosted our first Aunspaugh Fellow in the fall of 2009.  (An Aunspaugh Fifth Year Fellowship enables a U.Va. student who has an undergraduate degree to spend a year of intensive effort in a studio area within the McIntire Department of Art.)  Jeff Trueblood’s dark paintings filled every wall with their bold, haunting beauty.  Jeff’s talent for creating an atmosphere with light and dark still excites my curiosity. Taking a flashlight to one of his pieces will illuminate what is beneath the trees or behind the lamppost. We tried something different in the spring of 2010, opting for architectural renderings of Alderman Library.  Students in the School of Architecture’s graduate Architecture Studio 6020 were assigned a project to re-vision Alderman’s West Wing as a catalyst for new learning environments.  Their large pieces and a few 3-D models were hung throughout the Scholars’ Lab for the spring semester and gave all of us interesting points to consider for the future of our beloved building. In the fall of 2010, Aunspaugh Fellow Emily Corazon Nelson shared a series called “Nourish(ment)”.  Emily’s use of collage, staining, painting, and lamination with her photography created a dynamic show in the Scholars’ Lab.  Her work was inspired by an earlier project in which she and a few friends ran a garden and kitchen out of a biofuel bus and traveled around America, feeding people and developing relationships, while asking “what does nourishment mean to you?”   More of Emily’s art is here . Daniel Ballard’s work in paper making, printing, watercolor, and collage became, in the spring of 2011, a subtle exhibit of colors and forms to represent cityscapes and nature.  At the core of the show was Dan’s exploration into the implications of proposals for the future of cities. In Fall 2011, we showcased a small exhibit by Bena Dam, another Aunspaugh Fellow.  Bena’s photographs were beautiful, challenging and, in some ways, frightening.  Her goal was “to inject both a sense of whimsy and discomfort in [her] art.”  Her piece called “Hair Sew” (in which she sewed her fingers together with a strand of her own hair) and “Hung Up” (an optical illusion of Bena, headless, hanging by a wire hanger in her empty closet) were more startling than some might have liked, but personally I loved them because they did exactly what Bena set out to do: create feelings of discomfort and whimsy. Last spring, Takahiro Suzuki shared his black-and-white photographs in an exhibit he called “Beauty Through the Stillness”.  Taka’s goal was to highlight the barrenness of winter landscapes, while challenging the viewer to see the beauty in the quiet, desolate images .  They were at times dark, but not heavy, and I especially enjoyed the surprise of noticing a cemetery in one image after having looked at it repeatedly without noticing it. Our current exhibit is by Aunspaugh Fellow Elisabeth Hogeman.  Ellie named her exhibit “ Meander Lines,” an homage to map-making and ecology.  Ellie’s primary idea in her photographs is to, in her words, “play with the idea of the female body as a natural form, making contained topographical landscapes out of the body, set alongside areas of uncontained dense vegetation located near sources of water.”  Her art is beautiful, soft, and unflinching.  As with our previous art exhibits, “Meander Lines” fits well into our Scholars’ Lab space because it is as dynamic, creative, and bold as the Scholars’ Lab itself. Every semester we are challenged to think again about our surroundings, ourselves, our fears, and our hopes – kind of like every semester for our students!  I can’t wait to unveil our new exhibit (yet to be determined) soon.  Stay tuned!"},{"id":"2013-02-12-dancing-with-ruby","title":"Dancing with Ruby","author":"chris-peck","date":"2013-02-12 10:57:13 -0500","categories":["Grad Student Research"],"url":"dancing-with-ruby","layout":"post","content":"As the Praxis Team grapples with the necessity of making decisions (difficult decisions!) about our priorities when it comes to creating at tangible product between now and May, I continue to find new uses for Ruby in my own non-Prism-related work. Last week a choreographer friend asked for help with a problem that came up in her rehearsal last week for our new project. She had devised a system of textual instructions to guide a dance improvisation. This meant that in rehearsal one of the dancers would have to sit out and play the role of reading the instructions. Her system was a chance procedure, so it had already occurred to her that it might be possible to record the instructions one by one and place the soundfiles in a shuffled iTunes playlist. But she wanted another level of organization as well: an instruction drawn at random from one of three categories in sequence. A random instruction from the first category, another random selection from the second category, one from the third, then back to the first category, and so on. She had seen me whip up Max Patches to shuffle soundfiles in various ways and wondered if I could do something like that for this case. Using Ruby’s ability to execute shell commands, and the CLI for Mac’s speech synthesizer, I was able to come up with a solution that didn’t require recording all of the instructions. require \"csv\"\nadjectives_file = \"adjectives.csv\"\npause = 5 adj = CSV.read(\nadjectives_file,\n{\n:headers =&gt; true,\n:header_converters =&gt; :symbol\n}\n) adj_by_cat = {} adj.each do |a|\nif adj_by_cat[a[:category]].nil?\nadj_by_cat[a[:category]] = []\nend\nadj_by_cat[a[:category]].push a[:adjective]\nend 20.times do\nadj_by_cat.each do |cat, adjs|\ncmd = “say “#{adjs.shuffle.pop}”; sleep #{pause};”\n#puts cmd\n%x( #{cmd} )\nend\nend This little script reads in the instructions (organized into categories in a CSV file) and speaks them in a synthesized voice like this . Fun, right?"},{"id":"2013-02-13-neatline-drawing-svg-on-maps","title":"Neatline Feature Preview - Importing SVG documents from Adobe Illustrator","author":"david-mcclure","date":"2013-02-13 05:27:13 -0500","categories":["Research and Development"],"url":"neatline-drawing-svg-on-maps","layout":"post","content":"[Cross-posted with dclure.org ] ;tldr - The new version of Neatline makes it possible to take SVG documents created in vector editing software like Adobe Illustrator and Inkscape and “drag” them directly onto the map, just like a regular polygon. This makes it possible to create really sophisticated illustrations that go far beyond the blocky, “sharp-edge” style that we usually associate with digital maps. Check out the screencast (and scroll down for screenshots): The first version of Neatline implemented a pretty standard set of GIS controls for sketching vector geometry onto maps - points, lines, and polygons. It was easy to sketch out simple shapes, but more difficult to create really intricate, complex illustrations. Really, this is a sort of ubiquitous problem with digital maps, which tend to be good at representing points, but bad at representing curves . Under the hood, shapes on digital maps are represented by a series of X/Y coordinate pairs, wrapped up into different geometry types that store information about how the points should be displayed. For example, in Well-Known Text - the serialization format used by databases like PostGIS and MySQL - a line is represented by LINESTRING(1 2,3 4,5 6), a polygon by POLYGON((1 2,3 4,5 6,1 2)), and so on and so forth. At the end of the day, everything is just a series of hard-coded points, strung together to form shapes. This low-level organization in the data tends to bubble up to the level of user interfaces in the form of map sketching tools that make it easy to draw jagged shapes but hard to draw smooth shapes. For example, in the first version of Neatline, drawing this is easy: But this is much harder: It’s still possible, but it’s time-consuming and brittle - if you change your mind later and want to adjust the curvature of the arrow, you have to manually reposition dozens of points. This especially frustrating since, in other domains, this is a well-understood problem with lots of high-quality solutions: Vector graphics editors like Adobe Illustrator, Inkscape, and even in-browser tools like svg-edit make it easy to create smooth, complex vector-based geometries that can be serialized to a portable XML format called SVG (Scalable Vector Graphics). In the upcoming release of Neatline, we’ve made it possible to take SVG markup created in any vector editing tool and place it directly onto the map. Just save off any vector graphic as a SVG document, open up the file in a text editor, and paste the raw markup into the Neatline editor. Then just drag out the shape to any position, dimension, and orientation on the map. Once the new geometry is in place, it behaves just like regular points and polygons added with the default controls - it can be styled and edited just like anything else on the map. This also opens up a whole new front of high-fidelity text-based annotation on digital maps. Since vector editors can convert strings of text into SVG paths, this makes it possible to sketch out labels, snippets, or even little paragraphs of content directly onto the map itself."},{"id":"2013-02-14-rails-is-kind-of-hard-to-get-up-and-running","title":"Rails is kind of hard to get up and running","author":"shane-lin","date":"2013-02-14 06:09:26 -0500","categories":["Grad Student Research"],"url":"rails-is-kind-of-hard-to-get-up-and-running","layout":"post","content":"To pile onto an overrused trope: for a web framework famed for its use of use, Rails (and maybe Ruby itself) are really surprisingly difficult to get up and running. In hindsight, the very existence of tools like RVM and Bundler to handle the management of Ruby environments and libraries is a bit of a hint. I’ve set up Prism twice now, on my old Thinkpad running Ubuntu Precise and now on my new Thinkpad running Ubuntu Quantal and I think I’ve had a bit of a different experience each time. This time, I first ran into the usual problems with Ruby Bundler. I’m sure that once things are set up and running, Bundler is a lifesaver and that the speedbumps I ran into are more the fault of individual gems. But it’s very clearly not the automated process that it’s made out to be out of the box. Nokogiri and SQLite both caused the process to fail; figuring out why required me to manually gem install them: it was, of course, because my Linux distro doesn’t come with their dependencies. It’s certainly no big deal to Google what these dependencies are and apt-get them and it’s reasonable that a Ruby gem manager shouldn’t be expected to manage non-gem software, but a more streamlined process would be nice. Then, db:migrate threw back the “you don’t have a javascript runtime” error, which directs me to the execjs gem. Again, it seems reasonable that it isn’t a dependency because in theory any JS runtime will work, but it also seems reasonable that there should be a better mechanism somewhere upstream to handle this. Then, db-migrate returned the more inscrutable ”no such file to load – ripper” error. The first problem was that  ”ripper” is not a very unique name, even within the ruby namespace (there’s apparently a ruby-ripper gem for audio encoding). The ripper I was looking for is a 1.8 port of implementation built into Ruby 1.9. I was confused that this wasn’t just handled by Bundler (or at least uncovered by it). When it didn’t work, I eventually realized that Prism actually simply required Ruby 1.9. Maybe I should have known this from the start (I seem to recall figuring it out faster the first time around), but I also feel that this fact would have been less ambiguously messaged and discovered earlier with other languages with split versions (Python 2 vs 3, for example). So, getting RVM installed on Ubuntu also required a bit of configuration, revealed through the web documentation rather than the actual error reporting. But even after getting it and Ruby 1.9 installed, Prism wouldn’t run without Readline support. That came with a helpful error message about what packages to install, which actually turned out to be a Bad Thing, because installing it actually broke recompiling Ruby with Readline support on Ubuntu 12.04+… Long story short: it’s a series of issues that aren’t fundamentally unreasonable in themselves, but in aggregate leech away all of a person’s resolve and aspiration, leaving only a desiccated and hollow husk."},{"id":"2013-02-18-a-first-look","title":"A First Look","author":"gwen-nally","date":"2013-02-18 10:20:34 -0500","categories":["Grad Student Research"],"url":"a-first-look","layout":"post","content":"On Friday, the design team made a few mock-ups of what we’d like Prism to look like. We started with some of the basic pages that the next version of prism will probably have, a homepage, a login pop-up, etc. I feel that I have a much clearer idea of what needs to be done now that I’ve seen where we hope to take the site refresh. I’ve also noticed that the design team is making a number of important decisions as we go. For example, while designing the homepage mock-up we found ourselves asking questions about when and where people will be asked to login. Because we’re making these decisions on the fly and we’re not all working on the same things, I hope that others will have a chance to respond and revise. I also hope that these mock-ups will help the team figure out where to focus our energy in the weeks to come. Here’s how we imagine the site might work. The Homepage will have two large buttons, something like DEMO and PLAY. The DEMO button will hopefully lead to a tutorial page. The PLAY button will lead to a pop-up window where users can login. Brandon is working on open authentication as we speak. Once a user has logged in, she’ll have the choice to UPLOAD or BROWSE texts. The UPLOAD button might lead to some simple interface like the one below. Any thoughts? Revisions? Likes? Dislikes? I imagine Cecelia and I will start with the homepage over the next week, so it might be good to start our conversation there. The rest of this will certainly change as others on the team start to take on particular tasks."},{"id":"2013-02-18-svg-to-wkt","title":"SVG-to-WKT: Converting vector graphics into spatial coordinates","author":"david-mcclure","date":"2013-02-18 06:03:26 -0500","categories":["Research and Development"],"url":"svg-to-wkt","layout":"post","content":"[Cross-posted with dclure.org ] Last week, I wrote about the some of the new functionality in Neatline that makes it possible to take SVG documents created in vector-editing programs like Adobe Illustrator and drag them out as spatial geometry on the map. Under the hood, this involves converting the raw SVG markup - which encodes geometry relative to a “document” space (think of pixels in a Photoshop file) - into latitude/longitude coordinates that can be rendered dynamically on the map. Specifically, I needed to generate Well-Known Text (WKT), the serialization format used by spatially-enabled relational databases like PostGIS and MySQL. It turned out that there wasn’t any pre-existing utility for this, so I wrote a little library called SVG-to-WKT that does the conversion. The top-level convert method takes a raw SVG document and spits back the equivalent WKT GEOMETRYCOLLECTION : SVGtoWKT.convert('&lt;svg&gt;&lt;polygon points=\"1,2 3,4 5,6\" /&gt;&lt;line x1=\"7\" y1=\"8\" x2=\"9\" y2=\"10\" /&gt;&lt;/svg&gt;');\n&gt;&gt;&gt; \"GEOMETRYCOLLECTION(POLYGON((1 -2,3 -4,5 -6,1 -2)),LINESTRING(7 -8,9 -10))\" The library supports all SVG elements that directly encode geometry information, and exposes the individual helper methods that handle each of the elements: line SVGtoWKT.line(1, 2, 3, 4);\n&gt;&gt;&gt; \"LINESTRING(1 -2,3 -4)\" polyline SVGtoWKT.polyline('1,2 3,4');\n&gt;&gt;&gt; \"LINESTRING(1 -2,3 -4)\" polygon SVGtoWKT.polygon('1,2 3,4');\n&gt;&gt;&gt; \"POLYGON((1 -2,3 -4,1 -2))\" rect SVGtoWKT.rect(1, 2, 3, 4);\n&gt;&gt;&gt; \"POLYGON((1 -2,4 -2,4 -6,1 -6,1 -2))\" circle SVGtoWKT.circle(0, 0, 10);\n&gt;&gt;&gt; \"POLYGON((10 0,9.95 -0.996,9.802 -1.981,9.556 -2.948,9.215 -3.884,8.782 -4.783,8.262 -5.633,7.66 -6.428,6.982 -7.159,6.235 -7.818,5.425 -8.4,4.562 -8.899,3.653 -9.309,2.708 -9.626,1.736 -9.848,0.747 -9.972,-0.249 -9.997,-1.243 -9.922,-2.225 -9.749,-3.185 -9.479,-4.113 -9.115,-5 -8.66,-5.837 -8.119,-6.617 -7.498,-7.331 -6.802,-7.971 -6.038,-8.533 -5.214,-9.01 -4.339,-9.397 -3.42,-9.691 -2.468,-9.888 -1.49,-9.988 -0.498,-9.988 0.498,-9.888 1.49,-9.691 2.468,-9.397 3.42,-9.01 4.339,-8.533 5.214,-7.971 6.038,-7.331 6.802,-6.617 7.498,-5.837 8.119,-5 8.66,-4.113 9.115,-3.185 9.479,-2.225 9.749,-1.243 9.922,-0.249 9.997,0.747 9.972,1.736 9.848,2.708 9.626,3.653 9.309,4.562 8.899,5.425 8.4,6.235 7.818,6.982 7.159,7.66 6.428,8.262 5.633,8.782 4.783,9.215 3.884,9.556 2.948,9.802 1.981,9.95 0.996,10 0))\" ellipse SVGtoWKT.ellipse(0, 0, 10, 20);\n&gt;&gt;&gt; \"POLYGON((10 0,9.98 -1.268,9.92 -2.532,9.819 -3.785,9.679 -5.023,9.501 -6.241,9.284 -7.433,9.029 -8.596,8.738 -9.724,8.413 -10.813,8.053 -11.858,7.66 -12.856,7.237 -13.802,6.785 -14.692,6.306 -15.523,5.801 -16.292,5.272 -16.995,4.723 -17.629,4.154 -18.193,3.569 -18.683,2.969 -19.098,2.358 -19.436,1.736 -19.696,1.108 -19.877,0.476 -19.977,-0.159 -19.997,-0.792 -19.937,-1.423 -19.796,-2.048 -19.576,-2.665 -19.277,-3.271 -18.9,-3.863 -18.447,-4.441 -17.92,-5 -17.321,-5.539 -16.651,-6.056 -15.915,-6.549 -15.115,-7.015 -14.254,-7.453 -13.335,-7.861 -12.363,-8.237 -11.341,-8.58 -10.274,-8.888 -9.165,-9.161 -8.019,-9.397 -6.84,-9.595 -5.635,-9.754 -4.406,-9.874 -3.16,-9.955 -1.901,-9.995 -0.635,-9.995 0.635,-9.955 1.901,-9.874 3.16,-9.754 4.406,-9.595 5.635,-9.397 6.84,-9.161 8.019,-8.888 9.165,-8.58 10.274,-8.237 11.341,-7.861 12.363,-7.453 13.335,-7.015 14.254,-6.549 15.115,-6.056 15.915,-5.539 16.651,-5 17.321,-4.441 17.92,-3.863 18.447,-3.271 18.9,-2.665 19.277,-2.048 19.576,-1.423 19.796,-0.792 19.937,-0.159 19.997,0.476 19.977,1.108 19.877,1.736 19.696,2.358 19.436,2.969 19.098,3.569 18.683,4.154 18.193,4.723 17.629,5.272 16.995,5.801 16.292,6.306 15.523,6.785 14.692,7.237 13.802,7.66 12.856,8.053 11.858,8.413 10.813,8.738 9.724,9.029 8.596,9.284 7.433,9.501 6.241,9.679 5.023,9.819 3.785,9.92 2.532,9.98 1.268,10 0))\" path SVGtoWKT.path('M10 10 C 20 20, 40 20, 50 10Z');\n&gt;&gt;&gt; \"POLYGON((10 -10,10.722 -10.689,11.474 -11.344,12.255 -11.964,13.062 -12.551,13.894 -13.102,14.747 -13.62,15.62 -14.103,16.51 -14.552,17.417 -14.968,18.339 -15.35,19.273 -15.7,20.219 -16.018,21.175 -16.304,22.139 -16.558,23.112 -16.782,24.09 -16.974,25.075 -17.137,26.064 -17.269,27.056 -17.371,28.051 -17.443,29.048 -17.486,30.045 -17.5,31.043 -17.484,32.04 -17.438,33.035 -17.363,34.027 -17.258,35.015 -17.123,35.999 -16.958,36.977 -16.763,37.949 -16.536,38.913 -16.279,39.868 -15.99,40.813 -15.67,41.746 -15.317,42.666 -14.931,43.571 -14.512,44.461 -14.06,45.332 -13.574,46.183 -13.053,47.012 -12.498,47.817 -11.909,48.595 -11.285,49.345 -10.627,49.909 -10,48.911 -10,47.914 -10,46.916 -10,45.918 -10,44.92 -10,43.923 -10,42.925 -10,41.927 -10,40.929 -10,39.932 -10,38.934 -10,37.936 -10,36.939 -10,35.941 -10,34.943 -10,33.945 -10,32.948 -10,31.95 -10,30.952 -10,29.954 -10,28.957 -10,27.959 -10,26.961 -10,25.964 -10,24.966 -10,23.968 -10,22.97 -10,21.973 -10,20.975 -10,19.977 -10,18.98 -10,17.982 -10,16.984 -10,15.986 -10,14.989 -10,13.991 -10,12.993 -10,11.995 -10,10.998 -10,10 -10))\" If you look at the output strings, you’ll notice that the Y-axis coordinates in the WKT are inverted relative to the input: SVGtoWKT.polyline('1,2 3,4') returns LINESTRING(1 -2,3 -4), not LINESTRING(1 2,3 4) . This is because the Y-axis “grows” in the opposite direction on maps as it does in document space. In Illustrator, the coordinate grid starts at the top left corner, and the Y-axis increases as you move down on the page; on maps, the Y-axis increases as you move “up,” to the north. SVG-to-WKT just flips the Y-axis coordinates to make the orientation correct on the map. TODO Make it work in Node.js. This is actually a bit trickier that I thought it would be, because Node doesn’t implement the browser-native methods that jQuery’s parseXML uses. It may make sense to move to a generic XML parser that works in Node, which would be lighter-weight than jQuery anyway. Instead of just being purely functional (SVG in, WKT out), it might be useful to return some sort of SVGDocument object that could then be used to generate specific WKT strings at different density levels, orientations, etc. This would have come in handy while writing the custom OpenLayers handler that Neatline uses to actually position the generated WKT on the map (more on this later). Get rid of the Underscore.js dependency."},{"id":"2013-02-19-design-team-progress","title":"Design Team Progress","author":"cecilia-márquez","date":"2013-02-19 06:47:07 -0500","categories":["Grad Student Research","Research and Development"],"url":"design-team-progress","layout":"post","content":"This week the design team, myself, Brandon and Gwen,  met for the first time to start to map out our vision for the exciting redesign.    This week I am spending figure out the CSS behind the Prism site and me and Gwen are working on wireframes (mostly I yell out random ideas and Gwen makes them look amazing).  We are trying to figure out how we want the site to progress etc.  This week I am also trying to figure out how to make the header work the way that we want. Brandon is working on getting OAuth up and running so that people can sign in using facebook, twitter or google.  We have some other exciting plans in the works if we can finish all of these tasks!  Our next major goal is try and get user uploads working so that people can upload their own text for Prism analysis.  That’s all for now…next week we can start posting our design mockups so we can hear your feedback!"},{"id":"2013-02-19-works-in-progress","title":"Works in progress: Survey results, Praxis Network","author":"katina-rogers","date":"2013-02-19 05:10:42 -0500","categories":["Announcements"],"url":"works-in-progress","layout":"post","content":"[Cross-posted on my personal website ] This spring marks a new phase for my work with SCI. Data collection for the survey on career paths is complete, and analysis is underway, meaning that the next step will be much more focused on sharing outcomes. In some ways, this is a less comfortable step in the process for me (nerves! public speaking!), but also an exciting and satisfying one. I’m honored to be giving several invited talks over the next few months: March 8, 12:30–2:30 p.m., NYU (hosted by the Humanities Initiative ) April 10, 5–6:30 p.m., University of Delaware (hosted by the Interdisciplinary Humanities Research Center ; here’s a flyer ) April 17, 12–1 p.m., Stanford University (hosted by the Humanities Education Focal Group, which Russell Berman chairs) All talks are open to the public, so please come if you’re in the area! I’d love to see friendly faces, and I’m very much hoping for dynamic discussion at each event. Also in the spirit of sharing information and outcomes, I’ve been working with Jeremy Boggs on a website that will showcase a small handful of innovative programs for humanities graduate and undergraduate students. I can’t wait to unveil it; the programs are exciting, the website is beautiful, and overall I think it will be very useful for a range of audiences. In particular, I hope that it can be used to support the development of other new programs with similar goals of equipping humanities scholars to excel in the paths that they choose. The site is designed to be something of a response to the survey results—where the survey underscores opportunities for improvement in graduate curricula, the site (called the Praxis Network) points to specific efforts to rethink methodological training with an eye toward collaboration, project-driven scholarship, and public engagement. Finally, I’m incredibly pleased to be presenting a long paper at DH2013 in July that will include elements of the survey as well as the Praxis Network, and I’m working on a final report to be published around the same time. We’ll also publish the data so that others can build on the research we’ve done in the past year. It seems crazy, but by the time the DH conference rolls around, my time at SCI will be nearly finished. I’m thrilled that I’ll have so many opportunities to share our work between now and then."},{"id":"2013-02-20-highlighting","title":"Highlighting (some design proposals for Prism)","author":"chris-peck","date":"2013-02-20 04:24:17 -0500","categories":["Grad Student Research"],"url":"highlighting","layout":"post","content":""},{"id":"2013-02-21-on-learning-code","title":"On Learning Code","author":"brandon-walsh","date":"2013-02-21 05:44:03 -0500","categories":["Grad Student Research"],"url":"on-learning-code","layout":"post","content":"As we gear up for Prism development proper, I have been trying to get up to speed with some coding basics so that I can hit the ground running. Here are the learning aids that I have found most useful over the past couple weeks. Nearly all of these materials can be found in the  Praxis Program’s scratchpad . Ruby Learn Ruby the Hard Way - I enjoyed the learning approach here: the site displays a chunk of code that you type out, fix, and then examine more closely. It might seem like working backwards, but it works well for people who learn by doing. Ruby Warrior  - Learn Ruby by programming your own video game! There is nothing quite like tricking your brain to work. Each new level presents a problem that you must solve by changing a Ruby file. This one is great for learning the syntax, but it will only go so far in teaching the methods available to you in Ruby. You’ll be doing a lot of methods unique to the game, and I doubt that warrior.walk!(:backward) will come up in normal situations. Syntax is all, though. Rails Michael Hartl’s Ruby on Rails Tutorial  - Hartl walks you through building an application step by step via a Rails scaffolding. They are quite thorough and easy to follow. RailsCasts - These very short video tutorials walk you through Ruby on Rails concepts with screencasts and commentary. JavaScript Code Academy  - I’ve been using Code Academy for JavaScript, but they also have HTML, Ruby, Python, and several other coding tracks. There are a wealth of resources here arranged in interactive exercises, and the site also has lots of extra opportunities for practice once you finish the main courses. It also gives out points and badges as you code for added incentive!"},{"id":"2013-02-26-announcing-my-blog-songs-of-the-victorians-and-augmented-notes","title":"Announcing My Blog, Songs of the Victorians, and Augmented Notes","author":"annie-swafford","date":"2013-02-26 08:37:34 -0500","categories":["Digital Humanities","Grad Student Research","Research and Development"],"url":"announcing-my-blog-songs-of-the-victorians-and-augmented-notes","layout":"post","content":"I’ve been having an excellent and productive time as a Scholars’ Lab Fellow: thanks to the amazing advice and support I’ve received from the Scholars’ Lab staff, I’ve been making good progress on my two digital projects, Songs of the Victorians and Augmented Notes. For those of you who don’t know about them, I’ll give you a brief summery: Songs of the Victorians, which I’ve written about before here and here, is an archive and analysis of parlor and art song settings of Victorian poems.  I specialize in Victorian poetry and its intersections with music, and this project I’m building helps me in my interdisciplinary work.  For the four songs I include, users can examine an archive page with high resolution scans of the 1st edition printing of the score and an audio file of the work; when users click the audio file, each measure is highlighted in time with the music so scholars who can’t read music can still follow along.  Each song also has an analysis component, in which I include an essay on the music’s interpretation of the text, and users can click on parenthetical notes that occur after descriptions of the effect of a particular musical passage, and the relevant measures of the score will become highlighted in time with the music.  In case an image will make it more tantalizing, here’s the “Coming Soon” page that’s live on the web: Augmented Notes, which is not yet live, is a tool that will help users generate their own sites like Songs of the Victorians.  Users will upload a scan, audio file, and MEI file and then follow simple instructions to output css, html, and javascript template files that they can then alter to make their own complete interdisciplinary site. If you’re interested in learning more about either of these projects or in how I’m building them, I have a blog, Anglophile in Academia, where I post updates.  I write a new post every Monday, and I’d love to hear comments and feedback from all of you! If you don’t know where to jump in, here are some highlights so far: Progress report :  Here’s where I announce the date of the sneak-peak release of Songs of the Victorians. Content :  If you’re interested in the content of the analysis pages for Songs of the Victorians, read this pos t on Caroline Norton’s song “Juanita.” Design :  Here’s my most recent post on designing the homepage and song display page for Songs of the Victorians, here’s a post on the redesign of Songs of the Victorians and the rationale behind it, and here’s my post about logo designs for both sites. I’ll be making my weekly updates for the rest of the semester (and beyond), so make sure to subscribe or check it out every week!"},{"id":"2013-02-26-neatline-and-omeka-2","title":"Neatline and Omeka 2.0","author":"david-mcclure","date":"2013-02-26 08:53:00 -0500","categories":["Research and Development"],"url":"neatline-and-omeka-2","layout":"post","content":"[Cross-posted with dclure.org ] We’ve been getting a lot of questions about when Neatline plugins will be ready for the newly-released Omeka 2.0 . The answer is - very soon! In addition to migrating all of the plugins ( Neatline, Neatline Time, Neatline Maps, Neatline Features ) over to the new version of Omeka, we’re also using this transition to roll out a major evolution of the Neatline feature-set that incorporates lots of feedback from the first version. Some of the new, Omeka-2.0-powered things on tap: Real-time spatial querying on the map, which makes it possible to work with really large collections of data (as many as 1,000,000 records in a single exhibit); The ability to import SVG documents from vector-editing programs like Adobe Illustrator, making it possible to render complex illustrations on the map; A portable stylesheet system that allows exhibit-builders to use a CSS-like syntax to apply bulk updates to large collections of records; An improved workflow for displaying Omeka items in Neatline exhibits - mix and match individual Dublin Core fields, entire metadata records, images, and other item attributes; A flexible workflow for adding custom base layers in exhibits, which makes it possible to use Neatline to annotate non-spatial materials: paintings, drawings, abstract maps, and anything else that can be captured as an image. A new set of hooks and filters - both on the server and in the browser - that make it easy to for developers to write modular add-ons and customizations for Neatline exhibits - legends, sliders, record display formats, integrations with long-format texts, etc. The new version is just about feature-complete, and we’re now in the process of tying up loose ends and writing the migration code to upgrade projects built on the 1.1.x releases. We’re on schedule for a public beta by the end of March, and a full release by the end of the semester. Going forward, we’ll continue supporting the Omeka 1.5.x-compatible releases of Neatline from a maintenance standpoint, but we’re moving all new development efforts into the new versions of the plugins, which only work with Omeka 2.0. As the final pieces fall into place over the course of the next couple weeks, we’ll start posting a series of alpha releases for developers and other folks who want to test-drive the new feature set. Between now and then, check out some of the feature-preview articles we’ve posted in the last couple weeks: Neatline Feature Preview – 1,000,000 records in an exhibit Neatline Feature Preview – Importing SVG documents from Adobe Illustrator And watch this space for ongoing weekly updates!"},{"id":"2013-02-28-prism-site-map","title":"Prism Site Map","author":"cecilia-márquez","date":"2013-02-28 11:20:40 -0500","categories":["Grad Student Research","Research and Development"],"url":"prism-site-map","layout":"post","content":"The design team got together again this week and we have mocked up a map of how the site will proceed.  Those who are interested can see it here:  prismmap There are quite a few new pages that we are adding so we hope this map will make it easier to see how the progression works.  As we conference with Shane and other folks we may have to reorganize some of the order but I think this is more or less what it will look like.  We also have a second round of page mockups coming soon!"},{"id":"2013-03-01-gender-and-computing-ctd","title":"Gender and Computing (ctd)","author":"shane-lin","date":"2013-03-01 05:34:54 -0500","categories":["Grad Student Research"],"url":"gender-and-computing-ctd","layout":"post","content":"To add to the conversation about gender in computing (from Claire and Cecilia ), I just wanted to very briefly point out that while the gendered culture and gender gap in computing are not recent phenomena (on the former, Jennifer Light’s brief article on the submerging of women’s roles on ENIAC is a good read), it’s actually been getting much worse in the last few decades rather than better. Women’s participation in both academic computer science and the information technology industry has been on the decline since the 1980s, the exact opposite trend as the great progress women have made over this period in education and business at large. As this gender division has widened and gendered stereotypes have become crystallized,  even the iconic figures - the Ada Lovelaces and Grace Hoppers - have been shunted off into “token woman” status that robs them of the titanic contributions they made as computing pioneers. There’s been alot of commentary on the reasons for this decline and I feel rather unqualified to judge their relative merits (I am personally convinced though that tech’s ”startup”  culture, originating in the 1970s but reaching its apex in the 1990s and 2000s, is a major contributor). Whatever the cause, this trend demands active measures to correct it. From my short exposure, DH has seemed a rare and pleasant refuge surrounded by the larger technological sea of gendered assumptions and toxic sexism. I don’t have any better ideas of how, but Claire and Cecilia’s suggestions that DH or Praxis be wielded to challenge norms within its larger communities (be it technology or academia) seems especially important now."},{"id":"2013-03-01-restarting-marionette-applications","title":"Restarting Marionette applications","author":"david-mcclure","date":"2013-03-01 04:45:25 -0500","categories":["Research and Development"],"url":"restarting-marionette-applications","layout":"post","content":"[Cross-posted from dclure.org ] Over the course of the last couple months, I’ve been using Derick Bailey’s superb Marionette framework for Backbone.js to build the new version of Neatline. Marionette sits somewhere in the hazy zone between a library and a framework - it’s really a collection of architectural components for large front-end applications that can be composed in lots of different ways. I use Marionette mainly for the core set of message-passing utilities, which make it easy to define interactions among different parts of big applications - pub-sub event channels, command execution, request-response patterns, etc. I’ve come to completely rely on these structures, and can’t really imagine writing non-trivial applications without them anymore. The only big kink I’ve encountered is in the Jasmine suite. Since almost all of the integration-level test cases mutate the state of the application (trigger routes, open/close views, etc.), I needed to completely burn down the app and re-start it from scratch at the beginning of each test to ensure a clean slate. The top-level Marionette Application has a start method that walks down the tree of modules and runs the initializers. As it exists now, though, start can only be called once during the lifecycle of the application, and does nothing if it’s called again later on. I was getting around this by defining independently-callable init methods for all of my modules and wiring them up to the regular Marionette start-up system: But then manually calling all of the init methods in my Jasmine start-up routine to force-restart the application: This is icky - I have to exactly recreate a specific start-up order that’s automatically enforced in the application itself by before: and after: initialization events. And it introduces lots of opportunities for false-negatives - if you add a module, and forget to explicitly start it in the test suite, everything falls apart. Really, I wanted to just re-call Neatline.start() before every test. I realized tonight, though, that the application object can be tricked into restarting itself by (a) stopping all of the modules and (b) resetting the top-level Callbacks on the application: Much cleaner. Assuming all state-bearing components are instantiated in the initializers, this has the desired effect of completely rebooting the application. I’d imagine this is a pretty common issue - is there any philosophical reason for the prohibition against re-calling Application.start() more than once?"},{"id":"2013-03-05-gradient-highlights","title":"Gradient Highlights","author":"chris-peck","date":"2013-03-05 06:47:22 -0500","categories":["Grad Student Research"],"url":"gradient-highlights","layout":"post","content":"While playing around with the CSS gradients that put the highlighting for each “facet” into lanes over the text, I think I’ve come up with an interesting new proposal for what the highlighting might look like: I like this because it allows each facet’s highlight to be full height—thus feeling more like a highlighter pen and clearly bonding visually with its text—while still allowing the colors to be distinct in the areas of overlap. I think this approach might work well with up to four or five colors, and maybe that’s what I’ll try next. What was that about the value of failure and mistakes (a theme from some of our first meetings with the Praxis cohort in the fall)? I got this idea from mistyping a value when making a change to the CSS and seeing how easy it was to make this sort of gradient. I doubt I would have thought to try it otherwise. And in some ways it’s far better than any of the mock-ups I came up with in Illustrator last week."},{"id":"2013-03-07-css-victory","title":"CSS Victory!","author":"cecilia-márquez","date":"2013-03-07 07:13:27 -0500","categories":["Grad Student Research","Research and Development"],"url":"css-victory","layout":"post","content":"So the other day I spent most of my day working through the CSS on the Prism site.  It was incredibly annoying at first but at some point I really hit my stride and started to get it.  In order to start just figuring out where everything was located I made a feature branch (appropriately titled Honey Boo Boo) and started playing around until I could get it close to what I wanted it to look like.  I succeeded (as you can see above) in getting the homepage to look like what I imagined.  Currently I am using three of my femme superheroes as placeholders for the buttons (for those of you who don’t know they are [from left to right] Honey Boo Boo, Nicki Minaj, and RuPaul).  So this is a rough sketch of what the homepage might look like, although we will probably tone down the fabulosity that is the current buttons.  Thoughts?  Feedback?"},{"id":"2013-03-07-the-place-of-beauty-in-scholarly-writing","title":"The place of beauty in scholarly writing","author":"katina-rogers","date":"2013-03-07 03:35:27 -0500","categories":["Announcements"],"url":"the-place-of-beauty-in-scholarly-writing","layout":"post","content":"[Cross-posted on my personal website ] I’ve just returned from two thought-provoking days of conversations about assessment and authority in new modes of scholarly production, the second in a series of three SCI meetings on the topic. We’ll synthesize the key outcomes and insights into a report very soon. For the moment, though, I want to think a little more about a question that occurred to me after the meeting: What is the place of beauty in academic writing? While this wasn’t something the group discussed directly, it did seem to be an undertone of certain threads of conversation. I got home from CHNM on Friday evening feeling pretty brain-dead from the hybrid (and quintessentially #altac) work of wrangling meeting logistics and absorbing  stimulating and thoughtful discussion. Ready to relax, I sat down to watch Pina and was entranced within minutes; the film is stunning. The clips of Pina Bausch’s dance company, Tanztheater Wuppertal, are mesmerizing; they are made even more compelling by Wim Wenders’ directorial work. Something about the visual beauty of the film and the dance it portrayed helped me to think about the preceding conversations about scholarly work in a new light. One topic of discussion at SCI was the significance of the editorial process to the perceived quality and authority of scholarly work. Thinking about this while watching the film, I was struck first of all by the interviews with individual dancers that fill a substantial portion of screentime. Each dancer speaks admiringly of Pina (always referring to her by her first name), many of them noting her ability to draw out astonishing performances through her perceptiveness and laconic guidance. The task of ferreting out talent in academic spheres can happen at many different junctures, and is the touchstone of good mentors (and editors) everywhere. But I’m not sure that we give enough credit to the role, as stories of scholarly enterprise often favor a notion of individual struggle and success. Pina’s influence, by contrast, is clearly credited as a guiding force and catalyst, both for individuals and for the company as a whole. The second thing that I thought about while watching the compelling visual display was the necessity of expertise and practice in the dance productions, no matter how unlike traditional repertoire they may have been. Pina’s company was known for innovative and risky works that departed significantly from traditional dance productions, but that doesn’t mean that the dances are sloppy or unrehearsed. On the contrary, it is clear that the dancers have a deep foundation in traditional training, that the unusual choreography is equally demanding of precision, and that the productions are meticulously rehearsed. The result is both beautiful and powerful. As we talk about new modes of scholarly production that depart from the traditional mechanisms of academic authority, it’s worth considering what careful research and new lines of inquiry look like when separated from the formats that have long been customary. As the velocity of publication increases (and is done on an ever-thinner shoestring, even at traditional presses), the editorial process is condensed. Writers may not polish their prose to the same degree, and the work may not benefit from thorough content refinement, copyediting, or layout decisions that publishers have historically taken on. Generally speaking, I think that making scholarly work public more quickly is a significant enough benefit that it can bear the risk of a few rough edges. At the same time, perhaps especially for literary scholars whose work revolves around the ways that words are put together into sentences and stories to create both meaning and beauty, I’m acutely aware of the power of a beautifully-written text. The care and precision with which we construct our arguments is, I think, directly related to the ideas that we express. It’s useful to think about written style in terms of code, too, in which syntax and precision are strictly necessary to create a functioning program. Someone might prefer the flexibility of Perl or the comparative strictness of Python or C, but once she has chosen a language for the program, the corresponding rules must be followed. Precision isn’t an aesthetic choice in this case, but a requirement for functionality. All of this brings me back to my initial question: What is the place of beauty in scholarly writing? In a Twitter conversation with Kari Kraus, I floated three possibilities: It may be a core value to our scholarly enterprise; it may be a pleasant ancillary; or it may be a risky distraction. I haven’t yet mentioned the risk factor, but it’s part of what initiated this line of thinking in the first place. Scholarly writing is, at its core, about the creation and dissemination of new knowledge; if that is the goal, then perhaps the packaging shouldn’t matter. Jason Priem, co-founder of ImpactStory and a participant at the SCI meeting, worried that too much emphasis on polished grammar or design could serve as a choke point, preventing innovative ideas and arguments from reaching an audience. Scrutinizing the surface of the work, Jason argued, means that only those who have learned the codes afforded by elite education will see their work accepted as valuable, which potentially reinforces problematic classist limitations on the creation of new knowledge and lines of inquiry. The risk that Kari and I mentioned in our conversation considers a somewhat different angle. Rather than focusing on the rejection of good ideas that lack polish, we mused about the potential acceptance of weak arguments couched in beautiful prose. While I don’t think that this is an especially common problem in academic writing—I would love it if our problem was an excess of gorgeous prose!—it is plausible enough that it makes me pause when I think about whether beautiful writing could be considered a core value of scholarly work in the humanities. Ultimately, I think that beautiful writing is akin to precise, well-rehearsed movements in dance. The movements themselves are not sufficient to establish an interesting, cohesive work, but they are both elements of the piece’s beauty, and signposts indicating the care and work that are its foundation. The same is true with stylistic precision or fine visual design: they not only affect the audience’s encounter with the work, but also suggest the hard work and craftsmanship that have gone into it. Admittedly, that would mean that beauty is one part substance and one part signal, and I think there’s a fear that signals are mere dissimulation. But we’re affected by signals all the time, whether they are intended or not, and so we might as well be aware of the ways those signals are created and received. But what about the realities of contemporary scholarly production, in which editorial oversight and refinement are increasingly unavailable to scholars wishing to share their work as widely as possible? This is where a dose of cautious optimism comes in. As I’ve watched the innovative models of SCI’s partner projects— PressForward, MLACommons, and Scalar —I am hopeful that scholars will have more and more ways to participate in ongoing conversations about their work that lead to increased refinement. Post-publication review mechanisms, whether in the form of CommentPress or the multi-layered curation and editing of Digital Humanities Now and the Journal of Digital Humanities, provide (arguably) richer opportunities for a scholar to work through ideas with input from a community of peers. The resulting work has the potential to be of higher quality than an article seen by only a few sets of eyes before its publication, and it is also likely to reach a wider and more diverse audience. In the end, to recycle my own tweet, I just want to read (and, ideally, produce) more beautifully-written work. I hope that we’re creating systems that make that possible, and cultivating values that reward it."},{"id":"2013-03-08-the-blind-leading-the-blind-a-noob-and-program-management","title":"The Blind Leading the Blind:  A Noob and Program Management","author":"claire-maiers","date":"2013-03-08 08:29:39 -0500","categories":["Grad Student Research"],"url":"the-blind-leading-the-blind-a-noob-and-program-management","layout":"post","content":"A better title for this post would be “The Blind leading the Slightly-less-blind.”  I thought it a little too wordy for a title, but it really does accurately express my experience of project management during these first few weeks of actually working on Prism. Figuring out how to manage a project when I know little of how that project will actually be completed has been daunting.  While most of our team has found a little niche for themselves—some area where they are more adept than the rest—I have found myself staring at the existing Prism code with that terrified-deer-in-the-headlights look.  This has left me uncertain of whether or not I can actually make decisions about the work plan for the rest of the semester. However, after several pep talks from both Bethany and Wayne and an incredibly beneficial tour of the Ruby code for Prism from Eric, I am starting to find my way.  I have a small sense of how our work will proceed this semester and how the larger pieces of the puzzle fit together.  I am hoping that as we move forward, this aspect of program management will get a bit easier. The other challenge for me lies in understanding my relationship to the rest of the group.  This comes both from my unfamiliarity with project management in general and from the special circumstances of our group.   Given that I am not producing any tangible code or changes, I have been feeling that I am not really contributing to the project.  This has made me feel somewhat uncomfortable with suggesting deadlines for others or checking in to see how they are making progress. In addition, I am sensitive to the fact that early in our time together as a team we discussed resisting traditional organizational structures.  This has occasionally left me even more uncertain of my role: am I a leader?  a liaison between smaller projects and tasks?  Should I make final decisions over things on which we do not reach consensus?  How do I hold my team members accountable without being authoritarian?   I am also happy to report that I have developed friendships with my teammates—but this can then lead to additional confusion when it comes to management. Hopefully by the time of my next post I will be feeling a little less blind.  Until then, your suggestions and comments are welcome!"},{"id":"2013-03-11-size-matters","title":"Size Matters","author":"kelly-johnston","date":"2013-03-11 09:28:47 -0400","categories":["Geospatial and Temporal","Visualization and Data Mining"],"url":"size-matters","layout":"post","content":"In geography, size matters.  On maps, large always wins over small.  We’re human.  We’re wired to quickly spot patterns and make visual comparisons.  See Tufte, Edward . Picture a map of your own state.  How does it compare in size to the states next door, the largest states, the smallest, or Texas? I recently joined with map-minded folks to build GeoTron 5000 to put the power of comparative geography and spatial literacy in hand.  Choose two places and the GeoTron 5000 robot spins up two maps to show exactly how those places compare. So what’s going on behind the scenes in GeoTron 5000 to enable these mappy comparisons? GeoTron 5000 houses an international map library based primarily on Natural Earth, a fantastic public domain vector dataset.  The Natural Earth maps were pre-processed using Quantum GIS geographic information systems software to present consistent comparisons of land area from California to Kyrgyzstan.  International country lists and official land areas were harvested from the United Nations Statistics Division via their World Statistics Pocketbook and Demographic Yearbook .  Domestic datasets are from the US Census . GeoTron 5000 is free at the Apple App Store and includes all 50 US States and the District of Columbia.  Additional geographies outside the USA are available for comparison via in-app purchase.  The app requires no cell service, no internet connection, and no international data plan when traveling. Travel is one of the best tests of our spatial literacy.  When away from familiar territory we can use the size of places we know well to better understand places we’ve never visited.  Travel guide books assume a high degree of spatial literacy when offering comparisons like “Germany is about half the size of Texas”.  But spatial thinking is best served when we choose familiar frames of reference.  For example, to understand the relative size of China’s Great Wall, HowBigReally.com displays the massive wall scaled and centered over any location, here Charlottesville, Virginia: Visualizing comparative size and shape requires skill in spatial thinking . Packing a suitcase, parking a car, finding a restaurant, finding your car when leaving the restaurant…all involve visualizing spatial relationships based on size, distance, shape, and changing points of reference.  Artsy  infographics  overlay the world on Africa and  popular television explores Mercator’s map distortions. We’re all thinking spatially every day. “Spatial literacy is the competent and confident use of maps, mapping, and spatial thinking to address ideas, situations, and problems within daily life, society, and the world around us.” - Diana Stuart Sinton, Geographer and Spatial Thinker Universities host spatial studies centers, organize spatial studies conferences, and offer graduate level training in spatial literacy .  And spatial literacy is a topic of growing academic focus beyond the higher ed classroom .  Kids love maps.  Using maps to illustrate comparative size promotes spatial thinking at an early age . “Spatial thinking can be learned, and it can and should be taught at all levels in the education system.” -  National Research Council Much of Edward Tufte’s brilliant work on visual literacy is centered around maps.  In Envisioning Information he writes of maps: “No other method for the display of statistical information is so powerful.” Scaled maps for geographic comparison using How Big Really or GeoTron 5000 inform spatial reasoning by answering the key question: compared to what? Size matters. [Cross-posted with johnston9494.blogspot.com ]"},{"id":"2013-03-18-prism-on-spring-break","title":"Prism on Spring Break","author":"chris-peck","date":"2013-03-18 07:38:31 -0400","categories":["Grad Student Research","Research and Development"],"url":"prism-on-spring-break","layout":"post","content":"Last week I was in Little Rock, Arkansas for the Society for American Music conference, but Prism seemed to be following me: [![peerless](http://static.scholarslab.org/wp-content/uploads/2013/03/peerless_little_rock-1024x764.jpg)](http://static.scholarslab.org/wp-content/uploads/2013/03/peerless_little_rock.jpg) peerless rainbow logo This logo looks uncannily like one of our sketches for Prism highlighting. So far I can’t find anything on the internet about this logo or what kind of company it might be for. It seems like it must be a paint or lighting company? In any case, I bet our use of this visual treatment of text is probably cooler. There’s something very compelling about refraction as a metaphor for collective interpretation of text. The crowd is a prism that reveals facets of the text, and the text is a prism that reveals facets of the crowd. The rainbow splayed across the text is an apt image for Prism’s aspirations for bringing text to life through accumulated interpretations."},{"id":"2013-03-19-speaker-series-dr-shawn-graham-on-practical-necromancy","title":"Speaker Series: Dr. Shawn Graham on \"Practical Necromancy,\" March 21","author":"eric-johnson","date":"2013-03-19 12:45:27 -0400","categories":["Announcements"],"url":"speaker-series-dr-shawn-graham-on-practical-necromancy","layout":"post","content":"On Thursday, the Scholars’ Lab will have the pleasure of hosting Dr. Shawn Graham of Carleton University to talk about simulation and agent-based modeling in the humanities–and a bit about his life as a digital humanist.  He has provided a preview of part one of his talk . Details: Thursday, March 21 2:00pm - 3:00pm Scholars’ Lab, Alderman Library 4th floor Practical Necromancy: Simulation and Agent Based Modeling in the Humanities Dr. Shawn Graham\nAssistant Professor of Digital Humanities, Department of History\nCarleton University Raising the dead presents certain difficulties, but computation suggests a way forward. In Practical Necromancy, Dr. Graham discusses the use of agent based simulations to understand aspects of Greco-Roman antiquity, its perils and potentials, and how all of this fits into a worldview informed by the digital humanities. Shawn Graham is a Roman archaeologist by training, and a digital humanist by accident. After many years in the academic wilderness, where he did everything from teaching high school, to starting up some heritage-based businesses, to hunkering down in the trenches of for-profit online education, he discovered that his interests in games for learning and teaching, and simulation more generally, had turned him into a Digital Humanist. He escaped to Carleton University in Ottawa, Canada, shortly thereafter, which also happens to be in his home town. His work surveys the ways new media are used to construct cultural heritage knowledge, from the perspectives of practicing archaeologists, historians, and the wider public. It’s an exploration of how ‘historical consciousness’ informs, and is formed by, digital media. He teaches primarily historical methods and digital history at all levels, including a graduate seminar in public history/digital history. In that class, the final project involves using various augmented reality platforms for public storytelling. Currently he is teaching a third year seminar on video games as historical artefacts which form a digital historical consciousness; the final project is the ‘perfect video game for expressing history, giving a voice to the voicelss’. His students blog at http://3812.graeworks.net and he would love to have your comments on their work."},{"id":"2013-03-20-announcing-the-praxis-network","title":"Announcing the Praxis Network","author":"katina-rogers","date":"2013-03-20 05:12:08 -0400","categories":["Announcements","Grad Student Research"],"url":"announcing-the-praxis-network","layout":"post","content":"How can humanities graduate programs better equip students for a wider range of careers, without sacrificing the core values or approaches of the discipline? We are delighted to announce the launch of the Praxis Network, a new partnership of innovative graduate and undergraduate programs that are making effective interventions in the traditional models of humanities pedagogy and research. The Praxis Network features graduate programs at the University of Virginia, Michigan State University, CUNY Graduate Center, University College London, and Duke University, as well as undergraduate programs at Hope College and Brock University . The partnership is one of three complementary projects in the Scholarly Communication Institute’s current work on rethinking graduate education . First, we are convening a series of experts’ meetings in conjunction with the Consortium for Humanities Centers and Institutes (CHCI) and centerNet, its digital counterpart, to discuss ways that traditional and digital humanities centers can effect change both within and across institutions. Second, we have conducted a study on the level of career preparation provided by graduate programs in order to assess the most important points of leverage. It is clear from the results, which will be published along with the data later this year, that most graduates and their employers find that they do not gain many of the skills that are important in their professional environments—such as collaboration, project management, and communication with varied audiences—through their graduate study. Finally, the Praxis Network provides a closer look at select programs that have taken unusual and effective approaches to addressing some of the issues that the survey uncovered. The goals of each unique program are student-focused, digitally-inflected, interdisciplinary, and frequently oriented around collaborative projects. The website, which is the first product of the partnership, takes the important step of sharing information about the commonalities and unique properties of these programs in a way that makes it easy to compare them. Humanities programs have the opportunity to better serve their students as well as the public by examining our core values and rethinking the methods we use to teach them. The Praxis Network programs show just a few possible ways to move toward **collaborative projects, public engagement, and embracing an ethos of openness and exploration. **"},{"id":"2013-03-20-head-graduate-programs","title":"Are you our new Head of Graduate Programs?","author":"katina-rogers","date":"2013-03-20 06:25:57 -0400","categories":["Announcements","Grad Student Research"],"url":"head-graduate-programs","layout":"post","content":"We are delighted to announce an exciting job opportunity here at the Scholars’ Lab as the Head of Graduate Programs, which includes both the Praxis Program and the Graduate Fellows in Digital Humanities program. Read on for more details! Head of Graduate Programs The University of Virginia Library seeks an experienced, versatile digital scholar and administrator to lead programs for graduate students in our internationally recognized Scholars’ Lab; home of the Praxis Program and a vibrant community of Graduate Fellows in Digital Humanities. The ideal candidate will have: deep familiarity with humanities scholarship and digital methods at the graduate level; an interest in experimental approaches to analysis, authoring, and publication; experience in teaching and administrative roles in higher education; and a commitment to the training of emerging scholars and alternative academic humanities professionals. Reporting to the Director of Digital Research and Scholarship for UVa Library, the Head of Scholars’ Lab Graduate Programs joins an accomplished and forward-looking digital scholarship team, and is eligible for the self-directed research time that all of our staff members are granted for professional engagement and to pursue their own, often collaborative, R&amp;D projects. Primary Responsibilities Mentoring, managing day-to-day operations, and coordinating staff support for both team-based and individual graduate fellowship programs at U.Va. Library. Developing intellectual programming in the digital humanities for the Scholars’ Lab and building community among emerging scholars at U.Va. Fostering collaboration on humanities training and research support with internal and external partners, including the Praxis Network . Knowledge, Skills, and Abilities Working knowledge of digital humanities technologies and directions. Strong interest in mentoring junior scholars from project conceptualization to published outcomes. Excellent communications skills, including the ability to present complex technical information to a generalist audience and a clear understanding of humanities perspectives and needs. Previous experience in higher education administration and experience in scholarly research, writing, and digital project development preferred. Education Graduate study (PhD preferred) in a field related to humanities scholarship or humanistic aspects of information science. Experience 4 to 7 years, with demonstrated ability as an instructor, mentor, writer, and researcher. Familiarity with development and delivery techniques for digital humanities content and software. Project management or supervisory experience highly desirable. Salary and Benefits Salary is commensurate with experience, and expected to range between approximately $65K and $75K per annum. Excellent benefits, including paid leave, TIAA/CREF and other retirement plans, along with generous funding for travel and professional development. For full details, and to apply for the position, please see the official posting . (If you need to search the Jobs@UVa portal, the posting number is 0611761). The University of Virginia is an Equal Opportunity/Affirmative Action employer strongly committed to achieving excellence through cultural diversity. The University actively encourages applications and nominations from members of underrepresented groups. Don’t miss a chance to work with our wonderful students and incredible Scholars’ Lab team!"},{"id":"2013-03-21-slab-out-about","title":"SLab Out & About","author":"ronda-grizzle","date":"2013-03-21 06:38:27 -0400","categories":["Announcements"],"url":"slab-out-about","layout":"post","content":"SLab folks get out and about! Here’s where we’ve been over the last few months: Bethany Nowviskie was invited to give the Japanese Association for Digital Humanities keynote address in Tokyo in September 2012 (“ Too Small to Fail ”) and to participate in one of Michael Bérubé’s MLA Presidential Forum  events on Avenues of Access at the 2013 Modern Language Association meeting . Her invited talk was called “ Resistance in the Materials .” In Boston, Bethany also participated in  roundtable discussion with fellow members of MLA’s Task Force on Doctoral Study. Last semester, she and David McClure were invited to give a lecture at the University of Maryland as part of MITH’s  Digital Dialogues series. That talk and demo, entitled Space, Time, and the Problem of Scale: Digital Storytelling with Neatline, took place in November 2012 and was repeated at UVa. This month, she will give a workshop, seminar, and public lecture as a Lansdowne visiting scholar at the University of Victoria. David McClure has been traveling here and there teaching and talking about Neatline . In addition to the talk with Bethany (which they repeated at the Scholars’ Lab), he and Jeremy Boggs led a web seminar for the National Institute for Technology in Liberal Education (NITLE) entitled Geotemporal Storytelling with Neatline . David also gave a two-day workshop at Beloit College and a lecture introducing Neatline at the Alabama Digital Humanities Center in October 2012 entitled Maps, Timelines, and Archives: Using Neatline to Plot Digital Collections in Space and Time . Katina Rogers guest taught a session at Matt Gold’s graduate seminar at CUNY entitled Debates in the Digital Humanities: Towards a Networked Academy and ran a session at THATCamp CHNM on graduate education reform. Katina has also guest blogged for the ProfHacker blog, taken part in a roundtable at MLA 2013 on rebooting graduate education  (along with SLab Fellow Annie Swafford), and had her work featured in the Chronicle of Higher Education ( #alt-ac survey results and graduate education reform ). In January 2013, Eric Rochester spoke about digital tools in the Association for Computers and the Humanities panel on interoperability entitled Open Sesame at MLA 2013. Nancy Kechner taught the Pharm D group at the UVa Medical Center to incorporate SPSS into their clinical research, allowing them to present their cutting edge results as Rounds and at conferences. Nancy’s also helped the Infectious Disease group to use tools provided by the SLab to further their research. What are we looking forward to next? Digital Humanities 2013 at the University of Nebraska-Lincoln on July 16-19!"},{"id":"2013-03-21-spring-newsletter","title":"Spring Newsletter","author":"ronda-grizzle","date":"2013-03-21 07:58:14 -0400","categories":["Announcements"],"url":"spring-newsletter","layout":"post","content":"Our Spring 2013 newsletter (pdf) is available for download . In this issue, you’ll find information about Chris Gist and Kelly Johnston’s GIS Workshop series, an update on the Scholarly Communication Institute’s activities, and a round up of what the SLab staff have been up to as we get out and about, spreading the digital humanities joy. We hope you enjoy it!"},{"id":"2013-03-28-installing-omeka-through-amazon-web-services","title":"Installing Omeka through Amazon Web Services","author":"cory-duclos","date":"2013-03-28 11:56:09 -0400","categories":["Research and Development"],"url":"installing-omeka-through-amazon-web-services","layout":"post","content":"This set of instructions was developed, in part, during the Digital Humanities Winter Institute at the University of Maryland. Under the direction of Wayne Graham, a small group set out to install Omeka through Amazon Web Services . The directions were put together collaboratively by David Kim and Cory Duclos. Unlike other Omeka installation guides, this one does not pretend to be “easy.” There are some technical abilities you will need, including being comfortable using a terminal. This guide was written using OS X, but if you are a Windows user, the Git Bash prompt that ships with Git should work. Ideally, you should be doing this install on your own, private computer so that you can add security permissions for future server access. Step 1 : Create an amazon web server account at aws.amazon.com . This will require entering a credit card and going through a phone verification process. Step 2 : After setup, click on “My Account”, then “AWS Management Console”, then find the link to EC2 (Virtual Serves in the Cloud) and click on it Then click “Launch Instance”: Select the “Classic Wizard” and click continue: Step 3: In Classic Wizard: Choose an AMI, select Ubuntu, 12.04.01 LTS  64Bit : Step 4 : In Classic Wizard: Instance Details, Continue with default settings through the next three settings (1 instances; T1 Micro, etc.) Step 5 : In Classic Wizard: Instance Details - Key/Value tables: Under Value add a name for your project: Step 6 : In Classic Wizard: KeyPair Create key pair, adding in a unique username (which will be used again in step 14). This will cause your system to download a .pem file . Leave it where it is for now. Step 7 : Classic Wizard: Configure Firewall From the drop down menu on the left, select select “HTTP” and click “add rule”. It will appear on the right. Step 8 : Classic Wizard: Review Click Launch then Close in the next window. Step 9 : The easy part is over. Step 10 : Back in the EC2 dashboard, click on “0 Running Instances” Step 11 : In the subsequent panel, RIGHT click on the code under “ instance ” and select ‘ connect ’. Step 12 : Select “ Connect with standalone SSH Client ” do not close window yet. Step 13 : Open the terminal (Applications/utilities/terminal for OS X users); note that your color scheme may vary from these screenshots. Create a new directory called ec2 by running command mkdir -p ~/.ec2 Step 14 : run mv ~/Downloads/[username].pem ~/.ec2 The username is what you generated in Amazon EC2 in step 6. This moves the pem file to the new directory named .ec2 Step 15 : Enter and run the command: chmod 400 ~/.ec2/*.pem Step 16: Back in your web browser, copy the line in the box labled “ Enter the following command line: ” (begins with ‘ ssh ’) and paste it in terminal. If it tries to run by itself, you copied the return. Hit the up arrow to call back the command so that it can be edited. You won’t run the code exactly as you copied it. After -i add ~/.ec2/[username].pem Change amazon to ubuntu so that it looks something like the last line here: Step 17: run sudo apt-get update to update the server libraries. This command will generate a wall of text. Step 18: run sudo apt-get upgrade . Type “ Yes ” when prompted. Your server is up and running. Now we can shift our attention to installing all the libraries Ubuntu needs to run Omeka. Step 19: Installing Server Packages After the server is up and running, we need to get the components that are needed to run a web server installed. I’ll use a short-hand here to install a bunch of packages (and their dependencies). Then tell the Apache daemon to enable the mod_rewrite module that Omeka uses to make “pretty” URLs. NOTE: When logging onto the AWS server, you may be put into the ubuntu directory. You need to get to the main directory. Change the directory and verify by listing the files in that directory. cd /\nls sudo apt-get -y install apache2 php5 php5-xsl php5-mysql php5-curl mysql-server zip imagemagick sendmail\nsudo a2enmod rewrite As part of the installation process, you’ll be asked to create a ‘root’ account for the MySQL server. Just remember whatever you use for this account as you’ll need it later to create the database and user for Omeka. After the installation and configuration has finished, you can test that the web server is running by pointing your browser at the server name you have (it’ll be something like http://ec2_123.345.56.78/, whatever the server connection in the AWS panel is). If everything is correctly to this point, you should see a page in the browser that says “It Works!” Step 20: Download Omeka The default location for the web applications for Apache2 is /var/www/ . For the purposes of this tutorial, we’ll download the Omeka application and mv the files here. Assuming you’re still logged on to your server, you will need to issue the following commands to download Omeka: cd /tmp\ncurl -O http://omeka.org/files/omeka-1.5.3.zip\nunzip omeka-1.5.3\nsudo mv omeka-1.5.3 /var/www/omeka\nsudo chmod -R 777 /var/www/omeka/archive Configure MySQL: Assuming you’re still on the server you’re wanting to run Omeka on (and you’re not wanting to mess with the Amazon RDS), you will need to configure the MySQL database to create a user, a database, and allow the user to work with the database locally. You’ll be writing this password to the filesystem, so whatever password you choose, you don’t really need to remember what the password is, just where it’s at. For this reason, I recommend using a password generator (I use Strong Password Generator for these purposes). Whatever the password is, you will need to replace where I type ‘[your password]’ in the following examples (and don’t type the ‘$’or ‘mysql’; these just differentiate the difference between the terminal and mysql prompts): $ mysql -u root -p\nEnter password:\nmysql&gt; create database omeka;\nQuery OK, 1 row affected (0.00 sec)\nmysql&gt; grant usage on *.* to omeka_user@localhost identified by '[your password]';\nQuery OK, 0 rows affected (0.00 sec)\nmysql&gt; grant all privileges on omeka.* to omeka_user@localhost;\nQuery OK, 0 rows affected (0.00 sec) Now that the database is set up, we need to let Omeka know where to go to connect to the database. Step 22: Editing the Omeka db.ini file If you went to the Omeka path on your system right now (e.g. http://yourEC2.instance/omeka), you’ll notice that there’s a big error on the page. We need to tell Omeka where to look for the database connection, and the tools for doing this can be kind of scary on the terminal. First get into you Omeka folder and find the db.ini file cd /var/www/omeka/\nls Then you can get in to edit this file using the sudo vim command. sudo vim db.ini You’ll see the contents of the db.ini file, and instructions to replace the X’s with your own information. This can be done by pressing the i, which will allow you to insert your own text as follows: host = \"localhost\"\nusername = \"omeka_user\"\npassword = \"(password generated in step 20)\"\ndbname = \"omeka\" Leave everything else the same. Hit control + c to exit the edit mode and :wq to write and quit out of the program. Now restart apache sudo service apache2 restart In your browser navigate to  http://yourEC2.instance/omeka and you should be ready to install. Step 23: Possible Error\nIt may be the case that you see an error about the mod_rewrite not being activated. To fix this, do the following: cd /etc/apache2/sites-available/\nsudo vim default Now you’ll see the default file. You need to change the allow from to all under &lt;Directory /var/www/&gt; using the vim commands you used to change the db.ini file (see image) Now you need to change the .htaccess file in the Install directory cd /var/www/omeka/install\nsudo vim .htaccess Find the instruction in this document which tells you to uncomment a line and add your own directory. Make the changes using the vim commands from above.   Then restart apache. sudo service apache2 restart Then navigate in your browser back to your site and you should be able to run the install. Step 22: Adding Plugins To add a plugin, navigate to the plugins folder in terminal, copy the link for the plugin download, and run the following commands. This will download a .zip file, unzip the file, and delete the original .zip file. The plugin should then be available in your omeka. The code below shows how to install the Neatline plugin, but this could work for any other plugin. `````` cd /var/www/omeka/plugins\nsudo curl -O http://omeka.org/wordpress/wp-content/uploads/Neatline-1.1.2.zip\nsudo unzip Neatline-1.1.2.zip\nsudo rm Neatline-1.1.2.zip ``````"},{"id":"2013-03-29-2013-14-dh-fellowship","title":"Call for Applicants: UVa Graduate Fellowship in DH","author":"eric-johnson","date":"2013-03-29 10:17:58 -0400","categories":["Announcements","Grad Student Research"],"url":"2013-14-dh-fellowship","layout":"post","content":"The Scholars’ Lab is proud to host prestigious fellowship program for ABD graduate students doing significant and innovative work in the digital humanities at the University of Virginia. Supported by the Jeffrey C. Walker Library Fund for Technology in the Humanities, the Matthew &amp; Nancy Walker Library Fund, and a challenge grant from the National Endowment for the Humanities, the Graduate Fellowship in DH is designed to advance the humanities and provide emerging digital scholars with an opportunity for growth. In collaboration with other Library departments, the Scholars’ Lab offers Grad Fellows consulting services and assistance with the creation and analysis of digital content, as well as advice on intellectual property issues and best practices in digital scholarship and DH software development. Fellows join our vibrant community, have a voice in intellectual programming for the Scholars’ Lab, make use of a dedicated grad lounge, and participate in one formal colloquium at the Library per semester. Applications for the 2013-14 fellowship year are now being accepted.  Please see the DH Fellowship page for more information about the program, eligibility requirements, and application information. Deadline: April 15th! Please contact Eric Johnson, Head of Outreach &amp; Public Services at the Scholars’ Lab, with any questions."},{"id":"2013-03-29-seeking-praxis-fellows","title":"Call for Applicants: Praxis Program Fellows","author":"eric-johnson","date":"2013-03-29 10:18:23 -0400","categories":["Announcements","Grad Student Research"],"url":"seeking-praxis-fellows","layout":"post","content":"UVa grad students! Apply by April 15th for a unique, funded opportunity to work with an interdisciplinary group of your peers, learning digital humanities skills from the experts, and collaboratively designing and executing an innovative digital project. Each year, the Scholars’ Lab Praxis Program provides six UVa graduate students with hands-on experience in digital humanities collaboration and project creation. Team members work to take a shared project from conception to design and development, and finally to publication, community engagement, and analysis. Praxis is a unique and well-known training program in the international digital humanities community. Our fellows blog about their experiences and develop increased facility with project management, collaboration, and the public humanities, even as they tackle (most for the first time, and with the mentorship of our wonderful faculty and staff ) new programming languages, tools, and digital methods. The 2012-13 Praxis cohort is in full swing, thanks to a generous grant from the Andrew W. Mellon Foundation, through UVa Library’s Scholarly Communication Institute (SCI).  The 2013-14 cohort will be supported by the University of Virginia Library .  Recently, the Scholars’ Lab joined with like-minded institutions to create a recently-launched Praxis Network, made up of allied but differently-inflected humanities education initiatives from around the world, mainly focused on graduate training, and all engaged in rethinking pedagogy and campus partnerships in relation to the digital humanities. A new Head of Graduate Programs will join us in 2013, working closely with Scholar’s Lab Grad and Praxis fellows . We will welcome six new, competitively-selected Praxis students in late August 2013 . Each will be awarded $8000 in fellowship funds, and will be expected to devote approximately 10 hours per week in the fall and spring semesters, to learning together and building a collaborative digital humanities project in the Scholars’ Lab. Fellows join our vibrant community, have a voice in intellectual programming for the Scholars’ Lab, and can make use of a dedicated grad lounge. All University of Virginia graduate students working within or committed to humanities disciplines are eligible to apply to join the 2013-14 cohort. We particularly encourage applications from women, LGBT students, and people of color, and will be working to put together an interdisciplinary and intellectually diverse team. Information session, Monday, April 8th Please join Scholars’ Lab staff and current Praxis Fellows on Monday, April 8th, for a Q&amp;A session on the program, to be held in the Scholars’ Lab at 4:00 p.m. RSVP with Eric Johnson, Head of Outreach &amp; Public Services at ej9k@virginia.edu . If you’re unable to join us for the session but have questions about the program, don’t hesitate to be in touch. Application deadline: Friday, April 15th The application process is simple: direct an email to Eric Johnson at the address above. Please indicate why you’re interested in the Praxis Program, what you think you will gain from it, and what you feel you would bring to a collaborative digital humanities project. A small committee, consisting of Scholars’ Lab faculty and staff and past Praxis Program participants, will evaluate expressions of interest and schedule group interviews with finalists."},{"id":"2013-04-01-a-million-likes","title":"Scholars' Lab Campaigns for a Million \"Likes\"","author":"bethany-nowviskie","date":"2013-04-01 05:08:43 -0400","categories":["Announcements"],"url":"a-million-likes","layout":"post","content":"TRANSCRIPT A new social media campaign is taking Facebook by storm! Staff of the Scholars’ Lab, a prominent University of Virginia-based academic technology center, have gone viral with an adorable picture and charming plea. This is not the first time the SLab has captured the collective imagination of the international digital humanities community – but it may be the last! [PHOTO: Smiling staff gather around a whiteboard that reads: “If the SLab page gets a million “likes,” Bethany will buy us an EMP doomsday device! Zap! Lights OUT! (Sci-Fi 4-EVA) Bye-bye, DH! (She doesn’t think we can do it!) Please LIKE!”] SEAL: It all began in the early hours of March 28th, when Wayne Graham, head of Scholar’s Lab R&amp;D, received a call from systems administrators informing him he was to spend the next 16 hours tracking down the source of unusual activity on one of the lab’s servers. GRAHAM: I just couldn’t BEEEP_ing believe the BEEP BEEEP BEEP-BEEP_ers BEEP BEEEP_ed the BEEP BEEP BEEEP BEEP -box again._ BEEEEEEP. Meanwhile, Ronda Grizzle, who handles communications for the SLab, was waking to a message of her own. [CLOSEUP: Inbox. Email title reads: “Re: Re: RE: Newsletter Draft 47b-rev6 - just 1 more little tweak, plz!”] In a characteristic display of Scholars’ Lab camaraderie, Neatline developer David McClure (who quite possibly sleeps at his desk) was waiting to comfort Grizzle on her arrival at the office. But things went rapidly downhill. Investigating “ongoing, illicit humanities crowdsourcing / sweatshop activities,” student reporters from the Cavalier Daily delivered a FOIA request to stunned graduate fellows in the SLab Grad Lounge, site of the Library’s innovative Praxis Program . At the same time, a malfunctioning Ron Swanson chat-bot attached to the SLab’s IRC channel began to exhibit what Brooklyn-based staffer Dr. Katina Rogers called “disturbing signs of sentience.” ROGERS: [via chat] or maybe I’ve just been hanging out in here too long &lt;j/k&gt; waynebot++ #slab IRC FTW! By the end of what some observers called a “not atypical” Thursday, emergency personnel had been called to Alderman Library twice (first treating administrative assistant Becca Peters for severe paper-cuts incurred while processing fifty thousand Scholarly Communication Institute travel reimbursements, and later hurriedly wheeling a large, struggling man, prostrate and bound, to a waiting, unmarked van). Colleagues identified the man as Design Architect Jeremy Boggs. UVa Today was on the scene for this incident, and approached public services chief Eric Johnson for comment. JOHNSON: [sitting morosely beside a 3d-printer adorned with Scholars’ Lab stickers] He’s been trying to level the build-plate. I – I just can’t talk about it! [sobs] The same afternoon, GIS specialists Chris Gist and Kelly Johnston saw their popular program in “ DIY aerial photography ” crash to earth, as Charlottesville city council members authorized aggressive counter-measures to halt the Scholars’ Lab’s use of a helium balloon and small, mechanized quad-copter to photograph archaeological sites and public art installations at the request of UVa faculty. GIST: A sling-shot, Councilman? Seriously? JOHNSTON: [angrily] That was NOT a drone! But it was in his support for the lab’s core activity – enabling UVa faculty and students to perform research and scholarship using new technology – that Senior Developer Dr. Eric Rochester found a way forward. ROCHESTER: I was painstakingly refining our web-based approach to delivering scanned historical maps to scholars for analysis and display, when my fingers slipped. Delete! It was… life-changing. His colleagues agreed, and Scholars’ Lab director Dr. Bethany Nowviskie issued a challenge. One packet of dry-erase markers and a Facebook post later, the campaign was underway. We caught up with Nowviskie, to hear her response to staff ambitions to destroy the digital humanities once and for all, using a devastating global electromagnetic pulse from a “doomsday device.” NOWVISKIE: Well, it’s important to say that I’m still waiting to hear from Procurement about whether I can put equipment capable of forever ending worldwide electronic communications on my UVa VISA card. But, to speak to the viral aspects of the project: perhaps I should have seen this coming. After all, a few years ago I had the experience of popularizing a Twitter hashtag among DHers and so-called alt-academics which, by some accounts, sparked an important movement in higher education. SEAL: You refer, of course, to #fortranDH . NOWVISKIE: The wave of the future. But not according to her staff and their many supporters. At last count, the winsome smiles and can-do attitude the team exhibited on Facebook had garnered their SLab page a whopping 323 “likes” – well on the way, say Scholars’ Lab-trained digital humanities librarians Dr. Alex Gil (Columbia U) and Jean Bauer (Brown University), to the winning million. BAUER: I clicked! And so have all my friends and colleagues, who work unceasingly to re-mediate, describe, and contextualize humanities texts and artifacts; to collect, visualize, and preserve born-digital information; to develop and test new analytic methods and modes of scholarly communication; and to use technology to question received notions of canonicity and expert knowledge. We see this as a game-changer – a once-in-a-lifetime opportunity for the entire field of DH. GIL: [from atop the barricades] Faculty, staff, and students! The time has come! I really, really, really need a nap. UVa Library grants officer Raylon Johnson could not be reached for comment. Friends in Financial Services indicated he may be headed “somewhere tropical,” adding, “Don’t worry about Ray.” Long-term humanities computing practitioners identify the past sixty years of interdisciplinary, technology-assisted scholarship as a pragmatic and intellectual struggle toward new interpretive paradigms for historical understanding and the ongoing study of human arts and cultures. Others see the digital humanities as a fresh, recently-emerging, sweeping trend – the academy’s “next big thing.” But thanks to the Scholars’ Lab at the University of Virginia Library, a different consensus is building in social media: when it comes to DH, we’re almost done. ** * * END TRANSCRIPT * * **"},{"id":"2013-04-01-scholars-lab-talk-archive-page-and-future-plans-songs-of-the-victorians-and-augmented-notes","title":"Scholars' Lab Talk, Archive Page, and Future Plans: Songs of the Victorians and Augmented Notes","author":"annie-swafford","date":"2013-04-01 11:29:28 -0400","categories":["Grad Student Research"],"url":"scholars-lab-talk-archive-page-and-future-plans-songs-of-the-victorians-and-augmented-notes","layout":"post","content":"[Cross-posted from Anglophile in Academia ] This week, I’ve been preparing for my  Scholars’ Lab   talk  on Wednesday, April 3rd at noon.  I’ll be speaking about  Songs of the Victorians   and  Augmented Notes  and demonstrating both of them.  Here’s the poster  Ronda Grizzle  designed for it:  I hope I’ll see you there if you live in the area!  There will be a podcast of the talk, and I’ll also put my slides up on my blog.  To help with the upcoming talk, I added the archive page for  Michael William Balfe’s “Come into the Garden, Maud” .  I’ll be adding the analysis page in the next few weeks. In terms of Augmented Notes development, I added a new feature that lets users upload multiple pages of a score.  Users can click on the “+ Add another page” link, and a new upload button appears: Over this coming week, I will try to add two new features: 1. When users click the submit button after setting the measure times, the measure time information will be added to a JSON file;  and 2. Once the previous feature is built, the site will output a .zip file with the html, css, and javascript files necessary for users to have their own very basic archive page like those in Songs of the Victorians.  Stay tuned for a blog post later this week with my slides from my talk!"},{"id":"2013-04-02-omniauth","title":"Omniauth","author":"brandon-walsh","date":"2013-04-02 07:46:22 -0400","categories":["Grad Student Research"],"url":"omniauth","layout":"post","content":"After several weeks of grinding away with the generous help of Eric and Shane, Omniauth works with our current setup. Now a user can arrive at our site and log into the system using their Facebook or Google accounts as an alternative to creating their own Prism-specific login. One significant difficulty we encountered was how to handle multiple authentications. In the default framework, the system creates a new user when you authenticate through a service. So, if a user clicks on Facebook authentication, that action creates a new user in the database. If that same user comes back and clicks Twitter instead, that user will then have a separate account unlinked to the first in the database. Rather than implement an authentication management system, we decided to add a disclaimer to the myPrisms page directing users to login with the correct accounts if they aren’t seeing the information that they expect. Another difficulty: initially we also wanted to include Twitter authentication, but Twitter returns only the user’s nickname instead of an email. This is a problem for Devise, the gem that we use to handle the logins more generally. Devise has emails pretty firmly integrated into their framework, and things got quite whacky when we tried to disable email as the verifying function for new users. We successfully did so, but each new success raised new problems related to the nickname/email interactions. At length, we decided just to go with Facebook and Google for now. Any future authentications that we implement will be chosen based on whether or not they return user emails. Next, I’ll be helping Shane move forward with allowing User Uploads."},{"id":"2013-04-17-matrix-time","title":"matrix time","author":"chris-peck","date":"2013-04-17 07:14:14 -0400","categories":["Grad Student Research"],"url":"matrix-time","layout":"post","content":"From a recent e-mail exchange between Praxis team members. Praxer 1: I’m befuddled. Praxer 2: Blah blah blah line 173 blah blah comment out the blah blah reroute the encryptions blah blah facet table blah ruby this ruby that blah rake db:somethingorother blah blah… Praxer 1: Wow…you’re totally living in The Matrix. You must be, like, seeing code  everywhere . Praxer 2: Yes, and Praxer 3 is my Mr. Smith . I feel like what I’ve been doing for the last couple of weeks with regard to Prism is too embarrassing to blog about. Who really wants to hear about me staring at fifteen lines of JavaScript for several hours before I realize that it doesn’t just look weird because I don’t know JavaScript very well, or because it’s actually not JavaScript but CoffeeScript, but because the code is using d3, even though there’s nothing that looks d3-ish on the page (I suspect because d3 would be the perfect thing to realize some of last year’s proposed visualizations: bar graphs and such from the word highlight frequencies)? All this so I can eventually get a JavaScript pop-up when I click on a word that displays the percentage of users who have highlighted that word for a particular facet. My life in Praxis this week has also been about context switching. Going back and forth between squinting at the JavaScript console and feeling  incompetent, and writing essays for my qualifying exams where I’m mustering my best performance of  mastery over specific subfields of contemporary music. I’ve noticed that programming requires a very different kind of time—or seems to anyway—than other sorts of work: reading, writing, or even composing. I can sit down to write or compose for twenty minutes and accomplish meaningful work. But with code it seems like several hours of uninterrupted time are often necessary. Even one hour is barely enough to get the development environment up and running and realize it’s broken. This morning it took two hours—and help from two of the slab’s excellent staff developers—just to get it up and running and realize that it was broken. I’m not trying to be a downer here, and I should be clear that I love programming and could do it all day if I didn’t have other things to do (like these scary exams). But time management for DH projects might be a particular challenge because of the different kinds of time required for the D and the H."},{"id":"2013-04-17-out-on-a-small-limb","title":"Out on a (Small) Limb","author":"brandon-walsh","date":"2013-04-17 06:24:45 -0400","categories":["Grad Student Research"],"url":"out-on-a-small-limb","layout":"post","content":"In writing the following post, I was struck by how close it felt to one Alex Gil wrote last year about a similar Prism disaster that he called the  herokulypse . Alex wrote his post on April 23 of last year, so we’re a few days ahead of schedule for apocalyptic project events. – Over the past few weeks the Praxers have been diligently working on their own pieces of Prism. Gwen and Cecilia have been making strides with design, Chris has worked on highlighting and now visualizing, Shane has been doggedly attacking user uploads, and I worked on pieces of Omniauth, design, and uploads. On Monday we made our first attempt to really put things back together, and the result was pretty horrific . My sense was that we had all broken Prism a little bit in our own special way. Some of these broken pieces weren’t really noticeable until we combined everything back together, when suddenly Prism became a big goopy mess. CSS was screwed up, the databases didn’t work right, migrations were failing, and the basic functionality of the highlight tool erupted as whole pages somehow morphed into copies of other pages. The crisis was mitigated thanks to the swift responses of Wayne and Jeremy, and things seem to be working again. A lot of this could have been mitigated, I think, if we had better coordinated our feature branches. Instead, a lot of us were working on a big feature branch: “user uploads” or “omniauth.” It would have been better to break those out into smaller pieces so that any damage from merging would be manageable and less the nightmarish event that ensued. An “omniauth” branch could become multiple branches, one each for Facebook, Twitter, Google, etc. Admittedly, some of the work that we have been doing has been of the sort that I don’t think you can really divide up easily. Shane and I have been working on refactoring the databases, and large scale changes like that seem like they should happen all at once. Even so, I think we could have found ways to organize our work better. This would have, of course, necessitated better planning on our part ahead of time. Surprise: the SLab gurus have been telling us all along to make more and smaller feature branches. Listen to your digital elders. They know all the things."},{"id":"2013-04-22-prism-in-my-unconscious","title":"Prism in my Unconscious","author":"cecilia-márquez","date":"2013-04-22 08:46:46 -0400","categories":["Grad Student Research"],"url":"prism-in-my-unconscious","layout":"post","content":"As we rapidly approach the re-launch of Prism  I am spending more and more of my waking hours concerned about CSS and the site redesign.  We’ve made a lot of exciting changes including making the site aesthetic that of a “scandanavian child’s room” (thank you Eric Rochester).  However, this week Prism has finally seeped into by unconscious. As I was reviewing code this week there were occasional notes from Wayne about places to fix a table or ideas on how to redo a page.  Aside from hearing a military drill sergeant in the code I thought nothing was out of the ordinary.  The next night I had a dream that the code was yelling at me.  I’m sure there are some deeper psychoanalytic implications of this but for now I’m going to take it as a good sign that I am fully integrated with the code.  We are speaking the same language, even if its only in my dreams. Its either that or Wayne Graham is haunting me."},{"id":"2013-04-23-humanities-unbound-careers-scholarship-beyond-the-tenure-track","title":"Humanities Unbound: Careers & Scholarship Beyond the Tenure Track","author":"katina-rogers","date":"2013-04-23 05:59:09 -0400","categories":["Research and Development"],"url":"humanities-unbound-careers-scholarship-beyond-the-tenure-track","layout":"post","content":"_[Cross-posted from my personal site .] I’ve had the privilege of talking about graduate education reform and career preparation for humanities scholars at several universities this spring, including Stanford, NYU, and the University of Delaware. I’ve adapted the following from those presentations. The full dataset from the study that I discuss will be available later this summer, along with a more formal report. Already familiar with the background of this project? Jump straight to the survey results._ Image source Graduate students in the humanities thinking about their future careers face a fundamental incongruity: though humanities scholars thrive in a wide range of positions, many graduate programs operate as though every PhD student will become a tenured professor. While the disconnect between the number of tenure-track jobs available and the single-minded focus with which graduate programs prepare students for that specific career is not at all new, the problem is becoming ever more urgent due to the increasing casualization of academic labor, as well as the high levels of debt that many students bear once they complete their degrees. Before I say anything more, though, I’d like to dispel a couple of associations that the title of my talk might call to mind. Source: image one ; image two First, I don’t intend to compare the humanities as a discipline to Prometheus, though at times the angst of the job market may make it feel like an apt image. I also don’t mean to imply that the humanities are unraveling, though there is a useful connection to be made with that language. You may recall a recent article in the Chronicle of Higher Education by Michael Bérubé, past president of the Modern Language Association, which was called “ Humanities Unraveled .” In the article, Bérubé refers to the humanities as “a seamless garment of crisis”—noting that every element, from the dissertation, to scholarly publishing, to time-to-degree, to labor issues, are so deeply interrelated and in such advanced states of disrepair that it feels impossible to pull on one thread without the entire system coming apart. Bérubé’s article does an excellent job of showing the ways in which these elements fit together, and why it’s imperative that we not lose sight of the system as a whole and the many points of intersection as we work to find solutions. As important as that is, it isn’t quite what I want the focus of this talk to be. Instead, the connection I want to make is something a bit more positive. I think that the discipline of the humanities should be disentangled—or, unbound—from the rigid academic pathway leading to the single goal of the tenure track job. Image source Instead of imagining graduate school as a pipeline, keeping everyone contained and moving in one direction to a pre-determined endpoint, what if we thought more about a sprinkler, with water exploding out in all directions? Image source It’s a rather simplistic metaphor, but I do think it’s a helpful image, especially since its outward spray is reminiscent of something that is not only practical, but that also brings joy. Rather than focusing academic work inwardly, exclusively within academic institutions, humanities programs should be preparing students for much more flexibility in terms of audience and engagement. Even traditional scholarship has a growing potential to reach a wider and less specialized audience as scholarly publishing increasingly moves to open-access venues. But that’s only the tip of the iceberg. Scholars are publicly working through ideas on blogs and on Twitter, reaching a range of people from personal contacts, scholars, and grad students to interested strangers. Digital humanities projects are often public-facing and open to specialists and amateurs alike. And of course, people working in environments like museums, libraries, and presses have tangible public impact. Some humanities disciplines, such as history, do have a strong tradition in public engagement; public history is a well-developed sub-discipline, and many scholars work in museums, government, archives, and so on. But even there, the public branch has often been considered distinct from—and, unfortunately, less prestigious than—the research-focused side of the discipline. The humanities must get more serious about its role relative to the public for a number of reasons. First, if we believe that our work can be a social good, with broad relevance to our cultures and societies, then we should be attentive to the public value of that work for reasons intrinsic to the work itself. Second, as public funding for the humanities becomes increasingly scarce, we must make a case for continued and strengthened support. If our work is not even visible, let alone relevant, to more than an academic audience, then we will have a weak case indeed. Third, it has long been the case that a large proportion of graduates engage in careers outside the professoriate, and yet those roles have typically been undervalued. We owe it to current and future graduate students to equip them for a broader range of roles, and to attribute to those roles the merit that they deserve. Embracing a broad spectrum of career options—in and around universities, as well as cultural heritage organizations, non-profits, government, and the private sector—won’t fix everything, but it would represent a significant step in the right direction. Graduate programs can help prepare their students for a much broader professional world than simply the professoriate, but lacking data or strong models, it can be extremely difficult for individual programs to know what kinds of changes would be effective, and also to make a case for those changes. That’s why the Scholarly Communication Institute embarked on its recent study of perceptions of career preparation among humanities scholars: to determine a baseline from which to make specific recommendations for curricular changes. In the absence of data that we thought was a crucial foundation for reform, SCI decided to survey what has come to be known as the “alt-ac” community. But first we had to think about how to answer a common question: what does alt-ac even mean? The term was coined—more or less accidentally—in a 2009 Twitter conversation between Bethany Nowviskie (of UVa and SCI) and Jason Rhody (at the National Endowment for the Humanities). Short for “alternative academic,” the term became useful shorthand to refer to jobs in and around the academy, but outside the professoriate. At the time, jobs tended to be classified as either “academic” or “non-academic,” with everything outside the professoriate being relegated to the “non” category. But many scholars—including Nowviskie and Rhody—were (and are) doing innovative, productive, thought-provoking work in all kinds of positions. “Alt-ac” was a way to capture the kind of intellectually-stimulating roles that at that time were being brushed aside as viable scholarly career paths. Nowviskie went on to create #Alt-Academy, an online volume of essays treating a range of topics related to the pursuit and development of these various careers. Because the term is not at all formal, it means different things to different people. While the concept is something that people are eager to talk about, and while the term itself has both expanded in meaning and proven useful beyond what was initially imagined, there’s also a certain degree of discomfort with the phrase. Some find that it perpetuates an unfortunate (and false) binary of career options or reinforces the primacy of tenure-track employment—which, of course, is exactly what it was meant to alleviate. Others simply don’t find that the term resonates for them, considering it too narrow, or redundant with existing categories. Still others have expressed concern that #alt-ac is being held up as an unrealistic panacea for the perpetually abysmal academic job market, yet without necessarily creating new jobs. All of these are valid critiques, and really, I would love it if the term became unncessary and we could simply speak of career choices without the looming dominance of the tenure track. Until that day comes, I tend to take a broad view of what “alt-ac” can signify, and I’ll continue to use it here as shorthand for a wide variety of career paths. I’m actually quite comfortable with the unsettled nature of the term, as I think that it helps us to continue having useful and important conversations. I highlight the disagreement just to say that because of the differing understandings of the term, before we could even begin to study the various constellations of alternative academic careers, we needed a better idea of who felt that the terminology resonated with the kind of work they were doing. To get a sense of what others mean by “alt-ac”, we took the exploratory step of creating a public database where people could voluntarily add their names to a loose, public community of other alt-ac practitioners. We built the database within the framework of the #Alt-Academy project in order to leverage the energy of existing conversations, then stepped back to see what the results would be. We quickly had over two hundred and fifty entries from individuals in many different positions. Browsing the database is a great starting point for grad students or others who want to see the kinds of work that humanists are doing in the world. It’s still open to new entries, so feel free to check it out and add your own name if you think of your work along these lines. Once we created the database, we moved on to the second, more formal phase of the study, which included two confidential surveys. The primary survey targeted people with advanced humanities degrees who self-identify as working in alternative academic careers—precisely the people who identified themselves in the database, and we so we used the database as an initial source of potential respondents. The secondary survey targeted employers that oversee people with advanced humanities degrees. By questioning both, we were able to gain a more well-rounded perspective that balances some of the challenges inherent in self-reported answers. A couple of things about the response rate highlighted important discrepancies. We set an initial goal of 200 responses on the main survey, and 100 on the employer survey. Responses to the main survey totaled nearly 800 when we closed it, for almost four times our goal. Interestingly, this is a much higher number than the public database saw, despite the fact that, first, the survey required a more significant investment of time, and second, it was open for responses during a much shorter window than the database, which is still open now. I suspect that this is in part due to a lingering discomfort with publicly stating that one has pursued a path outside the professoriate; in fact, when the database launched, a number of people contacted me to say that while they supported the project, they didn’t want advisors or others from their institution to know the kind of work they were doing. I find this incredibly distressing. The employer survey, on the other hand, attracted far fewer responses, totaling around 75. This is a significant finding in itself, as it shows a pronounced disconnect between the motivations of job seekers compared to employers in thinking through the issues at hand. The questions of career paths and graduate education reform are understandably much more urgent for those that have gone through the process and made any number of difficult decisions. Now, to turn to the data. Much of the data from the surveys will be unsurprising to people who have already been thinking about these questions. As we might anticipate, people report entering graduate school expecting to become professors; they receive very little advice or training for any other career; and yet many different circumstances lead them into other paths. But even if the broad contours of the results are unsurprising, the specific data is useful to ground the general anecdotal impressions that many of us have long shared. Indeed, one of the primary goals of the survey is to help move the conversation from anecdote to data. For instance, asked to identify the career or careers they expected to pursue when they started graduate school, 74% indicated that they expected to obtain positions as tenure-track professors. As you can see, that response far outpaces any others, even though respondents could select multiple options. (As an aside, that’s also why the results here and on many subsequent slides add up to more than 100% . Because so many alt-ac positions are hybrid, we didn’t want to limit people to one option in their responses.) What is perhaps more interesting is their level of confidence: of that 74%, 80% report feeling fairly certain or completely certain that professorship was the career they would pursue. Keep in mind that because we targeted alt-ac practitioners, none of the survey respondents are tenured or tenure-track professors; they are all working in other domains. So even among the body of people who are working in other roles, the clear expectation at the outset of graduate school was for a future career as a professor. These expectations are not at all aligned with the realities of the academic job market, and they haven’t been for some time. The labor equation for university teaching has continued to shift dramatically in recent years, with non-tenure-track and part-time labor constituting a majority of instructional roles . The 2011 report on the Survey of Earned Doctorates reported another alarming statistic: 43% of humanities PhD recipients have no job or postdoctoral commitment on graduation—and that’s for any commitment, including temporary or contingent positions. Many graduate students begin their studies without a clear picture of their future employment prospects. What this signals to me is that we are failing at bringing informed students into the graduate education system. But whose responsibility is it to provide information about career prospects? It seems clear to me that it’s something that must be addressed before graduate education begins, and then reiterated in the admissions process and throughout the program, ideally by a trusted advisor. To be honest, I see this as an ethical issue: it is deeply problematic to admit students to a program if their expectations for the program’s outcome are not accurate. This is even more true if students are admitted to unfunded positions and must incur debt as they earn their degrees. Deepening the problem, students report receiving little or no preparation for careers outside the professoriate, even though we’re at a moment when the need for information about a variety of careers is most acute. Only 18% reported feeling satisfied or very satisfied with the preparation they received for alternative academic careers. The responses are rooted in perception, so there may be resources available that students are not taking advantage of—but whatever the reason, they do not feel that they are being adequately prepared. Again, this probably comes as no surprise, but it does reveal that we have significant room for improvement. Many programs have access to two rich resources that they leave untapped: their own staff, and their own graduates. Image source An easy first step for universities to take would be to list all of their non-professorial staff members that hold PhDs. Stanford has done something like this, compiling information about people in the Stanford system (including the Press and Libraries) that are willing to serve as mentors for humanities graduate students . Stanford has also developed a speaker series that highlights scholars working in hybrid and non-faculty roles on campus. Tracking graduates is a little more complicated, but no less important. There may be confidentiality issues involved, but there are surely ways that positions could be reported in aggregate and supplemented with a few individual examples. The more significant hurdle is that because prestige continues to be linked to tenure-track placement rate, many programs either do not track the careers of anyone outside this category, or if they do, they do not publish the information they have for fear (I suspect) of tarnishing their reputation relative to other schools. But the vibrant alt-ac movement suggests that the time is ripe to measure prestige in other ways. I would think that programs would be eager to publicize the often high-level alt-ac careers of their graduates, and that such information could be a strong draw for prospective students. I’ve long wondered why graduate programs find it so difficult to track their former students, when development offices seem to be the very first to know the whereabouts of alumni. New research by a third-party consultancy, the Lilli Research Group, has shown that it’s possible to determine the professional outcomes of graduates with a surprising degree of accuracy, even using only public records, but I think that this work should be taken on internally. Programs simply must do a better job of knowing what kinds of work their graduates are doing. So, where are people working after graduate school? As you can see, instead of the professoriate, the respondents reported working in a number of different types of workplaces. A large majority of respondents categorized their positions as being within universities, libraries, and other cultural heritage organizations. Parenthetically, this is one area where the definition of alt-ac gets fuzzy; some people prefer to think of alternative academic careers as only those that are within the university. I’ve mentioned that I take a broader view. Personally, I think it’s useful to consider not so much a specific job or career, but rather an approach: a way of seeing one’s work through the lens of academic training, and of incorporating scholarly methods into the way that work is done. It means engaging in work with the same intellectual curiosity that fueled the desire to go to graduate school in the first place, and applying the same kinds of skills—be they close reading, historical inquiry, written argumentation, or whatever else—to the tasks at hand. Thinking this way encourages us to seek out the unexpected places where people are finding their intellectual curiosity piqued and their research skills tested and sharpened. The next slide might actually be a bit of a pleasant surprise. Despite a concern that encouraging graduate students to pursue more varied lines of employment pushes them into short-term positions with unstable funding, in fact relatively few respondents report this as their situation. Only 18% are in positions currently funded wholly or partially by grants, and, while this chart doesn’t show it, just 5% are in positions with a specified end date. That is not to say that alternative academic positions are a quick fix; after all, we’re not talking about creating new jobs where none existed, but rather about recognizing existing opportunities. I especially want to reiterate that issues surrounding the academic labor market are pervasive and serious, particularly as universities rely on contingent labor to an ever-greater degree. But there are good, solid opportunities available that should figure into the ways that we train and advise graduate students. And the public will benefit from the perspective and expertise of humanities scholars whose voice extends beyond the classroom or academic journal. The reasons that people pursue careers beyond the tenure track are varied and complex. Location tops the list, which makes sense as a contrast to the near total lack of geographic choice afforded by academic job searches. Beyond that, people report pursuing alt-ac jobs for reasons ranging from the practical and immediate—salary, benefits, family considerations—to more future- and goal-oriented reasons, such as the desire to gain new skills, contribute to society, and advance in one’s career. Many people filled in their own responses as well, with the most common trends in the open text field being a desire for greater freedom, dissatisfaction with what a faculty career would look like, and much more simply, the need to find a job. A note of urgency and, sometimes, desperation came through in a number of these responses. Keeping in mind that the employer sample was quite small compared to the main sample (and therefore less reliable), I’d like to look at some of the competencies that both groups considered important to the alt-ac positions that they hold or supervise. Some of the skills are core elements of graduate work, such as writing, research skills, and analytical skills. Both groups value many of the skills at similar levels; however, there are a couple of discrepancies. I find it particularly interesting that alt-ac employees undervalued their research skills relative to employers. I suspect that there are two reasons for this: first, there may be some activities that employees do not recognize as research because it leads to a different end result (such as a decision being made, rather than a journal article being published). Second, it may be a skill that has become so natural that former grad students fail to recognize it as something that sets them apart in their jobs. On the other hand, alt-ac employees tended to overvalue the importance of project management among the competencies that their jobs required. That said, project management actually tops the list of areas where alt-ac employees needed training, according to employers: To me, this suggests that employees overvalued the skill because they found it to be a challenging skill that they needed to learn on the job, and so it took on an outsized importance in their minds. Employers also cited technical and managerial skills as areas that needed training. While the importance of those two skills would certainly depend on the type of position, others, such as collaboration, are useful in almost any work environment. Even simple things, like adapting to office culture, can also prove to be surprisingly challenging if graduates have not had much work experience outside of universities. The good news is that all of the elements that make stronger employees would also be hugely beneficial for those grads that do go on to become professors. While students are generally well prepared for research and teaching, they aren’t necessarily ready for the service aspect of a professorship, which incorporates many of the same skills that other employers seek. Many of the skills can also contribute to more creative teaching and research. By rethinking their curricula in such a way that students gain experience in things like collaborative project development and public engagement, departments would be strengthening their students’ future prospects regardless of the paths they choose to take. It’s not surprising that employers find that alt-ac employees need training in skills like project management and collaboration. Employees themselves also recognize that these are by and large not skills that they acquire in graduate school. Even among those who felt that their skills in these areas were strong, they noted that they gained them outside of their graduate program—for instance, through jobs or internships. Of course, the core skills of graduate training—especially research, writing, and analytical skills—are highly valued by employers. These skills are the reasons that employers will often hire PhDs even if the degree is not strictly required by the position. It’s important that students don’t undervalue (or insufficiently articulate) the ways that graduate study already equips them for broader roles, particularly in the methods and generalized skills that are critical to the process. One thing seems clear: the persistent myth that there’s nothing but a single academic job market available to graduates is damaging, and extricating graduate education from the expectation of tenure-track employment has the potential to benefit students, institutions, and the health of the humanities more broadly. However, as long as norms are reinforced within departments—by faculty and students both—it will be difficult for any change to be effective. Again, low tenure-track employment rates are not a new problem, but as the survey responses show, departments by and large are not succeeding at providing accurate and realistic information to their students, and many graduates still feel stigmatized when they pursue different types of careers. For change to be possible, it’s essential that institutional norms and measures of prestige shift in favor of highlighting successful outcomes across a broader spectrum of possibilities. And it’s important that graduate programs begin exploring ways that they can better prepare their students while maintaining the integrity of humanistic scholarship. Image source Admittedly, that can seem an incredibly difficult barrier to cross without successful examples to emulate. Having good models can not only ease the technical challenges of establishing a new program—like building support, establishing a program’s structure, and so on—but can also alleviate the potential social challenges of striking out on a new path. To that end, SCI has just launched another project that we hope will be a useful complement to the survey data: the Praxis Network . The Praxis Network is a new showcase of a small handful of truly excellent programs that are already up and running. Each of these programs can be thought of as one possible response to the question of how to equip emerging scholars for a range of career outcomes without sacrificing the core values or methodologies of the humanities, and without increasing time-to-degree. Institutions that wish to explore making changes can really benefit from seeing existing models, but currently, finding information about, and comparing, some of these new programs can be quite challenging. There are a lot of reasons for this: for instance, they may be housed in different parts of their institutions—such as departments, traditional or digital humanities centers, or libraries. They are often, but not always, small and nimble. Most of all, they are all quite different, though there are trends and commonalities among them. We wanted to create a space that made it easy for people to see what makes these great programs tick, and we wanted to present the information in a way that makes sense for anyone, whether they are administrators, faculty, students, or the general public. The site doesn’t aim to be a comprehensive database of all praxis-type programs; instead, it’s meant to be a small cross-section of programs that are different enough from one another that together they give an impression of the landscape. The anchor of the network is UVa’s Praxis Program, which brings together interdisciplinary cohorts of six doctoral students to collectively build a single tool that will be useful for humanities research or pedagogy. In the course of the year-long fellowship, they learn technical skills and project management under the mentorship of the Scholars’ Lab research and development team. But they also learn innumerable “soft” skills as they navigate the creation of a group charter, determine their priorities, think through their disciplinary values and assumptions, blog about the process, and publicly launch the tool. Along with UVa’s Praxis Program, which Bethany Nowviskie directs, we selected a few differently-inflected programs to highlight. Most are graduate programs. These include: the Cultural Heritage Informatics Initiative, led by Ethan Watrall at Michigan State University; CUNY Graduate Center’s Digital Fellows, under Matt Gold; the joint MA/MSc program in Digital Humanities at University College London, led by Simon Mahony; and the new PhD Lab in Digital Knowledge at Duke University, run by Cathy Davidson and David Bell. We’re also working with two undergraduate programs: the Mellon Scholars program at Hope College, which William Pannapacker leads; and Brock University’s Interactive Arts and Science Program, under Kevin Kee. For the moment, the website is the main product, and sharing information is the main goal. We hope that the ideas found on the site prove to be both useful and inspiring. There’s a wealth of exceptional work represented here, and we hope that it’s presented in a way that makes it easy for people to understand and compare various aspects of each program. The mission of each program might be the category that has the greatest degree of similarity across institutions. The goals of each are student-focused, digitally-inflected, interdisciplinary, and frequently oriented around collaborative projects. One common tenet is learning in public and exposing the process, warts and all. Many of the students grapple with the notion that our learning happens in unexpected ways—including through failure—and come to a greater degree of comfort with exposing their own uncertainties. Because humanities students are socialized to show their work only once it’s polished, this can be a very uncomfortable experience, but it’s one that leads to particularly fruitful results. All of the programs encourage some kind of public engagement, whether through blogs, public-facing projects, or more traditional venues like conference presentations. Some of the programs include partnerships with local cultural heritage sites, tech companies, or other potential employers, which allows students to immediately see potential applications for the work they’re doing. We then look at structure, which shows a lot more variety. Some of the programs are fellowships that offer a stipend, while others require tuition; they may be as short as five weeks, or as long as three years. They may be credit-bearing or extracurricular, with requirements that are formal or loose. One thing worth noting here is that most of the programs are fairly small and competitive. This has some real advantages: students benefit from strong mentorship and close collaborations with one another in small cohorts. When other institutions move toward making praxis-like changes, they often take the form of these kinds of small, competitive programs. This is a great start, but I’d also love to see more movement toward broad-based change that touches entire departments. To make that happen, we would have to see strong advocacy for curricular change and the acceptance of new modes of scholarship for credit and degrees. This kind of change is happening, but it’s happening very slowly. For digital and other non-traditional work to be recognized as scholarship, universities need examples of existing work to point to. Importantly, the site highlights the research products that the students are generating through the course of the programs. They often look quite different from the typical seminar paper, and demonstrate not only rigorous scholarly work, but also a creativity and vibrancy not often found in standard papers. Highlighting this kind of work is aligned with the efforts of organizations like the Modern Language Association, which establishes guidelines and offers workshops on the evaluation of digital work for tenure and promotion. The site then focuses on the people that are the core of each program. Every single program is characterized by strong, dedicated leaders and curious, intelligent students, and is invariably colored by the interactions among them. This can be both an asset and a challenge, as programs that rest on the shoulders of just a few individuals need to think about questions of continuity. But creating something new requires risk-taking, which is often easier for an individual or small group than for an entire department. Resources may feel like one of the most significant barriers to implementing programs of this nature, so we’ve detailed the support structure of each. Money is a crucial aspect of this: some programs are grant-funded, of course, while others have hard budget lines within their institutions. Physical space and personnel are also key resources, and play an important role in the scope of the program. A program’s future goals depend on a combination of many of the factors above. In some cases, the programs hope to grow, either within their institution or by forming cross-institutional partnerships. In other cases, however, the program’s success depends on its smallness, so some do not wish to expand, but rather intend to focus on how to continue innovating while staying light-weight. And finally, the nuts and bolts of the programs. If you’re thinking of developing something like what you see on the site, the information in this section will help you to know some key questions, potential risks, and other things to keep in mind as you consider how to proceed. I think of each program in the Praxis Network as an instantiation of the kinds of innovative solutions that can alleviate the issues that the survey uncovered. Humanities programs have the opportunity to better serve their students as well as the public by really examining what our core values are, and rethinking the methods we use to teach them. The Praxis Network programs show just a few possible ways to move toward collaborative projects, public engagement, and embracing an ethos of openness and exploration. Praxis Network members will be meeting during the coming months to discuss what kinds of cross-institutional collaborations might be most effective. At the same time, SCI will convene meetings in conjunction with CHCI (the Consortium of Humanities Centers and Institutes) and centerNet, its digital counterpart, to further explore points of intervention and to develop potential pilot programs that could leverage the particular strengths of traditional and digital humanities centers. So, those programs are great examples to follow, but you might be wondering what you can do right now, especially if you’re a graduate student. Image source First, if you’re not getting the information you need from your program, seek it out. If nothing else, start with the resources that are available on my website. Second, connect with people. Browse #Alt-Academy and the database that I mentioned earlier. Find people that are doing interesting work and see if they’d be willing to talk with you about how they got there—many of us are, especially since our own paths were so often not what we expected them to be! Encourage your department to help set up mentorships with alumni from the department, or with scholar-practitioners working in the university. Engage in online networks that are relevant to your interests. Find a mentor that can support you. Third, think about the skills and experience that you already have, how you might frame it for different kinds of jobs, and what else you might need to be a competitive applicant for positions that interest you. Depending on your circumstances, you might carefully consider whether to take an external job for a year of your graduate program rather than work as a TA if you have limited job experience outside of teaching. Finally, there are lightweight training opportunities that you can pursue even if your home department is quite traditional. The Digital Humanities Summer and Winter Institutes are great examples of short programs that enable students (or faculty, staff, or anyone else) to pick up new skills while also building a strong network. Beyond what I’ve talked about here, so much more is possible within and across existing programs. SCI hopes that our current work will help begin to rise the tide of transparency and innovation more broadly."},{"id":"2013-04-24-plot-your-course-in-space-and-time-a-look-at-scholars-labs-neatline","title":"Plot Your Course in Space and Time: A Look at Scholars’ Lab’s Neatline","author":"ronda-grizzle","date":"2013-04-24 10:29:12 -0400","categories":["Geospatial and Temporal","Podcasts"],"url":"plot-your-course-in-space-and-time-a-look-at-scholars-labs-neatline","layout":"post","content":"Scholars’ Lab Speaker Series: Bethany Nowviskie &amp; David McClure Plot Your Course in Space and Time: A Look at Scholars’ Lab’s Neatline In January, Bethany Nowviskie, Director of Digital Research &amp; Scholarship at UVa Library, and David McClure, Web Applications Specialist in the Scholars’ Lab spoke about Neatline . Summary:\nWhat do you get when you cross texts and archival collections with rich, interactive maps and timelines? Neatline. Join us for a demo of current Neatline projects and upcoming features and to learn why Neatline emphasizes hand-crafted visualization and “small data” in a big-data world. Speaker Bio:\nBethany Nowviskie: Computing humanist/humane computationalist since 1996. Director of Digital Research &amp; Scholarship at the University of Virginia Library and Associate Director of the Scholarly Communication Institute . President, Association for Computers and the Humanities and current chair of MLA ‘s Committee on Information Technology. David McClure: Web Applications Developer on the Scholars’ Lab R&amp;D team, David graduated from Yale University with a degree in the Humanities in 2009 and worked as an independent web developer in San Francisco, New York, and Madison, Wisconsin before joining the lab in 2011. David is the lead developer on Neatline and works on research projects that use software as a tool to advance traditional lines of inquiry in literary theory and aesthetics. As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.23169448143/enclosure.mp3”]"},{"id":"2013-04-25-scholars-lab-speaker-series-gretchen-gueguen","title":"Scholars' Lab Speaker Series: Gretchen Gueguen","author":"ronda-grizzle","date":"2013-04-25 04:39:37 -0400","categories":["Podcasts"],"url":"scholars-lab-speaker-series-gretchen-gueguen","layout":"post","content":"Scholars’ Lab Speaker Series: Gretchen Gueguen Do Digital Archivists Dream of Electronic Records? Born Digital Collections in the Small Special Collections Library In February, Gretchen Gueguen, Digital Archivist in UVa Library’s Digital Curation Services unit spoke about the challenges to fundamental principles of the archival practice by electronic communication._. Summary:\nThe information age has ushered in the biggest changes in human communication since the rise of printed text. The dynamic and ephemeral nature of electronic communication presents stark challenges to the fundamental principles of the archival practice. Join us for a look at how the tradition of collecting and creating archives is facing this paradigm shift and how the historical record will be shaped for the future. As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.23237371565/enclosure.mp3”]"},{"id":"2013-04-26-digital-humanities-speaker-series-walter-sheidel","title":"Digital Humanities Speaker Series: Walter Sheidel","author":"ronda-grizzle","date":"2013-04-26 10:46:07 -0400","categories":["Podcasts"],"url":"digital-humanities-speaker-series-walter-sheidel","layout":"post","content":"Digital Humanities Speaker Series: Walter Scheidel Redrawing the Map of the Roman World In March, Dr. Walter Scheidel, Dickason Professor in the Humanities, Professor of Classics and History, and Chair of the Department of Classics at Stanford University, spoke about ORBIS, a tool developed at Standord that models the geospatial network of the Roman world. Summary:\nAncient societies were shaped by logistical constraints that are almost unimaginable to modern observers. “ORBIS: The Stanford Geospatial Network Model of the Roman World” ( orbis.stanford.edu ), for the first time, allows us to understand the true cost of distance in building and maintaining a huge empire with pre-modern technology. This talk explores various ways in which this novel Digital Humanities tool changes and enriches our understanding of ancient history. The Digital Humanities Speaker Series is co-sponsored by IATH, SHANTI, and the Scholars’ Lab . As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.23529002200/enclosure.mp3”]"},{"id":"2013-04-26-scholars-lab-presentation-using-juxta-commons-in-the-classroom","title":"Scholar's Lab Presentation: Using Juxta Commons in the Classroom","author":"dana-wheeles","date":"2013-04-26 06:18:18 -0400","categories":["Podcasts"],"url":"scholars-lab-presentation-using-juxta-commons-in-the-classroom","layout":"post","content":"Scholars’ Lab Speaker Series: Andrew Stauffer &amp; Dana Wheeles Using Juxta Commons in the Classroom _In February, Andrew Stauffer, Professor of English and Director of NINES, and Juxta Project Administrator Dana Wheeles spoke about using the newly released Juxta Commons in the classroom. Because the presentation was a show-and-tell, Dana has been kind enough to create this blog post as a companion to the podcast of their talk. As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU ._ [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.23530048967/enclosure.mp3”] [![Juxta Commons work space](http://static.scholarslab.org/wp-content/uploads/2013/04/workspace-300x136.jpg)](http://static.scholarslab.org/wp-content/uploads/2013/04/workspace.jpg) The Juxta Commons Work Space During our recent presentation of Juxta Commons in the Scholar’s Lab, NINES Director Andrew Stauffer and I showed a number of visualizations of texts collated within the interface of our newly-released application. Because this aspect of the presentation does not translate for an audience listening to the podcast audio, this blog post is meant as a visual companion to our talk. We began the presentation with a tour of the Juxta Commons workspace, from the library section at the top of the page (for managing your source files, witnesses and comparison sets) to the visualization pane that dominates the lower portion of the window. Using a set comparing Lewis Carroll’s Alice’s Adventures Underground with the more well-know Alice’s Adventures in Wonderland, Dr. Stauffer showed how the heat map overlays color over variants - the deeper the color, the more different the passage. He also showed how the histogram offers a more global view of the collation, and allows the user to target the regions with the most difference quickly, even for long documents. [![Alice Underground vs Alice in Wonderland: heat map and histogram](http://static.scholarslab.org/wp-content/uploads/2013/04/alice_set-300x170.jpg)](http://static.scholarslab.org/wp-content/uploads/2013/04/alice_set.jpg) Alice Underground vs Alice in Wonderland: heat map and histogram Dr. Stauffer also showed a comparison of two versions of D.G. Rossetti’s review, “ The Stealthy School of Criticism,” illustrating how the author toned down his rhetoric in the version published in the Athenaeum. [![Highlight of variant in Rossetti's text](http://static.scholarslab.org/wp-content/uploads/2013/04/creeping-300x114.jpg)](http://static.scholarslab.org/wp-content/uploads/2013/04/creeping.jpg) Highlight of variant in Rossetti's text When I took the podium, I chose to focus on other ways of utilizing Juxta Commons, from authenticating texts found on the web, to exploring the history of news items and Wikipedia articles. For example, a look at two versions of an article posted on the New York Times website in November shows that the same article might be drastically different 30 minutes after posting. A full compendium of the sets we shared can be found at the main Juxta blog at juxtasoftware.org . If you have any questions about Juxta or Juxta Commons, please visit us at our development list, or write us directly at technologies at nines dot org."},{"id":"2013-04-26-scholars-lab-speaker-series-shawn-graham","title":"Scholars' Lab Speaker Series: Shawn Graham","author":"ronda-grizzle","date":"2013-04-26 08:13:59 -0400","categories":["Podcasts"],"url":"scholars-lab-speaker-series-shawn-graham","layout":"post","content":"Scholars’ Lab Speaker Series: Shawn Graham Practical Necromancy: Simulation and Agent Based Modeling in the Humanities In March, Dr. Shawn Graham, Assistant Professor of Digital Humanities in the Department of History at Carleton University, spoke about using agent based simulations to understand aspects of Greco-Roman antiquity. Summary:\nRaising the dead presents certain difficulties, but computation suggests a way forward. In Practical Necromany, Dr. Graham discusses the use of agent based simulations to understand aspects of Greco-Roman antiquity, its perils and potentials, and how all of this fits into a worldview informed by the digital humanities. As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.23530048996/enclosure.mp3”]"},{"id":"2013-04-29-graduate-fellows-forum-joanna-swafford","title":"Graduate Fellows Forum: Joanna Swafford","author":"ronda-grizzle","date":"2013-04-29 05:04:19 -0400","categories":["Podcasts"],"url":"graduate-fellows-forum-joanna-swafford","layout":"post","content":"Graduate Fellows Forum: Joanna Swafford Victorian Songs and Digital Tools: Facilitating Sound Studies Scholarship Joanna Swafford\nPhD Candidate, Department of English\nScholars’ Lab Fellow 2012-2013 Respondent\nDr. Herbert Tucker\nJohn C. Coleman Professor of English\nUniversity of Virginia Summary:\nAlthough sound studies and interdisciplinary music and poetry scholarship have increased over the last decade, scholars have not had the digital tools necessary to make their auditory arguments accessible to a wider audience. This talk will present two tools built by Scholars’ Lab Fellow Joanna Swafford that will help change that: Songs of the Victorians, an archive and analysis of parlor and art song settings of Victorian poems, and Augmented Notes, a tool that will let scholars build their own interdisciplinary websites like Songs of the Victorians . As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.23538270765/enclosure.mp3”]"},{"id":"2013-04-30-team-praxis","title":"Team Praxis!","author":"cecilia-márquez","date":"2013-04-30 07:42:12 -0400","categories":["Grad Student Research"],"url":"team-praxis","layout":"post","content":"This past week we started a soft (internal) launch of the new Prism so we could begin to work out all of the kinks. This of course meant that we spent  a lot of time together working out last minute concerns.  This was a really different “finals” type experience than the one I am currently experiencing.  Finals in graduate school basically means that I find a quiet place and don’t leave until all of the papers have been written.  This crunch time collaboration felt exciting, dynamic, and fun.  It made me realize how much I have come to value the teamwork time that I get in Praxis each week.  Grad school can be isolating and socializing can feel like a distraction.  What was great about Praxis was that we were able to come together weekly in a work-related activity but really enjoy each other’s company. I have always known that I’m a social worker but this experience has confirmed that in order to be successful in my graduate career I will have to build a team of people around me who are willing and able to support me.  I am also increasingly thinking about what type of alt-ac careers would allow me to keep this team experience central to my work life.  I’m going to miss these weekly work/hang sessions."},{"id":"2013-05-03-one-day-of-praxis","title":"One day of Praxis","author":"shane-lin","date":"2013-05-03 11:07:23 -0400","categories":["Grad Student Research"],"url":"one-day-of-praxis","layout":"post","content":"Here is a bunch of photos from our most recent team meeting!      "},{"id":"2013-05-03-random-skills-check","title":"Random Skills: Check","author":"claire-maiers","date":"2013-05-03 12:25:12 -0400","categories":["Grad Student Research"],"url":"random-skills-check","layout":"post","content":"This week Cecilia and took a few hours and completed one of our goals for this semester: creating a tutorial video for using Prism.   Not only did this turn out to be surprisingly fun (due mostly to Cecilia’s amazing antics), but it also added a few more skills to the list of things we have learned through Praxis. First—though this might seem trivial– we got to draw upon out teaching skills, devising a plan for how to teach someone to use Prism in under three minutes.  I authored the tutorial, and thanks to some great ad lib from Cecilia, I think the tutorial manages to capture the playful attitude with which we have approached our work and which we hope Prism engenders. In addition, we learned how to use the iShowU HD software program.  Admittedly, this is a very user-friendly program which allows you to record an area on your screen as a video.  Still, it has left us with one more skill which we can confidently claim for our own. And finally, and perhaps most importantly, we got to use this awesome microphone.   Need I say more? The launch of Prism is just around the corner.  In the mean time, check out our  tutorial here !"},{"id":"2013-05-06-digital-humanities-speaker-series-alan-liu-rama-hoetzlein","title":"Digital Humanities Speaker Series: Alan Liu & Rama Hoetzlein","author":"ronda-grizzle","date":"2013-05-06 04:37:38 -0400","categories":["Podcasts"],"url":"digital-humanities-speaker-series-alan-liu-rama-hoetzlein","layout":"post","content":"Digital Humanities Speaker Series: Alan Liu &amp; Rama Hoetlein The History of Thought as Networked Community: The RoSE Prototype In March, Dr. Alan Liu, Professor in the Department of English at University of California, Santa Barbara,\nand Rama Hoetzlein, Project Scientist, spoke about their work developing the RoSE Prototype. Summary:\nWhat if bibliographies of past authors and works could be modeled as a dynamic, evolving society linked to today’s scholars and students?  What if scholars and students could add data about biographical, historical, and intellectual relationships to the bibliographical entries, thus using present-day crowdsourcing to make more socially meaningful the crowds of history?  And what if visualizations could help us actively “storyboard” intellectual movements and not just spectate them?  Alan Liu and Rama Hoetzlein present the conceptual framework and some of the discoveries and challenges of the RoSE Research-oriented Social Environment (in beta at the conclusion of a NEH Digital Humanities Start-up grant). The Digital Humanities Speaker Series is co-sponsored by IATH, SHANTI, and the Scholars’ Lab . As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.23700755297/enclosure.mp3”]"},{"id":"2013-05-06-on-tasks-large-and-small","title":"On Tasks Large and Small","author":"brandon-walsh","date":"2013-05-06 10:06:03 -0400","categories":["Grad Student Research"],"url":"on-tasks-large-and-small","layout":"post","content":"The biggest issue that I have faced with my work with the Praxis Program has to do with how I judge the difficulty of the tasks before me. I have proven to be singularly inadequate at distinguishing quick fixes with a large payoff from larger problems that would yield only small utility. This issue surfaced early on, when I boldly suggested that I could implement Omniauth in a week. Two and a half months later I am still trying to get it to work: the Omniauth code structure works more generally, but each authentication service is its own unique little snowflake, different from all the other snowflakes in the code breaking eccentricities that it brings to the table. We had to jettison Twitter early on because it would not return user emails, which are central to the way that Devise handles logins. Facebook worked on local rails servers, but something whacky is happening on staging (and in deployment, for that matter) so that it throws a redirect uri failure at us that did not appear before. I feel as though I could have pounded away at these problems for ages and gotten nowhere. But for every task like Omniauth there have been other jobs that seemed large but turned out to be much easier than expected. I implemented a destroy function on prisms last week in just a couple of hours ( Shane had already provided a lot of the scaffolding on an older database setup), which meant that I also had to implement user roles and permissions using cancan. Cancan felt like a bigger job than it wound up being, and the whole thing seems to be working in the feature branch now. Now a user can delete a prism that they have uploaded. Great success! I think this ability to distinguish between large and small tasks is something that can only come with time and experience. We Praxers may be unable to tell how much work a particular feature will require, but the SLab team has been incredibly helpful in that regard. Where we see mountains, they see molehills, and vice versa. And when you get right down to it, Prism seemed like the biggest mountain of all a couple months ago. Prism felt like an insurmountable task until last week when everything came together in a flurry of productivity. I am impressed with what we have done. Now, without warning, a couple of inspirational  climbing   videos ."},{"id":"2013-05-06-scholars-lab-speaker-series-alan-liu","title":"Scholars' Lab Speaker Series: Alan Liu","author":"ronda-grizzle","date":"2013-05-06 07:11:59 -0400","categories":["Podcasts"],"url":"scholars-lab-speaker-series-alan-liu","layout":"post","content":"Scholars’ Lab Speaker Series: Alan Liu 4Humanities: Values, Strategies, Technologies for Humanities Advocacy in the Digital Age In April, Dr. Alan Liu, Professor in the Department of English at the University of California, Santa Barbara, spoke about ways in which the skills and resources of the DH community can help advocate for the humanities. Summary:\nAlan Liu will present an informal talk exploring such issues as assessing values and narrative frames for communicating the worth of the humanities. He will also brainstorm next-generation methods for using digital/networked technologies to create material for public view of scholars’ normal research and teaching work. As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.23697875127/enclosure.mp3”]"},{"id":"2013-05-07-graduate-fellows-forum-david-flaherty","title":"Graduate Fellows Forum: David Flaherty","author":"ronda-grizzle","date":"2013-05-07 04:23:24 -0400","categories":["Podcasts"],"url":"graduate-fellows-forum-david-flaherty","layout":"post","content":"Graduate Fellows Forum: David Flaherty Mapping the British Vision of Empire in 1731 David Flaherty\nPhD Candidate, Corcoran Department of History\nScholars’ Lab Fellow 2012-2013 Respondent:\nDr. S. Max Edelson\nAssociate Professor, Corcoran Department of History\nUniversity of Virginia Summary:\nThe British Board of Trade, a bureaucratic body responsible for overseeing the 18th-century Atlantic colonies, had a broad geographic vision of the British Atlantic based on their extensive communication with colonies from Newfoundland to Honduras.  This project maps the Board’s correspondence for a single year, showing which points on the map it received information about and where those letters came from, in an attempt to illustrate that vision. As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.23698365867/enclosure.mp3”]"},{"id":"2013-05-08-drum-roll-please","title":"Drum Roll Please.....","author":"claire-maiers","date":"2013-05-08 12:06:32 -0400","categories":["Announcements","Grad Student Research"],"url":"drum-roll-please","layout":"post","content":"After many months of brainstorming, debating, dreaming big, getting down to business, panicking, refocusing, programming, and fine tuning, Prism is here! It has been a great journey, and I think my fellow Praxers would agree when I say that we have learned a lot.  Not only have we been introduced to the world of DH and received hands-on training in things like HTML, CSS, Ruby on Rails, database construction, and a smidge of JavaScript and Coffee Script, but we have also learned a great deal about working in a team environment and collaboration. Although I plan to share some of the lessons learned throughout this year with you all in a subsequent blog post, I thought I would take this opportunity to tell you about what we have done with Prism this year. Oauth:  With the new Prism, users are able to sign in through a variety of options.  We have retained the ability for users to create their own account on Prism.  However, users can also sign in through existing accounts with Facebook, Google, and Mozilla Persona.   Props go out to Brandon who tackled OmniAuth early on in the semester and then stuck with it despite many unforeseen hurdles! Redesign:  From the beginning of our meetings, we discussed creating an online environment that was playful and would invite users to explore and participate.  In tandem with our vision of playfulness, we wanted to design the site with special attention to the user interface.  We pictured a site where the design itself would direct users on how to interact and lead them through the workflow from Prism creation to visualization in a streamlined manner.  Manifesting this vision fell to Gwen and Cecilia, and they have done a remarkable job creating a beautiful and functional site. Database Refactoring: On perhaps a less glamorous note, Brandon and Shane (or “Brane” as I call them) have refactored the database for Prism.  This refactoring was crucial to some of the improvements we’ve made to visualizations, allowing for user uploads, and will hopefully allow for others to build upon our code more easily. User Uploads: Thanks to Shane and Brandon, Prism now also provides the ability for users to upload their own texts to the site.  Our hope is that this contribution allows for Prism to be deployed in a variety of classroom and scholarly settings.  As users create a new Prism, they have the option to either make their Prism public and available to anyone for highlighting or they can choose to make an unlisted Prism.  Unlisted Prisms are not listed in the Browse page, allowing users to limit participation to desired audiences.  I encourage you to check this feature out for yourself! A New Visualization:  This version of Prism also includes a second option for visualizing collaborative interpretation, the Winning Facet Visualization, developed by Chris.  This option allows users to see a combination of all the facet categories at once.  In addition, users can interact with the pie chart to see the exact break down of user highlights by category. There are many other smaller improvements that I have not listed here, so go take a look at Prism  and experience it for yourself! Before I sign off, I want to give a shout out to the SLab faculty and staff.   They invested many hours in our team and rescued each of us from a precipice of panic or frustration more than once this year.  None of this would have been possible without them.  I will have more to say on that later, but for now, I will just say THANKS!"},{"id":"2013-05-09-prism-for-play","title":"Prism, for Play","author":"bethany-nowviskie","date":"2013-05-09 11:31:20 -0400","categories":["Digital Humanities","Grad Student Research","Research and Development"],"url":"prism-for-play","layout":"post","content":"[cross-posted from nowviskie.org ] This week marks the release of a new version of Prism, a web-based tool for “ crowdsourcing interpretation,” constructed over the course of two academic years by two separate cohorts of graduate fellows in our Praxis Program at the Scholars’ Lab . Praxis fellows are humanities and social science grad students across a variety of departments at UVa, who come to our library-based lab for an intensive, team-based, hands-on experience in digital humanities project-work, covering as many aspects of DH practice as our practiced Scholars’ Lab staff can convey. (By the end of the year, our fellows have negotiated a project charter; learned to create and appreciate robust ontologies and database designs; programmed or at least hacked around in Ruby and Javascript/CoffeeScript; raised up a Rails scaffold and become competent in HTML/CSS; managed the versioning of open source code in GitHub and deployed staging and production instances of a project; made design decisions and analyzed and drawn conclusions about user-experience aspects of a real-world project; communicated the value of their work and grown more comfortable sharing it in iterations and open-access venues; honed their skills at speaking across disciplinary and professional lines; learned hard project-management lessons; expanded their contacts in the DH world; engaged in conversations about funding, academic personnel, professionalization, and broadened career paths for scholars; and had fun and survived it all.) Where our 2011-12 cohort of Praxis fellows laid the groundwork (resurrecting an old SpecLab game that evolved into the finest bit of vaporware never to be produced by the humanities computing community at UVa, and creating a multilingual, prototype system that allowed multiple readers to mark up a pre-set list of texts according to a shared vocabularly), our 2012-13 team had the opportunity to refine the concept into a usable, open-ended tool. Thanks to their work, it’s easy to create a Prism account (including by logging in via existing services) and launch your own markup games, by uploading texts and defining the facets available to readers for the kind of blunt-force, collaborative annotation Prism allows.  Users now have a catalogue of texts they’ve added to the system or participated in marking up, and can get a sense of the evolving, shared reading of those texts through two visualization modes – one new (showing a quantified breakdown of crowdsourced readings), and one (showing the affective frequency of reader agreement) refined. Best of all, Prism has become lovely and light.  A design refresh and attention to ease-of-entry should make it an attractive tool for classroom use, and for experimentation and play. Please try it out and let our students know what you think. (They are Claire Maiers, Sociology; Brandon Walsh, English; Gwen Nally, Philosophy; Cecilia Marquez, History; Chris Peck, Music; and Shane Lin, History – emerging scholars and scholar-practitioners to watch!)  We would be especially interested in pedagogical applications of Prism.  And, since next year’s Praxis cohort – soon to be announced – will be moving on to a new project (reviving and re-thinking another SpecLab classic, the Ivanhoe Game ), we also encourage developers to send pull requests for bug fixes and new features.  Much remains possible with the “crowdsourcing interpretation” concept at the heart of Prism, which one early reviewer called “potentially the beginning of a new research field.”  Further visualizations?  Image-based or non-textual approaches to collaborative markup?  Computational linguistic analysis based on comparison of crowdsourced readings to larger corpora?  The sky is the limit. For now, we’re just enjoying the way the new, bright, child-like design for Prism matches the current mood in the Scholars’ Lab grad lounge: “Look! We made this!”"},{"id":"2013-05-13-announcing-neatline-2-0-alpha1","title":"Announcing Neatline 2.0-alpha1!","author":"david-mcclure","date":"2013-05-13 06:38:10 -0400","categories":["Geospatial and Temporal"],"url":"announcing-neatline-2-0-alpha1","layout":"post","content":"[Cross-posted with dclure.org ] It’s here! After much hard work, we’re delighted to announce the first alpha release of Neatline 2.0, which migrates the codebase to Omeka 2.0 and adds lots of exciting new things. For now, this is just an initial testing release aimed at developers and other brave folks who want to tinker around with the new set of features and help us work out the kinks. Notably, this build doesn’t yet include the migration to upgrade existing exhibits from the 1.1.x series, which we’ll ship with the first stable release in the next couple weeks once we’ve had a chance to field test the new code. 45 minutes of Neatline 2.0 alpha testing, compressed to 90 seconds, set to Chopin. In the interest of modularity (more on this later), the set of features that was bundled together in the original version of Neatline has been split into three separate plugins: Neatline - The core map-making toolkit and content management system. NeatlineWaypoints - A list of sortable waypoints, the new version of the vertical “Item Browser” panel from the 1.x series. NeatlineSimile - The SIMILE Timeline widget. Just unpack the .zip archives, copy the folders into the /plugins directory in your Omeka 2.x installation, and install the plugins in the Omeka admin. For more detailed information, head over to the Neatline 2.0-alpha1 Installation Wiki, and take a look at the change log for a more complete list of changes and additions. We’re really excited about this code. Since releasing the first version last summer, we’ve gotten a huge amount of incredibly helpful feedback from users, much of which has been directly incorporated into the new release. We’ve also added a carefully-selected set of new features that opens up the door to some really interesting new approaches to geospatial (and completely non -geospatial) annotation. It’s a leaner, faster, more focused, more reliable, and generally more capable piece of software - we’re excited to start building projects with it! Some of the additions and changes: Real-time spatial querying, which makes it possible to create really large exhibits - as many as about 1,000,000 records on a single map; A total rewrite of the front-end application in Backbone.js and Marionette that provides a more minimal, streamlined, and responsive environment for creating and publishing exhibits; An interactive “stylesheet” system (inspired by projects like Mike Migurski’s Cascadenick ), that makes it possible to use a dialect of CSS - built directly into the editing environment - to synchronize large batches of records; The ability to import high-fidelity SVG illustrations created in specialized vector editing tools like Adobe Illustrator and Inkscape; The ability to add custom base layers, which, among other things, makes it possible to annotate completely non-spatial entities - paintings, photographs, documents, and anything else that can be captured as an image; A revamped import-from-Omeka workflow that makes it easier to link Neatline records to Omeka items and batch-import large collections of items; A flexible programming API and “sub-plugin” system that makes it easy for developers to extend the core feature set with custom functionality for specific projects - everything from simple JavaScript widgets (legends, sliders, scrollers, etc.) up to really deep modifications that extend the core data model and add completely new interactions. Over the course of the next two weeks, I’ll be writing in much more detail about some of the new features. In the meantime - let us know what you think! We’re going to be pushing out a series of alpha releases in pretty rapid succession over the course of the next couple weeks, and we’re really keen to get feedback about the new features before cutting off a stable 2.0 release. If you find a bug, or think of a feature that you’d like to see included, be sure to file a report on the issue tracker ."},{"id":"2013-05-14-arduino-rainbow-hack","title":"Arduino Rainbow Hack","author":"brandon-walsh","date":"2013-05-14 05:58:37 -0400","categories":["Experimental Humanities"],"url":"arduino-rainbow-hack","layout":"post","content":"The following was co-authored and co-hacked with Claire . Claire and I went to the Arduino Hackday hosted by the Scholars’ Lab on Friday. We had no idea what we were getting into, which made it all the more fun. Jeremy brought in a bunch of Arduino kits of all shapes and sizes, and various people went to work to see what they could make out of them. First Claire and I got a blinking light to work. From there we decided to skip several chapters to make a little music player using a piezo speaker component. At that point we went off the map, dreaming big and working off the page. We started out working with this schematic, but we wanted to expand things a little. I apologize in advance for whatever vocabulary gap there might be in the discussion below; it was my first time coding in C. For one, this set-up only uses a single octave major scale. We opened things up by expanding the range of tones the speaker could play, including both a chromatic scale and adding an extra octave. One difficulty here was in the way that way that accidentals would be read by the board. At first we tried to use standard notation – f sharp would be represented by f#. But the code finds the notes in the melody by reading character by character through an array. So when the computer sees “cc#”, it processes it as two ‘c’s and one nonsense syllable that it can’t process. There is probably a more elegant solution, but we got around this by associating the accidentals with new characters entirely C#/Db             =&gt;       l D#/Eb              =&gt;       m F#/Gb              =&gt;       n G#/Ab             =&gt;       o A#/Bb             =&gt;       p Next, we added an extra octave. To extend the range, we hard coded frequencies for the new notes according to the formula given by the original Arduino code: timeHigh = period / 2 = 1 / (2 * toneFrequency) We probably could have done this in a more dynamic way. Or, as Eric and Ronda showed, we could have just downloaded a tone library to do this for us. They were jamming out to the Star Wars theme while we were still trying to get things working.  But eventually, we were able to program in the opening of “Mary Had a Little Lamb,” Mozart’s Lacrimosa, John Coltrane’s “26-2,” and “Somewhere Over the Rainbow.” Here is where things got really ambitious, we came up with the idea to incorporate a RGB LED light that would change colors each time there was a note change in “Somewhere Over the Rainbow.” Rainbow lights for a song about rainbows. To get the light working we worked off a slightly modified version of this schematic, mashed in with the piezo tutiorial. With some quick help from Eric, we modified the light arrangement so that each generates a random RGB combination. We then synchronized this with the rhythm of the melody. In terms of the actual circuitry, we just split the digital signal so that it went out to two different parts of the bread board simultaneously and then fed information to both pins. Later we added a volume knob to reduce the maddening noise. You can see attached photos below, though you’ll have to meet us halfway with our diagram of the breadboard. A later, unsuccessful attempt to key each frequency to a particular color resulted in a light that got brighter or darker depending on the note. I think with a little more time I could fix that by hard coding particular RGB values to particular frequencies, but we were trying to do it dynamically by converting the frequency directly into an intensity value. Behold our final product! [gist id=5569210]"},{"id":"2013-05-14-interactive-css-in-neatline-2-0","title":"Interactive CSS in Neatline 2.0","author":"david-mcclure","date":"2013-05-14 08:23:53 -0400","categories":["Geospatial and Temporal"],"url":"interactive-css-in-neatline-2-0","layout":"post","content":"[Cross-posted with dclure.org ] Neatline 2.0 makes it possible to work with really large collections of records - as many as about 1,000,000 in a single exhibit . This level of scalability opens up the door to a whole range of projects that would have been impossible with the first version of Neatline, but it also introduces some really interesting content management challenges. If the map can display millions of records, it also needs utilities to effectively manage content at that scale. This often involves a shift from working with individual records to working with groups of records. When there are a million records on the map, it’s pretty unlikely that you’ll want to change the color of just one of them. More likely, that record will exist as part of a large grouping of related records (eg, “democratic precincts,” or “photographs from 1945”), all of which should share a certain set of attributes. There needs to be a way to slice and dice records into overlapping clusters of related records, and then apply bulk updates to the individual clusters. Really, this is a familiar problem - it’s structurally identical to the task of styling web pages with CSS, which makes it possible to address groupings of elements with “selectors” and apply key-value styling rules to the groups. Inspired by projects like Mike Migurski’s Cascadenick, Neatline 2.0 makes it possible to use a Neatline-inflected dialect of CSS to update groups of records linked together with “tags,” which can be applied in any combination to the individual records. Neatline Stylesheet Basics Let’s take a look at how this works in practice. Imagine you’re plotting results from the last four presidential elections. You load in a big collection of 800,000 records (200,000 precincts for each of the four elections), each representing an individual polling place with a point on the map. Each point is scaled to represent the number of ballots cast at that location, and shared red or blue according to which party won more votes. In this case, there are really seven different nested and overlapping taxonomies in the data. All of the records are precincts, but each falls into one of the our election seasons - 2000, 2004, 2008, or 2012 . And each precinct went either democrat or republican, regardless of which election cycle it belongs to. Each record can be tagged with some combination of these tags: Each of the groupings needs to share a specific set of attributes - and also not share some attributes that need to be assigned separate values on individual records. For example, all of the precincts - regardless of date or party - should share the same basic fill-opacity and stroke-width styles. All records in each of the groupings for the four election seasons need to share the same after-date and before-date visibility settings so that the records phase in and out of visibility in unison. And all republican and democratic records should share the same shares of red and blue. Meanwhile, none of the groupings should define a standard point-radius style, which is used on a per-record basis to encode the number of ballots cast at that location. Neatline-inflected CSS makes it easy to model these relationships. To start, I’ll define some basic styles for the top-level precinct tag, which is applied to all the records in the exhibit: Now, when I click “Save,” Neatline instantaneously updates the stroke-width and fill-opacity styles on all records tagged with precinct : Next, I’ll set the before-date and after-date properties for each of the for election season tags, which ensure that the four batches of records phase in and out of visibility in unison as the timeline is scrolled back and forth: Now, when I open up any individual record, the before-date and after-date fields will be updated with new values depending on which election the record belongs to: Last, I’ll define the coloring rules for the two political parties. First, the Democrats: Click “Save,” and all democratic precincts update with the new color: Auto-updating stylesheet values So far, we’ve just been entering hard-coded values into the stylesheet. This often makes sense for properties that have inherently semantic values (eg, dates). For other attributes, though (namely colors), it’s much harder to reason in the abstract about what value you want. For example, I know that I want the republican precincts to be “red,” but I don’t know off-hand that #ff0000 is the specific hexadecimal value that I want to use. It makes more sense to open up the edit form for an individual record and use the color pickers for the “Fill Color” field to find a color that looks good. And even for styles that can be reasoned about in the abstract, it’s often easier and more intuitive to use the auto-previewing functionality on one of the record forms to tinker around with different values. Once you’ve decided on a new setting, though, it’s annoying to have to manually propagate the value back into the stylesheets so that all of the record’s siblings stay in sync - you’d have to copy the value, close the form, open up the stylesheet, find the right rule, and paste in the new value. To avoid this, Neatline also automatically updates the stylesheet when individual record values are changed, and immediately pushes out the new value to all of the record’s siblings. Let’s go back to the election results. For the republican precincts, instead of pasting in a specific hex value for the fill-color style, we’ll just “register” fill-color as being one of the properties controlled by the republican tag by listing the style and assigning it a value of auto : When I click “Save,” nothing happens, since a value isn’t defined. Now, though, I can just open up any of the individual republican records, choose a shade of red, and save the record. Since we activated the fill-color style for the republican tag, Neatline automatically updates all of the other republican records just as if we had set the value directly on the stylesheet : And now, when I go back to the stylesheet, the fill-color rule under republican is automatically updated with the value that we just set in the record form: This also works for styles that already have concrete values. For example, say I change my mind and want to tweak the shade of blue used for democratic precincts. I can just open up any of the individual democrat -tagged records, pick a new value with the color picker, and save the record. Again, Neatline automatically replaces the old value on the stylesheet and propagates the change to all of the other democratic precincts."},{"id":"2013-05-16-neatline-1-1-3-maintenance-release","title":"Neatline 1.1.3 Maintenance Release","author":"david-mcclure","date":"2013-05-16 12:17:57 -0400","categories":["Geospatial and Temporal"],"url":"neatline-1-1-3-maintenance-release","layout":"post","content":"This morning, Kiyonori Nagasaki noticed that one of the remote API’s used by the Neatline 1.x releases went offline, which had the effect of breaking exhibits that included the SIMILE Timeline widget. To fix this, we just posted a 1.1.3 maintenance release that patches up the timeline problem and also includes a couple of other improvements: Disabled animated opacity transitions on WMS tiles, which were causing performance problems in recent builds of Chrome; Fixed a bug that was causing the map not to focus correctly when a record is selected that has a default focus position/zoom, but no vector geometry. Download Neatline 1.1.3 . Meanwhile, lots of activity on the Neatline 2.0 front - we’re almost done with a second alpha release, which gets us one step closer to a stable 2.0 release, which will include the migration to update existing installations from the 1.x series. Stay posted!"},{"id":"2013-05-17-graduate-fellows-forum-lydia-rodriguez","title":"Graduate Fellows Forum: Lydia Rodríguez","author":"ronda-grizzle","date":"2013-05-17 10:36:28 -0400","categories":["Podcasts"],"url":"graduate-fellows-forum-lydia-rodriguez","layout":"post","content":"Graduate Fellows Forum: Lydia Rodríguez The Time Has Come: Ethnography, Gesture Research, and Digital Technology Lydia Rodríguez PhD Candidate, Department of Anthropology Scholars’ Lab Fellow Respondent Dr. Eve Danziger Associate Professor, Department of Anthropology University of Virginia Summary: In Western societies, time is usually perceived as a linear progression of events, but not all cultures think about and experience time in this particular way. In this presentation I analyze the relationship among linguistic, conceptual, and cultural notions of time through ethnographic observation of spoken interactions in Chol, a Maya language spoken in Chiapas, Mexico. In particular, I describe how the concept of time is depicted in the spontaneous gestures which are produced in conversational exchanges among speakers of Chol Maya. I also discuss the role that digital technology has played in the collection and analysis of gestural data, and how digital tools can be used to complement and enhance traditional ethnographic research about temporal conceptualization. As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.23987868880/enclosure.mp3”]"},{"id":"2013-05-17-testing-asynchronous-background-processes-in-omeka","title":"Testing asynchronous background processes in Omeka","author":"david-mcclure","date":"2013-05-17 06:16:22 -0400","categories":["Research and Development"],"url":"testing-asynchronous-background-processes-in-omeka","layout":"post","content":"[Cross-posted from dclure.org ] I ran into an interesting testing challenge yesterday. In Neatline, there are a couple of controller actions that need to spawn off asynchronous background processes to handle operations that are too long-running to cram inside of a regular request. For example, when the user imports Omeka items into an exhibit, Neatline needs to query a (potentially quite large) collection of Omeka items and insert a corresponding Neatline record for each of them. Jobs extend Omeka_Job_AbstractJob and define a public perform method: And can be dispatched asynchronously by getting the job_dispatcher out of the registry and passing the job name and parameters to sendLongRunning : It’s easy enough to directly unit test the perform method on the job, but, since actual execution of the process is non-blocking, the jobs can’t be tested at the integration level in the ordinary manner. For example, I’d like to just dispatch a request with a mock item query, and check that the correct Neatline records were created. This can’t be asserted reliably, though, since there’s no guarantee that the job will have completed before the testing assertions are executed. The job itself is non-blocking, but the job invocation in the controller code is blocking, and can be tested pretty easily by replacing the job_dispatcher with a testing double and spying on the sendLongRunning method. Since this is a pattern that needs to be implemented in more than one test, I started by adding a mockJobDispatcher method to the abstract test-case class that mocks the job dispatcher and injects it into the registry: Then, in the test, we can just call this method to mock the dispatcher, assert that the dispatcher is expecting a call to sendLongRunning with the correct job and parameters, and then fire off a mock request to the controller action under test: This is a pretty good solution, but not perfect: The integration test is really asserting an intermediate step in the implementation of the controller action, not the end result - it tests that the job was called with certain parameters, not the final effect of the request. This opens up the door to false positives. For example, in the future, I might make a breaking change to the public API of the Neatline_ImportItems . Assuming I’ve changed the job’s unit tests to assert against the new API, the test suite would pass even if I completely forget to update any of the job invocations, since the integration tests are just asserting the structure of the invocation, not the final effects. I’ve encountered a version of this problem more than once, and I’ve never really found a good solution to it. Short of moving up to something like in-browser Selenium tests, or resorting to hacky execution pauses in the integration tests, has anyone ever come across a better way to do this?"},{"id":"2013-05-22-neatline-time-1-1-3-release","title":"Neatline Time 1.1.3 Release","author":"jeremy-boggs","date":"2013-05-22 05:00:12 -0400","categories":["Research and Development"],"url":"neatline-time-1-1-3-release","layout":"post","content":"Thanks to Neatline Time user nancymou, we’ve addressed a bug in Neatline Time where fields added by other plugins to the advanced search form were not recognized by Neatline Time. (In nancymou’s case, the other plugin was Exhibit Builder.) So we’ve put together a bugfix release, version 1.1.3, that addresses this issue. Download Neatline Time 1.1.3 to upgrade. We’re also wrapping up work on Neatline Time to make it compatible with Omeka 2.0 . There isn’t a tagged version yet, but if you’re interested in trying it out (and sending us feedback), feel free to check out the develop branch from our Github repository. You can also download a zip of the develop branch ; you’ll just need to rename the unzipped folder to “NeatlineTime”. If you do try it out, and run into any problems, feel free to add a ticket to our issue tracker . There’s also plenty of time to contribute a translation for Neatline Time, or any of the Neatline plugins!"},{"id":"2013-05-28-announcing-neatline-2-0-alpha2","title":"Announcing Neatline 2.0-alpha2!","author":"david-mcclure","date":"2013-05-28 09:30:41 -0400","categories":["Geospatial and Temporal"],"url":"announcing-neatline-2-0-alpha2","layout":"post","content":"[Cross-posted with dclure.org ] We’re pleased to announce Neatline 2.0-alpha2, a second developer-preview version that gets us one step closer to a stable 2.0 release! For now, this is still just an testing release aimed at engineers and other folks who want to experiment with the new set of features (for more information, check out the announcement for the first testing release ). Grab the code here: ** Neatline-2.0-alpha2 NeatlineWaypoints-2.0-alpha2 NeatlineSimile-2.0-alpha2 ** This revision fixes a couple of bugs and adds two new features that didn’t make it into the first preview release: A user-privileges system, which makes Neatline much easier to use in collaborative, multi-user settings like classrooms and workshops. In a lot of ways, this feature reflects an expanded focus for Neatline. During the first cycle of development last year, we were mainly focused on building a tool designed for individual scholars and students working on focused projects. In that setting - when just a handful of trusted collaborators are working on a project - it’s often not necessary to assign “ownership” to individual pieces of content to protect them from being changed or deleted by other users. Over the course of the last year, though, we’ve realized that there’s a lot of interest in using Neatline in a classroom setting, which introduces a new set of requirements. When 50 students are all building their own Neatline exhibits inside a single installation of Omeka, it would be easy for someone to accidentally edit or delete someone else’s work - there need to be guard rails to prevent users from modifying content that doesn’t belong to them. In Neatline 2.0-alpha2, we’ve added an ACL (access control list) that makes it possible to enforce a three-level user privileges system: * **Admin** and **Super** users can do everything - they can create, edit, and delete all Neatline exhibits and records, regardless of who they were originally created by.\n\n\n\n* **Contributor** users can add, edit, and delete their own exhibits, but can't make changes to exhibits or records that they didn't create.\n\n\n\n* **Researcher** users are denied all Neatline-related privileges - they can't create, edit, or delete any Neatline exhibits or records. This is a simple approach, but we think it addresses most of the basic patterns for classroom use that we’ve encountered here at UVa and elsewhere. If students are working on individual projects, each can be given a separate “Contributor” account, which allows them to create and update their own exhibits, but blocks them from changing anyone else’s work. If students are working together in groups, each group can be assigned an individual “Contributor” account, which allows group members to update each other’s work, but prevents them from making changes other groups’ exhibits. An exhibit-specific theming system that makes it possible to create completely custom “sub-themes” for individual Neatline exhibits. Before, it was possible to customize the layout and styling of the Neatline exhibit views by editing the Omeka theme, which would change the appearance of all the exhibits on the site. In many cases, though, individual exhibits have specific requirements. Depending on the content, it might be useful for different exhibits to have different page headers, typography, or viewport layouts; and it’s also really useful to be able to load exhibit-specific JavaScript files, which can be used to define custom interactions for individual exhibits. In this release, every aspect of an exhibit’s public view can be completely customized by adding an “exhibit theme” that sits inside of the regular Omeka theme. For example, if I have an exhibit called “Testing Exhibit” with a URL slug of testing-exhibit, I can define a custom theme for the exhibit by adding a directory in the public theme at neatline/exhibits/themes/testing-exhibit . With the directory in place, Neatline will automatically load any combination of custom assets: * If a `template.php` file is present in the directory, it will be used as the view template for the exhibit in place of the default `show.php` template that ships with Neatline.\n\n\n\n* All `.js` and `.css` files in the directory will be loaded in the public view. This makes it possible to break additional styling and JavaScript functionality across multiple files, which makes it easier to break complex customizations into smaller units. This gives the theme developer full control over the appearance and behavior of each individual exhibit, making it possible to build a extremely diverse collection of Neatline projects inside a single installation of Omeka. Check out the change log for more details. And let us know what you think!"},{"id":"2013-05-31-mla14-roundtable-on-the-praxis-network-rethinking-humanities-education-together-and-in-public","title":"MLA14 Roundtable on the Praxis Network: Rethinking Humanities Education, Together and In Public","author":"katina-rogers","date":"2013-05-31 08:53:07 -0400","categories":["Announcements"],"url":"mla14-roundtable-on-the-praxis-network-rethinking-humanities-education-together-and-in-public","layout":"post","content":"[Cross-posted from my personal site ] I’m delighted to announce that our proposed roundtable on the Praxis Network has been accepted for the 2014 MLA Convention . Here are the details: Session Proposal How can humanities programs better equip students for a wider range of careers, without sacrificing the core values or approaches of the disciplines? While not new, the question becomes more urgent as public funding for the humanities shrinks and the proportion of contingent faculty grows. Rather than see these pressures as threats, however, many programs see in them an opportunity to develop vibrant programs that take a broader view of possible methodological approaches, research products, and desirable career outcomes. The participants in this proposed roundtable are all members of the Praxis Network, a new international partnership of graduate and undergraduate programs that are making effective interventions in the traditional models of humanities pedagogy and research. They represent programs that are embarking upon collaborative, interdisciplinary, project-based approaches to humanities education. The Praxis Network features graduate programs at the University of Virginia, Michigan State University, CUNY Graduate Center, University College London, and Duke University, as well as undergraduate programs at Hope College and Brock University. By bringing together a collection of diverse programs that all aspire to similar goals of increasing the effectiveness of humanities educational practices and making their methodologies more widely applicable, we hope to spark ideas among institutions that are exploring similar initiatives. Each roundtable participant will give brief remarks to introduce their program, leaving substantial time for broader discussion and questions. The partnership is one of three complementary projects in the Scholarly Communication Institute ’s latest work on rethinking graduate education. A recent SCI study on the level of career preparation provided by graduate programs makes it clear that most graduates and their employers find that they do not gain many of the skills that are important in their professional environments—such as collaboration, project management, and communication with varied audiences—through their graduate programs.  The Praxis Network provides a closer look at select programs that have taken unusual and effective approaches to addressing some of the issues that the survey uncovered. Beyond preparing students for a broader range of careers, the Praxis Network programs also provide excellent models for the relevance of humanities scholarship in a changing public landscape. With federal and state funding for higher education facing tremendous pressure, making humanities scholarship meaningful to a much broader audience is critical. Fortunately, scholarly work is becoming increasingly available to a broader and less specialized public, whether through open-access journals, via blogs and personal websites, or as standalone digital projects. The programs in the Praxis Network address these two trends by encouraging students to develop public-facing projects that are accessible to non-specialists, without sacrificing disciplinary rigor. In fact, the students’ research output shows that encouraging students to think critically about their intended audience helps them to better grasp not only what is appropriate for the general public, but also what matters to their academic peers. Humanities programs have the opportunity to better serve their students as well as the public by examining our core values and rethinking the methods we use to teach them. Increased public engagement is not only valuable to general audiences, but also healthy for academic disciplines and for individual graduates. Still, a great deal of work remains before humanities departments will commonly evaluate their success through outcomes other than tenure-track job placement. For wide-scale change to be possible, programs must find it valuable to equip students for varied careers in universities, libraries, cultural heritage organizations, non-profits, government offices, and more. The programs in the Praxis Network show the tremendous potential of encouraging students to approach humanistic inquiry in new ways as the discipline moves toward embracing increased collaboration, meaningful public engagement, and an ethos of openness and exploration. Bringing together representatives of each program in a roundtable discussion will provide a fruitful opportunity for others in the humanities community to learn about the developments, to ask questions relevant to the goals and directions of their own institutions, and to spark new ideas for growth and change. Participant Bios David F. Bell, co-director of the Duke PhD Lab in Digital Knowledge, is Professor of French Studies at Duke University. His research focuses on nineteenth-century French literature and culture, including urban space and technologies of communication. He also works on the concept of tact as a discursive strategy, and on the notion of debt in French literature. He is the co-editor of SubStance. Matthew K. Gold directs the Digital Fellows program at the Graduate Center, City University of New York, and holds joint appointments there and at the New York City College of Technology. Gold is an Associate Professor of English at City Tech, while his roles at the Graduate Center include Director, CUNY Academic Commons; Advisor to the Provost for Master’s Programs and Digital Initiatives; and Acting Executive Officer, MA Program in Liberal Studies. Gold recently edited a collection of essays related to the digital humanities, titled Debates in Digital Humanities. Other projects include “Looking for Whitman”, a multi-campus experiment in digital pedagogy sponsored by two NEH Digital Humanities Start-Up Grants, Commons In A Box, funded by the Alfred P. Sloan Foundation, and JustPublics@365, funded by the Ford Foundation. Kevin Kee is Canada Research Chair of Digital Humanities and Associate Professor in the Department of History and the Centre for Digital Humanities at Brock University, where he also directs the Interactive Arts and Science program. Kee’s research focuses on the intersection of history, computing, education, and game studies, with a particular interest in the use of computing for innovative expressions of culture and history. Part of his innovative approach includes a strong emphasis on partnerships across the university and with the broader community; his development of the business incubator nGen enables a deep level of crossover between the two. Cecilia Márquez is a PhD student at the University of Virginia’s Corcoran Department of History, and is one of the 2012-2013 Praxis Fellows. She became interested in the digital humanities through the South Atlantic Studies Fellowship for the Public Humanities, funded by the Virginia Foundation for the Humanities.  Márquez’s recent work focuses on the experience of Latina/os in the American South during the Civil Rights Movement.  Her research interests include African American, Latina/o, and Gender and Sexuality Studies. Kelli Massa earned her MSc in the Digital Humanities Program at University College London. As a graduate student in the UCL DH program, Massa had the opportunity to take a variety of stimulating courses connecting the humanities with digital technologies. Entering the program with a humanities background (MA in Literature), she focused primarily on the computer science side of the program and pursued an MSc. Her internship with JISC (Joint Information Systems Committee) sparked an interest in academic software sustainability and repositories, which she plan to research for her dissertation. William Pannapacker is Professor of English at Hope College and directs the Mellon Scholars program. His research and teaching interests include American literature and culture and digital humanities. Pannapacker is also a columnist for the Chronicle of Higher Education and a contributor to The New York Times and Slate, where he writes about a range of issues related to higher education in the humanities. Donnie Sackey is a PhD candidate in the Writing, Rhetoric, and American Cultures department at Michigan State University, and a 2012-2013 Cultural Heritage Informatics Initiative Graduate Fellow. Through his dissertation research on invasive species, Sackey explores the relationship between rhetoric and the environment, as well as ways in which creative applications of information and computing technologies can help map environments and subsequently allow for alternative levels of engagement. Katina Rogers researches graduate education reform and career paths for humanities scholars in her capacity as Senior Research Specialist at the Scholarly Communication Institute. Rogers collaborated on the development of the Praxis Network website and conducted a study of career preparation in humanities graduate programs. She has given invited lectures on her work on graduate education reform and alternative academic career paths at New York University, Stanford University, and the University of Delaware, and her work has been written up in the Chronicle of Higher Education and Inside Higher Ed. She holds a PhD in Comparative Literature from the University of Colorado, Boulder. Effective September 2013, she will begin a new role as Managing Editor of MLA Commons ."},{"id":"2013-06-17-reflections-on-project-management-i","title":"Reflections on Project Management I","author":"claire-maiers","date":"2013-06-17 05:35:56 -0400","categories":["Grad Student Research"],"url":"reflections-on-project-management-i","layout":"post","content":"At our final official Praxis meeting, I shared an overview of my experience as project manager with the rest of the team, and I thought I would share some of those same reflections in a short series of blog posts.  This first post reflects on some of the more technical and organizational aspects of project management.    Early on, I read Sharon Leon’s piece about project management, and these reflections follow much of her conversation.  As Leon discusses, the PM is responsible for the delegation of roles and tasks, the allocation of resources, establishing a work plan and a time line, and developing a method for tracking progress and the completion of tasks.  In addition, the PM needs to be able to articulate the vision and goals of the project, distinguishing between primary deliverables and those that are secondary.   Rather than deciding on a set of goals and then assigning people to different roles, in the case of Praxis, these processes happened in tandem.  As each of us gravitated toward certain roles, it became more clear which deliverables would become prioritized and which would be secondary.  To help keep myself focused, I drafted a very general project vision statement and, at Bethany’s suggestion, collected statements from the other Praxisers about their goals for the semester.   This left me with the job of laying out the calendar and work plan for the semester.  As I blogged about earlier, this was a really daunting task for me given that I am still new to the DH world and did not have the skills to conceptualize how all the pieces fit together.  From that process, however, I have learned that PMs do not need to know every detail of how the project will go from the outset.  Rather than feel uncomfortable with my lack of knowledge, I started to see how I was surrounded by a team of knowledgeable consultants who could assist me in figuring out the relevant information needed to coordinate different aspects of our project.  When it came to the calendar, I basically just worked backward.  I started by establishing our launch date.  Then, in conversation with Wayne and Bethany, I worked to figure out the approximate dates when each task needed completion.  Leon and others recommend that PMs work with team members to establish deadlines and goals rather than just dictating them.  While this is a good idea (and something which I plan to try again in my next PM gig), it didn’t work so well for Praxis—mostly due to the fact that all of us were novices and not really sure how much time various tasks required.  I did, however, collect information from everyone with regard to their commitments over the semester.  I wanted to know when people would be leaving for the summer, when they would be out of town for a significant period, and if there were other commitments that might take time away from Praxis.  That information helped me to set up the calendar and to know when people were most and least available throughout the last few months.  Although we did not have to worry about financial resources or the allocation of equipment or software, I want to say something briefly about time and space.  One of our most important resources has been the graduate lounge.  Often our best work happened when we were in that shared spaced for a good chunk of time.  Although the design team had regular weekly meetings, the rest of us were rarely in the lounge collectively.  Rather, we came in one by one when our schedules allowed.  As the semester quieted down, we found more time to be in the lounge collectively.   I am happily in awe of the amount of work and progress that we accomplished in the last three weeks of Praxis.  Had I known how valuable that shared time and space would be, I would have done more to institute habits of collective work meetings early To keep track of progress, we used the issues tracker in github.  Deciding what counted as a task or a milestone was also difficult initially.  Personally, I found that it helps to have the project broken down into small enough parts that you can actually track progress from week to week.  If tasks are too large, there is little to check off of the “to do” list until the project approaches completion.  In the end, each of our core deliverables became a milestone to which a list of issues (or tasks) were attached.   I will have more to say about the articulation of tasks in my next post."},{"id":"2013-06-19-reflections-on-project-management-ii-know-your-team","title":"Reflections on Project Management II: Know your Team","author":"claire-maiers","date":"2013-06-19 06:28:45 -0400","categories":["Grad Student Research"],"url":"reflections-on-project-management-ii-know-your-team","layout":"post","content":"In my last post, I talked through some of my experiences with the organizational aspects of project management.  Here, I want to talk about one of the aspects of project management which can be more difficult to articulate and to learn.  While each project needs a timeline, deadlines, and clear goals, a project manager does more than keep track of the organizational aspects of the project.   The way I see it now, a successful project manager is able to facilitate the coordination of her team in a way that maximizes the team’s potential. This involves both the ability to be a liaison between various aspects of the project and team members and the ability to learn how each team member works and which kind of guidance (or opinion) is either useful or disruptive.  For example, I found that some Praxisers preferred very specific guidance on tasks, while others preferred just general instruction and to be left (more or less) to their own devices.  In this way, knowing your team members and how they work is an important piece of the puzzle when it comes to identifying milestones and tasks to track. I have to say that I only figured out this aspect of project management through trial and error.  Most weeks I would send out an email that recapped our Monday meetings and outlined goals for the week.  Sometimes those emails worked to facilitate the work plan for the week, and sometimes they did not.   It was only over time that I began to realize that some of my teammates were not getting the kind of guidance that they expected from a PM and that I was able to start adapting my strategy for thinking about tasks and goals. Having said that, I am looking forward to an opportunity to work on multiple projects with the same team.  It is only through time spent together and learning experiences brought on by both failures and successes that you can really get to know your team members.  While it may be true that anyone with decent organizational skills can set up a calendar and track tasks, I think the best project managers know and understand how a team works and can use that knowledge to shape their own approach to leadership."},{"id":"2013-07-02-reflections-on-project-management-iii-it-is-all-about-communication","title":"Reflections on Project Management III: It Is All About Communication ","author":"claire-maiers","date":"2013-07-02 08:09:56 -0400","categories":["Grad Student Research"],"url":"reflections-on-project-management-iii-it-is-all-about-communication","layout":"post","content":"I have saved the discussion of my biggest struggle with Project Management for my final post in this miniseries.  Much to my surprise, communication with the rest of the team was one of the more difficult aspects of this semester.  This caught me completely off guard because I actually think that I have pretty good communication skills: I don’t shy away from talking about topics just because they are uncomfortable, I write clearly, and I make an effort to be sure I am hearing and understanding others. And yet, I found communication challenging. After a lot of reflection, there are at least three related issues that contributed to my communication woes.  First, I assumed that others would react to the communication of various sentiments as I would react.  If someone asks me to “do something if I have time,” I just about always get it done.  This is due to my own inability to say “no,” even when I should.  I eventually realized that my team members were not all as overly socialized and prone to guilt as myself.  When I included statements such as “if you have time,” they took me literally (as they should).  If I really needed something done, I had to learn to drop the conditional statement from my emails.   It wasn’t so much that my statements were not clear, but that people did not react to certain statements as I expected. However, this first point is not entirely removed from two other contributions to my communication troubles: insecurity about my own knowledge in DH and a worry over seeming bossy.  I discussed this insecurity about my knowledge and my role in telling my teammates what to do in two earlier posts ( here and here ).  How could I—with very little DH knowledge—really have the authority to tell other when to do what?  As the semester wore on, this got easier. The worry about bossiness has been persistent however.  Worried that statements would be taken the wrong way, I often inserted wishy-washy language and conditional statements into my emails.  As I prepared to talk with the Praxis team about my experience as PM, it occurred to me that this might be a gendered worry.  When I brought this issue up at our final meeting, I was surprised by the amount of conversation that it generated.  I worried that, because I am a woman, certain statements and requests would be seen as me over stepping my grounds or being temperamental and demanding (I have a particular female adjective in mind that is not appropriate for public space, but I hope you get the drift). As became clear through our group conversation, others were not prone to actually reading clear and to-the-point statements with this bossy tone simply because of my sex.  But, the fact that I worried so much about it was a problem.  I’ve come to see that the worry itself is actually a gendered aspect. Because of gendered stereotypes of female leaders and bosses, I constantly worried over the perception of my words and decisions.  Men, it seems, might be less prone to this particular type of over analyzing.  As Wayne pointed out, men also worry about striking the right tone and being considerate to their colleagues, but women (or at least this woman), worried a lot about invoking the particular feminine adjective mentioned above in a way that would undermine my own legitimacy and credibility with my teammates. Due to my concern for seeming too bossy, I actually undermined my abilities all on my own by generating vague and imprecise communication with my team.  I did eventually get better.  As crunch time neared, I learned to just articulate explicitly what needed doing without all the wishy-washy stuff.  I am thankful to Gwen for pointing out during our group conversation both that she was relieved and that our progress improved once I was able to just drop all these concerns and do my job by telling the team what to do. I cannot thank the SLab staff enough for the opportunity to be a part of the Praxis team this year and for their encouraging us to reflect about the experience as we went along.  The opportunity to blog and converse with the wider DH community has been just as valuable as the experience of working on Prism.  I hope that some of you out there will comment on this miniseries of PM posts.  Although my time with Praxis is wrapping up, I am looking forward to continued dialogue about DH and project management!  Thank you SLab staff and fellow Praxisers—it has been great!"},{"id":"2013-07-03-are-you-the-new-head-of-scholars-lab","title":"Are you the new Head of Scholars’ Lab?","author":"katina-rogers","date":"2013-07-03 10:42:31 -0400","categories":["Announcements"],"url":"are-you-the-new-head-of-scholars-lab","layout":"post","content":"We are thrilled to announce an exciting job opportunity on the leadership team here in the Department of Digital Research &amp; Scholarship . Read on for more details! Head of Scholars’ Lab Are you an experienced digital humanities scholar-practitioner with a strong background in project management and public service? The University of Virginia Library seeks an energetic, adaptable leader for the digital consultation services and intellectual programming of our internationally-respected Scholars’ Lab. The ideal candidate is detail-oriented, eager to work collaboratively with diverse faculty and staff, and able to muster and effectively communicate UVa Library’s deep resources for digital scholarship. This supervisory position is responsible for day-to-day operations in the Scholars’ Lab and, together with the heads of Graduate Programs and Research &amp; Development, completes the leadership team reporting to the director of Digital Research and Scholarship for UVa Library. The Head of the Scholars’ Lab should have a strong service ethic, broad technical knowledge, and ability to collaborate as a true partner with faculty and graduate students, enabling next-generation digital scholarship in a library lab setting. He or she should also be able to take good advantage of the “20% time” afforded to all in the Department of Digital Research &amp; Scholarship to pursue professional development and their own (often collaborative) research projects related to the mission of the Scholars’ Lab. This is a full-time, permanent managerial and professional staff position at UVa. Primary Responsibilities: Community-building and Leadership : strategic leadership of digital humanities services, oversight of day-to-day operations, design of intellectual programming and instruction, and collaboration with local and national/international peers; Project-based Collaborations : coordinates development of high-level goals, intake processes, workplans, and MoUs for digital project collaborations, in consultation with departmental, Library, and University partners; Research and Professional Development : pursuit of own scholarly R&amp;D agenda related to the humanities or social sciences with publication of results and/or presentation at appropriate venues. **Specialized Knowledge and Skills: **\nStrong knowledge of digital humanities history, technologies, and intellectual directions. Strong public service orientation and interest in guiding scholarly projects from conceptualization to implementation and audience-building. Excellent communication skills, including the ability to present complex technical information to non-specialists and a clear understanding of the perspectives and needs of scholars. Previous experience in public service in an academic library setting preferred. **Education: **\nGraduate study (PhD preferred) in a field related to humanities scholarship or humanistic aspects of social or information science. **Experience: **\n5+ years’ experience with project management and/or hands-on development of digital projects related to digital humanities or cultural heritage. Supervisory experience preferred. Salary and Benefits: Salary is commensurate with experience, and expected to range between approximately $60 and $85k per annum. Excellent benefits, TIAA/CREF and other retirement plans along with generous funding for travel and professional development. To Apply:\nReview of applications to begin immediately and continue until position is filled. Apply through the University of Virginia online employment website . (If you need to search the Jobs@UVa portal, the posting number is 0612479.) Complete application, and attach cover letter and CV, with contact information for three current, professional references. The Scholars’ Lab is also seeking a Head of Graduate Programs. Learn more here . Don’t miss a chance to work with our wonderful students and incredible Scholars’ Lab team!"},{"id":"2013-07-09-neatline-2-0","title":"Announcing Neatline 2.0.0! A stable, production-ready release","author":"david-mcclure","date":"2013-07-09 10:59:18 -0400","categories":["Announcements","Geospatial and Temporal"],"url":"neatline-2-0","layout":"post","content":"[Cross-posted from dclure.org ] It’s finished! Today we’re excited to announce Neatline 2.0.0, a stable, production-ready release of the new codebase that can be used to upgrade existing installations. If you’re starting fresh with a new project, just download the new version and install it like any other Omeka plugin. If you’re upgrading from Neatline 1.x, be sure to read through the 2.0 migration guide before getting started ( most important - the 2.0 migration runs as a “background process,” which means that there could be a 10-20 second lag before your old exhibits are visible under the “Neatline” tab ). Then, if you want to use the SIMILE Timeline widget and item-browser panel that were built into the first version of Neatline, download NeatlineSimile and NeatlineWaypoints, the two new sub-plugins that integrate those features seamlessly into the Neatline core. For more information, check out the (all new!) documentation, which walks through the installation process in detail. Download the plugins: Neatline NeatlineWaypoints NeatlineSimile Neatline 2.0 is a major update that significantly expands the scope of the project. Building on the core set of geospatial annotation tools from the first version, we’ve turned Neatline into a general-purpose visual annotation framework that can be used to create interactive displays of almost any type of two-dimensional material - maps, paintings, drawings, photographs, documents, and anything else that can be captured as an image. We’ve also made a series of changes to the user interface and code architecture that are designed to make Neatline more accessible for new users (college undergraduates working on class assignments) and, at the same time, more flexible for advanced users (professional scholars, journalists, and digital artists who want to use Neatline  for complex projects). Some of the highlights: Improved performance and scalability, powered by a real-time spatial querying system that makes it possible to work with really large collections of records - as many as about 1,000,000 in a single exhibit; A more sophisticated set of drawing tools, including the ability to  import high-fidelity SVG documents created in programs like Adobe Illustrator or Inkscape and interactively drag them into position as geometry in Neatline exhibits; An interactive, CSS-like stylesheet system, build directly into the editing environment, that makes it possible to quickly perform bulk updates on large collections of records using a simplified dialect of CSS; A flexible user-permissions system, designed to make it easier to use Neatline for class assignments and workshops, that makes it possible to prevent users from modifying or deleting content they didn’t create; Expanded support for non-spatial base layers that makes it possible to build exhibits on top of any web-accessible static image or non-spatial WMS layer - paintings, drawings, photographs, documents, etc. A more powerful theming system, which makes it possible for designers to completely customize the appearance and interactivity of each individual Neatline exhibit. This makes it possible to host completely independent and thematically-distinct projects inside a single installation of Omeka. A total rewrite of the front-end JavaScript applications (both the editing environment and the public-facing exhibit views) that provides a more minimalistic and responsive environment for creating and viewing exhibits; A new programming API and “sub-plugin” system that makes it possible for developers to add custom functionality for specific projects - everything from simple user-interface widgets (sliders, legends, scrollers, forward-and-backward buttons, etc.) up to really extensive modifications that expand the core data model and add totally new interactions. And much more! Over the course of the next week, leading up to our panel about Neatline at the DH 2013 conference in Lincoln, Nebraska (“Circular Development: Neatline and the User/Developer Feedback Loop,” Wednesday at 10:30), we’re going to be fleshing out the new documentation and building a set of Neatline-2.0-powered projects designed to put the new feature set through its paces. Also, watch this space later in the week for another code release - we’ve built an extension called NeatlineTexts that connects Neatline exhibits with word-level annotations in long-format documents, which makes it possible to use Neatline as a publication platform for essays, blog posts, scholarly articles, monographs, etc., and built a special Omeka that’s specifically designed to frame these interactive editions. Until then - grab the new code, give it spin, and let us know what you think!"},{"id":"2013-07-10-summer-of-travel-dh-edition","title":"Summer of travel, DH edition","author":"katina-rogers","date":"2013-07-10 07:32:10 -0400","categories":["Digital Humanities"],"url":"summer-of-travel-dh-edition","layout":"post","content":"[Cross-posted from my personal website ] This spring and summer has been the busiest travel season I have ever had. While I won’t deny that I’m happy to be rounding the corner on my last two trips this summer, I’ve learned a tremendous amount as I’ve bounced from city to city, and feel lucky to have had so many outstanding opportunities. A good portion of the travel has been to share findings from the survey work I did on career preparation among humanities scholars. After doing so much work on the topic, it was incredibly exciting and rewarding to talk with various groups about the results, and to see what questions and ideas arose from my remarks. I’ve posted a version of these talks, so I won’t go into more detail here. Aside from the occasional conference presentation, public speaking is pretty new for me, and it’s been a few years since I’ve even been in front of a classroom. I found myself most comfortable only after extensive preparation: while I admire those who can speak extemporaneously, I learned that I need to craft my talks carefully at this stage. I think that I’ve become a more effective communicator even in the last few months, and I hope that means that the results from SCI’s study will be more likely to reach the right audiences in ways that encourage discussion and action. Another unusual trip for me was to attend the Digital Humanities Summer Institutes . Those of you who have been to DHSI know the unique atmosphere that I encountered: part conference, part classroom, part summercamp for grown-ups. The welcoming environment that Ray Siemens and his team create suffuses the week, and the backdrop of beautiful Victoria, BC is intoxicating. (I rented a bike my last day there so that I could explore the shoreline, which I highly recommend!) One thing that strikes me as unique about DHSI is the way that it brings together people with similar values but very different backgrounds and goals. I took a course in visual design, and while it’s not a topic that’s of unique interest to digital humanists, the fact that we were all approaching it with goals related to DH meant that we had some strong overlap in terms of audience, anticipated work products, questions to consider, and more. Unlike a conference, you spend the bulk of your time at DHSI with a single group of people—your classmates—and while that’s wonderful in terms of fostering deeper relationships with a small group, I also found it important to make sure to spend time with people that I wasn’t seeing each day. Our conversations tended to be rich and varied, and brought fresh perspectives to the new skills we were learning. And now, the two remaining trips are a small SCI meeting to close out our work on rethinking humanities graduate education, and my very first DH conference . The abstract of the paper I’ll be giving is available here . I’m nervous about DH—it’s my first time attending, and I feel like something of an interloper—but also incredibly excited. My talk again report on the survey findings, but from a somewhat different angle, and I expect that the DH community will have some very productive questions, critiques, and feedback to offer. The program is packed with an impressive variety of talks, many of which are outside my areas of expertise, and I’m looking forward to learning a lot and seeing firsthand the kinds of work that the community has been developing. In all, the past few months have been a bit of a whirlwind (though I’m pleased to say that I’ve become a pro at packing, unpacking, and grabbing the right number of bins in the airport security line). All this travel has really helped to broaden my perspective, giving me a window into both the day-to-day and long-term questions, goals, and challenges that face individuals in different contexts within higher education. It has helped get me out of my head (a problem sometimes, especially since I’m working from home) and talking with grad students, faculty, and administrators, which has in turn grounded my own perspective. And it has helped me to more accurately assess the value and limitations of my own work."},{"id":"2013-07-18-homers-catalogue-of-ships","title":"Homer's Catalogue of Ships","author":"gwen-nally","date":"2013-07-18 11:53:23 -0400","categories":["Announcements","Digital Humanities","Experimental Humanities","Geospatial and Temporal","Grad Student Research"],"url":"homers-catalogue-of-ships","layout":"post","content":"Ben Jasnow and Courtney Evans (UVA Classics Graduate Students) just presented their findings at DH2013. They hypothesize that their spatial-linguistic analysis of the catalogue of ships could aid in the discovery of ancient sites. Check out their presentation here:   Mapping the Catalogue of Ships Here’s a preview:"},{"id":"2013-07-29-scholars-lab-speaker-series-james-smithies","title":"Scholars' Lab Speaker Series: James Smithies","author":"ronda-grizzle","date":"2013-07-29 07:19:10 -0400","categories":["Podcasts"],"url":"scholars-lab-speaker-series-james-smithies","layout":"post","content":"Speaker Series Brown Bag: James Smithies The UC CEISMIC Digital Archive: Co-ordinating Libraries, Museums, Archives, Individuals and Government Agencies in a Disaster Management Context On July 22, Dr. James Smithies, Senior Lecturer in Digital Humanities at the University of Canterbury in Christchurch, New Zealand, spoke in the Scholars’ Lab about his work designing and developing the CEISMIC Digital Archive. Summary:\nThe Canterbury region in the South Island of New Zealand has experienced over 11,000 earthquakes since September 2010, including a devastating magnitude 6.3 quake on February 22nd 2011 that resulted in the loss of 185 lives. Only months after the February quake, while university staff were teaching from tents in the approach to winter, a fledgling digital humanities programme was established that had as its first goal the development of a national federated digital archive to preserve the vast quantities of content being produced as a result of the earthquakes. Paul Millar and James Smithies drew together a Consortium of 10 local and national agencies representing New Zealand’s libraries, museums, archives and cultural organisations in an effort to ensure a co-ordinated response.They then led technical development of a national federated archive, ceismic.org.nz, and a bespoke research archive, quakestudies.canterbury.ac.nz . The CEISMIC archive has recently completed Phase 1 of its technical development, and includes over 20,000 items. Consortium member organisations, local government agencies, and commercial companies provide content next to community groups and individuals. Projections indicate that the archive will hold 100,000 items by the end of 2013. The intention is to remain operational for the 10-15 years it is expected to take to rebuild the region. This talk will describe the current state of the archive, and explain how Millar and Smithies used methods inspired by the digital humanities community to achieve their goals. www.ceismic.org.nz You can find Dr. Smithies online on his website ( jamessmithies.org ) and on twitter ( @jamessmithies ). As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.25360114143/enclosure.mp3”]"},{"id":"2013-08-07-announcing-neatline-2-0-2","title":"Announcing Neatline 2.0.2!","author":"david-mcclure","date":"2013-08-07 09:38:14 -0400","categories":["Geospatial and Temporal"],"url":"announcing-neatline-2-0-2","layout":"post","content":"Today we’re pleased to announce the release of Neatline 2.0.2 ! This is a maintenance release that adds a couple of minor features and fixes some bugs we’ve rooted up in the last few weeks: Fixes a bug that was causing item-import queries to fail when certain combinations of other plugins were installed alongside Neatline (thanks Jenifer Bartle and Trip Kirkpatrick for bringing this to our attention). Makes it possible to toggle the real-time spatial querying on and off for each individual exhibit. This can be useful if you have a small exhibit (eg, 10-20 records) that can be loaded into the browser all at once without causing performance problems, and you want to avoid the added load on the server incurred by the dynamic querying. Fixes some performance issues with the OpenStreetMap layer in Chrome. And more! Check out the release notes for the full list of changes, and grab the new code from the Omeka add-ons repository . Also, watch this space for a couple of other Neatline-related releases in the coming weeks. Jeremy and I are working on a series of themes for Omeka specifically designed to display Neatline projects, including the NeatLight theme, which is currently used on the Neatline Labs site I’ve started playing around with (still a work in progress). We’re also just about ready to cut off a public release of the NeatlineText plugin, which makes it possible to connect records in Neatline exhibits to individual sections, paragraphs, sentences, and words in text documents (check out this example). Until then, give the new code a spin, and let us know what you think!"},{"id":"2013-08-08-speaking-in-code","title":"Speaking in Code","author":"bethany-nowviskie","date":"2013-08-08 07:02:33 -0400","categories":["Announcements","Digital Humanities"],"url":"speaking-in-code","layout":"post","content":"We’re pleased to announce that applications are open for a 2-day, NEH-funded symposium and summit to be held at the Scholars’ Lab this November 4th and 5th. “ Speaking in Code ” will bring together a small cohort of accomplished digital humanities software developers. Together, we will give voice to what is almost always tacitly expressed in DH development work: expert knowledge about the intellectual and interpretive dimensions of code-craft, and unspoken understandings about the relation of our labor and its products to ethics, scholarly method, and humanities theory. Over the course of two days, participants will: reflect on and express, from developers’ own points of view, what is particular to the humanities and of scholarly significance in DH software development products and practices; and collaboratively devise an action-oriented agenda to bridge the gaps in critical vocabulary and discourse norms that can frequently distance creators of humanities platforms or tools from the scholars who use and critique them. In addition to Scholars’ Lab developers and project managers, facilitators include Steve Ramsay, Bill Turkel, Stéfan Sinclair, Hugh Cayless, and Tim Sherratt .  The SLab particularly encourages and will prioritize participation of developers who are women, people of color, LGBTQ, or from other under-represented groups. (See “ You Are Welcome Here ” for more info.) This will be the first focused meeting to address the implications of tacit knowledge exchange in digital humanities software development. Check out the Speaking in Code website to apply ! Deadline September 12th."},{"id":"2013-08-12-alt-ac-report-and-data","title":"Now available: Report and data from SCI’s survey on career prep and graduate education","author":"katina-rogers","date":"2013-08-12 06:05:25 -0400","categories":["Research and Development"],"url":"alt-ac-report-and-data","layout":"post","content":"[Cross-posted at my personal website ] I am delighted to announce the release of a report, executive summary, data, and slides from the Scholarly Communication Institute ’s recent study investigating perceptions of career preparation provided by humanities graduate programs. The study focused on people with advanced degrees in the humanities who have pursued alternative academic careers. Everything is CC-BY, so please read, remix, and share. I’d especially welcome additional analysis on the datasets. All of the materials are openly accessible through the University of Virginia’s institutional repository: Report, executive summary, and slides (one package): http://libra.virginia.edu/catalog/libra-oa:3480 Or, download the PDFs separately here for easier sharing: report, executive summary, slides Data from the main survey: http://libra.virginia.edu/catalog/libra-oa:3272 Data from the employer survey: http://libra.virginia.edu/catalog/libra-oa:3500 (Note that the files available for download are listed in the top left-hand corner of each Libra listing.) Having worked on this for over a year, I’m more convinced than ever about the importance of incorporating public engagement and collaboration into humanities doctoral education—not only to help equip emerging scholars for a variety of career outcomes, but also to maintain a healthy, vibrant, and rigorous field. It has been fascinating to connect with scholars working in such a diverse range of stimulating careers, and to see some of the patterns in their experiences. Many, many thanks to everyone who has contributed time and energy to this project—from completing the survey, to reading (or listening to) the preliminary reports, to providing feedback and critique."},{"id":"2013-08-12-displaying-recent-neatline-exhibits-on-your-omeka-home-page","title":"Displaying Recent Neatline Exhibits on your Omeka Home Page","author":"jeremy-boggs","date":"2013-08-11 21:00:56 -0400","categories":["Research and Development"],"url":"displaying-recent-neatline-exhibits-on-your-omeka-home-page","layout":"post","content":"The charismatic Alex Gil submitted a feature request to Neatline asking to be able to browse Neatline exhibits on your Omeka home page . Turns out you can already specify which page you want as your home page in Omeka 2.0, so that helped with Alex’s original query. But as we discussed the issue, Alex also wondered about putting a list of recent Neatline exhibits on the home page, much the same way Omeka already does with recent items. While we’re not sure yet about putting this kind of thing in the plugin itself, I mentioned that it’s fairly easy to do in your own them using one of Omeka’s hooks, and promised him a blog post explaining more. Here’s me making good on that promise. In case you didn’t know, Omeka has plenty of ways for developers to add new content to an Omeka site or filter existing content using hooks and filters, respectively. To use them, you first need to write a function that adds or changes content to your preference, then you pass that function to the relevant hook or filter in Omeka. Some dummy code to illustrate: [sourcecode language=”php”]\n&lt;?php function my_custom_function() {\n  echo ‘Hello world!’;\n} add_plugin_hook(‘hook_name’, ‘my_custom_function’); [/sourcecode] You could put this kind of code anywhere that Omeka could run it, particularly a new plugin or your activated theme’s custom.php file. (An Omeka theme’s custom.php file is a great place to put some custom code for your Omeka site, without having to go to the trouble of creating and activating a plugin.) In our case, we want to append some new content to the home page of an Omeka site, so we’ll need to find a hook to let us do that. Fortunately, we have one available— public_home —so let’s use that to display some recent Neatline exhibits. (Keep in mind that the following code should work in Omeka 2.0 and Neatline 2.0; you can take a similar approach for earlier versions of each, but some of the functions would be different.) First, we’ll need to create a custom.php file in your current active theme, if one doesn’t already exist. (If it does exist, we’ll use that one.) Make sure the file is in the root of your theme: omeka/themes/your-theme/custom.php . Next we’ll need to write a function that gets a certain number of Neatline exhibits and lists them out, and put that in our custom.php file. We’ll name our function display_recent_neatline_exhibits, and put all our goodies in there. Let’s create the function: [sourcecode language=”php”]\n&lt;?php function display_recent_neatline_exhibits() { } [/sourcecode] After we’ve created the function, we’ll go ahead and pass that function to the public_home hook: [sourcecode language=”php”]\n&lt;?php function display_recent_neatline_exhibits() { } add_plugin_hook(‘public_home’, ‘display_recent_neatline_exhibits’);\n[/sourcecode] We still shouldn’t see any changes on the home page, since our function isn’t actually doing anything. But you shouldn’t get any errors on the page either. If you do, make sure you have every curly brace and semicolon and all the other characters right; PHP is quite dramatic about syntax errors. Now lets add some stuff to our function to get some recent Neatline exhibits. First, let’s define a variable $html and set that equal to an empty string. In the end, we’ll echo the value of $html, so we want it equal to at least something, in case you actually don’t have any Neatline exhibits to display. [sourcecode language=”php”]\n&lt;?php function display_recent_neatline_exhibits() {\n    $html = ‘’; echo $html; } add_plugin_hook(‘public_home’, ‘display_recent_neatline_exhibits’);\n[/sourcecode] Next we’ll create a variable, $neatlineExhibits, and assign it to the results of a query using Omeka’s get_records function. The get_records function takes three arguments: the type of record, an array of query parameters, and number to limit results. We’ll query for ‘NeatlineExhibit’ record type, make sure that the recent parameter is true, and limit our results to five: [sourcecode language=”php”]\n&lt;?php function display_recent_neatline_exhibits() {\n    $html = ‘’; // Get our recent Neatline exhibits, limited to five.\n$neatlineExhibits = get_records('NeatlineExhibit', array('recent' =&amp;gt; true), 5);\n\necho $html; } add_plugin_hook(‘public_home’, ‘display_recent_neatline_exhibits’);\n[/sourcecode] Now we’ll set the results in $neatlineExhibits for a record loop, and check to see if in fact we have exhibits to display in a PHP if statement: [sourcecode language=”php”]\n&lt;?php function display_recent_neatline_exhibits() {\n    $html = ‘’; // Get our recent Neatline exhibits, limited to five.\n$neatlineExhibits = get_records('NeatlineExhibit', array('recent' =&amp;gt; true), 5);\n\n// Set them for the loop.\nset_loop_records('NeatlineExhibit', $neatlineExhibits);\n \n// If we have any to loop, we'll append to $html.\nif (has_loop_records('NeatlineExhibit')) {\n\n}\n\necho $html; } add_plugin_hook(‘public_home’, ‘display_recent_neatline_exhibits’);\n[/sourcecode] Inside our if statement, we’ll update the value of $html so that, instead of echoing an empty string, it echos some HTML that includes links to each of our recent Neatline exhibits. Remember that this will only get printed if we actually have Neatline exhibits in the database, otherwise we’ll just return an empty string. [sourcecode language=”php”]\n&lt;?php function display_recent_neatline_exhibits() {\n    $html = ‘’; // Get our recent Neatline exhibits, limited to five.\n$neatlineExhibits = get_records('NeatlineExhibit', array('recent' =&amp;gt; true), 5);\n\n// Set them for the loop.\nset_loop_records('NeatlineExhibit', $neatlineExhibits);\n \n// If we have any to loop, we'll append to $html.\nif (has_loop_records('NeatlineExhibit')) {\n    $html .= '&amp;lt;ul&amp;gt;';\n    \n    foreach (loop('NeatlineExhibit') as $exhibit) {\n        $html .= '&amp;lt;li&amp;gt;'\n               . nl_getExhibitLink(\n                     $exhibit,\n                     'show',\n                     metadata($exhibit, 'title'),\n                     array('class' =&amp;gt; 'neatline')\n                 )\n               . '&amp;lt;/li&amp;gt;';\n    }\n\n    $html .= '&amp;lt;/ul&amp;gt;';\n}\n\necho $html; } add_plugin_hook(‘public_home’, ‘display_recent_neatline_exhibits’);\n[/sourcecode] As you can see, we append an opening unordered list tag, &lt;ul&gt; to $html . (Using .= in PHP lets us append additional strings onto an existing variable.) Then, we use Omeka’s loop function to loop through our set of Neatline exhibits. Inside that loop, we once again adding something to the value of $html : A list item wrapping a link to a the current Neatline exhibit in the loop. To help us make that link, we’ll use a function provided by the Neatline plugin: nl_getExhibitLink . We’re passing values for four arguments: The exhibit object (defined in $exhibit in the foreach loop); the action or route you want the link to take; the text of the link (here we’ve used Omeka’s metadata function to give us the title of the exhibit); and an array of attributes for the link (we’ve added a class attribute equal to ‘neatline’). Then we end with a closing list item tag. And that should do it. You can see a version more or less the same as what I demonstrate here in a public gist I published earlier in the week. If you’d like to display a recent list of Neatline exhibits on your Omeka home page, just grab this code, and put it in your theme’s custom.php template."},{"id":"2013-08-12-why-do-we-trust-automated-tests","title":"Why do we trust automated tests?","author":"david-mcclure","date":"2013-08-12 05:08:50 -0400","categories":["Research and Development"],"url":"why-do-we-trust-automated-tests","layout":"post","content":"[Cross-posted from dclure.org ] I’m fascinated by this question. Really, it’s more of an academic problem than a practical one - as an engineering practice, testing just works, for lots of simple and well-understood reasons. Tests encourage modularity; the process of describing a problem with tests makes you understand it better; testing forces you to go beyond the “happy case” and consider edge cases; they provide a kind of functional documentation of the code, making it easier for other developers to get up to speed on what the program is supposed to do; and they inject a sort of refactoring spidey-sense into the codebase, a guard against regressions when features are added. At the same time, though, there’s a kind of software-philosophical paradox at work. Test are just code - they’re made of the same stuff as the programs they evaluate. They’re highly specialized, meta-programs that operate on other programs, but programs nonetheless, and vulnerable to the same ailments that plague regular code. And yet we trust tests in a way that we don’t trust application code. When a test fails, we tend to believe that the application is broken, not the tests. Why, though? If the tests are fallible, then why don’t they need their own tests, which in turn would need their own, and so on and so forth? Isn’t it just like fighting fire with fire? If code is unreliable by definition, then there’s something strange about trying to conquer unreliability with more unreliability. At first, I sort of papered over this question by imagining that there was some kind of deep, categorical difference between resting code and “regular” code. The tests/ directory was a magical realm, an alternative plane of engineering subject to a different rules. Tests were a boolean thing, present or absent, on or off - the only question I knew to ask was “Does it have tests?”, and, a correlate of that, “What’s the coverage level?” (ie, “How many tests does it have?”) The assumption being, of course, that the tests were automatically trustworthy just because they existed. This is false, of course [1]. The process of describing code with tests is just another programming problem, a game at which you constantly make mistakes - everything from simple errors in syntax and logic up to really subtle, hellish-to-track-down problems that grow out of design flaws in the testing harness. Just as it’s impossible to write any kind of non-trivial program that doesn’t have bugs, I’ve never written a test suite that didn’t (doesn’t) have false positives, false negatives, or “air guitar” assertions (which don’t fail, but somehow misunderstand the code, and fail to hook onto meaningful functionality). So, back to the drawing board - if there’s no secret sauce that makes tests more reliable in principle, where does their authority come from? In place of the category difference, I’ve started to think about it just in terms of a relative falloff in complexity between the application and the tests. Testing works, I think, simply because it’s generally easier to formalize what code should do than how it should do it. All else equal, tests are less likely to contain errors, so it makes more sense to assume that the tests are right and the application is wrong, and not the other way around. By this logic, the value added is proportional to the height of this “complexity cliff” between the application and the tests, the extent to which it’s easier to write the tests than to make them pass. I’ve starting using this as a heuristic for evaluating the practical value of a test: The most valuable tests are the ones that are trivially easy to write, and yet assert the functionality of code that is extremely complex; the least valuable are the ones that approach (or even surpass) the complexity of their subjects. For example, take something like a sorting algorithm. The actual implementation could be rather dense (ignore that a custom quicksort in JavaScript is never useful): The tests, though, can be fantastically simple: These are ideal tests. They completely describe the functionality of the code, and yet they fall out of your fingers effortlessly. A mistake here would be glaringly obvious, and thus extremely unlikely - a failure in the suite almost certainly means that the code is actually defective, not that it’s being exercised incorrectly by the tests. Of course, this is a cherry-picked example. Sorting algorithms are inherently easy to test - the complexity gap opens up almost automatically, with little effort on the part of the programmer. Usually, of course, this isn’t the case - testing can be fiendishly difficult, especially when you’re working with stateful programs that don’t have the nice, data-in-data-out symmetry of a single function. For example, think about thick JavaScript applications in the browser. A huge amount of busywork has to happen before you can start writing actual tests - HTML fixtures have to be generated and plugged into the testing environment; AJAX calls have to be intercepted by a mock server; and since the entire test suite runs inside a single, shared global environment (PhantomJS, a browser), the application has to be manually burned down and reset to a default state before each test. In the real world, tests are never this easy - the “complexity cliff” will almost always be smaller, the tests less authoritative. But I’ve found that this way of thinking about tests - as code that has an imperative to be simpl er than the application - provides a kind of second axis along which to apply effort when writing tests. Instead of just writing more tests, I’ve started spending a lot more time working on low-level, infrastructural improvements to the testing harness, the set of abstract building blocks out of which the tests are constructed. So far, this has taken the form of building up semantic abstractions around the test suite, collections of helpers and high-level assertions that can be composed together to tell stories about the code. After a while, you end up with a kind of codebase-specific DSL that lets you assert functionality at a really high, almost conversational level. The chaotic stage-setting work fades away, leaving just the rationale, the narrative, the meaning of the tests. It becomes an optimization problem - instead of just trying to make the tests wider (higher coverage), I’ve also started trying to make the tests lower, to drive down complexity as far towards the quicksort-like tests as possible. It’s sort of like trying to boost the “profit margin” of the tests - more value is captured as the difficulty of the tests dips further and further below the difficulty of the application: [1] Dangerously false, perhaps, since it basically gives you free license to to write careless, kludgy tests - if a good test is a test that exists, then why bother putting in the extra effort to make it concise, semantic, readable?"},{"id":"2013-08-14-parsing-bc-dates-with-javascript","title":"Parsing BC dates with JavaScript","author":"david-mcclure","date":"2013-08-14 09:29:07 -0400","categories":["Research and Development"],"url":"parsing-bc-dates-with-javascript","layout":"post","content":"[Cross-posted from dclure.org ] Last semester, while giving a workshop about Neatline at Beloit College in Wisconsin, Matthew Taylor, a professor in the Classics department, noticed a strange bug - Neatline was ignoring negative years, and parsing BC dates as AD dates. So, if you entered “-1000” for the “Start Date” field on a Neatline record, the timeline would display a dot at 1000 AD. I was surprised by this because Neatline doesn’t actually do any of its own date parsing - the code relies on the built-in Date object in JavaScript, which is implemented natively in the browser. Under the hood, when Neatline needs to work with a date, it just spins up a new Date object, passing in the raw string value entered into the record form: Sure enough, though, this doesn’t work - Date just ignores the negative sign and spits back an AD date. And things get even funkier when you drift within 100 years of the year 0. For example, the year 80 BC parses to 1980 AD, bizarrely enough: Obviously, this is a big problem if you need to work with ancient dates. At first, I was worried that this would be rather difficult to fix - if we really were hitting up against bugs in the native implementation of the date parsing, it seemed likely that Neatline would have to get into the tricky business of manually picking apart the strings and putting together the date objects by hand. It’s always feels icky to redo functionality that’s nominally built into the programming environment. But I didn’t see any other option - the code was unambiguously broken as it stood, and in a really dramatic way for people working with ancient material. So, grumbling at JavaScript, I started to sketch in the outlines of a bespoke date parser. Soon after starting, though, I was idly fiddling around with the Date object in the Chrome JavaScript terminal when stumbled across an unexpected (and sort of inexplicable) solution to the problem. In reading through the documentation for the Date object over at MDN, I noticed that the constructor actually takes three different configuration of parameters. If you pass in a single integer, it treats it as a Unix timestamp; if you pass a single string, it treats it as a plain-text date string and tries to parse it into a machine-readable date (this was the process that appeared to be broken). But you can also pass three separate integers - a year, a month, and a day. Out of curiosity, I plugged in a negative integer for the year, and arbitrary values for the month and day: Magically, this works. A promising start, but not a drop-in solution for the problem - in order to use this, Neatline would still have to manually extract each of the date parts from the plain-text date strings entered in the record forms (or break the dates into three parts at the level of the user interface and data model, which seemed like overkill). Then, though, I tried something else - working with the well-formed, BC date object produced with the year/month/day integer values, I tried casting it back to ISO8601 format with the toISOString method. This produced a date string with a negative date and… … two leading zeros before the four-digit representation of the year . I had never seen this before. I immediately tried reversing the process and plugging the outputted ISO string back into the Date constructor: And, sure enough, this works. And it turns out that it also fixes the incorrect parsing of two-digit years: I am deeply, profoundly perplexed by this. The ISO8601 specification makes cursory note of an “expanded” representation for the year part of the date, but doesn’t got into specifics about how or why it should be used. Either way, though, it works in all major browsers. Mysterious stuff."},{"id":"2013-08-14-reprinting-printed-parts","title":"Reprinting Printed Parts","author":"jeremy-boggs","date":"2013-08-14 06:00:12 -0400","categories":["Experimental Humanities"],"url":"reprinting-printed-parts","layout":"post","content":"As some of you know, the Scholars’ Lab has a spiffy 3D printer, a Makerbot Replicator 2. We’ve had fun with it, printing all sorts of wonderful things. As time went on and we continued using it, we ran into a problem plenty of other folks encountered, where the plunger that pushes filament against the drive gear was weakening. The first solution we tried was tightening up the plunger, but we’d have to do this frequently. A better solution was to print a new set of parts—a spring-loaded arm and mount—to replace the plunger. So I ordered up the hardware I’d need (spring, bearing, and bolts), and when those arrived in the mail, I printed the arm and other parts, disassembled the drive block on the printer, and replaced the plunger with new spring-loaded arm. You can see the fully-assembled drive block in my made stuff on Thingiverse . After I put the drive block back on the printer, I could tell an immediate difference in the prints. The plastic was extruding more smoothly than before. Loading the filament was a tiny bit trickier, but well worth it. Then last month, I hauled the printer halfway across the country to use in a workshop on 3D modeling and printing at the Digital Humanities conference in Lincoln, Nebraska. When I started a test print to calibrate the printer, I noticed the printer wasn’t extruding plastic as well as it had been. so I took the drive block off again, and noticed the arm was loose, that the spring wasn’t actually pushing up on it enough to exert pressure on the filament coming it. I assumed I had just cracked the arm somehow during the drive out to Nebraska, so I just stretched the spring out a bit so it would better push the arm up, and went on with the workshop as planned. Yesterday I finally got around to printing a replacement arm. After taking the old arm and mount off, and comparing the old arm with the new one, it seems like my initial thought that I had broken the arm were incorrect: [![IMG_0334](http://static.scholarslab.org/wp-content/uploads/2013/08/IMG_0334-768x1024.jpg)](http://static.scholarslab.org/wp-content/uploads/2013/08/IMG_0334.jpg) Comparing printed arms for the drive block. The black arm is the replacement, and the orange arm is the old one. Upon further inspection, the old one isn’t broken as I had first suspected. Comparing the two (The orange arm is the first one I printed, and the black arm is the replacement I just printed) , it looks like the orange one has warped, very likely due the plastic arm being so close to the heated extruder.  It could also be that the spring was strong enough to warp the plastic arm bit by bit over time. I’m sure the proximity to the heated extruder helped with that too. [![Assembled drive block with spring-loaded arm.](http://static.scholarslab.org/wp-content/uploads/2013/08/IMG_0338-1024x768.jpg)](http://static.scholarslab.org/wp-content/uploads/2013/08/IMG_0338.jpg) Assembled drive block with spring-loaded arm. I decided to go ahead and use the newly printed arm for now. Needless to say, if you’ve tried this solution for your Replicator 2, you’ll be better off long-term sending for the machined replacement parts  as I did. But for now, the new arm is working great, and should hold us over until the machined parts arrive."},{"id":"2013-08-15-problem-solving-with-html5-audio","title":"Problem Solving with HTML5 Audio","author":"wayne-graham","date":"2013-08-15 05:19:16 -0400","categories":["Research and Development"],"url":"problem-solving-with-html5-audio","layout":"post","content":"Several years ago I worked on a project to take recordings made of William Faulkner while he was the Writer-in-Residence at the University of Virginia in 1957 and 1958. The project, Faulkner at Virginia, transcribed the audio and then keyed the main components of the audio to the text using TEI. In order to provide playback of an individual clip, we used a streaming server ( Darwin Streaming Server ) that was being managed by another group. This allowed me to provide “random” access to the components of the audio, without needing to split up the files. Using the associated API, I could generate a clip of the data with something like this: [gist id=”5830678” file=”gistfile1.js”] While this is kind of a nasty bit of JavaScript, it (somewhat) abstracts the Object embed code: [gist id=”5830678” file=”gistfile2.html”] At the time, the WHATWG specifications for audio were still pretty nascent, and didn’t have a lot of actual implementation saturation in browsers. At the time (late 2000s), the approach of using a third-party plugin to provide “advanced” interaction with a media element was pretty much the only game in town. As with any project that relies on web technology, eventually things start to break, or just flat-out not work on devices that can access the Internet (e.g. mobile). Browsers have been in a war with each other for speed, implementation of “standards”, and market share. This has been a real boon for users as it has allowed developers to really push what the web is capable of as a run-time environment. Unfortunately for the Faulkner audio, the code got to the point where the approach stopped functioning consistently across all desktop browsers (interestingly, Chrome seemed to manifest this issue most consistently), and oh yeah, there are those iOS-based mobile devices that can’t play this either. HTML audio to the rescue You know that modern browsers (everything but IE &lt; 9) can play audio natively (i.e. without a plugin), right? Really the only really horrible thing is that not every browser handles the same “native” format. You can check out a good table for codec support for audio, but it basically boils down to needing an MP3 and an Ogg Vorbis version of the audio files to provide for nearly all the browsers (IE being the outlier, with this working of IE 9+). [gist id=”5830678” file=”gistfile3.html”] This provides something on your page like this: The great thing is that this will work on a mobile device as well. Score one for the Internet! Now to figure out the best way to do this. Split the files My first instinct was to take the files and split them into “clips” on the page. This would allow the browser to provide its native playback mechanism, and allow individuals to grab the segments for remixing (still waiting for an auto-tuned remix of “Spotted Horses”). In the TEI source are the start and end times for each of the “snippets.” My go-to tool for working with media is ffmpeg, and I knew I could break up the files into components, copying the bitrate into a new mp3. I wrote a quick XSLT to generate a shell script that would generate the ffmpeg commands to run. [gist id=”5830678” file=”convert_mp3.xsl”] This generated a nice file of the commands to run. [gist id=”5830678” file=”gistfile5.sh”] At this point, all the data has been processed, so I need to see if this this going to actually work. I wrote another XSLT to preview what was going on and make sure this approach was going to work ok. Nothing too fancy, just an HTML wrapper, with most of the “work” happening in the div2 element. [gist id=”5830678” file=”htmlaudio.xsl”] Since the segment file names were derived from their id attributes, I was able to just point at the file without a lot of poking around. Now for the test! I started playing with it and it appeared to work just fine. I then asked one of my colleagues who was working remotely to take a look at it, and she ran into a show stopper. She observed that when she loaded the page, only the first several of the clips were loading. In the audio element, I had added the preload=\"auto\" attribute to allow the file to buffer and play before then entire file had downloaded. When I profiled what was going on, I realized that somewhere around 20Mb of download, the browser was giving up preloading the audio for immediate playback. If you remove that attribute, the browser won’t buffer the file and you would have to download the entire file before you could start the playback. Definitely not what I was aiming at. Time to try something else. Audio Time Range In reading the MDN docs on HTML5 audio, I came across a section on Specifying a playback range . This looks promising! There is one file reference, and I just need to get the playback times in. It is unclear, however, from the description if the browser treats this as a single file transfer, or each segment as it’s own download thread. Fortunately it’s just a small tweak to the XSLT generating the audio elements. [gist id=”5830678” file=”time_ranges.xsl”] After checking the browser again, looks like the same issue is there; the browser treats each segment as its own download thread and chokes when it gets around 20Mb. Meh; the Internet. Ok, time to try something different. Audio Sprites When I was writing my book on developing HTML5 games, I ran across a great article Audio Sprites (and fixes for iOS) by Remy Sharpe. The idea draws inspiration from CSS sprite sheets where you put all your image assets into a single file then display the portion of the image you want on an HTML page. With audio sprites, instead of shifting coordinates of an image around, you shift the playhead of an MP3 file and tell it how long to play. This is really great for games as you can have a single file for players to download with all the audio files. Maybe this technique will work here… Since I wanted to see how well this would work, and not necessarily write a library to support this, I used the howler.js library which has support for audio sprites. Back to the XSLT. The howler.js API defines sprites by names to allow you to refer to then as variables in your code (again, it’s written for developing games). It also wants you to (in milliseconds) tell it where to start playing, and for how long to play. Ugh, my start and end times are in hh:mm:ss.s format. I wrote a quick function to explode the timestamps and add them together as milliseconds (actually this is a bit off, but I didn’t spend the time to work in the actual conversion units, but I wanted to see if this is going to work before I put that time in). [gist id=”5830678” file=”timeToMilliseconds.xsl”] Now I can set up JavaScript for the Howler object literal for use on the page. [gist id=”5830678” file=”timeToMilliseconds.xslt”] A few notes here, iOS devices require user interaction before an audio asset can be loaded. To handle the mobile devices, I added a button to the page that is hidden with a media query for desktop browsers. When the user clicked on it, they would see a thumper and a notice when the file had been loaded. I also had to add my own “play” buttons as this API is really meant for games. Awesome, it works! But is this really a good idea? This is kind of an exotic (“clever” in programming terms) approach. It also relies on an obscure library that may not be maintained in the future. This probably isn’t the best path forward… Blended Approach After some more thought, maybe what’s needed here is a blended approach. I liked the fact that with the timestamps, I only have to create one extra file (and not 2 * n clips for both mp3 and ogg formats), but there was that sticky preload issue. This is where JavaScript can also help. What if there was an obvious mechanism for a user to click and then I could use JavaScript to dynamically construct an audio element in the DOM, and only start streaming the “segment” the user requested? This just may work :) With a little JavaScript, I take a look at the DOM and construct an audio element, passing back to the browser the smallest version of the file (ogg then mp3) the browser can play back natively. [iframe src=”http://jsfiddle.net/wsgrah/3EfAD/15/embedded/js,html,css/” allowfullscreen=”allowfullscreen” frameborder=”0”] So the final product results in this, which has an animation to remove the icon, replacing it with the native audio playbar: [iframe src=”http://jsfiddle.net/wsgrah/3EfAD/15/embedded/result/” allowfullscreen=”allowfullscreen” frameborder=”0”] Generating Ogg Now that I’ve got a basic system for playing the audio that works on just about every browser, time to take a look at converting these audio files. There are about 60 MP3s that needed to get transcoded for this project. If it were just a handful, I may have just manually done the transcoding in something like Audacity, but there were a lot of files, and I’m a “lazy” developer. Obviously this is a another job for ffmpeg . I had to recompile it from homebrew (an OS X package manager) to include the libvorbis bindings. [gist id=”5830678” file=”gistfile4.txt”] After getting the proper transcoding libraries installed, I wrote a quick bash script to convert all MP3s in a directory to ogg. [gist id=”5830678” file=”transcode_ogg.sh”] After this ran (it took a few hours), I had a complete set of MP3 and OggVorbis files for the project. Conclusion After rethinking how to address the problem of streaming audio to multiple platforms, with various limitations on how the audio specification is implemented, I finally landed on something that is not novel. What it does do, however, is move away from an approach that is no longer widely supported (the use of QTSS), to a single method that leverages the native support of modern browsers to do something reasonably simple…play a bit of sound. I also got rid of a lot of JavaScript (which breaks), the reliance on another server (which breaks), and sped up the delivery of the audio to the client. Additionally, since this isn’t an exotic (or complicated) replacement, the next person who has to do something with this code in five years will have a fighting chance at figuring out what is going on!"},{"id":"2013-08-26-welcome-new-slab-grad-fellows","title":"Welcome, new SLab grad fellows!","author":"bethany-nowviskie","date":"2013-08-26 05:48:28 -0400","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"welcome-new-slab-grad-fellows","layout":"post","content":"The Scholars’ Lab is pleased and proud to announce our partnership with nine new graduate fellows for the 2013-2014 academic year! They represent seven academic disciplines in the humanities and social sciences at the University of Virginia, and join a distinguished group of past recipients of Scholars’ Lab fellowships. (Since 2007,  UVa Library has offered 44 fellowships to deserving grad students in fields as diverse as History, Archaeology, Computer Music, Anthropology, Economics, English, Ethnomusicology, French, Religious Studies, Art History, Linguistics, and Architecture.) First, we have our three winners of the UVa Library Graduate Fellowship in Digital Humanities . They are: Erik DeLuca of the Composition and Computer Technologies Program in UVa’s McIntire Department of Music\n“Community Listening in Isle Royal National Park, a sonic ethnography” Gwen Nally of the Corcoran Department of Philosophy\n“When Socrates Misleads: Falsehood and Fallacy in Plato’s Dialogues” and Tamika Richeson of the Corcoran Department of History\n“I Know What Liberty Is: Black Motherhood, Labor, and Criminality, 1848-1878” These fellows will have the opportunity to work closely with Scholars’ Lab staff over the course of the year, in applying digital methods to their dissertation research and presenting that work online.  P lease join us on September 10th at noon in the Scholars’ Lab, when we welcome Erik, Gwen, and Tamika with a casual luncheon, and have the opportunity to hear a brief summary from each of them, about what they hope to accomplish this year! Next, we’re getting started with a third year of the Praxis Program here at UVa, which is now home base for the new, international Praxis Network !  Last year saw the refinement and reimagining of Prism (not that Prism), a tool created by the first Praxis cohort in 2011-12. Prism is a web application for crowdsourcing interpretation, and for thinking through the relationship of humanities inquiry to the methods and motives of crowdsourcing. This year, the 2013-14 Praxis team will rethink, revive, and (we expect) utterly remake the Ivanhoe Game, another platform for playful, collaborative interpretation of documents and artifacts. 2013-2014 Praxis Fellows include: Scott Bailey (Religious Studies) Elizabeth Fox (English) Veronica Ikeshoji-Orlati (Classical Art &amp; Archaeology) Stephanie Kingsley (English) Francesca Tripodi (Sociology)\nand Zachary Stone (English) Keep an eye on the Scholars’ Lab blog for news throughout the year, on the work of all of our wonderful graduate fellows!"},{"id":"2013-09-05-and-so-it-begins","title":"And so it begins...","author":"veronica-ikeshoji-orlati","date":"2013-09-05 13:15:48 -0400","categories":["Grad Student Research"],"url":"and-so-it-begins","layout":"post","content":"This is my first blog post (ever), so I have spent a good deal of time hemming and hawing over an appropriately novel and pithy title to headline my blogging debut. Needless to say, that hasn’t happened. Anyway, I’m Veronica Ikeshoji-Orlati, a 4th year PhD Candidate in Classical Art &amp; Archaeology in the McIntire Department of Art here at UVA. I took a BA in Classics, with minors in Art History and Philosophy, from the University at Buffalo (2003), later returned to UB for an MA in Art History (2010), and am now weaving together the disparate threads of my personal and academic interests into my dissertation, entitled Music, Performance, and Identity in 4th century BCE South Italian Vase-Painting . The field of Classical Archaeology, and South Italian Archaeology in particular, offers up many challenges, incredible opportunities, and fascinating methodological questions, so I find my research engaging on many levels. I am thrilled to be part of the 2013-14 Praxis cohort for a panoply of reasons. The two most important to me are 1. the opportunity to work with, and learn from, people from vastly different backgrounds with diverse personal, professional, and academic interests, and 2. the chance to plunge into the field of Digital Humanities with patient, knowledgeable guides and amicable, resourceful accomplices. Getting to work on the Ivanhoe game is an added benefit, since pedagogy happens to be a topic which occupies a significant corner of my mind and the role of digital spaces and teaching tools in the classroom is a developing interest. That just about sums up what an introductory post should say. I look forward to sharing what we’re doing, and how it impacts my own research and thinking, here!"},{"id":"2013-09-08-a-bit-about-me","title":"A bit about me","author":"francesca-tripodi","date":"2013-09-08 12:30:46 -0400","categories":["Grad Student Research"],"url":"a-bit-about-me","layout":"post","content":"Hello readers! My name is Francesca Tripodi, and I am one of the 2013/2014 Praxis Fellows at UVa. I come to academia from a more circuitous route. Unlike many graduate students that I meet, I didn’t realize that I wanted to be in academia until much later in life. As a student at the Annenberg School at the University of Southern California my immediate interests were working in media. But after an extremely fulfilling internship at Fox Cable Networks Group, I caught the travel bug and took off to Australia where I spent six months backpacking “down under” followed by a month exploring New Zealand and month in Thailand. When I returned to The States, I yearned for a more global metropolis and spent the better part of my twenties working in Washington, DC. My first job was at the United States Telecommunications Training Institution (USTTI). I worked as a liaison between the private sector (Cisco Systems, Bechtel, Qualcomm, and Microsoft) and the public sector (FCC, NTIA) to help deliver low-cost training programs to citizens of developing countries looking to expand and improve their digital infrastructure.  In addition to organizing course content, I worked with USAID offices and the State Department to coordinate the logistics of participants traveling to the US for the training (including visa processing).  After that job, I moved to Georgetown University and eventually became the Program Director of Pathways to Success - an academic immersion program that brings high school students from rural America to Georgetown University in an effort to improve  minority involvement in STEM education. One of my greatest achievements in this position was helping to secure additional funds (1.3million) to continue financing the program through 2012. As an employee at Georgetown I also took advantage of their tuition remission benefits and earned my MA in Communication, Culture and Technology. It was there that I learned I ask very “sociologically oriented” questions and with the help of my advisor decided to continue my education at the University of Virginia. As a fourth year PhD Candidate in the Sociology Department, I am currently working on data that I collected from an ethnographic study in rural Louisiana on alligator hunting. Some of my more immediate findings are the importance of female hunting in the community and the parallels between the Cajun culture I experienced and the media representations of Cajun life on the show “Swamp People.” In the spring I hope to defend my dissertation proposal in an effort to answer the central research question that currently occupies my mind: To what extent does media influence a community’s boundary making process? In what ways do these boundaries shift depending on who controls the mediated narrative? I am also happily married to a wonderful guy and six weeks ago we welcomed to the world a beautiful baby boy."},{"id":"2013-09-08-greetings-from-stephanie-new-praxis-fellow","title":"Greetings from Stephanie, new Praxis Fellow!","author":"stephanie-kingsley","date":"2013-09-08 15:23:41 -0400","categories":["Grad Student Research"],"url":"greetings-from-stephanie-new-praxis-fellow","layout":"post","content":"I am excited to be a part of the new Praxis cohort and would like to take a few moments to introduce myself before a flurry of–ideally, great and innovative–thoughts populate the blog.  I am a second-year MA student in the English department, specializing in American literature, textual studies, and digital humanities.  My academic interests include Colonial and 19th-century American literature and history, as well as American book history.  My goal is to graduate this Spring and work in publishing or alternate academia. Alongside my literary studies, a key component of my career at the University of Virginia has been to learn as much as possible about the digital humanities.  I have assisted on Alison Booth’s Collective Biographies of Women, a database of women grouped in communities based on the 19th-century biographies in which they are featured.  I continued my DH education with Documents Compass’s People of the Founding Era, an online archive of individuals mentioned in the Papers of the Founding Fathers .  Additionally, I took David Seaman’s Rare Book School Course “XML in Action” this past summer.  These projects have introduced me to digital editing and archiving, while also getting me thinking about other applications of DH.  An area I have yet to break into is crowdsourcing, inviting user participation and contribution, and this is an area which Praxis will plunge me into immediately.  Already, I am blogging… reaching out to people… and excited to be doing it. We discussed in our first Praxis meeting a somewhat conflicted relationship to technology which we all share.  For my part, I still have no Smart Phone, forget to check my Facebook, and–lacking a GPS–have been known to use a paper map; notwithstanding all this, a hobbyist dream of mine is to create a digital archive of old family letters and photographs, alongside ancestor profiles.  I believe that DH is the key to preserving and disseminating this sort of material throughout the world.  At the same time, I will never cease to love the smell of a brand-new paperback book, or the feel of one of the many treasures housed in UVA Special Collections.  As a textual-studies and DH scholar, I inhabit both worlds and exist in a constant state of perplexity and wonderment… a state in which I now turn my attention to the work at hand.  To the SLab and Praxis cohort, and all our Praxis-blog followers, I am glad to make your acquaintance and thrilled to begin our work this year!"},{"id":"2013-09-09-greetings-and-salutations","title":"Greetings and Salutations","author":"elizabeth-fox","date":"2013-09-09 04:08:28 -0400","categories":["Grad Student Research"],"url":"greetings-and-salutations","layout":"post","content":"Hello all! My name is Eliza Fox, and I’m a third-year PhD student in the English Department.  My research focuses on the Victorian novel, along with secondary interests in children’s and young adult literature.  I’m not a total newcomer to the DH world: I spent last year as a NINES Fellow, working on projects that ranged from encoding a manuscript of Prometheus Unbound to updating metadata for NINES’s vast collection of aggregated digital objects.  I’ve studied databases at DHSI, and my undergraduate career included the rudiments of HTML.  I’ve also spent an inordinate amount of time enjoying Prism, otherwise known as the fruits of Praxis Past.  (My masterpiece: Prism on Prism, in which I highlighted articles on NSA surveillance for “Uncertainty about the future” and  “Fear.”  Clearly, irony is my medium.) I recognize, however, that none of this has prepared me for the whirlwind year that lies ahead.  If my past experiences have allowed me to dip my toe in the DH pool, Praxis will force me to jump into the deep end and to figure out – live, in public, and in the moment – how to swim. But, strange as it may sound, it’s a jump that I’m immensely excited to take.  Despite my love/hate relationship with technology (I once nicknamed my computer Cher – short for Chernobyl), I’m perpetually intrigued by the opportunities DH offers to represent, reconsider, and reconstitute our studies.  Praxis, in particular, offers the kind of full-throttle, hands-on, learning-by-doing approach that allows for a true appreciation and mastery of the field.  I’ve had many friends go through this program, and I’ve watched them transform, in just a few months, from technological novices into committed DH scholars.  Whatever their introductory levels of experience, they’re now designing archives, teaching coding, and project managing. I can’t wait to join them."},{"id":"2013-09-11-a-bit-more-medieval","title":"a bit more medieval","author":"zachary-stone","date":"2013-09-11 14:30:38 -0400","categories":["Grad Student Research"],"url":"a-bit-more-medieval","layout":"post","content":"Well. I must admit to some surprise and no small degree of trepidation regarding my presence here (both in the Praxis program and here online as a blogger being read by you [blog readers and, I guess, bots]). For example when, in our first meeting, I asked what a ‘wiki’ actually was I found out that they are ‘like Google Docs,’ which of course only begged the question on my part: what is a Google Doc? So. As we can see, this will be a fun year. In truth, I applied to the Praxis program not because of a strong background in the Digital Humanities (hence forth ‘DH’ [incl. the definite article]) but rather on account of my relative lack of any background in DH. Lack of background, however, ought not be equated with lack of interest. As a medievalist, specifically a student of medieval books, manuscripts, and textual cultures, I have both pragmatic and theoretical interests in DH. Pragmatically DH offers new ways to share previously impossible (or at least highly improbable) amounts of data, specifically visual data. As a palaeographer/book historian this allows me to avoid certain compromises forced on previous generations by the exigencies of print. For example, when cataloguing the medieval manuscripts of Wadham College, Oxford I looked to other, printed, manuscript catalogues for guidance regarding the type and amount of information to include.In those catalogues choices regarding how much description of certain facets of a given book to include required consideration of the volume’s overall publishability, especially given the expected low sales volume. Conversely, online cataloguing would allow the inclusion of all the material the cataloguer finds relevant and useful regardless of length. This is an admittedly conservative example, but nevertheless, my experience cataloguing suggested to me the extent to which my scholarly thought, or perhaps more properly ‘imagination’ in its strictest medieval sense, is constrained to the medium in which I am thinking. So pragmatic interest is pretty easy to grasp but positing ontological stakes might seem a bit much. Nor can I provide a neat, concrete, example. It’s more of a feeling that the types of textual production and consumption which occur in pre- and post- print environments share a certain resonance that itself poses an ontological challenge- a challenge the very being-ness- of a print-centric intellectual culture. As we begin to think about our charter we, the new Praxis team, have begun to think about credit, i.e. who gets it. In print world this is pretty tidy. Publication represents a convenient end stop to the production process at which juncture credit and the rewards therein may be distributed along traditional lines of authorship, etc. Online, things seem a bit more medieval. By that I mean the lines of authorship and production are blurred to the extant that disentangling them becomes not only impossible but somewhat ludic in principle. Kind of like the manuscripts I spend most of my time buried in. Anyhow, as usual, I run on. In short, I am excited about the chance Praxis offers to both learn new DH skills and understand how those skills fit in the long, fluid, tradition we conventionally call the ‘Liberal Arts’ or ‘Humanities.’"},{"id":"2013-09-11-mapping-crowd-sourced-bicycle-data","title":"Mapping Crowd Sourced Bicycle Data","author":"chris-gist","date":"2013-09-11 06:14:53 -0400","categories":["Geospatial and Temporal","Visualization and Data Mining"],"url":"mapping-crowd-sourced-bicycle-data","layout":"post","content":"Background Charlottesville is not the easiest place to ride a bicycle.  There are obstacles beyond the narrowness of the streets.  Let’s take a look at a few of these. ![](http://static.scholarslab.org/wp-content/uploads/2012/08/cvilleElevRoad-1024x768.png) Cville street grid overlaid on elevation surface The above map shows the elevation around Charlottesville with dark green being the lowest areas and bright red being the highest.  The Charlottesville street system is primarily laid out on top of a series of connected ridges.  This fragmented grid leaves only a small number of routes between different “sections” of the street network.  Not only that, most of the ridges run in a north/south direction with one going east/west.   In the following map, circles (blue for downtown and orange for UVa Central Grounds) show two “employment centers” (those circles will be used as reference in other maps). ![](http://static.scholarslab.org/wp-content/uploads/2012/08/cvilleCentersl-1024x768.png) Downtown and UVa reference circles To make matters worse, there are two rail lines running through town. ![](http://static.scholarslab.org/wp-content/uploads/2012/08/rail-1024x768.png) Cville rail corridors These two lines cross at the train station on W. Main St.  They esentially cut the city into four quads. [![](http://static.scholarslab.org/wp-content/uploads/2012/08/quads1-1024x768.png)](http://static.scholarslab.org/wp-content/uploads/2012/08/quads1.png) Cville quadrants The rail lines further limit the ability to traverse the street network.  Below are all the street crossings (over/under rail lines) from one quad to another.  The crossings marked in red are not, in my opinion, suitable for bicycles in current configurations. [![](http://static.scholarslab.org/wp-content/uploads/2012/08/quadsCrossings1-1024x768.png)](http://static.scholarslab.org/wp-content/uploads/2012/08/quadsCrossings1.png) Cville quad passes That leaves us with some serious bottlenecks for bicycle movement that, not coincidentally, tend to be along busy streets. Here is a street map to provide context for anyone not familiar with Charlottesville. [![](http://static.scholarslab.org/wp-content/uploads/2012/08/referenceMap-1024x768.png)](http://static.scholarslab.org/wp-content/uploads/2012/08/referenceMap.png) Base map uses Open Street Map data Cville Bike mApp Back in April of 2012, the  Thomas Jefferson Planning District Commission  (TJPDC) launched a bicycle route data collection project called   Cville Bike mApp .   The TJPDC  adapted a smart phone apps that allowed users to log all their bicycle trips and share the location data with the TJPDC.  Data collection officially closed on May 18.  The apps are still available and the TJPDC is still collecting data.    The source code can be downloaded  here . Kelly Johnston and I were asked to consult with the TJPDC intern working with the data.  We discussed various visualizations and spatial analyses.  We asked for and were given permission to have the data to see what we could do.  Let the fun begin! Data Issues The data arrived after being “cleaned” of many data points.  This was necessary because there were many obvious cases where people had left their logging app running after they were at home, work or driving.  There were also routes with only a few datapoints over a very large geographic area.  The data was in Excel with each record containing a single latitude/longitude pair with trip ID, date and time.  There was a separate table listing user ID, trip purpose, weather and notes.  There is no personal information in the data we received.  Of the data mapped, there were 1011 trips made by and 118 users. [![](http://static.scholarslab.org/wp-content/uploads/2012/08/badTrip1-1024x768.png)](http://static.scholarslab.org/wp-content/uploads/2012/08/badTrip1.png) Someone forgot to turn off their app [![](http://static.scholarslab.org/wp-content/uploads/2012/07/badTrip2-1024x768.png)](http://static.scholarslab.org/wp-content/uploads/2012/07/badTrip2.png) Clearly a few points missing along this route The first idea for visualizing these data was trying to quantify the trips by route.  I thought the idea of using a spatial join to aggregate the trips seemed a useful technique.  However, there were a few issues with the data.  Relying on GPS coordinates from a variety (and quality) of smart phones is problematic. [![](http://static.scholarslab.org/wp-content/uploads/2012/07/routesRaw-1024x768.png)](http://static.scholarslab.org/wp-content/uploads/2012/07/routesRaw.png) Main St. Routes The first technique to try would be to expand the road centerlines using buffers to increase the catchment area for the roads.  Then, spatially join the routes to the roads creating a count of unique trips along any roadway section.  After a little trial and error, I settled on 60 foot buffers around all the road segments.  Of course, this still does not capture all the trips as shown the the image below. [![](http://static.scholarslab.org/wp-content/uploads/2012/07/bufferIssue1-1024x768.png)](http://static.scholarslab.org/wp-content/uploads/2012/07/bufferIssue1.png) Red lines indicate edge of 60ft buffer The Visualizations I ran the spatial join twice, once to capture the total number of trips and once to capture the unique users.  I then took these data from the buffers and joined them back to the road centerline to associate the trip and user counts with the centerline files in order to create something like the image below. [![](http://static.scholarslab.org/wp-content/uploads/2012/07/uniqueTrips-1024x768.png)](http://static.scholarslab.org/wp-content/uploads/2012/07/uniqueTrips.png) Unique Trips [![](http://static.scholarslab.org/wp-content/uploads/2012/07/uniqueUsers1-1024x768.png)](http://static.scholarslab.org/wp-content/uploads/2012/07/uniqueUsers1.png) Unique Users This technique creates some interesting artifacts.  When you spatially join relatively inaccurate data like these, some of the aggregated features will end up being associated with features that were not meant to be.  In other words, side streets that were not or lightly used end up with much higher counts than they should have.  Case in point, W. Main St: is it possible that riders deviated to all the side streets and returned back to Main? [![](http://static.scholarslab.org/wp-content/uploads/2012/07/spatialJoinProblem11-1024x768.png)](http://static.scholarslab.org/wp-content/uploads/2012/07/spatialJoinProblem11.png) Side street issue Of course, the answer is no and the problem becomes acutely clear when the trip data is overlaid. [![](http://static.scholarslab.org/wp-content/uploads/2012/07/spatialJoinProblem2-1024x768.png)](http://static.scholarslab.org/wp-content/uploads/2012/07/spatialJoinProblem2.png) Lack of accuracy So, the buffer segments of the side streets nearest to W. Main St. were picking up trips along Main St.  I tried several times to deal with this using various techniques and found nothing very helpful.  Next option?  Move to raster analysis.  I used a tool in ArcMap called Line Density to create a surface raster of all the unique trips. [![](http://static.scholarslab.org/wp-content/uploads/2012/08/trip2-1024x768.png)](http://static.scholarslab.org/wp-content/uploads/2012/08/trip2.png) Trip density surface raster The map above clearly shows the most traveled streets in the study are W. Main St., Rugby Rd./Dairy Rd., Alderman Rd., Water St., Market St., Preston Ave., Rose Hill Dr. and 10th St. NE/Locust Ave.  There is also heavy traffic on the path between Rugby and Emmett adjacent to Lambeth Field. The phone app also asked people to log the type of trip.  This is mapable! [![](http://static.scholarslab.org/wp-content/uploads/2012/07/tripType-1024x768.png)](http://static.scholarslab.org/wp-content/uploads/2012/07/tripType.png) Trips by type As you can see in the map, commuting seems to outweigh the other trip types.  What about densities for these types?  I decided to make a map with a series of small multiple maps to demonstrate the densities of the four categories of trips I identified. [![](http://static.scholarslab.org/wp-content/uploads/2012/07/tripsByType-1024x768.png)](http://static.scholarslab.org/wp-content/uploads/2012/07/tripsByType.png) A series of small multiples - term coined by E. Tufte Clearly, there is a lack of data evident in at least one category.  However, I think there is a definite difference between the four types of rides.  This led me to look at the origination and destination of commuting trips. [![](http://static.scholarslab.org/wp-content/uploads/2012/08/commuteOrigDest-1024x768.png)](http://static.scholarslab.org/wp-content/uploads/2012/08/commuteOrigDest.png) Originations/destinations raster Other than the fact that downtown is both a hot spot in morning and afternoon for originations and destinations, I am not sure what else can be gleaned from that map. I also wanted to see how steep slopes compared to the heaviest used routes.  I created a slope layer to show the steep slopes and overlayed the route density layer. [![](http://static.scholarslab.org/wp-content/uploads/2012/08/slopes-1024x768.png)](http://static.scholarslab.org/wp-content/uploads/2012/08/slopes.png) Trip density raster overlaid w/ steep slopes As you can see, there is not a great deal of overlap between the most heavily used routes and high slope areas.  In those areas where there is overlap, most tend to be where the slope is just off the road and does not represent the road grade as along 5th St. Extended or Market St between 2nd St. NW and McIntire Rd. Findings I don’t think there are many surprises in this data.  Cyclists tend to take the flattest routes or at least the ones that don’t have steep climbs.  W. Main St. is the highest trafficked area.  Locust Ave., Rose Hill Dr. and Rugby Rd. are north/south collectors for that corridor.  This phenomenon has two causes in my opinion.  First, W. Main is the flat, straight corridor between the two largest population/employment centers in Charlottesville, UVa and Downtown.  Secondly, as mentioned earlier, W. Main is the only east/west oriented ridge in the city. What I do find interesting is some of the alternative paths cyclists are taking.  The 8th St. NW connector (one of my favorite short cuts) under the railroad to the 10th and Page neighborhood connects to a parking lot underpass that bypasses W. Main St.  The extension of the parking lot leads to 7th St. SW which leads to a road within a condominium development (Walker Square).  This route extends to Grove St. and Roosevelt Brown Blvd. (and the UVa Hospital).  This path traverses all four quads, takes advantage of three underpasses, and uses very little in the way of busy streets without bike lanes. [![](http://static.scholarslab.org/wp-content/uploads/2012/08/alternatePaths-1024x768.png)](http://static.scholarslab.org/wp-content/uploads/2012/08/alternatePaths.png) The four quad alternative Here is a rather heavily used route along the bike/ped pathway from from Ruby Rd. to Emmett St.  This path connects UVa to Barracks Road Shopping Center. [![](http://static.scholarslab.org/wp-content/uploads/2012/08/alternatePaths2-1024x768.png)](http://static.scholarslab.org/wp-content/uploads/2012/08/alternatePaths2.png) Lambeth bike/ped path Another large barrier is the Rivanna River.  The past few years have seen a good deal of development on Pantops east of town.  The only way to get there is over Free Bridge on the US 250 Bypass.  Neeldess to say, that is not an inviting route for cyclists.  There are paths on both sides of the river.  There needs to be a bridge connecting them. [![](http://static.scholarslab.org/wp-content/uploads/2012/08/alternatePaths3-1024x768.png)](http://static.scholarslab.org/wp-content/uploads/2012/08/alternatePaths3.png) Dream crossing for the Rivanna River Conclusion This type of data collection has inherent issues.  Only people with smart phones can participate, which really limits the sample size and variety.  While always improving, there is a lack of locational accuracy with smart phones.  The phone apps require the user to start and stop the app at the appropriate times.  These issues lead to a small amount of  collected data that has to be further slimmed into “good data.”  There is also seems to be a distinctive lack of data around UVa.  The timing of the study did not allow for UVa students to fully participate since they were either in exams or on break for a good portion of the study time.  However given a larger sample of riders over a longer period of time, I think some meaningful results would be forthcoming.  I urge the planning commission and local governments to consider more of this type of survey to gather better data. If you have some ideas about better ways to visualize or analyze these data, I would love to hear about it."},{"id":"2013-09-16-greetings","title":"Greetings","author":"scott-bailey","date":"2013-09-16 06:09:11 -0400","categories":["Grad Student Research"],"url":"greetings","layout":"post","content":"Hello! My name is Scott Bailey, and I’m one of the new Praxis Fellows. I am also a Ph.D. student in Religious Studies, writing a dissertation on vulnerability as a locus of dogmatic reflection. Taking a cue from Brené Brown’s work on vulnerability, I’m asking what it means to think through the vulnerability of Christ, leading us to think of the vulnerability of God and of humanity. Much of this is done within the context of 20th Century Protestant theology, with a particular focus on Eberhard Jüngel’s theology. I am also avidly interested in technology, though, both inside and outside the classroom. For the past three years, I was the Teaching + Technology Support Partner for the Department of Religious Studies, and helped faculty and grad students learn to use and incorporate different applications into their teaching practice. Of particular interest were applications like WordPress and NowComment. I applied to Praxis in order to keep pushing further, to learn about the what makes some of these applications work, and to learn more broadly about the world of Digital Humanities. From just a bit of exposure to HTML/CSS, I’ve already found that the concrete character of writing code is a welcome balance to the often abstract and speculative questions in theological ontology with which I am concerned. I look forward to working with the other Praxis Fellows and the rest of the Scholars’ Lab in the year to come."},{"id":"2013-09-19-praxis-time-capsule","title":"Praxis Time Capsule","author":"cecilia-márquez","date":"2013-09-19 12:22:44 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"praxis-time-capsule","layout":"post","content":"Obviously this is a little late, but comprehensive exams have a way of stealing whatever time you thought you had.  I wanted to write a post that reflected on my time in Praxis, hopefully share a bit about what I am taking away, suggestions I have for future Praxis generations and an opportunity to share some general gratitude! What I’m Taking Away New level of computer/digital literacy: as I’ve mentioned in other blog posts, the way I frequently dealt with computer problems was to either cry, punch my computer, or take a nap.  While I will miss these fantastic coping strategies, Praxis has actually made me someone that other people ask for help with their computers. Clearer sense of what my interest in digital and alt-ac careers might be: I have learned that I hate ruby and love CSS and design aspects of website development. Fuller understanding of what “digital humanities” actually is and how I might integrate it in my own work Excitement about alt-ac possibilities New network of friends and colleagues Suggestions for Future Praxi Don’t put off the hard conversations–take on questions like what is our goal for this tool? What are the major things we want a user to get out of it? Tackle those early and just force yourself to sit in a room until you hammer them out Learn to love (or at least tolerate) conflict. As someone who has done a lot of “interdisciplinary” work in the past, that was never as hard (or rewarding) as it was in Praxis.  It becomes a tricky balancing act of maintaining an awareness for what is useful/helpful for your discipline and what the tool might be separate from those disciplinary constraints. There really is no such thing as a stupid question The Scholars’ Lab team are beyond lovable so don’t be scared to ask lots of questions. Gratitude Thanks to everyone in the Scholars’ Lab faculty and staff who resisted the temptation to roll their eyes at my one millionth question about html and CSS formatting.  Also thanks for helping me work through all of my insecurities about technology and not judging my borderline obsession with Honey Boo Boo. Thanks to the Praxis team.  I learned more about the practice of “interdisciplinarity” with you 5 than I have ever have.  I also learned a lot about collegial and productive conflict from you all. it also affirmed my love of collaborative and team-based work and gave me hope that there are spaces within (or adjacent to) the academy that can be full of laughter and making mistakes openly."},{"id":"2013-09-25-2013-2014-praxis-charter-ratified","title":"2013-2014 Praxis Charter ratified!","author":"stephanie-kingsley","date":"2013-09-25 09:41:25 -0400","categories":["Grad Student Research"],"url":"2013-2014-praxis-charter-ratified","layout":"post","content":"Last week the new Praxis cohort ratified its charter .  This important document ended up demanding much more deliberation than we had anticipated.  Nonetheless, after a couple weeks of thinking about what really mattered to us in commencing our program, we established a set of core beliefs and structuring principles which I believe will help guide us through a very exciting year. We took inspiration from the previous cohorts’ charters in several respects because in many ways we feel we are continuing in the same tradition.  We, too, will conduct our work in the spirit of open source.  We, too, feel that a key part of this experience will be our all sharing credit for the project.  We also hope to learn programing skills central to DH professions, and we plan to launch a digital tool at the end of the year as an outcome of our participation in the program. A key tenet which is of primary importance to our particular cohort is that of flexibility, and this ideal influences many aspects of our charter.  For instance, we want the tool we build to be adaptable for various scholarly needs.  As of yet, we are in the early stages of conceptualizing this tool, and the issue of flexibility and utility will no doubt arise as we progress.  (I anticipate many reflective blogs to come on that topic.)  Perhaps even more importantly, we plan to be flexible–understanding, sensitive–with each other.  We all come from different scholarly and professional backgrounds, and we all have personal lives with various demands and responsibilities.  It will be our goal to be supportive of each other personally while working together to make the Praxis experience an enriching one for all. Last Wednesday, Eric and Wayne showed us how to use a text editor, Github, and a rake task to publish the new charter on the Praxis website.  This was our first lesson in programing.  Our brilliant SLab computer mentors encoded the new charter text in Markdown, committed it to our code repository, and then let us do the simple–but no-less-important–step of hitting “Enter.”  Upon striking the key, watching a whir of yet incomprehensible code flash across the screen, and thus finalizing our newly forged charter, we felt a rush of glee.  In that moment, we had plunged headfirst into the new and intriguing world of Digital Humanities, and we had a charter to guide our voyage."},{"id":"2013-09-25-neatline-2-1-0","title":"Neatline 2.1.0","author":"david-mcclure","date":"2013-09-25 06:55:46 -0400","categories":["Announcements"],"url":"neatline-2-1-0","layout":"post","content":"We’re pleased to announce the release of Neatline 2.1.0 ! This is a fairly large maintenance release that adds new features, patches up some minor bugs, and ships some improvements to the UI in the editing environment. Some of the highlights: A “fullscreen” mode (re-added from the 1.x releases), which makes it possible to link to a page that just displays a Neatline exhibit in isolation, scaled to the size of the screen, without any of the regular Omeka site navigation. Among other things, this makes it much easier to embed a Neatline exhibit as an iframe on other websites (eg, a Wordpress blog) - just set the src attribute on the iframe equal to the URL for the fullscreen exhibit view. Eg: Thanks coryduclos, colonusgroup, and martiniusDE for letting us know that this was a pain point. A series of UI improvements to the editing environment that should make the exhibit-creation workflow a bit smoother. We bumped up the size of the “New Record” button, padded out the list of records, and made the “X” buttons used to close record forms a bit larger and easier-to-click. Also, in the record edit form, the “Save” and “Delete” buttons are now stuck into place at the bottom of the panel, meaning that you don’t have to scroll down to the bottom of the form every time you save. Much easier! Fixes for a handful of small bugs, mostly cosmetic or involving uncommon edge cases. Notably, 2.1.0 fixes a problem that was causing item imports to fail when the Omeka installation was using the Amazon S3 storage adapter, as we do for our faculty-project installations here at UVa. Check out the release notes on GitHub for the full list of changes, and grab the new code from the Omeka add-ons repository . And, as always, be sure to send comments, concerns, bug reports, and feature requests in our direction. In other Neatline-related news, be sure to check out Katherine Jentleson’s Neatline-enhanced essay “ ‘Not as rewarding as the North’: Holger Cahill’s Southern Folk Art Expedition,” which just won the Smithsonian’s Archives of American Art Graduate Research Essay Prize. I met Katherine at a workshop at Duke back in the spring, and it’s been a real pleasure to learn about how she’s using Neatline in her work!"},{"id":"2013-09-30-are-we-gaming-or-just-simulating","title":"Are we gaming or just simulating? ","author":"francesca-tripodi","date":"2013-09-30 09:38:02 -0400","categories":["Grad Student Research"],"url":"are-we-gaming-or-just-simulating","layout":"post","content":"As we Praxis Program fellows embark on just what exactly we want our version of the Ivanhoe Game to accomplish we’ve been doing some “research” - good old fashion game playing. However, I’ve noticed a sense of frustration within our play, or perhaps it is just me that becomes a bit frustrated with the games at this point. I think part of my frustration comes from limited time to devote to the process of the game. Since the rules of the games we’ve play thus far are limited or nonexistent, I’m often unclear about what exactly I should be doing and feel like I spend more time trying to figure out what’s going on than actually contributing. I’m also unsure if my “moves” are being received (and subsequently not enjoyed by the team) or just ignored all together. In the end my take away from our games is that it feels like we’re all playing our own game alongside one another without communicating what we are doing, why we’re doing it, and most importantly why we think it’s fun. What I’m getting at here is that we call Ivanhoe a “game” but thus far, our gameplay doesn’t have any game-like properties. So one might ask, what is a game? Well in his blog, Digital Scholar Tom Chatfield provides a fairly clear definition:  ”A game means submitting to an external set of rules defining particular things you are supposed to achieve: goals, achievements, points, a certain amount of exploration or action, kills, items, whatever.”  The use of the word “game” gets even more specific when applying it to a pedagogical framework. As Gredler (2004) argues, when we create games for learning they should be competitive exercises with the ability to win. Winning, she notes, should be based on existing knowledge or skill sets.  Perhaps this is where we run into trouble. As an interdisciplinary endeavor we seem to lack a common set of skills by which we could create challenges. Moreover, we seem hesitant to assign a leader over our games to outline what exactly is the goal and how one can achieve it. Finally, there lacks the element of competition that Gredler indicates is essential for differentiating between a game and a simulation. Therefore, I think that what we’ve been doing up to this point is playing simulations . “Unlike games, simulations are evolving case studies of a particular social or physical reality. The goal, instead of winning, is to take a bona fide role, address the issues, threats, or problems arising in the simulation, and experience the effects of one’s decisions” (Gredler 573). While simulations are also fun, we must acknowledge the potential constraints of a simulation - most importantly that of time. If our goal is to use Ivanhoe in a classroom, this is less of an issue. Students could figure out their roles, address problems, and figure out the simulation while subsequently learning the topic at hand. However, if we’re wanting to create a tool that is used both for pedagogy and in a research setting among peers, time could be an issue. At an already hectic conference will attendees have time to figure out what’s going on, what role they should take, and devote time to contributing when their contributions might not be read or acknowledged? So a big thing that I think we need to decide on is do we want to create a simulation or do we want to create a game? Perhpas we could do both - in a “ Type 2 ” Platform we could provide links to each resource. But, if we think we want to create a game (as well as a simulation), isn’t it time we started playing games? References: Gredler, M. E. (2004). Games and simulations and their relationships to learning. In D. H. Jonassen (Ed.),  Handbook of research for educational communications and technology  (2nd ed., pp. 571-82). Mahwah, NJ: Lawrence Erlbaum Associates."},{"id":"2013-10-07-experimental-typesetting-with-neatline-and-shakespeare","title":"Experimental typesetting with Neatline and Shakespeare","author":"david-mcclure","date":"2013-10-07 06:45:05 -0400","categories":["Research and Development"],"url":"experimental-typesetting-with-neatline-and-shakespeare","layout":"post","content":"[Cross-posted from dclure.org ] Click here to view the exhibit . I’ve always been fascinated by the geometric structure of text - the fact that literature is encoded as physical, space-occupying symbols that can be described, measured, and manipulated just like any other two-dimensional shapes. There’s something counter-intuitive about this. When I look at a letter or a word, I see particles of sound and meaning, transcendental cognitive forms, not things that could be straightforwardly described as a chunks of vector geometry. And there’s definitely a truth to this - I do think that texts have a kind of extra-physical cognitive essence that’s independent of their visual instantiations on pages or screens, and that it’s usually this common denominator that’s most interesting when we talk about literature with other people. And yet, in the context of any individual reading, the physical structure of documents - the set of pragmatic decisions that go into the design, layout, and formatting of text on the page - can have subtle but significant effects on how a text feels, on the imaginative dreamscape that surrounds it in your mind when you think back on it days or weeks or years after the fact. This is definitely true, for example, at the level of some-thing like font selection, which encodes a kind of “meaning metadata” about the text - where it comes from, who it’s intended for, how serious it imagines itself to be, etc. But I think it also holds at the level of more incidental, pseudo-random aspects of typesetting. For example, how does the vertical line traced out by the right margin of a paragraph or stanza color the reader’s affective reaction to the literary content? Does a jagged, unjustified border make the text feel more tumultuous and Dionysian? Would the same text, printed with a justified margin, become more emotionally controlled and orderly? I think of cases like Whitman’s long lines, which often have to be prematurely broken at the right edge of the page, resulting in a kind of clumpy, disorganized visual gestalt. I doubt that Whitman intended this, in the strong sense of the word (although I don’t know that he didn’t), but it has a kind of symbolic affinity with the poetry itself - sprawling, organic, uncontainable. When I think of Whitman, the image that appears in my mind consists largely of this. I wonder what it would be like to read an “unbroken” Leaves of Grass, printed on paper wide enough to accommodate the lines? Would it become more metaphysical, detached, ironical? Or would it be just the same? Anyway, this kind of speculation fascinates me. I’ve been thinking a lot recently about experimental modes of digital typesetting that would be completely impossible on the analog page - new ways of presenting text on screens to evoke certain feelings or model intuitions about the structure of language. As a quick experiment, I decided to use Neatline to try to capture a certain aspect of my experience of reading Shakespeare. I’ve always been interested in the notion of language as kind of progressive enveloping of words - they’re printed side-by-side on the page as equals, but the meaning of a syntagm grows out of the ordering of the tokens. Each exists in the context of the last and casts meaning onto the next; each word is contained, in a sense, inside the sum of its predecessors. I was taken by this idea when I read Saussure and company in college, because it seemed to map onto my own experience of reading poetry - the sensation of scanning a line always felt more like a descent than a left-to-right movement, a shift from the surface (the beginning) to the center (the end). To play with this, I built a Neatline exhibit that typesets a single Shakespearean couplet in a kind of recursive, fractal, Prezi -like layout in which each successive word is “physically” embedded inside of one of the letters of the previous word. Reading the couplet literally becomes a matter of magnification, zooming, burrowing downwards towards the end of the syntagm. To scan the fragment, either pan and zoom the environment manually, as you would a regular, Google-like slippy map, or click on the words in the reference text at the bottom of the screen to automatically focus on individual slices of the text."},{"id":"2013-10-09-chartering-our-project","title":"Chart(er)ing Our Project","author":"veronica-ikeshoji-orlati","date":"2013-10-09 10:18:57 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"chartering-our-project","layout":"post","content":"This post is a couple weeks late, but I would still like to share a few thoughts on the process of drawing up our 2013-14 Praxis charter . Though we had the eloquent and insightful charters of the previous two cohorts as models, the Scholars’ Lab faculty encouraged us to think about what_ our_ goals, principles, and priorities were and how we could best articulate them. Reflecting on what we wanted to accomplish this year – ‘accomplish’ being itself a difficult term to define – was more challenging than anticipated, but the result is a charter which – in my opinion – lays out a path for all of us to learn, play, think, and work together amicably and productively. Stephanie has already discussed some of the specific aspects of our charter, but there are a couple which I would like to elaborate on further. Therefore, I’ll throw in my two cents on our goals as we did and why ‘flexibility’ is one of our core values. We organized our goals by what we all wanted to get out of our year as Praxers – a solid grounding in DH, an interesting tool for our scholarship, and an improved handle on how to harness the power of our social networks. Personally, what DH is, how it works, and its role in the humanities and social sciences as a whole are my primary questions. Specifically, one of the things which interests me is the role of databases as tools for building knowledge and not just as repositories of ‘preformed’ knowledge. By learning what computers can actually do – and how their capabilities have been applied within the humanities and social sciences – I hope to get a better grasp on what I can do with data in my own field. And that would have been the end of my charter. Fortunately, however, I am working with a dynamic cohort of Praxers and Scholars’ Lab faculty, and so the whole ‘outreach’ thing is laid out as a top priority, too. To be clear, I believe in working collaboratively, in public, and getting feedback throughout the process. However, I’m terrified of social media and my own insecurities have driven me to a back-seat position on the web, watching the fantastic on-going work of other people and then scurrying back to my private little hole (in the basement, no less) to obsess about my own research until it is perfect (or, more likely, I get completely stuck). Now, talking in person about my ideas and research is easy – I know my audience and there is no permanent record of the ridiculous things I say. Putting ideas out on the internet and accepting that they will never really go away completely…well, that’s scary. Hence this blog post is three weeks late. Finally, a quick note on our core value of ‘flexibility.’ It is worth noting that we came to this concept through various routes – we wanted a ‘flexible’ and multidisciplinary tool, we wanted to be ‘flexible’ with one another, we wanted to be ‘flexible’ with our definition of the Ivanhoe Game and how we would approach building it. I think this ethos really is fundamental to our group-definition, and I hope it is reflected in our work throughout the year."},{"id":"2013-10-09-forming-norming-storming-performing","title":"Forming, Norming, Storming & Performing ","author":"francesca-tripodi","date":"2013-10-09 10:55:13 -0400","categories":["Grad Student Research"],"url":"forming-norming-storming-performing","layout":"post","content":"This is my first time on a group project of this nature and as the project begins to take shape I am reminded of “The Tuckman Model.” Created by Bruce Tuckman in 1965, I can’t help but see the relevancy even though his findings are nearly fifty years old. For those unfamiliar with this article, here is a lovely image of his main points borrowed from SHIFT-IT Coach  (however the “adjourning” stage wasn’t part of the original 1965 model, that part wasn’t added until the 1970s). Right now I feel like we are definitely in the forming stages. Pleases and Thank Yous still fill our conversations and we seem to be operating as our most polite selves. Rarely do people interrupt one another and I often find myself nodding in appreciation while others speak (even if I might not actually agree with what they are saying…). I was compelled to write this post because, according to Tuckman, if we are ever going to move forward with our project and start “performing” our current formalities must begin to fad. While I’m not suggesting we move toward a path of rudeness, I hope that a leader among us will eventually emerge. When this happens, resistance from the group is bound to occur as it is unlikely that we will all agree on a direction for the project. Moreover, as we begin to figure out just exact it is that we want to build, our individual preferences will also begin to surface and clash with others in the group. This stage of development might not be the most pleasant but I hope my post helps to spur a bit of storming among us. For until we can articulate and battle out our actual desires, and subsequently overcome these obstacles, we won’t be able to achieve our goals (whatever they might actually end up being). References: Tuchman, Bruce W. (1965). “Developmental Sequence in Small Groups.” Psychological Bulletin . 63(6): 384-399"},{"id":"2013-10-09-more-fun-with-interactive-typesetting-a-coat-by-yeats","title":"More fun with interactive typesetting: \"A Coat,\" by Yeats","author":"david-mcclure","date":"2013-10-08 20:27:03 -0400","categories":["Experimental Humanities"],"url":"more-fun-with-interactive-typesetting-a-coat-by-yeats","layout":"post","content":"[Cross-posted from dclure.org ] Click here to view the exhibit . After spending the weekend tinkering around with an interactive typesetting of a couplet from Macbeth that tried to model reading as a process of zooming downward towards the end of the phrase, I became curious about experimenting with the opposite analogy - reading as an upward movement, an climb from the bottom (the beginning) to the top (the end), with each word circumscribing everything that comes before it. Really, this is just the flip side of the same coin. Meaning certainly flows “downhill” in a phrase - each word is informed by the previous word. But it also flows back “uphill” - each word casts new meaning onto what comes before it. What would it would feel like to visualize that? This time I decided to work with “A Coat,” Yeats’ wonderful little ode to simplicity (he renounces what he thinks to be the stylistic affectation of his work from the 1890’s, and announces an intention to write “naked[ly]”). Originally, I planned to exactly invert the layout of the Macbeth couplet - start with the “I” at the bottom of the stack, and work upwards towards the end with “naked,” which, in the final frame, would geometrically contain each of the preceding words. I started to do this, but quickly ran into an interesting computational obstacle, which actually cropped up in the Shakespeare example as well. Trip Kirkpatrick noticed the problem: @clured Are jaggies on deepest zoom artifacts results of zoomed pixels or aesthetic decisions? Both? – Trip Kirkpatrick (@triplingual) October 7, 2013 Indeed, the last two words - “way” and “comes” - are pixelated and malformed: This wasn’t on purpose - I couldn’t figure out why it was happening when I was working on the exhibit, but decided against trying to fix it, half out of laziness and half because the visual effect had some satisfying affinities with the content of the line, especially when paired with the “descending” motif - a plunge down to hell, where order disintegrates, smooth lines are forbidden, etc. Anyway - after thinking it over, I’m pretty sure I know what’s going on, although I’m not certain. At the extremely deep zoom levels (far beyond anything you’d ever need for a regular map), I think that OpenLayers is actually losing the floating point precision that it needs to accurately plot the SVG paths for the letters - the computer is running out of decimal places, essentially. I squeaked by with the Macbeth couplet, but this turned out to be a showstopper for the Yeats, since I was effectively trying to plot geometry about four and a half times deeper - 45 words versus 11. At that depth, the text becomes completely illegible, so I had to find a way to squeeze more content into fewer zoom levels. In the end, I managed to fit it all in by positioning each line into a geometric “notch” formed by the ascenders of two letters on the following line, which more or less preserves the philosophical rationale of the exhibit (each bit of text “envelops” the previous, if somewhat less completely than before) while limiting the zooming to just ten magnification contexts, one for each line. To scan the poem, just zoom out by clicking on the “minus” button (or scrolling the mouse wheel or pinching the screen, if applicable), or click on the lines in the reference text at the top left to auto-focus on a particular part of the poem."},{"id":"2013-10-10-on-games-that-just-fizzle-a-chronicle-and-reflection","title":"on games that just fizzle - a chronicle and reflection","author":"scott-bailey","date":"2013-10-10 17:43:02 -0400","categories":["Grad Student Research"],"url":"on-games-that-just-fizzle-a-chronicle-and-reflection","layout":"post","content":"For the past few weeks, we’ve been trying to get a feel for what Ivanhoe is and what one does when playing Ivanhoe. Francesca has already posted on the question of whether we are gaming or actually running simulations, so I’d like to focus on something else: the various media through which we have tried to play Ivanhoe games and how those games gently fizzled out. Our first game was a short-lived chaos of editing and creative additions to Poe’s “The Tell-Tale Heart,” carried out through a wiki. The medium, I think, determined much of how we played. We had a clean copy of Poe’s text, and then with various perspectives and goals we quickly turned the story into a perhaps painfully complex, psycho-analytic thriller, or a deeply tragic therapy session. Or something else. By the end I wasn’t sure, as our interventions directly altered and sometimes made incoherent the additions and edits of others. I think it was in part this experience that lead to some of Francesca’s questioning, though that concerned our more recent games as well. The wiki, as a platform or tool, or place of engagement, created only a single space within which to work, a single page, a single text which shifted and changed, where there was no sanctity to any move. Any text could be edited, and with every addition or edit the immediate horizon of the text’s interpretation changed. And the game fizzled out. In meetings shortly after that game gave up the ghost, we discussed other media through which to play Ivanhoe - tumblr, twitter, and the ever-humble email. As well, we looked at reasons why the first game fizzled, especially at the lack of commitment to and interest in Poe’s text that some felt. We gathered then to discuss topics or categories for new games that would be played with smaller groups, three or four people only. From there we began four games: an email game related to science-fiction, an email game on “citizen journalism,” a twitter game on the second amendment, and a tumblr game on Wallace Stevens’ “Anecdote of the Jar.” Only the first two truly took off, while the latter two are, I hope, waiting quietly for their time in the spotlight. But those first two did take off, with flurries of posts, enlightening creativity, and clear evidence of interest and commitment . Then they fizzled. And I think that’s okay, for a couple of reasons. One, as we’ve been told, many of the early games fizzled after a certain point, where players lost interest, got busy, or just couldn’t quite figure out what was left to say or do. No golden move was made that wrapped up all the various interventions from each player in a perfectly elegant fashion. Two, as an experiment, as a endeavor to figure out how different media affect the style of game play, both email games were clear successes. We know now that people are far more likely to play regularly if they receive notifications in their inboxes, places in which they dwell throughout the day. We also know that the discrete, differentiated character of emails encourages moves that perhaps stay more coherent and consistent within a single role per person and provides for some degree of protection for each intervention. An edit or intervention in someone else’s move shows up necessarily as another move, another intervention, while in a wiki it could, unless one looks at the version control, show up not at all. We’ve learned. We’ve gained some knowledge about what features, like a notification system, might need to be built into our own project. As we move forward in the next few weeks, storyboarding and wire-framing, I look forward to seeing what else reveals itself about the Ivanhoe game."},{"id":"2013-10-11-song-of-wandering-aengus","title":"\"The Song of Wandering Aengus,\" Neatline, and negotiation with the machine","author":"david-mcclure","date":"2013-10-11 07:43:03 -0400","categories":["Experimental Humanities"],"url":"song-of-wandering-aengus","layout":"post","content":"[Cross-posted from dclure.org ] Click here to view the exhibit . One last little experiment with Neatline-powered interactive typesettings - this time with the ending of Yeats’ endlessly recitable “The Song of Wandering Aengus,” which, like many great poems, seems to somehow signify the entire world and nothing really in particular. I chose to use just the last three lines so that it would be possible to play with a more dramatic type of geometric nesting that, with more content, would quickly run up against the technical limitation that I mentioned in Wednesday’s post about “A Coat” - the vector geometry used to form the letters starts to degrade as the Javascript environment runs out of decimal precision at around 40 levels of zoom, making it impossible to continue the downward beyond a certain point. With just three lines, though, I was able to place each consecutive line completely inside of one of the dots above an “i” in the previous line. So, the “silver apples of the moon” are inscribed into the dot over the “i” in the “till” of “till time and times are done,” and the “golden apples of the moon” are then contained by the dot on the “i” in “silver.” Since the nested lines are placed literally inside the shaded boundaries of letters (as opposed to the empty spaces delineated by the “holes” in letters, as was the case with the first two experiments), the color of the text has to alternate in order to be legible against the changing color of the backdrop. What I didn’t expect (although in retrospect I guess it’s obvious) is that this shift in the color palette completely modulates the visual temperature of the whole environment - the backdrop swerves from bright white to solid black back and then back to white over the course of the three lines, with the last transition mapping onto the night-to-day, moon-to-sun thematic movement in the final couplet. Interestingly, this effect was almost thwarted by another unexpected quirk in the underlying technologies, although I managed to maneuver around it with a little hack in the exhibit theme. The problem was this - it turns out that OpenLayers will actually stop rendering an SVG geometry ones the dimension of the viewport shrinks down below a certain ratio relative to the overall size of the shape. So, in this case, as the camera descends down into the black landscape of the dot over the first “i,” the black background supplied by the vector shape would suddenly drop away, as if the camera were falling through the surface, which of course had the effect of making the second-to-last line - typeset in white - suddenly invisible against the default-white background of the exhibit. I thought this was a showstopper, but then I realized that I could programmatically “fake” the black background by directly flipping the CSS background color on the exhibit container. So, I just fired up the Javascript console, inspected the zoom attribute on the OpenLayers instance to get the zoom thresholds where the color changes needed to happen, and then dropped a little chunk of custom code into the exhibit theme that manifests the style change in response to the zoom events triggered by the map: Weird, but effective. Whenever I work on projects like these I’m fascinated by the wrinkles that arise in the interaction between what you want to do and what the technology allows you to do. It’s very different from analog scholarship or art practice, where you have a more complete mastery over the field of play - you have a much more direct and unmediated control over the sound of your words, the shape of a line in a physical sketch, the pressure of a brush stroke. With digital objects, though, you’re building on top of almost unimaginably huge stacks of technology - the millions of man-hours of work that it took to create the vast ecosystem of Javascript and PHP libraries that Neatline depends on, the whole set of lower-level technologies that shape the underlying browser rendering engines and Javascript runtimes, which in turn are implemented in still lower-level languages, which eventually brush up against the dizzying rabbit hole of physical hardware engineering, which to my mind is about as close to magic as anything that people have produced. That kind of deep, massively-distributed collaboration can definitely exist offscreen (eg., all of intellectual history, in a sense), but it’s more loosely coupled, and certainly less fragile - if I write an essay about Yeats, Yeats can’t break in the way that a code dependency literally can (will). At first this really bothered me, but I’ve come to peace with it - digital work is by definition a relinquishing of control, a give-and-take with the machine, a negotiation with our current little slice of modernity about what’s possible."},{"id":"2013-10-14-more-musings-on-tuckman","title":"More Musings on Tuckman...","author":"stephanie-kingsley","date":"2013-10-14 13:49:46 -0400","categories":["Grad Student Research"],"url":"more-musings-on-tuckman","layout":"post","content":"Last week Francesca posted on Bruce Tuckman’s model of group work on a collaborative project.  To recap, a group passes through four phases: Forming, Storming, Norming, Performing, and Adjourning.  I find this an intriguing model, and I agree with Francesca that we are currently in a sort of “Forming” stage, but I also find myself revisiting our work on the charter a few weeks ago and thinking, “Wasn’t there a some Storming going on then?”  It was relatively courteous storming, but I think in working to establish our deepest desires for this project and the experience of the coming year, frustrations did occur and personalities clashed.  Encouraged by our SLab mentors, we determined to hastily conclude work on the charter and thus enabled ourselves to focus on game play, which we did for a spell with a vigor which I would consider… Formative Performing.  Now that we spend more time in the less emotionally charged world of command line and CSS, we have relaxed, are peaceful, take turns speaking, and have probably slipped into more casual Forming; but there was disagreement in the charter-writing days, and I wonder if Tuckman’s model might not require some rethinking. I envision a model of group work where we can view the team as passing through Forming, Storming, Norming, Performing, and Adjourning; but perhaps a group may need to work through the various stages more than once depending on what it is doing.  I think one of the virtues of our group is that we are all opinionated and feel strongly.  On the one hand, that can lead to the sorts of clashes we experienced with the charter, but on the other, we have a fabulous team of six passionate scholars whose cumulative ideas are certain to birth greatness by the end of the year!… To recover from that moment of enthusiastic wordiness, in short, I think our group will likely be revisiting the Storming stage when we get into determining what our Ivanhoe game will be.  Then we will make a decision, settle down, see how our various strengths will help the project as a whole and define individual roles, and create a plan (Norming?). Then we can begin Performing once again. This is a wonderful model to consider, and I applaud Francesca for bringing it to bear and encourage Storming.  It is easy to look on conflict as negative, but Tuckman incorporates it as a healthy part of group growth and development, and I look forward to the stormy days ahead as the forerunners of brilliant productivity."},{"id":"2013-10-14-nitle-presentation-on-geotemporal-storytelling-with-neatline","title":"NITLE Presentation on Geotemporal Storytelling with Neatline","author":"jeremy-boggs","date":"2013-10-14 06:20:58 -0400","categories":["Digital Humanities","Geospatial and Temporal"],"url":"nitle-presentation-on-geotemporal-storytelling-with-neatline","layout":"post","content":"About this time last year, David McClure and I had a great conversation with the folks from the National Institute for Technology in Liberal Education (NITLE) about geotemporal storytelling with Neatline . We had lots of great questions and comments from the audience, too. Video for the talk is now available on NITLE’s YouTube channel:"},{"id":"2013-10-16-a-review-of-the-suffragette-game","title":"A review of the suffragette game","author":"stephanie-kingsley","date":"2013-10-16 03:55:51 -0400","categories":["Grad Student Research"],"url":"a-review-of-the-suffragette-game","layout":"post","content":"I think Scott’s review of our Ivanhoe game play is insightful and timely, and it has inspired me to consider the once-thriving-but-now-dormant “citizen journalism” game.  I think it’s time to really look into the successful games and, aside from the fact that we could see what was going on more immediately via the email medium, examine what else contributed to their being taken up with such vim.  I think an element of fun is paramount.  The sci-fi people (Bethany, Eliza, Eric, Scott, and Veronica all contributed) played a game on Galaxy Quest Chompers entitled “Sci-fi Fun,” and the journalism people (Eliza, Francesca, Zach, Veronica, and I) began playing around with–and even revising–a major moment in history: c. 1909 women’s suffrage in England.  As the beginner–and as of right now, the finisher–of this game, I would like to review the major moves made and directions our game took in anticipation of building on them in Round 2 of game play. The premise: a mysterious first-person narrator discovers a news clipping with this photograph in her mother’s trunk.  The narrator says, “She had always fallen strangely silent when asked about the movement; perhaps now I might finally learn more about that momentous day in her life.  I began to read.”  This was all presented in email form, with the photograph pasted into the email.  The idea was to prompt reflections on the photograph, contributions to what the article might have said, and explanations for why the mother wouldn’t want to talk about the movement.  I figured that such a setup was open-ended enough to allow for creativity, topically engaging and therefore open to critical evaluations regarding feminism and history, and focused – which the Poe game was not. Eliza rapidly authored a wonderful article on the suffragette plot to assassinate Prime Minister Asquith–which I thought was a colorful fabrication until she informed me that it was a true event.  Because I had framed the game in fiction, I assumed that subsequent moves would be fictional; from the get-go, the line between history and alternative history had been blurred.  In the spirit of fun, I concocted a secondary plot line in which Asquith had an affair with suffragette Mary Leigh–totally fictitious.  I constructed this story in the form of telegrams and letters the narrator, Mary Leigh’s daughter, discovers in her mother’s trunk. Then, Mary’s daughter by Asquith (Etta and the narrator) falls ill, and she writes him about her illness (fictional); Mary gets thrown in prison, engages in a hunger strike, and gets force-fed (contributed by Zach and true); and Asquith’s wife and his daughter Violet get involved in the suffrage movement (contributed by Veronica and probably fiction?).  Francesca contributed an interview with Mrs. Banks, ) a delightful homage to Mary Poppins ; Mary Leigh seeks Poppins’ services for Etta; and all the women in Asquith’s life–wife, mistress, and daughters–band together in the end for the sake  of Women’s Suffrage (my move and meant to be a Golden Move of sorts, as the game by that point was indeed “fizzling”). The game clearly was a blend of delving into real history and absurdity.  A debate took place in one of our Praxis meetings shortly after the game had begun: was there a use to engaging in alternative history in this way?  The conclusion was that it depended on your purpose: if the goal is to establish fact in a historical research project, then sticking to fact is best; if, however, you are attempting to think in more outside-the-box ways about history, then breaking into alternative history could inspire new ways of interpreting a particular historical moment. In analyzing my game play, I realize that I was consistently attracted to ironies: the irony of Mary Leigh trying to influence Asquith to give women the vote by seducing him, the irony of her leaving her daughter every day and seeking Mary Poppins’ assistance while she protested for that same daughter’s rights (“Our daughters’ daughters will adore us…”), etc.  This was not to impose a particular criticism on this moment in history, but rather to parse out experiential elements–hypothetical effects these women’s public lives could have had on their personal lives and vice versa.  After all, even in Mary Poppins, Mrs. Banks asks her maid to hide their suffrage ribbons as she hears Mr. Banks at the door, saying, “You know how they infuriate him.” Francesca’s contribution of Mary Poppins was lighthearted and fun, but it got me to thinking about Mrs. Banks, so enthused about the movement that Katie Nanna’s giving notice (where the video breaks off) and the need to find a new governess to care for the children is an interruption.  We could have stayed more grounded in our Ivanhoe, but I think that by blending historical with alt-historical moves we ended up with a provocative game which, for me at least, prompted some interesting and troubling musings on gender politics.  Altogether, I think it was a very successful Ivanhoe, and I look forward to the ideas new games may produce."},{"id":"2013-10-17-welcoming-purdom-lindblad-laura-miller","title":"Welcoming Purdom Lindblad & Laura Miller!","author":"bethany-nowviskie","date":"2013-10-17 04:33:40 -0400","categories":["Announcements"],"url":"welcoming-purdom-lindblad-laura-miller","layout":"post","content":"With great pleasure, I write to announce two wonderful additions to the Scholars’ Lab team! Purdom Lindblad will join us on December 30th as our much-anticipated Head of Scholars’ Lab Graduate Programs (including, among other programs, Praxis and the Praxis Network ), and Laura Miller will shortly take on a dual position coordinating public programs in the SLab and assisting me with strategic planning for digital humanities in the UVa Provost’s office. ![Purdom Lindblad](http://static.scholarslab.org/wp-content/uploads/2013/10/purdom-300x200.jpg) Purdom Lindblad ![Laura Miller](http://static.scholarslab.org/wp-content/uploads/2013/10/laura-300x225.jpg) Laura Miller Purdom comes to us from Virginia Tech, where she works as the College Librarian for Humanities and Digital Humanities. There, she has radically expanded DH project support and programming, organized graduate student open-access events, and helped to establish Port, Tech’s digital research commons. Purdom has taught at both the undergraduate and graduate level. Her current course on Religion in America involves students in an interactive exhibit in the library. She is active at the Byron Society of America, is co-PI of an ACH micro-grant project to produce dm4dh, a blog and podcast on data management in the humanities, and studied at Doshisha University in Kyoto as the winner of a distinguished Monbukagakusho scholarship. Purdom holds an MA in American Studies from Michigan State University (where she worked at MATRIX, MSU’s digital humanities center) and an MS in Information from the University of Michigan. Purdom describes herself as an open access advocate, an “abuser of exclamation points, and a knitter (stereotype!).” Laura Miller comes to the Department of Digital Research &amp; Scholarship from a central role as Assistant to the Director in UVa’s Brown Science and Engineering Library, where she provided public service, outreach, and research instruction, project management, and user needs assessment for both space and interface design. Last year, she coordinated a successful Learning Spaces Renewal Project across four of UVa’s libraries. Laura is also co-creator of the Library’s data management portal for UVa graduate students, and recently presented on the subject at a J-term Data Management Bootcamp for Grad Students (a partnership with Virginia Tech). Laura holds a BA in English from William and Mary and an MS in Library and Information Science from Florida State. She has a past life in science-fiction book publishing and her research interests include usability and iterative, user-centered design. Laura also reports an interest in “quirky podcasts, negotiating with a three year-old, and live music of (almost) any sort.” Staff, students, and collaborating faculty in the SLab are hugely excited about the transformative impact Laura and Purdom will have on our work. I hope you will join me in welcoming them!"},{"id":"2013-10-21-2013-gis-day-wednesday-november-20th-2013","title":"2013 GIS Day - Wednesday, November 20th, 2013","author":"chris-gist","date":"2013-10-21 09:18:33 -0400","categories":["Announcements","Geospatial and Temporal"],"url":"2013-gis-day-wednesday-november-20th-2013","layout":"post","content":"In honor of GIS Day 2013, the Scholars’ Lab at UVa Library would like to invite you to participate in our celebration on November 20th. Starting at 1:30PM in the Scholars’ Lab, there will be a round of lightning talks followed by the cutting of the GIS Day cake.  We encourage everyone, including students (UVa, PVCC and high school), in the Charlottesville GIS community to contribute.  If you have never seen lightning round talks, they can be pretty entertaining: a rapid fire succession of speakers given a set, short amount of time and PowerPoint slides.  In previous years, we’ve had many great presenters giving a powerful message by showing the breadth of disciplines and fields in which GIS is used. In this year’s round, each speaker will be given five minutes with a maximum of ten slides.  It is a fairly easy task to create and give a lighting round talk (assuming you can deal with time constraints).  Help make this year’s event special by participating in the talks.  You can present on anything spatially related you like.  It could be about a project you have worked on, things going on at your office or just something of personal interest. If you wish to participate in the lightning round talks, please email cgist[at]virginia.edu .  If you cannot participate, please come to enjoy the talks and GIS Day cake and see what we concocted this years’ cake.  Previous years’ cakes are documented on our blog. We will wrap up the event by 3:30PM. Please let Chris Gist know if you have any questions or thoughts."},{"id":"2013-10-21-building-dwelling-coding","title":"Building, Dwelling, Coding","author":"scott-bailey","date":"2013-10-21 06:55:37 -0400","categories":["Grad Student Research"],"url":"building-dwelling-coding","layout":"post","content":"Over the past week, I’ve become rather occupied with customizing Sublime Text 2, the editor in which I’ve been writing code. I’ve been adding packages to make it work more efficiently, to cut down on the number of keystrokes, to make it so my fingers rarely have to leave the keyboard. With Jeremy’s help, I created a symbolic link between the terminal and Sublime, so I can open files in Sublime from the terminal itself. I’ve spent several hours watching workflow videos, trying to figure out what habits I need to cultivate to write better code more efficiently. And some it is wonderful. Why write vendor prefixes yourself when you can run your CSS through Prefixr with a simple keyboard shortcut? Why type out the basic html structure, when you can use Emmet, type !, and just hit tab to have it created? There are so many shortcuts to take advantage of, built in to Sublime and through the rich environment of plugins available, so many ways to be that much more efficient. In his well-known 1951 essay, “Building, Dwelling, Thinking,” Martin Heidegger, with a mix of etymological work and analysis of everyday life, draws our attention to the fundamental interconnection of the spaces we create through building and the way in which those built spaces reflect and determine how we exist and think within those spaces. Without stepping into the long-running and difficult debate on what precisely Heidegger means by the fourfold, the gods, mortals, earth, and sky, we can still take a few quick lessons from Heidegger’s insights. In acts of building, we affect and determine spaces themselves, physically and semantically. We intervene creatively in our landscape, adding material objects which become part of the web of meaning that any person enters when existing within that space. Taking the space itself as a phenomenon, we could say that acts of building alter the structure of the phenomenon, such that it presences differently and its semantic field is shifted. As we enter into this built space, we exist correspondingly within it, and if we can dwell, if we can let be what is, giving that which is the case the space in which to be, our own existence will be shaped toward allowing the freedom of all that is. If this seems a bit mystical, it is. Much of Heidegger’s late work, especially on Gelassenheit, bears the trace of Meister Eckhart and other Christian mystics, so focused on a type of being and thinking that runs counter to what will later be called the dominating subjectivity of the transcendental ego. It must be remembered, though, that Heidegger, from the beginning to the end, set out to describe concrete human existence. Whenever I’ve taught, especially working in constructive theology and in ethics, I’ve encouraged my students to take time to think, to give thinking time, as much time as it takes, and to keep giving the thought time as it shows itself in writing. My philosophical and theological heritage is one of thinkers who tended to have one really good thought that they remained with for decades, describing its contours and connections again and again, circling back around repeatedly, building an ever thicker description of some fundamental reality. I’ve taken the first step on that road myself, for over a year now trying to think one thought about vulnerability as a fundamental structure, ontic and ontological, in human existence. But. In modifying Sublime Text differently this week, building out the structure itself and learning the new signs of this environment, this place, I have been building a space that allows one to dwell in it through the quick tapping of a few keys. A space that encourages a form of dwelling suited more to the finishing of a product, toward thinking that thinks in short deadlines, in concrete results, in operability. We are, in Praxis, building a tool to be used after all. It is also a space, though, that will hopefully become something more like a home, something like Heidegger’s Black Forest hut, in which he found himself free, with the space to let himself be himself. It is hopefully the case that the careful habituation of fingers to keyboard shortcuts, the learned muscle memory that navigates the signs of this built space, will give me space and freedom to write code simply as myself, to let myself think in code, and, as an upcoming conference here at the Scholars’ Lab puts it, to let myself speak in code. If this does turn out to be the case, as I’m sure I’ll report on some months down the road, then it will have been the case that these two forms of dwelling, these two forms of thinking, will not have been so very different after all."},{"id":"2013-10-21-shut-up-legs-or-jens-voight-and-why-i-applied-for-praxis","title":"Shut up Legs! or: Jens Voight and Why I Applied for Praxis","author":"zachary-stone","date":"2013-10-21 07:05:17 -0400","categories":["Grad Student Research"],"url":"shut-up-legs-or-jens-voight-and-why-i-applied-for-praxis","layout":"post","content":"So. In our charter we committed to a goal we called ‘Outreach.’  As part of that commitment I pushed for a firm statement of two blog posts per person per month. This seemed reasonable at the time, and still does. And yet, here we find ourselves on October 16 and, discounting my introductory post I have yet to surface online.  This would be the moment to play the grad student ‘woe is me I am so busy grading student papers reading for orals having existential crises etc. etc.’ card but, unfortunately in my case it simply isn’t true. Deeper still, I try to convince myself (and thought about trying it on you kind folks) that I really need to be careful about what I say online as who knows what job committee Googling might turn up.  Again, though, this masks my deeper anxieties about not only blogging and DH, but also the entire practice of the interwebs. The reason I know I have time to blog is that I have time to read not 1, or even 3, but 6 cycling related blogs religiously.1 While I have enjoyed the first fruits of the DH revolution (or whatever DH’ers call the last fifteen odd years) in my professional work, the very same technologies that enable those ventures imperil my own work.  I am not being over dramatic.  Nor am I casting aspersions on the Internet as an institution. No, like Eliot, I am distracted from distraction by Jens Voigt memes (or facts ).2 The fault is all mine but the effect is real. Nor do I think I am alone. Judging by the number of cat/Ryan Gosling memes I see on the Facebook, I fear that the scholarship of an entire generation is being threatened by anything.GIF. In my fear of my PowerBook (or really any of Apple’s techno-crack that brings custom bike pictures straight to my lustful gaze in so many cunning ways), though, lies the root of my interest in the Praxis program.  I am not dumb. I cannot turn back the clock to a predigital age (though the Wifi in many of the medieval libraries I haunt is refreshingly antique), nor do I want to. What I need to learn, what I want to learn, is how to be both digitally engaged and human. How to make the machines work for me and how to think along with emerging technology and not against it. Three plus years deep in the [ Hors Catégorie _climb](http://en.wikipedia.org/wiki/Hors_catégorie) that is grad school (yes… I think “WWJVD?” [What Would Jens Voigt Do?] all the time, and the answer is always the same: Shut up Legs [or metaphorically whatever in me is wanting to quit at that day/time]) I realized that while I will learn many wonderful things from my colleagues and mentors ensconced traditional humanities departments, I will not learn how to be a humanist in an increasingly post human world. And by mean I mean _in, or online.  This is not in any way a criticism of the specific people and departments I know and have known, rather it is recognition of a persistent condition in the academy as it has been broadly configured. Praxis, for me, offered the chance to learn not just how to program or how to manage DH projects, but how to thrive in world in which I have no choice but to live. Most of my posts will explore my evolving relationship with not just DH but the Internet as cultural force in my entire life.  This may seem outside the remit of ‘DH’ but I may just be foolish enough to think that DH and old-fashioned ‘H’ finally aim at the same target: how to live, and live well, in the world in which you live. Oh yah. I ride bikes some times. And really love Jens Voigt. And you should too. Prolly is not Probably, Bike Snob NYC, Bike Rumor, Rapha, Road.cc, VeloNews.com TS Eliot, “Burnt Norton,” Four Quartets (Harcourt, 1943), loosely."},{"id":"2013-10-24-podcast-introducing-our-2013-2014-graduate-fellows","title":"Podcast: Introducing Our 2013-2014 Graduate Fellows","author":"ronda-grizzle","date":"2013-10-24 05:48:14 -0400","categories":["Podcasts"],"url":"podcast-introducing-our-2013-2014-graduate-fellows","layout":"post","content":"Graduate Fellows Forum Introducing Our 2013-2014 Graduate Fellows On September 10, 2013, the Scholars’ Lab hosted a lunch talk to welcome our new Digital Humanities Graduate Fellows, who gave an introduction to their research to be undertaken with the Scholars’ Lab during the next year. Eric DeLuca Composition and Computer Technologies Program\nMcIntire Department of Music “Community Listening in Isle Royal National Park, a sonic ethnography” Sounds not only change physically as they travel across and through spaces and places, but they also change, and shape, dense webs of relationships between people and things across socio-cultural contexts. Within this space, what can we learn from individualized listeners? And what can we learn by listening to how these people listen? My work with the Scholars’ Lab focuses on one of these relationships. Blurring the line between soundscape composition, audio documentary, and sonic ethnography, the work documents how I became part of a dialogue between wolf researchers of the world’s longest running wildlife study, and a community of park-exploring people. In short, the wolf researchers have tapped into a network of park visitors and employees scattered across the island, listening. The scientists collect listening reports from this network that leads to the determination of wolf reproductive success in the summer season. In the process, this network of listeners gains, among other things, a deep listening relationship with the place. During extensive fieldwork in the US National Park system as a composer/researcher through their Artist-in-Residence program I have become interested in how these ways of listening exists within, and are tied to complicated, interconnected environments. In order to understand this listening relationship it is necessary to understand an array of issues and topics directly related: noise, silence, chance, the relationship between science and the public, global climate change, National Park policy and politics, species reintroduction, togetherness, the romanticization and dramatization of the wolf, spirituality, and socio-aesthetics of the place. The work is framed by the intrinsic relationship between this listening network and the ecological well-being of the park, which is currently at risk of major change because the wolves, who play a vital role in maintaining this health, are on the brink of “blinking-out”. My time in the Scholars’ Lab will be spent experimenting with different forms of research transmission/creative expression geared toward connecting the research/story to people. I am interested in preparing an interactive version of the project that would grow into, or become a platform for, other community-based sound and listening projects that may include sound mapping using GIS, online sound databases, and data scraping. Gwen Nally Corcoran Department of Philosophy “When Socrates Misleads: Falsehood and Fallacy in Plato’s Dialogues” My dissertation challenges a powerful orthodoxy; scholars generally assume that the Plato’s dialogues portray Socrates as a sincere thinker, arguing in good faith and in search of the truth. It is my view, however, that Socrates often misleads his interlocutors, sometimes for sport and sometimes in the service of educating those who are resistant to philosophical argumentation. In particular, I have identified a number of conversations, in the Phaedo, Meno, and Phaedrus, where Socrates misrepresents his own convictions in order to convince an interlocutor of an important philosophical truth. These instances provide contextual and linguistic evidence not only that Socrates misleads but that he is aware of having done so. My digital research will focus on identifying other dialogues in which Socrates knowingly misleads his interlocutors. Because many of my arguments are philological, I purpose to use computational linguistic techniques, like topic modeling, to compare the vocabularies of the Phaedo, Meno, and Phaedrus to other dialogues of with similar themes, including the Gorgias, Symposium, Parmenides, and Theaetetus. Comparing these dialogues through statistical analysis will, I hope, reveal further linguistic indications of deceptive rhetorical practices. In addition to showing interesting results for my own research, I hope to demonstrate the extent to which topic modeling might be used as an aid to close reading. In recent years, those interested in topic modeling have tended toward corpus-based approaches, examining large swaths of text, across multiple authors and genres, in order to determine sweeping thematic and structural shifts.[1] I hope that this project will demonstrate the extent to which topic modeling might also be useful to scholars who are interested in detailed textual analysis. [1] Clay Templeton (August 1, 2011). “Topic Modeling in the Humanities: An Overview.” MITH: Maryland Institute for Technologies in the Humanities. UMD. http://mith.umd.edu/topic-modeling-in-the-humanities-an-overview. See also: http://mith.umd.edu/corporacamp/tool.php. Tamika Richeson Corcoran Department of History “SURVIVAL AND SURVEILLANCE: RECOVERING NARRATIVES OF BLACK FEMALE CRIMINALITY DURING THE CIVIL WAR” A critical component of her dissertation titled, “Black Sass: A Social and Cultural Examination of Black Female Criminality in Civil War Era Washington, D.C. 1850-1880,” Tamika Richeson’s digital humanities project applies the police precinct data of over 450 arrests of black women to develop a digital narrative of the lived experiences of “lower-class” black women across space. Applying crime as a focal point, her study of the spatial relationship between black women’s law-breaking activity and police surveillance in the nation’s capital, offers a window into the lives and labors of lower-class black women during an era of national conflict and fortified race-based legal restrictions. The nation’s capital was a shared space, occupied by local black and white inhabitants, immigrants, politicians, and soldiers. Her project involves a detailed examination of criminal data from 1861 and 1862 to capture the tensions and conflicts within that shared space and at centers of power. Her research demonstrates that black women strategically navigated wartime Emancipation in their quest for independence, stability, and survival. This case study at the heart of national politics and culture resituates lower-class black women from the margins to the center to examine the racialized and gendered context in which American criminal law took shape. As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.26842728989/enclosure.mp3”]"},{"id":"2013-10-24-podcast-sukanta-chaudhuri","title":"Podcast: Sukanta Chaudhuri","author":"ronda-grizzle","date":"2013-10-24 05:49:18 -0400","categories":["Podcasts"],"url":"podcast-sukanta-chaudhuri","layout":"post","content":"Institute of the Humanities &amp; Global Cultures Speaker: Sukanta Chaudhuri Many Tagores: Travels through a Variorum Website On September 12, 2013, Professor Sukanta Chaudhuri, Professor of English Literature at Jadavpur University and a Digital Humanities scholar, spoke in Alderman Library at the conclusion of his residence at UVa as an Institute of the Humanities &amp; Global Cultures Clay Distinguished Visiting Professor. Dr. Chaudhuri is the Principle Investigator of the Bichitra: Online Tagore Variorum . As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.26844676557/enclosure.mp3”]"},{"id":"2013-11-01-thinking-through-doing-while-losing-my-marbles","title":"Thinking Through Doing While Losing My Marbles","author":"veronica-ikeshoji-orlati","date":"2013-11-01 13:17:53 -0400","categories":["Grad Student Research"],"url":"thinking-through-doing-while-losing-my-marbles","layout":"post","content":"Last Monday, at the suggestion of our Scholars’ Lab mentors, the Praxis cohort divided into two teams to start wire-framing some of our ideas for the Ivanhoe Game. The initial thought was to divide into groups along a theoretical Type I/Type II DH divide, as identified by Stephen Ramsay . Many of us Praxers, however, are not absolutely committed to one or the other DH type, thereby throwing a bit of a wrench into that plan. So instead, we decided that we’d try wire-framing two versions of the Ivanhoe Game with no particular constrictions – not even feasibility. Yes, we’ve had the opportunity to design what we would build in an ideal world with unlimited time, resources, and abilities. And so the fun began. In the past ten days or so, Francesca, Zach, and I have met a couple of times to discuss how we visualize our version of Ivanhoe playing out (pun intended). Jeremy ran us all through the essentials of wire-framing and user interface design, and at his suggestion we’ve consulted J.J. Garrett’s The Elements of User Experience: User-Centered Design for the Web and Beyond to figure out how, exactly, we want to go about building our game and conceptualizing the user experience. Simultaneously, the entire Praxis fellows group has been engaged in a rather entertaining and informative e-mail-based game on the history of the Parthenon marbles. Stephanie chose the Parthenon marbles for us because of the wealth of information available, the extensive debate surrounding them, and their 2,500 year history touching upon questions close to many of our disciplines. We’ve taken carefully-defined roles (I myself am arguing a position I personally find rather unpalatable and untenable, other players taking on roles from history and mythology) and we have worked towards building a moderately-coherent narrative of the Parthenon marbles through our various game personae. One of the reasons we decided to start another communal game was to experiment with adding more structure to the format of game play (e.g. a points system, a game/move time-line, and explicitly-stated roles) to see if it would improve the quality of the game and, perhaps, better sustain game-play . Additionally, we thought it would be helpful to simultaneously play and design a game so that we could think more critically about what type of interface would enable the various moves, connections, and conversations we would like to foster but simply cannot in the platforms available to us. Upon further reflection, I realized what this whole process reminded me of: dissertation research and writing. Last Fall, I read Joan Bolker’s Writing Your Dissertation in Fifteen Minutes a Day in which the author advocates for writing each and every day to improve one’s productivity and quality of research. Additionally, Bolker insists that, while research and thought are (obviously) fundamental to a dissertation, writing is often the only way to think through ideas, questions, and problems. In essence, to push beyond the vague generalities and variably-substantiated theories which float around in our heads as we conduct research, it is necessary to articulate the arguments in concrete form to expose the strengths and weaknesses of their logical structures and underlying suppositions. Likewise, we have reached a point with Ivanhoe at which designing something, no matter how amateurish, incomplete, and flawed, is absolutely necessary to determine the next step in creating a workable game. We are, in theory, thinking by doing, while still pushing our ‘research’ forward with the Parthenon marbles game. As a bit of a post-script, I seem to be finding Ivanhoe-esque objects and ideas everywhere, such as J.J. Abrams and D. Dorst’s S., an annotated version of a book, the Ships of Theseus, written by the fictional author V.M. Straka. Though the game Francesca, Zach, and I are designing is more of an open-ended space for creating and visualizing the connections between various data-sets and less of a resource for annotating text (like our first game on Edgar Allen Poe’s The Tell Tale Heart ), I’m eager to get my hands on a copy of S. soon – and maybe I can call reading that research."},{"id":"2013-11-11-stephen-covey-intervenes-in-wire-framing-ivanhoe","title":"Stephen Covey intervenes in wire-framing Ivanhoe","author":"stephanie-kingsley","date":"2013-11-11 07:57:23 -0500","categories":["Grad Student Research"],"url":"stephen-covey-intervenes-in-wire-framing-ivanhoe","layout":"post","content":"When the SLab folks recommended we split into groups and begin creating wireframes for our Ivanhoe games, my first thought was, “How can we start building when we don’t really know what we’re doing yet?”  However, talking about what we were doing in the abstract had been generating somewhat circular discussion, so I took a leap of faith and began drawing.  Jeremy handed out colored pencils, and we all started sketching individually. It occurred to me in that moment that what we were doing was analogous to stream-of-consciousness writing—imagining in the very moment of creation.  So, inspired, I took up a violet pencil and commenced.  To get the creative juices flowing, I drew a rectangle to represent a computer screen, and the empty spaces suddenly demanded, “Fill me with something brilliant!”  Ideas began to arise spontaneously.  I mused first about what our game-play space would look like—which led to more musings on what Ivanhoe was all about.  This time, however, rather than just spinning my wheels, I drew out two visualizations to follow the two trains of thought arising concurrently: one, a linear view of Ivanhoe which would represent moves much the way Facebook posts pop up in the Newsfeed, and the second, a nonlinear model which displayed moves as a web representing them in relation to one another.  Simply by sketching out the tension between linear and non-linear Ivanhoe concepts, I had a couple concrete visualizations in hand—already a step forward! We split into groups of three to envision two possibilities for Ivanhoe: Eliza, Scott, and me; and Francesca, Veronica, and Zach.  Our goal was to create wireframes and present them to the SLab folks in our meeting today, so both groups met separately over the past week to prepare.  I realized that not only did sketching out my ideas help me delineate specific problems to address (such as the issue of linearity), but it helped us to collaborate more efficiently, as well.  We were able to communicate in concrete ways as opposed to trying to speak about abstract concepts. At this point, I could speak about a lot of aspects of wire-framing; it was a rich experience in a variety of ways.  What strikes me the most, however, is the way the simple act of having to get our ideas on paper helped us communicate better with one another—an unanticipated benefit.  For those familiar with Stephen R. Covey’s The 7 Habits of Highly Effective People, I understand this phenomenon best in light of Habit 5 : “Seek first to understand, then to be understood.”  The premise is self-explanatory: if you are trying to make another person understand your point of view, first try to understand his.  The reason I bring up this concept is that I saw drawing out our ideas together as a facilitator for understanding one another.  If Scott had an idea, he drew it on the board, and Eliza and I both could immediately understand it and then build on it by adding our own ideas to Scott’s drawing.  The space of time in which something was being drawn created a moment in which discussion ceased, and everyone waited to continue until the idea was fully formed on the board and could be considered.  In this way, Eliza, Scott, and I were able to lay out a solid plan for what our Ivanhoe might look like. I find group interaction fascinating.  This being my first major collaborative project in a working environment, I see it as an opportunity to grow socially in a professional sense.  Working with other people is not easy.  It is rewarding, fun, exhilarating and inspiring, but not necessarily easy.  We are a group of confident high-achievers, and that means that every one of us has excellent ideas that could stand alone.  The challenge is to meet each other in the middle, and by taking the best of each, to fashion something even better.  Covey’s Habit 6 is “Synergize,” which means combining the strengths of individuals to collaboratively create what no one person could create on his or her own.  This is what we’re doing now, and I look forward to seeing what we’ll build together. Sources: Stephen R. Covey. Web.  Accessed 6 Nov 2013 https://www.stephencovey.com ."},{"id":"2013-11-11-two-ivanhoes-one-direction","title":"Two Ivanhoes, One Direction ","author":"francesca-tripodi","date":"2013-11-11 07:59:33 -0500","categories":["Grad Student Research"],"url":"two-ivanhoes-one-direction","layout":"post","content":"Over the past few weeks our team of six divided into two subgroups to try and wireframe out our respective visions of Ivanhoe (see Veronica’s excellent post for more details on how these groups were organized). After coming together and presenting each of our ideas, I was immediately struck by the similarities between our projects (and a bit relieved to see that we won’t really need to make too many compromises in either direction). We both envisioned a central space or environment where users would come to collaborate (similar to Prism  all of the play would happen in one place). We both mapped out a profile page where users can easily access games in progress, chat with other players, and have the ability to edit  their profile. Yet we also had some important differences: Team II (Stephanie, Eliza, and Scott) had a role journal, a “contact us” feature, an option to sign in to the game by using Facebook/Twitter as well as the ability to link our environment to these social networking sites.  They also had a demo feature for people new to the space, the ability to star moves, activate move notifications, and assign points. Overall, I think most of these functions are good ones - especially the ability to star moves and have the option for players to receive notifications (they suggested an e-mail notification but perhaps we could also make a text option to facilitate more smartphone usage). Given smartphone proliferation, I also really like the idea of linking up to FB/Twitter because I think it would increase participation and frequency of use. I also think their idea of a demo is essential because it could help new users navigate the space but also serve as a marketing tool! However, I’m not sure I see the benefit of a role journal. I understand creating a restraint so players aren’t just making moves haphazardly, but our team presented a “rational” section whereby players had to justify why they made moves instead of a “role journal” which I believe is a bit excessive. For me, I want most of the time players spend in the environment to be devoted to playing/making moves and not writing. I’m unclear of what the benefits of a role journal might be and I’m concerned that by creating more work for users our environment will become a less fun place. I’m also a bit weary of a “contact us” section because it requires constant monitoring and after this academic year comes to a close I wonder who would maintain this function. Perhaps a FAQs might be a good alternative? Team I (me, Veronica, Zach) - one big difference of our presentation was the look and feel of the environment. While Team I envisioned a space that looks similar to WordPress, our team was thinking more of a Google Maps interface - where users could zoom in and out. We also want to have a function similar to a tag cloud whereby the images with more connections would grow in size as the game progresses (or if you have a team game, the team with the larger images is the one making more connections…aka “winning”).  The benefit of our layout is mainly aesthetic, but Team II might be on to something with navigation. It’s possible our layout could get too messy as more and more people make moves. We also included a taxonomy feature - a necessary component if we want users to have the ability to search for and find games they aren’t directly invited to. Finally, we proposed an image only game  (in the form of pictures, music, videos) and this is something that I really want to push for. While I realize that the original Ivanhoe was manipulation of texts, I think that Prism essentially does this (although in a less playful way). SO, if we want to have the ability to manipulate a text, perhaps we could embed some of the Prism tools into our gaming environment? Is this possible? Do those familiar with the old Ivanhoe feel that a text based game is essential or do you like the idea of image only? In general, I was impressed by all of our ideas and doing this exercise gave me a renewed energy for our project.  But as we move forward in figuring out which additions we want to include and ultimately what direction we end up taking, I think the final question we all need to ask ourselves is what else is missing?"},{"id":"2013-11-12-sticky-situations-lessons-group-cohesion","title":"Sticky Situations: Lessons in Group Cohesion","author":"veronica-ikeshoji-orlati","date":"2013-11-12 10:12:40 -0500","categories":["Grad Student Research"],"url":"sticky-situations-lessons-group-cohesion","layout":"post","content":"Over the past couple of weeks, we Praxers have been wire-framing two distinct visions of the Ivanhoe game in two groups – Eliza, Scott, and Stephanie in one, Francesca, Zach, and I in the other. On Wednesday we presented our visions to each other and our dedicated Scholars’ Lab mentors. (Francesca has written an excellent summation of both, highlighting some of the key similarities and differences.) As we await the second databurst from the people who know how to actually build our fuzzy visions, I thought I would share a couple of brief reflections on how our first stab at creating something together went. First Lesson: Ivanhoe = Archaeological Excavation. As an archaeologist, I am accustomed to – and very much relish – working with other people towards building a coherent interpretation of archaeological data. Indeed, one person has never and will never be able to collect, organize, and analyze all of the data which comes out of an excavation, not only due to the sheer quantity of information, but also because of the years required to develop sufficient expertise in any one sub-field of archaeology to properly contextualize finds. Ivanhoe is a bit like an archaeological site, and since it’s still in the early stages of development, it presents many of the same questions and problems. To add to the chaos, we Praxers do not yet have defined specialties, and our overall interests and goals are, for the most part, strikingly amorphous. So as of last week, we were effectively a group of first-time DH archaeologists looking at a gigantic field armed only with the knowledge that there was something _out there _somewhere for us to find. By wire-framing some of our visions of Ivanhoe we executed a field survey and, with the interpretative assistance of the SLab R&amp;D people, we’re finally going to figure out where to ‘dig’ and what sorts of tools and specialties we’re going to need to accomplish our project. Yes, that is a terribly convoluted metaphor. Nevertheless, I think it is helpful in conceptualizing how to work on a collaborative DH project. No one of us will hold ‘The Key’ to making Ivanhoe work, and our final product will be more than the sum of our individual efforts. (Or so one would hope.) Second Lesson: Thinking Brilliant Thoughts Means Nothing If You Don’t Articulate Them. Every once in a while, an absolutely Brilliant Idea enters into my head. (Allow me the illusion for the moment.) I tell myself that I won’t forget this Brilliant Idea and continue to go about the rest of my day, basking smugly in the radiant glow of my own genius. The end of the day arrives, and I finally sit at my computer to write the paper/essay/dissertation chapter which will elucidate for the world my Brilliant Idea. And it’s gone. I spend that evening trying to recreate the moment the Brilliant Idea struck me, combing my memory for any detail which might trigger the Brilliant Idea to reappear. But instead of suddenly recalling the Brilliant Idea in full, with additional layers of nuance added from my efforts in remembering it, I pass the evening writing laboriously around the topic of the Brilliant Idea and hoping it will manifest again if I just…write…enough. I suspect I am not the only person to have suffered the loss of a Brilliant Idea. I did not, however, realize that the very same thing happens in groups. Francesca, Zach, and I had a number of excellent conversations about our vision of Ivanhoe (greatly facilitated by our SLab mentors!). The problem, however, came after the conversations, when we had to figure out how to articulate everything we discussed in a coherent manner. Not until Jeremy finally forced us to diagram things out did we understand just what we were doing, and at that point we started to realize how many holes we had in our vision. This lesson reiterates what I blogged about a couple of weeks ago, but I think it is absolutely crucial to making sure we actually have a concrete product by the end of the year which reflects the quality conversations and vibrant energy of our group. Third Lesson: Disagreement Happens. This is another statement of the obvious, but making disagreements fruitful and productive is something that, in my opinion, we are struggling with as a group. Francesca’s blog post last month on the evolution of group dynamics and Stephanie’s subsequent evaluation of our group based on Tuckman’s Forming-Norming-Storming-Performing model are indicative of our general concern with being positive and productive as a group. The fact is that academia can be terribly isolating and often values individual, field-specific contributions by far more than collaborative, interdisciplinary ones. As graduate students in the humanities and social sciences, we often talk about my research, my contribution, my interests. But perhaps, as we work through the conflicts between our individual ideas, we can continue to better our project."},{"id":"2013-11-15-role-journals-texts-pedagogy-and-pragmatism","title":"Role Journals, Texts, Pedagogy, and Pragmatism","author":"scott-bailey","date":"2013-11-15 04:33:26 -0500","categories":["Grad Student Research"],"url":"role-journals-texts-pedagogy-and-pragmatism","layout":"post","content":"In her recent post on the Scholars’ Lab, Francesca gave a quick rundown on some of the similarities and differences between the approaches of our two wire-framing teams. I have to confess that I was surprised by a couple of her concerns, and I’d like here to clarify the reasoning behind our strong focus on the role journal and our sense of Ivanhoe as a textually centered game with significant multimedia capabilities. I’ll do so with two concerns in mind: teaching goals and practice and pragmatic concerns. Role Journals and Pedagogy Francesca makes a distinction between the “rationale” section that her own group built into their version, and the role journal which our group included. She understands the rationale section to be a place where “players had to justify why they made moves,” which might create a “restraint so players aren’t just making moves haphazardly.” This will be less “excessive” than the role journal, and will allow players to spend most of their time “devoted to playing/making moves and not writing.” As well, there is a concern that creating this extra bit of work for players will make the environment “less fun.” I am certainly sympathetic with many of these concerns, but I think clarification of our idea of the role journal in terms of its pedagogical use will ameliorate these significantly. I and a few others have thought of Ivanhoe especially in terms of pedagogy. As such, Ivanhoe is an application that allows players to intervene in a discursive space (this will be familiar language to those acquainted with it from a decade ago) and facilitates critical reflection upon acts of interpretation. But what does that mean? I’ll drop back to my own discipline in the hopes of giving a clear illustration. Suppose you are teaching a course in Reformation history and theology, and were teaching Martin Luther’s “On the Babylonian Captivity of the Church.” In groups, you have your students engage this text through Ivanhoe. Each chooses a role. Perhaps one decides to be Luther himself, another Pope Leo X. Someone else chooses to write as Elector Frederick. All of these would be significant figures in the historical controversy. But someone else decides to write as Pope Francis, the current Pope, another as the Nazi jurist Carl Schmitt, and yet another as the Daily Show’s Jon Stewart. Each of these roles will bring a distinct and interesting approach to the historical, political, and theological controversies which find expression in Luther’s text. As the game begins and then progresses, each player, maintaining his or her role, begins to make interventions. Perhaps in the guise of writing a history of the papacy, “Pope Francis” lays out in successive moves the critical development of the papacy throughout the medieval and early modern periods. Perhaps “Luther” submits as his or her moves autobiographical reflections which reveal Luther’s deep anxiety and distress in his own spiritual life. “Jon Stewart” provides short, witty, incisive engagements all the relevant figures, reacting especially to the moves of others, pointing out inconsistencies and moments of hypocrisy, expressing a voice of moral indignation at the faults of Luther, Leo X, and others. As the game progresses, the players attempt to respond more significantly to each other, all for the goal of understanding the initial text through the fusion of numerous horizons (do understand this in Gadamer’s terms). As a teacher, while I want my students to be engaging creatively, analytically, and critically with Luther’s text, I want even more for them to understand and engage the way in which they think and interpret and form judgments. I want them to reflect on the interventions or moves of other players, to think about how those moves open up dimensions of the text which they now can engage, to consider how those moves of others capacitate their own critical and creative interventions. And I want them thinking about how the roles that they have chosen come with certain hermeneutic positions with which they must reckon in order to understand how to coherently embody that role in its engagement with the discursive space of the text. To think about the text. To think about thinking about the text. As a tool, Ivanhoe allows us to engage an object of concern, and while I’ve used a text as an example, it could be an image, a video, a sound file, a physical object, or almost anything else. Around that object of concern we engage in acts of interpretation which could take a vast number of forms and styles. And then we can think about what we’re doing when we’re playing. We can raise awareness of how we ourselves think and perhaps how others think as well. This is the benefit of the role journal, to give our students the space to reflect consciously on what is happening in the game and what is happening in their own thinking. It is this second part, I think, which really takes us to the point of deep engagement with critical thinking. In the design my group offered, we placed a button on every page that had game content on it, and when you click that button it pops out a little no-frills text editor for the role journal. The idea was that while reading the interventions of others or one’s own interventions, the player could write quick notes and reflections. This method of quick entry was paired with a dedicated role journal page, with a more substantial WYSIWYG editor that would allow for more significant entries into the role journal. Could much of this be done with pen and paper, in a student’s notebook? Sure. But the idea of building it in is to give players a single environment in which the game and their production of content exists, and the idea of the pop out editor is to make critical reflection a common part of the experience of the game. There are other details, of course, like whether the role journals are public or private. There are reasons for either, and I think it would be an option for each game to choose one or the other. I could certainly see an interesting activity where at the end of a game, the participants share their private role journals and reflect on others reflecting on the game as it progressed. Is this a bit more work? Yes. But I think it is pedagogically worth it, and not so very different from the idea of a move “rationale.” I even think that for quite a few players, the critical, reflective activity of the role journal, and the sharing of them, might rather heighten the fun of the game. Texts and Pragmatism The example above and even my language of “reading” someone’s else moves undoubtedly reveal that I think of Ivanhoe in terms of texts. Working in philosophical theology, texts are my mainstay. Yet, I think Ivanhoe should be a game wherein we can play with any number of objects, whether they be texts or images, video or audio-clips, or anything else. I see Ivanhoe being incredibly flexible here and open to so many types of games. So, I was surprised to see the other group, and Francesca in her blog post, push for a non-textual game. I’ll focus my response on Francesca’s blog post. She writes there that they proposed, “an image only game (in the form of pictures, music, and videos),” which she “really want[s] to push for.” Francesca notes that the original Ivanhoe game was primarily “manipulation of texts,” but suggests that Prism, the product of our forebears in Praxis, “essentially does this (although in a less playful way).” She suggests that we consider “embedding Prism tools in our gaming environment,” but asks whether those “familiar with the old Ivanhoe feel that a text based game is essential or do you like the idea of image only?” I don’t know whether I count as one of those familiar with the old Ivanhoe, but as one of those building the new one, I have to say that I think the two options given are not the only ones. As I pointed out above, I think we need to build an application flexible enough to facilitate games of many varieties, some of which may involve a great deal of text and others which involve only a bit and others which involve not a whit. I’ll return to some of this reasoning in terms of pragmatism shortly. I want take a moment, though, to address Prism. Francesca interestingly asked if we could embed Prism within our gaming environment, and, also interestingly, within my group’s presentation, Stephanie had built a mockup of how something very close to this could be done as an alternate way to play the Ivanhoe game, an alternate way to make textual interventions. But here’s the rub, it was an alternate way, meaning not the only way, nor perhaps even the primary way. What Prism allows one to do is “collaborative interpretation” through highlighting a text according to different categories, or “facets.” If we look at our most successful games of Ivanhoe as our cohort has experimented with Ivanhoe (primarily textually, I will admit), the most interest moves have involved writing text and including media with that text at times. This is a different thing than Prism though, and doesn’t involve the same sort of highlighted interpretation of a text through determined categories. Rather, textual moves in Ivanhoe are often products, literary creations of different forms, interwoven with different media. So I have to say that I don’t think embedding Prism is sufficient to grapple with the ways in which we might engage texts through Ivanhoe. The two tools are simpy different, with different goals and different means. But why include text at all? Why not just build an image (very broadly conceived) game? Selfishly, I would want to play this game myself, to use it in thinking through theological and philosophical texts, and perhaps play it now and then with the science-fiction and fantasy literature I love. For me, that will mean writing text and incorporating others texts, as well as linking in images, videos, and sound clips. Disregarding my own interests, though, there is a significant pragmatic reason for making sure that Ivanhoe handles texts well. One of the tenets of the Praxis Program is to build a functioning tool or application that will actually be used. To do that latter part, we need to have an idea of our audience and what they might do with our tool. At the moment, while digital humanities is spreading, and while universities and colleges are pushing for innovative work in digital pedagogy, many of those who are incorporating digital tools in their teaching work within English, literature, and history departments. While all of these people probably engage with different media in their work and teaching, and would play games that incorporate different media, many of them substantively work with texts, and teach classes that are in a deep and abiding way about texts. These are the people who might be most open to embracing a tool such as Ivanhoe, that could enrich and enliven their teaching through productive, ludic engagements among students, especially if we can build a version of Ivanhoe that has a low-bar of entry to use. Understanding pragmatically who might buy in and use the tool we’re building more quickly and more thoroughly, then, I think it is necessary to make sure that Ivanhoe is text inclusive, just as it is inclusive of others forms of media. Despite this rather lengthy engagement with Francesca’s concerns, I actually agree with her when she writes that she, “was struck by the similarities between our projects (and a bit relieved to see that we won’t really need to make too many compromises in either direction).” The differences I’ve addressed here are important, and I’ve tried to offer cogent arguments regarding both. But I don’t think either difference is enormous or prohibitive of our team, all six of us along with the folks of the Scholars’ Lab, moving forward. I certainly look forward to the days ahead."},{"id":"2013-11-18-tongue-tied-in-css","title":"Tongue-tied in CSS","author":"francesca-tripodi","date":"2013-11-18 07:15:14 -0500","categories":["Grad Student Research"],"url":"tongue-tied-in-css","layout":"post","content":"Learning programming languages is both exciting, fascinating, and a bit overwhelming for me. While I find that I’m typically pretty good at learning new languages I’m having difficulty getting energized about learning the back end of building what we see on the web. Ironically, one of the main reasons I applied for Praxis was to learn how to program but the more I become familiar with it, the less I want to do it! Don’t get me wrong, I see the value and importance in the skill, I think I’m just way behind on the learning curve and the mountain in front of me seems daunting. Moreover, I don’t think I’m ever going to get to  ”computer guru” status so I wonder if having a mediocre set of skills will be particularly useful on the job market. That being said, I’m happy that our group has decided to work within WordPress and develop a game PlugIn instead of building an Ivanhoe from scratch. Not only are there cost benefits to this model, it seems like knowing how to use WordPress in a more meaningful way will be an important skill for me to have and something I will be able to take with me after my fellowship is over. Even though I originally thought I would be building something from the ground up I’m coming to realize that a year would never be enough time for me to do this well. I also have a newfound appreciation for the IT professionals at offices everywhere."},{"id":"2013-11-21-gis-day-2013","title":"GIS Day 2013","author":"chris-gist","date":"2013-11-21 06:22:44 -0500","categories":["Geospatial and Temporal"],"url":"gis-day-2013","layout":"post","content":"November 20, 2013 was GIS Day .  In our annual tradition here in the Scholars’ Lab, we hosted a round of lighting talks with a variety of speakers including several groups from the Shenandoah Valley Governor’s School . As always, the had a great mix of disciplines and uses of GIS.  Thanks again to all the participants.  A slide stack of the presentations is available here .  Oh, I almost forgot the other part of our GIS Day tradition, GIS Day cake! For images of previous years’ cakes, look here . Video of presentations to come."},{"id":"2013-11-25-better-focus","title":"Better :focus","author":"jeremy-boggs","date":"2013-11-25 08:48:52 -0500","categories":["Research and Development"],"url":"better-focus","layout":"post","content":"Whenever I’ve taught folks how to do some basic HTML and CSS, the first thing they want to change are the styles for links on the page. And who can blame them? The default colors for links are pretty lame, as you can see in my first example page on CodePen . For those who don’t know, there are a few states of a link you can style independently, using CSS pseudo-class selectors . These pseudo-class selectors include: :link - The default state of the link :visited - Allows you to style links to addresses you’ve already visited. :hover - Allows you to update link styles when a user moves their pointer over a link. :focus - Allows you to update link styles when a user tabs or otherwise changes their focus to a link. :active - Used to update link styles when the link is activated (when clicked or when a user hits enter on a focused link). You’ll notice that, if you use your “Tab” key to navigation through the links on the example CodePen page, you can move between links. The link you’re current on will be outlined, depending on your browser’s settings. (You can do the same for forms, too. This is a nice way,if the form is marked up correctly, to move between fields when filling out a form, instead of moving your cursor to the next field and clicking on it.) The styles of these focused elements can be updated using the :focus pseudoclass selector. Now, earlier versions (and some current versions) of CSS resets would set the “outline” property for everything to “0”, thus getting rid of the outline on focus and any other state. In their defense, they also recommended resetting this for accessibility, but I’ll admit that I rarely remembered to do this in my younger days. The result is that there are no indications when a user has focused on a link or form element. Similarly, but perhaps even more foolishly, I was for a while inclined to think that the act of hovering over a link or form element was equivalent to focusing, so I would very often style these two pseudoclasses in the same CSS rule: a:hover,\na:focus {\n   color: orange;\n} You can actually seen an example of this right now on our Speaking in Code site. Using the tab key to move between links on the page uses the same styles as hover, but they’re not very noticeable, certainly not noticeable enough to know where you’ve focused when the page scrolls down. Part of the problem here is that the hover/focus styles aren’t really different enough from the default link styles anyways, which is something I should address. But with focus, the way a user navigates the page is inherently different, and assuming that hover and focus are equivalent leads to a pretty inaccessible page, even for folks who just tab between links for convenience. Long story short: I need to do better focus styles. This is a very different method for navigating web pages, and the styles should help facilitate that navigation, not obscure it. As a quick example of exploring more prominent styles for link :focus, I created a new CodePen with some updated styles : a:focus {\n  outline: thin dotted #60c;\n  color: #60c;\n  background: #fff09e;\n  text-decoration:none;\n  padding: 2px 0;\n  font-weight:bold;\n} Now if you tab through the links on the page, you’ll notice they stand out a bit more. They’re intentionally ugly—to show you how different you can actually make these look—but you can change the styles to balance matching your own aesthetic with making the links noticeable enough to see where you’ve focused on the page. A few lessons to take away here: If you use a CSS reset, actually read through it and see what it does. They’re meant to be just that—a reset—and still expect you to set styles appropriately. Most of them are designed to give you a consistent starting point across browsers, but they still don’t supersede your responsibility to set styles appropriately for all your users. They’re also meant to be tinkered with and changed for your own preferences and design goals. Take particular care to style the :focus pseudoclass for both links and form elements. It’s a handy way to navigate parts of a page, but without noticeable style updates, users can quickly lose track of where they’ve actually focused. Now I’m off to go update the styles on Speaking in Code, and just about every other project I’ve had my hands on around here!\n  *[HTML]: Hypertext Markup Language\n  *[CSS]: Cascading Style Sheets"},{"id":"2013-12-09-podcast-open-access-week-speaker-gail-mcmillan","title":"Podcast: Open Access Week Speaker - Gail McMillan","author":"ronda-grizzle","date":"2013-12-09 06:07:17 -0500","categories":["Podcasts"],"url":"podcast-open-access-week-speaker-gail-mcmillan","layout":"post","content":"UVa Library Open Access Week Speaker: Gail McMillan Graduate Student Publishing and Open Access: Understanding the Digital Landscape of Open Access Publishing for Theses and Dissertations On October 29, 2013, Gail McMillan, Director of Digital Libraries and Archives at Virginia Tech, spoke in Alderman Library about her research on the effects of open access for theses and dissertations on publishing opportunities for graduate students. Joanna Swafford and Brandon Walsh, Ph.D. candidates in the Department of English and former Scholars’ Lab Fellows, served as respondents for Ms. McMillan’s talk. As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.28074082867/enclosure.mp3”]"},{"id":"2013-12-11-neatline-san-francisco","title":"Neighborhoods of San Francisco","author":"david-mcclure","date":"2013-12-11 07:14:03 -0500","categories":["Geospatial and Temporal"],"url":"neatline-san-francisco","layout":"post","content":"[Cross-posted from dclure.org ] View the Exhibit Built on the Stamen Toner layer . Back in October, about a month after moving from Scholars’ Lab HQ in Virginia out to Menlo Park (my partner started a PhD program at Stanford), I drove up the peninsula to San Francisco on a Saturday morning and set out on a long, rambling, 10-mile trek along the northwest shoulder of the city. It was a fantastic day - I walked west through Golden Gate Park, north along the Richmond beach, past the Cliff House, into the fog over Lincoln Park, through the mansions in Sea Cliff, past the abandoned artillery nests on the western coast of the Presidio, and finally out onto the Golden Gate bridge. From there, I headed south through the trails in the Presidio, into Richmond, and eventually back to where I started, near the top right corner of the park. From the middle of the Golden Gate Bridge, you can look out to the east over a large swath of the city - the skyscrapers of the financial district, the new span of the Bay Bridge hanging over Treasure Island, Alcatraz, and the faded outline of the East Bay, the Berkeley campus a little smudge at the base of the ridge line. But, scanning my eyes over the rest of the city, I realized that I had very little sense of what I was actually looking at. I could attach labels onto all the touristy landmarks, but I didn’t have any kind of intuitive mental geography of the place - the names of all the little hills and neighborhoods, what connects to what, how to string the pieces of the city together into workable routes and itineraries. So, over the course of the next few weeks, I slowly cobbled together a Neatline exhibit that plots out each of the neighborhoods in the city - 87 of them, by my count, although it’s somewhat a matter of interpretation as to how they should be sliced and diced. Working mainly from this image as reference, I started by tracing rough outlines of the boundaries (using Neatline’s standard-issue “Draw Polygon” tool) on top of Stamen’s Toner layer. Then, once the borders were in place, I used Neatline’s SVG import tool to place vector-geometry text labels on top of each of the individual neighborhoods, inspired by other spatial-wordcloud experiments like this and this . Adventures in geospatial typesetting This was great fun, and, interestingly, it ended up overlapping in unexpected ways with the interactive typesetting projects that I was playing with earlier in the semester. The process of positioning the labels becomes a kind of textual jigsaw puzzle, a game of trying to wrangle the raw, geometric instantiations of words into a coherent organizational scheme - except, this time, against the backdrop of actual geospatial coordinates and locations, not the abstract, featureless voids of the poetry experiments. Often, this is pretty straightforward - Noe Valley and the Inner Mission, for example, just get tagged as such: In other places, though, the names of the neighborhoods overlap with one another in ways that make it possible to find interesting “economies” in how the words can be laid out on the map - when adjacent neighborhoods share the same words, it’s sometimes possible to essentially atomize the names into their component parts, and then rebuild them according to their own spatial logic, in a sense, by visually stringing together the pieces on the map. Take Richmond, for example, which is divided into three side-by-side segments: Outer Richmond, Central Richmond, and Inner Richmond. Instead of cluttering things up by repeating “Richmond” for each of the three sections, I just dragged out a single, all-encompassing “Richmond” across the entire width of the three sub-neighborhoods, and the blocked in the three modifiers as smaller words on top of the corresponding sections: This worked much the same way for the Sunset and Parkside neighborhoods, which share the same cleanly partitioned spatial organization: With the exception of the “middle” portion of Parkside, which is just the un-prefixed “Parkside,” meaning that the center piece doesn’t get a separate modifier: In other cases, though, it gets much trickier, and much more interesting. Take the little cluster of neighborhoods at the southwest corner of the Presidio, the big park at the base of the Golden Gate Bridge. It’s a hodgepodge of repeated names, but in a much more scrambled and overlapping way - Presidio, Presidio Heights, Pacific Heights, Lower Pacific Heights. In this case, I had to take a bit more care to place the little black arrows in ways that didn’t connote incorrect linkages among the names. For example, the relationship between Presidio and Presidio Heights moves in just one direction - Presidio Heights (labelled with just “Heights” on the map) needs to “inherit” the “Presidio” modifier from the Presidio, but not the other way around, since the Presidio ceases to be the Presidio when “Heights” is tacked onto it: To encode these relationships, I settled on a rule of thumb that the arrows would always be contained inside the neighborhoods that they modify . So, the arrow pointing from “Presidio” to “Heights” is fully contained inside of the geographic boundaries of Presidio Heights, in the sense that it pulls “Presidio” downward into the “Heights,” without also pushing “Heights” back in the other direction (which would effectively mislabel the Presidio). Likewise the link between “Pacific” and “Heights” is contained within the Pacific Heights outline, since otherwise Presidio Heights would be implicitly but incorrectly prefixed by “Pacific.” Anyway, this is all completely useless as actual cartographic practice, but great fun as a kind of abstract étude of information design. It’s also incredibly useful as a mnemonic device - after untold hours in Palo Alto coffee shops sketching out all the outlines and positioning the labels, they’re all thoroughly burned into my mind. This is an interesting aspect of digital mapping projects that doesn’t get a lot of attention - we tend to focus on the final products, the public-facing visualizations and interactions (for good and obvious reasons), but much less on the process that goes into creating them, the personal acquisition of knowledge that takes place when you force yourself to spend dozens or hundreds of hours painstakingly positioning and repositioning things on maps. It gives you an incredible sense of cognitive intimacy with the space, the ability to load a little chunk of the world into working memory and reason about it in really complex and interesting ways."},{"id":"2013-12-14-turning-points-in-praxis-new-roles-wire-frames-and-programming-languages","title":"Turning points in Praxis: new roles, wire-frames, and programming languages","author":"stephanie-kingsley","date":"2013-12-14 04:33:56 -0500","categories":["Grad Student Research"],"url":"turning-points-in-praxis-new-roles-wire-frames-and-programming-languages","layout":"post","content":"The last couple of weeks have been exciting ones in our program.  Our team has now specified our individual roles for the year.  Eliza, Scott, and Veronica will be our coders; Francesca and Zach will be the design team; and I will be performing project management duties, with assistance from Francesca.  I am excited to see these groups forming, people beginning to specialize, and a greater sense of direction inspiring everyone. After weeks of theorizing what the Ivanhoe Game SHOULD be, our talks about what it actually COULD be based on time, money, skills, etc., inspired some new questions: What platform should we use to build our tool? Which type of tool will be the most accessible and inviting to users? Which type of programing language will we be able to master in this time frame? What kind of tool is most likely to prompt open-source contributions by other academics? What is the easiest to maintain time-wise (and time is, after all, money) for the SLab once the fellows leave the program? Our end decision has been to build our Ivanhoe as a WordPress plugin which users can download and use on their own WP sites, rather than as a stand-alone, hosted service.  We saw WP as familiar and widely-used enough that it would make start-up as easy as possible for users.  (See Francesca’s thoughts on our decision to use WordPress.)  It also means that other programers can play with our code and contribute to Ivanhoe’s development and maintenance. Eliza, Jeremy, Zach, and I met Wednesday to draw up some wire-frames for our revised Ivanhoe.  After a couple weeks of discussion about what our Ivanhoe would be, we have a more concrete–but still highly viscous–vision of what our game will look like.  We plan to have a game which will allow players to make moves which link to other moves–a key feature.  Players will be able to incorporate text, image files, sound files, and video files into their moves.  We also have plans for a role journal which will aggregate the “Rationale” entries made every time a player makes a move.  Ivanhoe strongly resembles blogging platforms such as Tumblr and WordPress blogs, but we hope our particular setup with linking possibilities will inspire new ways of posting.  We also hope, through our design strategies, to encourage a more multimedia approach to making posts. Our journey now takes us forth into the strange world of PHP–which, if you had asked me about a month ago, I’d have guessed was some sort of chemical.  To embrace our multimedia tenet, I will conclude with a not-so-subtle homage to our new best friend: As we ring in the new year, here’s to magic… only a few queries away:"},{"id":"2013-12-16-podcast-dot-porter","title":"Podcast: Dot Porter","author":"ronda-grizzle","date":"2013-12-16 04:10:47 -0500","categories":["Podcasts"],"url":"podcast-dot-porter","layout":"post","content":"Scholars’ Lab Speaker: Dot Porter Ceci n’est pas un manuscript: How Digitization and Presentation Practices Ignore and Obscure the Physicality of the Object On October 30, 2013, Dot Porter, Curator of Digital Research Services at the Schoenberg Institute for Manuscript Studies in the Kislak Center for Special Collections at the University of Pennsylvania, spoke in the Scholars’ Lab about the ways in which current manuscript digitization and presentation practices ignore and obscure the physicality of the object and offered some ideas for how to deal with it. Ms. Porter joined us as part of the Global Digital Libraries Symposium, co-sponsored by Rare Book School, the Scholars’ Lab, and the Buckner W. Clay Endowment for the Humanities at the Institute of the Humanities &amp; Global Culture. As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.28089203644/enclosure.mp3”]"},{"id":"2013-12-16-podcast-meg-stewart","title":"Podcast: Meg Stewart","author":"ronda-grizzle","date":"2013-12-16 04:28:41 -0500","categories":["Podcasts"],"url":"podcast-meg-stewart","layout":"post","content":"Scholars’ Lab Speaker: Meg Stewart A Fulbright Scholar Talks About Participatory GIS, the Caribbean, Google Earth and How a Fulbright Could Be in Your Future On December 3, 2013, Meg Stewart, Academic Technology Consultant and Fulbright Ambassador, spoke in the Scholars’ Lab about her experiences as a geospatial technologist in the Caribbean and about the Fulbright Scholar program and encouraged applications for a Fulbright grant. Ms. Stewart’s presentation slides can be viewed by clicking here . As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.28089203756/enclosure.mp3”]"},{"id":"2013-12-24-breaking-things-over-winter-break","title":"Breaking Things Over Winter Break","author":"veronica-ikeshoji-orlati","date":"2013-12-24 05:47:01 -0500","categories":["Grad Student Research"],"url":"breaking-things-over-winter-break","layout":"post","content":"It has been over a month (!) since my last post, but the long delay is certainly not due to lack of activity in the Praxis-Ivanhoe world. Our PM, Stephanie, has written a great post summing up the progress we’ve made as a group towards defining and building Ivanhoe. I’d like to underscore her point that, now that our roles within the team have started to crystallize, it feels like we’re finally all moving in the same direction. During the weeks between Thanksgiving and the end of the semester, Wayne et al. presented us with some lessons in PHP and have now sent us off for the winter break with the task of practicing what we learned. Again. And again. And again. And again. And that is exactly what I’m doing this winter break. I am committing to memory the syntax of foreach loops and if…elseif…else statements, and I’m stumbling my way through the (rather entertaining) process of translating what I want to do into a logical flow my fatal-error-loving computer can comprehend. But to tell the truth, I’ve also spent an inordinate amount of time breaking things and banging my head on my keyboard (figuratively and, on more than one occasion, literally). And no matter how many times our Scholars’ Lab mentors say it’s okay to break a piece of code, a wave of panic overcomes me every time I do so. (Sample internal monologue: “Oh no! It’s the end of the world and my computer will self-destruct in 5 seconds! How could I forget that semicolon? Noooo!”) Fumbling around with PHP has dredged up many of the anxieties which Francesca articulated in one of her posts, namely that I am so far behind on the learning curve that becoming adequately competent (and confident) with computer programming is beyond my reach. I suppose I knew that a magical panacea for my technophobia wouldn’t exist, and I even asked to work on the development side of Ivanhoe so that I would be forced to face my fears directly, but that hasn’t kept me from hoping to simply one day wake up and be okay with breaking things. After a brief reflection on the nature of my programming anxiety, I did a few Google searches. What I found surprised me. Books such as Gender and Computers: Understanding the Digital Divide (2003), Gender Codes: Why Women are Leaving Computing (2011), Recoding Gender: Women’s Changing Participation in Computing (2012) ; articles such as “Caring About Connections: Gender and Computing” (1999), “Why So Few Women Enroll In Computing? Gender and Ethnic Differences in Students’ Perception” (2009), “The Effect of Tangible Artifacts, Gender and Subjective Technical Competence on Teaching Programming to Seventh Graders” (2010) ; long-term, multi-part studies on women in CS ; clearly I need to think a bit more about the roots of my anxieties. I’ll save the topic of gender and coding for another post. In the meantime, for the rest of winter break, you’ll find me breaking code, breaking down over broken code, and trying to break through some of my anxieties and insecurities about computer programming."},{"id":"2013-12-24-lessons-for-christmas-a-sawzall-solves-all","title":"Lessons for Christmas: A Sawzall Solves All","author":"zachary-stone","date":"2013-12-24 05:32:54 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"lessons-for-christmas-a-sawzall-solves-all","layout":"post","content":"Christmas, this year, is for building stuff. Today’s work orbits around three projects: building my new dining room table, finishing off a bike I am building for someone, and building some stuff in CSS.  As we move into next year with our Praxis roles a bit more solidified, it looks as if I will be focusing on design.  Which seems kind of fun and kind of scary and kind of like building a table or a bike. Or at least I go about all three roughly the same way: if stuff doesn’t fit, hit with a hammer until it does and always remember: A Sawzall solves all. In the case of CSS that means forking stuff over to Wayne.  Anyhow, all three balance out The Other Thing (Orals on 30 Jan…gulp) and allow me to make small, tangible advances on something (anything, really) while slogging away at the mountain of textual criticism reading. In the course of all three projects, similar patterns, or at least aesthetics, emerge. I dislike clutter. If given the choice to excise or supplement I always opt for the Sawzall. I love the clean lines of a perfectly assembled bike, a well-balanced tabletop, and a clutter free website.  Thankfully the ghosts of Praxis past set a pretty good aesthetic precedent in the latest iteration of Prism . Hopefully, come the new year, I’ll have a new table, a happy relative on a bicycle, and some flashing dinosaurs in CSS."},{"id":"2013-12-24-praxis-holidays","title":"Praxis Holidays","author":"elizabeth-fox","date":"2013-12-24 17:42:38 -0500","categories":["Grad Student Research"],"url":"praxis-holidays","layout":"post","content":"Just before we departed for the break, Stephanie and I met with Jeremy to talk over some of our wireframes for Ivanhoe.  (Stephanie discussed our wireframing process in her post .)  “Right,” Jeremy told us.  “You’re going to want to clone the files that I’ve already created and start from there.” We blinked at him.  “Cloning?” we said.  “Is that like forking?  Do we need another repo?  Can we have another repo?” Thus began an epic Git recap, in which Jeremy heroically explained version control to us once again.  The result was not only a much-needed refresher on the workings of Github, but a reminder of just how much we’ve learned this semester.   In true Praxis soup-to-nuts form, we’ve considered quite a few topics—we’ve discussed Git, CSS, wireframes, and WordPress as well as PHP.  With only two meetings a week and a wealth of other commitments, we’ve all struggled with the act of learning and retaining information as we move from one subject to the next. For me, then, this winter break represents a time to catch up on things.  Like Veronica, I’ll be spending most of it working on PHP.  (As she has rightly noted in her post, the process represents equal parts studying and banging my head against the desk.)  In addition, though, I’ll take some time to review the many other skills that we have learned.  By keeping up my proficiency in various fields, I’ll be able to understand the design team as well as the development team, to discuss wireframes in addition to PHP.  I’ll also gain a better understanding of DH as a whole—what better holiday gift for a Praxer? Happy holidays to all, and have fun cloning in the new year!"},{"id":"2013-12-24-we-wish-you-a-merry-cssmas","title":"We wish you a merry CSSmas! ","author":"francesca-tripodi","date":"2013-12-24 05:37:50 -0500","categories":["Grad Student Research"],"url":"we-wish-you-a-merry-cssmas","layout":"post","content":"While I plan on taking some time off over Christmas and New Year’s to be with family and friends, I also plan on fine-tuning my programing skills over the break. At the suggestion of Jeremy the first step is diving into tutorials (which involves equal part determination and motivation). As you may have gathered from my last post   I plan on starting with the very first lesson: What is CSS?   ;) See you all in the New Year!"},{"id":"2013-12-25-a-little-bit-of-everything-christmas-for-the-praxis-project-manager","title":"A little bit of everything: Christmas for the Praxis project manager","author":"stephanie-kingsley","date":"2013-12-25 10:14:46 -0500","categories":["Grad Student Research"],"url":"a-little-bit-of-everything-christmas-for-the-praxis-project-manager","layout":"post","content":"Merry Christmas!  As you can see from Zach, Francesca, Veronica, and Eliza’s posts, we’re all equally busy preparing ourselves for the thrilling upcoming semester of building Ivanhoe.  Like my fellow Fellows, I too will be practicing my HTML, CSS, and PHP, because as project manager I need to understand the challenges the group faces in coding and designing Ivanhoe.  (I also find these computing languages fun–so why not?)  Besides this practice, I am currently in the process of coordinating a group post for New Year’s Day–perhaps with flashing dinosaurs.  This post will show off some of our new CSS skills–and hopefully some PHP.  So stay tuned. Although things have calmed down a bit now, before leaving Charlottesville for Winter Break, I had several very busy days in the Scholars’ Lab meeting with Bethany, Jeremy, and Wayne about various project management issues.  Bethany schooled me in being an administrator on the SLab WordPress site–a skill I’m now wielding in publishing Praxers’ blog posts.  Eliza described our meeting with Jeremy to wire-frame Ivanhoe in HTML.   To do this, we needed a tutorial on collaborating in GitHub–cloning, pulling, pushing, forking, etc.  Wayne and Jeremy both advised me in laying out a schedule for Ivanhoe construction next semester (more on this in the Spring).  Finally, I compared Doodle Polls for all Scholars’ Lab and Praxis people, sent a flurry of emails to coordinate meeting times for next semester, and–laptop in tow–drove home to Charlotte, where I now sit contentedly drinking a glass of eggnog. I will be working to familiarize myself with WordPress, as well as gain proficiency in setting up GitHub repositories and coordinating collaboration on Git.  Our team’s New Year’s post will be the result of my first self-organized Git project! Setting that up took quite a while, involving a combination of recall from Jeremy’s lessons and trial-and-error before I successfully created a repository that would display as a webpage which our whole team could work on.  (Veronica mentioned that learning PHP involved breaking things ; well, I set up and deleted three different repos before achieving success.)  I also needed to describe the process of cloning a Git repo to a teammate who had been absent for Jeremy’s lesson but would need to clone over break. Teaching something is truly the best way of solidifying your own learning, which is convenient, as I have a number of friends who want me to teach them HTML over the holidays for the purposes of building their own professional web pages. To all coders everywhere, teach your friends, and spread the joy in this festive time of year! So ultimately, my break is about learning a multitude of bits of computer/coding knowledge so that I can be competent in prioritizing, scheduling, and reporting back on our progress next semester.  I look forward to cracking CSS with my chestnuts, HTML with my hazelnuts, PHP with my pecans, and WordPress with my walnuts as I work through my mixed bag of skills–and nuts!–this winter."},{"id":"2013-12-31-happy-new-year-and-a-few-thoughts-to-begin-it-with","title":"Happy New Year! -- and a few thoughts to begin it with","author":"stephanie-kingsley","date":"2013-12-31 12:12:34 -0500","categories":["Grad Student Research"],"url":"happy-new-year-and-a-few-thoughts-to-begin-it-with","layout":"post","content":"To those who read my post last week, we do not have a flashy New Year’s post to show you; however, I have drawn some useful insights from this circumstance which I would like to share as we kick off another year. My first insight has to do with learning new computing skills and the SLab folks.  The plan was for our Praxis fellows to contribute some HTML, CSS, and PHP to a GitHub repository to create a web page which would read “Happy New Year!”  We would then display this page or include a link to it in a blog post on New Year’s Day.  The idea was to demonstrate the new skills we learned in Praxis this past semester which we will be using next semester to construct Ivanhoe.  I failed, however, to account for a few pitfalls along the way.  For instance, as Wayne pointed out to me over email, GitHub will not support PHP; I would instead need to use Heroku or AppFog to deploy it.  So my battle with Heroku commenced.  Setting up the account was easy enough, but problems began when I attempted to set up remote repositories and apps.  Despite my having set up several apps with names like “vast ocean” and “aqueous fountain,” in actually trying to verify the correct remote repository or add files to the app, either the repo would “already exist” or the push would fail for one reason or another.  This post will not be a particularly tech-savvy one, but the point of all this is rather to say how grateful I am for the wonderful support and patience of Eric, Jeremy, and Wayne (our development and design SLab mentors) in assisting us Praxers in tackling the most elementary of computing problems.  This was the day after Christmas, and all our SLab friends were probably relaxing or celebrating, as I should have been doing–so no assistance via chat, either.  I spent close to four hours waging war on Heroku when one of the SLab folks could have set me up in five minutes.  I stubbornly relish figuring things out for myself, but sometimes, that really can result in–like Veronica and Eliza –banging my head on my keyboard.  Thank you, Eric, Jeremy, and Wayne, for your teaching and patience. My second insight relates to project management.  I must admit, I concocted this plan myself.  As an optimist, I tend to see creative endeavors as solely fun and easier to execute than they actually are in the end.  I failed to anticipate the obvious: that no one wants to or should be badgered into doing more work–however fun–between December 24th and January 1… period.  I underestimated how much effort this thing would take to pull off (as my Heroku efforts demonstrate), and I got overly enthusiastic about this new creative endeavor, expecting others to feel the same.  I ended up calling off the project, and we all went off to revel with family and friends, just as we should.  To my fellow Praxis teammates, I apologize for my unseasonable exuberance.  The project manager’s job should be to figure out what work needs to be done and how it can be executed most efficiently–not to create more work for the team.  So even though we do not have a flashy New-Year’s post, I am happy to say that we do have a more sensible project manager. A Happy New Year to our Praxis team, to the entire SLab, and to all our followers!  I look forward to continuing work in January and keeping everyone posted on Ivanhoe and all our Praxis experiences along the way."},{"id":"2014-01-05-building-a-website-and-pulling-apart-wordpress-plugins","title":"Building a Website and Pulling Apart WordPress Plugins","author":"scott-bailey","date":"2014-01-05 08:02:03 -0500","categories":["Grad Student Research"],"url":"building-a-website-and-pulling-apart-wordpress-plugins","layout":"post","content":"For the first time in all my years as a graduate student, I decided to take some of Christmas break as an actual break from school work. So, for the first two weeks of break, I refused to read school related emails, read mostly neuroscience instead of theology or philosophy, only thought about my dissertation a couple of times a day, and only glanced at code here and there. After those restful two weeks, it’s time to get back to work though. Before school starts back up, I’ve got a few things I’m trying to do. Foremost, I’m finishing up building my own website, which will be hosted through Github Pages and run there with Jekyll . I’ve been playing around with different grid frameworks, such as Skeleton, but have decided finally to just write it from scratch with no grid given the simplicity of what I’m trying to do. A surprisingly hard part of the whole process, I’ve found, is picking good typography. As writing focused websites and blogging platforms, such as Medium, become more popular, more and more designers seem to be moving toward clear, minimal layouts and designs which serve as backdrops to more interesting typographical choices. As I am myself drawn toward sites with a lot of white space, minimal navigational elements, and larger text, I’ve been designing my own site similarly, and trying to choose a good font stack to really set off my own written content, as the primary focus of the website will be blog posts. Doing so, though, has led to hours of perusing fonts on Google Fonts and Font Squirrel and reading articles on typography in modern web design. More immediately relevant to Praxis, I’m reviewing our lessons on PHP from December, continuing to practice through resources provided by the SLAB staff, and beginning to pick through WordPress Plugins . With the last of these, I’m paying a lot of attention to the structure of the plugin, and the structure of the code. One of the most important things we can do in building the Ivanhoe plugin is to rigorously stick to best practices, including clear and consistent commenting, to make our code easily readable for future groups and/or outside developers who might want to work on it in the future. As we get closer to our official return to school, I look forward to hitting the ground running with the other Praxers and building something not just working but sustainable!"},{"id":"2014-01-06-are-you-our-new-digital-humanities-developer","title":"Are you our new Digital Humanities Developer?","author":"wayne-graham","date":"2014-01-06 08:56:56 -0500","categories":["Announcements"],"url":"are-you-our-new-digital-humanities-developer","layout":"post","content":"Are you an enthusiastic software developer with an interest in the humanities or cultural heritage? The internationally-recognized Scholars’ Lab is seeking a digital humanities software engineer to join its innovative Research and Development group. At the University of Virginia -based Scholars’ Lab, you’ll work on projects like Neatline and collaborations with UVa faculty and students, mentor graduate fellows, and help teach in our Praxis Program . In addition to contributing to all of these facets of digital humanities work at Virginia, you will also be eligible for ”20% time,” where you are encouraged to pursue your own (often collaborative) R&amp;D project or scholarship. As a Digital Humanities software engineer reporting to the Head of R&amp;D for the Scholars’ Lab, you will be responsible for building, testing, and debugging code, developing documentation, and helping to manage our server infrastructure. You should possess a fine attention to detail and a high level of accountability and responsibility. We’re looking for someone who enjoys technical challenges, likes to figure out how things work, and stays involved in the latest web and digitial humanities technologies. You will need to fit in with a creative and collaborative group of software engineers to help create the next generation of scholarly interfaces. We particularly encourage applications by women developers and members of other under-represented groups. Demonstrated ability with one (or more) of the following: Your Github/Bitbucket/other repository Your Open Source Work Your awesome blog Code samples from side projects Your production website (handling real traffic) Ability to work with technical and non-technical collaborators Duties and Responsibilities Build, test, and debug open source software Estimate effort for software projects Brainstorm and prototype new concepts and approaches into real things Server and service deployments, server and database installations and configuration management Ability to draft and communicate design concepts Writing and updating internal documentation of systems and processes Knowledge of systems and network security issues and trends Maintain distinct environments such as development, staging, and production Qualifications Experience with configuration management systems and concepts (e.g. puppet, chef, Ansible, cfengine) Experience building web applications Knowledge (or ability to learn) our technology stack, which includes: Ruby/Ruby on Rails/Sinatra PHP/Omeka MySQL/PostgreSQL/PostGIS JavaScript/CoffeeScript (Backbone, Ember, jQuery) Testing frameworks (PHPUnit, RSpec, Cucumber, Jasmine) git Cocoon JSP Capistrano/Vagrant Passion for growing your skills, tackling interesting work and challenging problems Ability to design and write well-structured, maintainable, well-documented code that balances beauty and pragmatism Strong communication skills Experience in the digital humanities is a plus To Apply For full details, and to apply for this position, see the official posting (posting number 0613484 if you search Jobs@UVa ). The University of Virginia is an Equal Opportunity/Affirmative Action employer, strongly committed to achieving excellence though cultural diversity. The University actively encourages applications and nominations from members of underrepresented groups."},{"id":"2014-01-07-faulty-format-or-user-error","title":"Faulty Format or User Error?","author":"francesca-tripodi","date":"2014-01-07 07:00:39 -0500","categories":["Grad Student Research"],"url":"faulty-format-or-user-error","layout":"post","content":"As you’ve already learned from Stephanie’s post   team delegates (Stephanie &amp; Eliza) have been wireframing ideas for Ivanhoe since we’ve decided on creating a plugin for WordPress . But before this happened, the entire team brainstormed the features that we would like to see included in our final product. As we became animated about our “must haves” (uploading visuals, URL linking, text options) - Bethany brought up a good point - WordPress already does all these things (Plugin not required!) Her statement got me thinking - for all of our desires to create “visual connections” - none of us do this! Perhaps some of the problem comes with the way WordPress is set up - for those of us who rarely notice here is a screen shot (click to make it larger): Perhaps part of the reason people don’t link or use media is that the primary space is geared toward writing. Even the symbols most prominent mimic those found on Word (a text-based tool). Think about how different it would be when you clicked on the button to create a new post, this is what came up (yes, it’s a bit ugly…but go with me here) While these are essentially the same features that already exist in basic WordPress the emphasis on the visual makes it seem like something else entirely (a point  McLuhan has been making since the seventies). So what does this mean for us? Well for starts I think we all need to look more carefully at what WordPress already offers and then think about how the organization/visualization of what we create will influence user application in ways we might not intend. Doing so will help us create a product that people want to use but also an environment that fosters our intended ideas."},{"id":"2014-01-07-map-sleuthing-in-africa","title":"Map Sleuthing in Africa","author":"kelly-johnston","date":"2014-01-07 04:36:20 -0500","categories":["Geospatial and Temporal"],"url":"map-sleuthing-in-africa","layout":"post","content":"One of the many fun things we do in the Scholars’ Lab is help people find geographic datasets. [![srh.noaa.gov](http://static.scholarslab.org/wp-content/uploads/2013/12/gislayers.jpg)](http://static.scholarslab.org/wp-content/uploads/2013/12/gislayers.jpg) Yes, these are geographic datasets via srh.noaa.gov Folks use geographic datasets to make maps and for spatial analysis using geographic information systems software.  Finding detailed local-scale datasets can be hard.  And finding local-scale geographic datasets for areas outside the United States is even harder, often approaching impossible, which takes a little longer. Charlottesville, the University of Virginia’s home town, is a sister city with Winneba, Ghana .  So it follows that the UVa School of Architecture chose Winneba as one of the cities for its interdisciplinary Resilient Communities project focusing on the challenge of climate change and rapid urbanization for coastal communities. Local-scale mapping and analysis for Resilient Communities will require detailed datasets describing both natural and man-made features. During the summer of 2013 our Scholars’ Lab colleague Matt West invested a good bit of time searching for Ghana datasets.  But the datasets found from afar were coarse in resolution, better suited for regional analysis than the kind of local-scale work planned for Resilient Communities.  Seemed our best option for success was to find local experts in Ghana with access to the best local datasets. [![MV Explorer via semesteratsea.org](http://static.scholarslab.org/wp-content/uploads/2014/01/20131202-MV-Explorer-Kite-Aerial-Photography-sailing-into-Rio-1024x768.jpg)](http://static.scholarslab.org/wp-content/uploads/2014/01/20131202-MV-Explorer-Kite-Aerial-Photography-sailing-into-Rio.jpg) Semester At Sea on the MV Explorer via semesteratsea.org On the Fall 2013 voyage of Semester At Sea, a ship-based study abroad program, I was fortunate to visit Ghana with Nancy Takahashi, distinguished lecturer of Landscape Architecture at UVa.  Nancy is leading the Resilient Communities effort in Winneba.  Joining us was Semester At Sea faculty member Sian Davies-Vollum, an expert on coastal geology. To kick-off the Ghana project, Nancy arranged a series of meetings with Ghanaian experts.  Throughout our time in Ghana, city planner and chair of the Winneba-Charlottesville Sister City Commission, Joe Baami gave us excellent guidance. [![20131016 Winneba 03 welcome ceremony, Joe Baami](http://static.scholarslab.org/wp-content/uploads/2013/12/20131016-Winneba-03-welcome-ceremony-Joe-Baami-1024x789.jpg)](http://static.scholarslab.org/wp-content/uploads/2013/12/20131016-Winneba-03-welcome-ceremony-Joe-Baami.jpg) Joe Baami introduces Winneba elders In Winneba we met with local government leaders to learn about the community and discuss plans for collaboration. We conferred with local school officials, visited the site where a new public library is planned, walked the downtown waterfront lined with traditional fishing boats, visited coastal and inland neighborhoods, and generally became more familiar with the lay of the land and local culture.  One goal was to lay the groundwork for future visits by Semester At Sea students who will contribute to Resilient Communities through fieldwork and research. [![Winneba Waterfront](http://static.scholarslab.org/wp-content/uploads/2013/12/20131017-Winneba-197-boats-beach-1024x768.jpg)](http://static.scholarslab.org/wp-content/uploads/2013/12/20131017-Winneba-197-boats-beach.jpg) Winneba Waterfront [![20131017 Winneba 206 Sean Davies-Vollum Kelly Iddrisu Shani](http://static.scholarslab.org/wp-content/uploads/2013/12/20131017-Winneba-206-Sean-Davies-Vollum-Kelly-Iddrisu-Shani-e1387847553766-565x1024.jpg)](http://static.scholarslab.org/wp-content/uploads/2013/12/20131017-Winneba-206-Sean-Davies-Vollum-Kelly-Iddrisu-Shani-e1387847553766.jpg) Kelly Johnston with Iddrisu Shani - Winneba Environmental Health Officer To better understand Winneba we needed maps.  Ideally we would find locally produced maps showing roads, streams, and elevation contours along with point locations for important sites like schools, clinics, and drinking water sources. But early in our conversations with Winneba government officials we learned the creation of maps built on local-scale GIS datasets was planned but had not yet happened.  So as we toured Winneba we used a printed Google Map (with minimal detail) to orient ourselves. [![20131017 Winneba 240](http://static.scholarslab.org/wp-content/uploads/2013/12/20131017-Winneba-240-1024x768.jpg)](http://static.scholarslab.org/wp-content/uploads/2013/12/20131017-Winneba-240.jpg) Joe Baami, Iddrisu Shani, and Winneba Assembly Representative Helping us in Winneba was our Ghanaian colleague Benjamin (Benjie) Akuetteh with the Centre for Remote Sensing and Geographic Information Services at the University of Ghana, Legon.  Benjie focused our discussion on the importance of understanding the spatial relationships at work in the community.  He believes maps should start every such discussion and invariably serve to bring folks from across disciplines more quickly to a common understanding.  Benjie’s many successes working with localities and geographic datasets in Ghana lent credibility to his views. [![20131017 Winneba 176 Kelly Benjamin Akuetteh (Benji) GIS guys](http://static.scholarslab.org/wp-content/uploads/2013/12/20131017-Winneba-176-Kelly-Benjamin-Akuetteh-Benji-GIS-guys-1024x768.jpg)](http://static.scholarslab.org/wp-content/uploads/2013/12/20131017-Winneba-176-Kelly-Benjamin-Akuetteh-Benji-GIS-guys.jpg) Kelly Johnston with Benjamin Akuetteh, Principal Applications Specialist, Centre for Remote Sensing and Geographic Information Services at the University of Ghana, Legon We traveled to Legon in the northern suburbs of Ghana’s capital, Accra, to visit the campus and tour the Centre for Remote Sensing and Geographic Information Services where we met Benjie’s colleagues.  It’s a very mappy place. Later, magic happened when Benjie made some calls and found a lead on some Winneba data.  So we set off on an adventure to the Survey Office of Ghana in Accra. [![](http://static.scholarslab.org/wp-content/uploads/2013/12/20131018-Accra-288-Survey-Office-1024x768.jpg)](http://static.scholarslab.org/wp-content/uploads/2013/12/20131018-Accra-288-Survey-Office.jpg) One of the many buildings at the Survey Office of Ghana A series of meetings starting with the Director of Survey at the Ghana Survey Office lead us into a cartographers’ paradise.  Detailed datasets for Winneba may already exist!  Then at the desk of  Charles Nanzo of the Survey Office we saw for the first time those very detailed Winneba GIS datasets flash up on the screen!  The appearance of street centerlines, political boundaries, contour lines, and building footprints generated excitement among all the map nerds. [![GIS Datasets!](http://static.scholarslab.org/wp-content/uploads/2013/12/20131018-Accra-290-survey-office-1024x768.jpg)](http://static.scholarslab.org/wp-content/uploads/2013/12/20131018-Accra-290-survey-office.jpg) Charles Nanzo displays GIS datasets for Winneba Unfortunately we were not able to access the data to examine it in detail but we believe these datasets can jumpstart the process of establishing a detailed basemap for Resilient Communities in Ghana.  Since we were not prepared for a large data purchase on the spot, we made some notes with contact information, snapped a few photos, and opted temporarily for a less-detailed paper map purchased from the nearby analog “Map Sales Office”. [![The Sign Says It All](http://static.scholarslab.org/wp-content/uploads/2013/12/20131018-Accra-293-survey-office-map-sales-office-1024x768.jpg)](http://static.scholarslab.org/wp-content/uploads/2013/12/20131018-Accra-293-survey-office-map-sales-office.jpg) The Sign Says It All Now back in Charlottesville we’re working with the University of Virginia Library to acquire the digital datasets from the Ghana Survey Office to make the best use of their content for the Resilient Communities project. Our experience tracking down these digital datasets points to the importance of local knowledge and local networks.  Without the help of our local experts in Ghana, these detailed local datasets would not have come to light.  Of course, this is only a small first step.  We look forward to a long and productive relationship working in collaboration with our friends in Ghana."},{"id":"2014-01-14-on-community-listening-1","title":"On Community Listening: 1","author":"erik-deluca","date":"2014-01-14 07:02:03 -0500","categories":["Digital Humanities","Experimental Humanities","Grad Student Research","Research and Development"],"url":"on-community-listening-1","layout":"post","content":"During the Fall semester, Scholars Lab Design Architect, Jeremy Boggs and I spent time conceptualizing and sketching a web interface to house a perusable version of my ethnographic composition, Community Listening in Isle Royale National Park . Check out the project abstract below. This blog post is the first of many posts that will track our developments. Meanwhile, check out past Scholars’ Lab Fellow, Wendy Hsu ’s recent series in Ethnography Matters, On Digital Ethnography . This four-part series provides a groundwork for discussing and making ethnography that exists beyond text and print. My project Community Listening _falls nicely into this category. Completely unrelated: I’ve been listening to THIS and THIS . Community Listening in Isle Royale National Park traces how I became part of a dialogue among a team of wolf biologists and a community of park-explorers who share a unique deep listening relationship. The scientists involved in this project are the primary investigators in the five-decade long wolf/moose project, the longest continuous wildlife study in the world. The primary way these researchers determine clues of wolf reproduction is to listen for the sounds of group howling during the summer months when excitement at den sites erupts during pup feeding time. For these clues, the researchers ears, or “antennas” as they put it, tap into a network of visitors and employees who are scattered across the island, listening. This listening network is directly tied to the ecological well-being of the park, which is currently at risk of major change because the wolves, who play a vital role, are on the brink of “blinking-out” due to global climate change. The ethnographic composition weaves together several different types of sound data collected during fieldwork: soundscape recordings of the place, interviews with the wolf/moose researchers, interviews with park visitors and employees, audio diaries, an audio essay derived from field notes, and archival recordings. The work will exist in three formats: a 25-minute radio work, an online web environment that mirrors the listening network as an interactive form, and a concert piece. Issues of access are important to this project because of its interdisciplinary nature. Therefore, the narrative frame of each format is meant to guide the listeners through the specific project content while also creating a space for self-perusal and discovery."},{"id":"2014-01-14-when-expectations-meet-reality","title":"When expectations meet reality ","author":"francesca-tripodi","date":"2014-01-14 05:45:06 -0500","categories":["Grad Student Research"],"url":"when-expectations-meet-reality","layout":"post","content":"As with the beginning of any new year, 2014 brings with it high hopes. When the clock struck 12:01, I turned to my husband and said that one of my professional new years resolutions is to be “more productive.” But as I reflect a bit more on this goal, it got me thinking, how much more productive can I actually be? This year in addition to taking on this Praxis Fellowship and TAing for the Media Studies Department (approximately 80 students each semester) I am trying to complete/defend my dissertation proposal. Since I’m finally done with coursework, I thought this year was going to be a breeze! Of course, I didn’t quite consider the  reality of my situation - in addition to all of these professional responsibilities I was taking on another large undertaking…raising my son who was born on July 23, 2013.  Insert on-going (and extremely convoluted) debate on  if women can really have it all. Being a new mom and a graduate student is quite frankly bizarre. In some respects I live a semi-charmed life. Unlike most of the other working moms that I know, I am able to spend most days with my son (in part this stems from the fact that my modest income does not begin to cover the ridiculous price of childcare). Luckily for me I work for amazing people who let me do a great deal of my work at home (both in Media Studies and Scholars’ Lab). Thanks to Bethany in particular, who doesn’t believe in the Yahoo approach to creativity, and an amazing mother-in-law who comes up once a week to help with childcare, I’m just barely making it work. However, even with all of this extra help I basically have no free time and spend most nights and weekends working.  I feel like I’m in a constant state of catching up. Basically, this is my life: What could this post possibly do with Praxis? Well, for one it helps to explain why I’m so lost when it comes to learning new programming languages! But it mostly reaches out as a thank you to the SLAB team for helping me with my schedule (Wayne for Web-conferencing me in and Jeremy for making himself available on IRC over the weekends). And, a shout out to the other members of my team: five highly skilled and motivated individuals who are juggling their own eccentricities of graduate life. Finally - I hope my post can serve as a reminder to all of us to set realistic expectations for ourselves. Perhaps my new years goal is not to be more “productive” but to be more “realistic” about my capabilities and do the best that I can. I’ll wrap this up here with a bit of humor  ( courtesy of PhD comics )"},{"id":"2014-01-15-neatline-2-2-0","title":"Neatline release-apalooza: Neatline 2.2.0, Neatscape, Astrolabe","author":"david-mcclure","date":"2014-01-15 07:28:07 -0500","categories":["Announcements"],"url":"neatline-2-2-0","layout":"post","content":"Today we’re excited to announce the release of Neatline 2.2.0! This is a big update that ships out a cluster of features and fixes that address a couple of rough spots identified by users over the course of the last couple months. 2.2.0 focuses on improvements in two areas - first, we’ve overhauled the workflows that connect Neatline records with Omeka items to make them more intuitive, flexible, and feature-rich, with the goal of making the overall integration between the two environments feel more seamless and low-friction. Second, we’ve added a system of interactive documentation to the editor that builds reference materials and tutorials directly into the interface, which should make it easier for new users to find their way around. We’re also pushing out maintenance releases of the two extensions, NeatlineSimile and NeatlineWaypoints, which add compatibility for Neatline 2.2 and deal with a couple of minor bugs. As always, grab the code from the Omeka addons repository: Neatline 2.2.0 NeatlineSimile 2.0.1 NeatlineWaypoints 2.0.2 What’s more, we’re also making release candidates available for two new Omeka themes designed to showcase Neatline exhibits: Astrolabe and Neatscape . Loyal readers may recall that a while back, we ran a theme naming contest, and we’re finally making good on our word! These are just release candidates, but we wanted to get them out in the open for testing and feedback before cutting off stable releases. Give them a spin, and be sure to file a ticket on the respective issue trackers ( Astrolabe and Neatscape ) if you find quirks or need new features. Some highlights in Neatline 2.2: Adds a new interface for linking Neatline records to Omeka items that makes it possible to browse the entire collection of items, run full-text searches, and instantaneously preview the Omeka content (metadata, images, etc.) as it will appear in the public Neatline exhibit. Frees up the “Title” and “Body” fields for modification on Neatline records linked to Omeka items . Previously, these fields were automatically populated with the item content imported from Omeka, making it impossible to add custom information not contained in the Omeka metadata. Now, Neatline leaves these fields open for editing and displays them above the content synced in from Omeka, making it possible (though not required) to add exhibit-specific headings and text descriptions for imported items. Makes it possible to import raw data from the Dublin Core “Coverage” field . When Omeka items are imported into Neatline exhibits, existing values in the Dublin Core “Coverage” field (either KML or WKT strings) are now automatically imported into Neatline and displayed on the map. Previously, this only worked if the coverage on the item was created with the Neatline Features plugin. With this functionality in place, it’s much easier to bulk-import existing spatial data sets - use the CSV Import plugin to populate a collection of items, and then push the new items to a Neatline exhibit. Adds interactive documentation to the editor that builds reference materials for each individual control directly into the interface. Now, the heading for each input is followed by a little “ ? ” button that, when clicked, overlays a document with information about what the control does, how to use it, and how it interacts with other functionality. The goal is to make the editor effectively self-documenting, so that it’s unnecessary to find separate documentation and toggle back and forth between different tabs as you work. Last semester was a busy one for Neatline - we were supporting twelve classes here at UVa that are using Neatline for research assignments, and had the good fortune to collaborate with a number of folks at Harvard, Stanford, Northeastern, Duke, Indiana, and elsewhere who were using Neatline or gearing up for upcoming projects in the new year. We’ve also got a couple of exciting ideas brewing here in the lab for new, Neatline-powered projects - keep an eye on this space over the course of the next couple months. As always, don’t hesitate to file bug reports on the issue tracker, post questions to the forums, or contact us directly . Happy new year!"},{"id":"2014-01-15-on-community-listening-2","title":"On Community Listening: 2","author":"erik-deluca","date":"2014-01-15 07:05:30 -0500","categories":["Grad Student Research"],"url":"on-community-listening-2","layout":"post","content":"[Don’t miss the first part of Scholars’ Lab graduate fellow Erik DeLuca’s series on his project, continued here.] Check out this “trailer” I made for Community Listening : [soundcloud url=”http://api.soundcloud.com/tracks/119822657” params=”color=ff6600&amp;auto_play=false&amp;show_artwork=true” width=”100%” height=”166” iframe=”true” /] The excerpts poetic opening of place is meant to link the listeners to the island environment and contextualize the content to come. This opening is composed of field recordings that I made in 2011 while I was the Artist-In-Residence in the park. We hear the high-frequency sounds of tiny invertebrates in tidal pools, underwater sounds of Lake Superior, fog horns, bell buoys, and the beeps of Rolf’s wolf radio collar receiver. These recordings are mixed with selections from a string quartet that I wrote for the park which was composed from spectral information of the fog horn recordings. To understand the diverse perspective of this network I conducted interviews with park visitors and employees. In addition, I gave several small sound recorders to visitors and asked them to keep audio diaries of their experience listening in the park. Using a digital audio workstation and a selection of this sound data, I composed the next section of the excerpt, a sonic representation of the community listening network. Using the technology I was able to poetically portray the polyvocality of the network in sound. Meanwhile, to maintain narrative continuity, a layer of environmental recordings continues, transitioning into a brief introduction of how the Peterson’s wolf/moose study benefits from the network. There is a considerable amount of noise in this recorded interview because I reverted to using my lo-fi, unobtrusive camera-sized compact sound recorder for interview tasks. I packed up my pro sound gear midway through fieldwork because it was acting as physical and cultural barrier between myself and the Peterson’s.  They seemed to be lumping me into the media category of journalists who were flocking to the island at the time to report on the low wolf numbers. When I switched to the small lo-fi recorder I noticed a significant difference in the Peterson’s attitude towards me and decided that I would much rather deal with lo-fi sound quality in exchange for a comfortable and communicative environment. In fact, I have come to like these lo-fi recordings because they remind the listener of the recordist in the field due to audible disturbances and artifacts. My goal for this excerpt is to create a narrative frame to guide listeners through which allows for optimal room for self-perusal and discovery."},{"id":"2014-01-16-building-from-scratch-then-scratching-that-build","title":"Building From Scratch Then Scratching That Build","author":"scott-bailey","date":"2014-01-16 06:00:25 -0500","categories":["Grad Student Research"],"url":"building-from-scratch-then-scratching-that-build","layout":"post","content":"Over the past couple of weeks I’ve been working hard on building my own website. I actually began this work last summer, in the midst of a code camp for humanities graduate students being run at U.Va. with the great help of Scholars’ Lab staff. There, I had gotten an intro to git, found out about Github Pages and Jekyll, and had learned enough PHP to build a couple different WordPress themes throughout July and August. But, I was never happy with what I had produced and when school started back, my priorities shifted. When the holiday break began, I knew I needed to go ahead and get something up and running. I decided to host the site with Github Pages using Jekyll for a couple of different reasons. One, it’s free and I’m a humanities graduate student. Two, I wanted my site to be focused around blog content, and Jekyll will auto-build a blog from your markdown files. I have written nearly everything over the past six months in markdown, so this fits well with my current habits and preferences. And three, hosting through Github Pages has made using git and the terminal a comfortable habit, all to the betterment of my future. This still left the question of what to build and how to build it. Throughout the year, I had spent time looking at different frameworks and boilerplates. I checked out the HTML5 Boilerplate, browsed through Bootstrap, and looked at a number of different grid systems, such as Skeleton . A number of these different frameworks and systems fit the design aesthetic I like - minimal clutter, clear navigation, focus on text and typography and the like. As I said in my previous blog post, I decided to build it from scratch, thinking that the design I had in mind and had sketched out was pretty simple to code. And it was, mostly. I built a blog site with a left sidebar navigation menu taking roughly 2/5 of the page with the rest displaying the blog content. I used Adobe Kuler to help generate a color scheme I liked. I spent quite a bit of time looking at typography. Wanting to keep up with the times, I made the whole thing responsive and wrote media queries to shift the navigation menu to the top at an appropriate breakpoint, and though I still needed to iron out some positioning and spacing kinks, it all worked. But there was something missing, some bit of polish and clarity that you find in great websites today. That’s when I realized that minimalism isn’t necessarily easier to code. The basics of the layout and style were straightforward, but when there is less on the page, every detail matters, and to code those details, those little bits of spaces, calculating the proper ems and such, that would take time. Unfortunately, time is not necessarily what I have in abundance. With the spring semester starting, my dissertation looms especially large, as does planning for the course I’m TAing and preparing for our work in Praxis on Ivanhoe. In the interest of pragmatism, I decided near the end of break to scratch my built-from-scratch build. I want my site to show a bit of my coding ability, but I also want it to meet a certain standard. My own from scratch build isn’t quite there, though given some time it will be. I decided, though, to rebuild using the PureCSS Framework . It’s a set of CSS modules that can be used all together or pulled apart. To get the layout and appearance I wanted, and still be responsive, I decided to use one of their layouts similar to my own. I worked off of the framework and adapted it to the templates that Jekyll needs, done with the Liquid templating language, to get content to display where and how I wanted it. I then started changing things a bit with my own CSS, using the color scheme I had decided on to style the links, to change a bit in the menu, to change spacing, and then put in my own font choices. I hooked in Font Awesome to get icons for links to my Github and Twitter pages. The whole time, I used git to track the changes I was making (as my commit history on Github shows) and ran Jekyll locally to know what was going on before pushing to Github. There are still things here and there I want to tweak, such as spacing, but the website is up and running at csbailey.org . And I learned this lesson, which I think has been dwelling in the back of my head for some months. The website that is up and running is better than the built from scratch website that sits on your computer, never to be seen by anyone else."},{"id":"2014-01-20-look-its-a-game-its-a-simulation-no-its-gamification","title":"Look, it's a game, it's a simulation, no...it's GAMIFICATION! ","author":"francesca-tripodi","date":"2014-01-20 04:00:01 -0500","categories":["Grad Student Research"],"url":"look-its-a-game-its-a-simulation-no-its-gamification","layout":"post","content":"Back in October I wrote a blog post  pushing back on the idea that we were actually creating a game.* Using Gredler’s (2004) work on games vs simulations I was arguing that while we were calling Ivanhoe a game we weren’t incorporating essential elements of game-play (i.e. rules, goals, the ability to win, etc). As we begin wire-framing, these same questions are coming into play again: should we have a role journal? what about points? Such a conversation got me thinking again about what we’re actually making. To help me with this struggle, I ordered a copy of The Gamification of Learning and Instruction Fieldbook   written by Karl Kapp, Lucas Blair, and Rich Mesh (2014). By reading their book I came to realize that we’re not producing a game _or _a simulation…we are actually incorporating gamification. Gamification means that we are using elements of games to create a more engaging pedagogical environment. As Kapp et al write, “Gamificiation is using game-based mechanics, aesthetics, and game-thinking to engage people, motivate action, promote learning, and solve problems” (2014: 54). Moreover, I think Ivanhoe is using more  content gamification _vs _structural gamification . While these are not mutually exclusive ideas, it seems that we want to create a product that “makes the content more game-like but doesn’t turn the content into a game” (pg 55). Such a revelation about what we were making gave me a renewed sense of energy about our product.  By working with the elements of gamification we can “encourage learners to progress through content, motivate action, influence behavior, and drive innovation” (pg 56). In doing so, we can make something that enhances learning and hopefully adds a bit more enjoyment to our classroom. *Side note, my original post was selected by Digital Humanities Now for “Editors Choice” - if you’ve got some time, you should peruse their other selections. It’s a great way to see what else is going on in the DH world."},{"id":"2014-01-23-spring-2014-scholars-lab-gis-workshop-series","title":"Spring 2014 Scholars’ Lab GIS Workshop Series","author":"chris-gist","date":"2014-01-23 10:48:14 -0500","categories":["Announcements","Geospatial and Temporal"],"url":"spring-2014-scholars-lab-gis-workshop-series","layout":"post","content":"All sessions are one hour and assume attendees have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials with expert assistance.  All sessions will be taught on Wednesdays from 10AM to 11AM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community. February 5th\n**Making Your First Map with ArcGIS\n**Here’s your chance to get started with geographic information systems software in a friendly, jargon-free environment.  This workshop introduces the skills you need to make your own maps.  Along the way you’ll get a taste of Earth’s most popular GIS software (ArcGIS) and a gentle introduction to cartography. You’ll leave with your own cartographic masterpieces and tips for learning more in your pursuit of mappiness at UVa.  February 12th\n**Getting Your Data on a Map\n**Do you have a list of Lat/Lon coordinates or addresses you would like to see on a map?  We will show you how to do just that.  Through ArcGIS’s Add XY data tool and Geocoding (address matching), it is easy to take your tabular lists and generate points on a map. February 19th\n**Georeferencing a Map\n**Would you like to see historic map overlaid on modern aerial photography?  Do you need to extract features of a map for use in GIS?  Georeferencing is the first step.  We will show you how to take a scan of a paper map and align in it in ArcGIS. February 26th\n**Easy Demographics\n**Need to make a quick demographic map or religious adherence?  This workshop will show you how easily navigate Social Explorer.  This powerful online application makes it easy to create maps with contemporary and historic census data and religious information. March 5th\n**Historic Census Data\n**Would you like to map the poverty in Philadelphia around the turn of the 20th Century?  How about a racial breakdown by state in the 1860s?  This workshop will focus on how to download historic census boundary and tabular data to make historic demographic maps. March 19th\n**Collecting Your Own Spatial Data\n**Research projects often rely on fieldwork to build new datasets.  In this workshop we’ll focus on tools for spatial data collection. First we’ll take a quick look behind the curtain to see how GPS really works and how to use that knowledge to our advantage.  Then we’ll evaluate free or low-cost options to gather locations and associated attributes using handheld GPS devices, smartphones, and apps.  This workshop will introduce you to a range of devices and methods for mobile spatial data collection. March 26th\n**Making Your First Map with QGIS\n**Here’s your chance to get started with geographic information systems software in a friendly, jargon-free environment.  This one-hour workshop introduces the skills you need to make your own maps.  Along the way you’ll get a taste of Earth’s most popular open source GIS software (QGIS) and a gentle introduction to cartography.  You’ll leave with your own cartographic masterpieces and tips for learning more in your pursuit of mappiness at UVa. April 2nd\n**Quantum GIS – Adding Remote Data Services\n**Would you like to show the live weather radar on your map?  How about other live and/or free data?  This workshop will show you how to add open web service (OWS) layers to Quantum GIS and use them in a map. April 9th\n**Working With Rasters Using Open Source Tools\n**QGIS is a popular open source software for working with spatial data.  The Geospatial Data Abstraction Library (GDAL) is an open source utility library for raster processing that’s integrated with QGIS.  In this workshop we will use QGIS and GDAL with freely available datasets to create elevation contour lines and identify the highest and lowest elevation points inside a study area.  Gain the skills you need to perform your own terrain analysis using free open source tools.      April 16th\n**Learning Old-School Mapping Techniques\n**How did folks make maps before GPS and satellite imagery?  In this workshop we’ll focus on plane table mapping.  Using just a flat surface, a sheet of paper, a straight edge, and a pencil we’ll learn techniques to create accurate maps for large geographic areas.   With plane table mapping, if you can see it, you can map it.  "},{"id":"2014-01-29-dh-speaker-series-presents-adeline-koh-roopika-risam","title":"DH Speaker Series: Adeline Koh & Roopika Risam on DHpoco","author":"laura-miller","date":"2014-01-29 05:08:13 -0500","categories":["Announcements"],"url":"dh-speaker-series-presents-adeline-koh-roopika-risam","layout":"post","content":"Theories and Practices of Postcolonial Digital Humanities Friday, January 31 at 10:00 am in Alderman Library, Room 421 Both postcolonial studies and the digital humanities have gained currency within the academy but are subject to strident critiques from interlocutors. Postcolonial studies has been criticized for being overly reliant on jargon and apolitical, whereas the digital humanities have been taken to task for failing to interrogate questions of race, power, and identity fully. The Postcolonial Digital Humanities (#dhpoco) intervenes in these gaps through theory and praxis. #dhpoco engages postcolonial studies to address global issues relating to race, gender, class, sexuality, and disability within cultures of technology while bringing the activist praxis of the digital humanities to the work of postcolonial studies. Co-founders Adeline Koh and Roopika Risam will discuss the theoretical underpinnings of #dhpoco and outline the tactics that #dhpoco employs. Adeline Koh is the director of DH @ Stockton and an assistant professor of literature at Richard Stockton College. Koh is the co-founder of Postcolonial Digital Humanities with Roopika Risam and directs Digitizing Chinese Englishmen, a digital archival project, and The Stockton Postcolonial Studies Project, an online magazine of postcolonial studies. She is the designer of Trading Races, a historical role-playing game for undergraduates,  and is a core contributor to the Profhacker column at the Chronicle of Higher Education. Roopika Risam is an Assistant Professor of English at Salem State University. Her research interests include postcolonial studies and minority discourse in the United States, and the role of technology in mediating between the two. She is the co-founder of Postcolonial Digital Humanities with Adeline Koh and is co-director of the open-access public domain critical edition of Claude McKay’s Harlem Shadows with Chris Forster of Syracuse University. This event is co-sponsored by The Carter G. Woodson Institute and the Scholars’ Lab.  As always, it is free, open to all, and requires no advance registration. Image courtesy of http://dhpoco.tumblr.com/tagged/comics"},{"id":"2014-01-30-potential-ivanhoe-users","title":"Help us design the Ivanhoe Game!","author":"stephanie-kingsley","date":"2014-01-30 12:00:47 -0500","categories":["Grad Student Research"],"url":"potential-ivanhoe-users","layout":"post","content":"Attention, game players and digital-pedagogy enthusiasts! This year’s Praxis Program is rebooting SpecLab’s Ivanhoe Game as a WordPress theme that users can install on their own websites and style and configure according to their individual needs. We would greatly appreciate feedback on the ways potential users already employ WordPress in their course websites or personal homepages.  If you are interested in installing Ivanhoe and hosting games for students or friends, please help us by answering a few questions. Thank you!"},{"id":"2014-01-30-spring-2014-scholars-lab-tech-workshop-series","title":"Spring 2014 Scholars' Lab Tech Workshop Series","author":"ronda-grizzle","date":"2014-01-30 11:32:21 -0500","categories":["Announcements"],"url":"spring-2014-scholars-lab-tech-workshop-series","layout":"post","content":"We’re pleased to announce our Spring 2014 Tech Workshop series roster. All workshop sessions are free, open to the public, and require neither previous experience nor registration. February 12 Introduction to Neatline Join us for a hands-on introduction to Neatline, a set of plugins for Omeka developed by the Scholars’ Lab. With Neatline, anyone can create beautiful, complex maps and narrative sequences from collections of archives and artifacts, and connect maps and narratives with timelines that are more-than-usually sensitive to ambiguity and nuance. See neatline.org for more information. Time: 3:00-4:00pm Location: Alderman Library Electronic Classroom (ALD 421) Instructor: Ronda Grizzle February 26 Neatline Timelines You say you’ve got the basics of the tools available in Neatline, but want to know more about how to use timelines to illustrate your data? This is the class for you. Please join us for this 1-hour, hands-on workshop. Prior experience with Neatline or previous attendance at the Introduction to Neatline workshop is helpful, but not required. Time: 3:00-4:00pm Location: Alderman Library Electronic Classroom (ALD 421) Instructor: Ronda Grizzle March 5 Introduction to Screen Scraping Have you ever found data on the web that you need to use, but when you look for the “download” button, it’s not there? If so, we can help you. In this workshop, we’ll start with a quick introduction to Python. We’ll also talk about several libraries that are commonly used to download pages and pull information out of them. And by the end of the session, we’ll have downloaded a web page and extracted data from it into a format that we can easily load into Excel, a database, or a statistical package. Time: 1:00-2:00pm Location: Alderman Library Electronic Classroom (ALD 421) Instructor: Eric Rochester March 19 Version Control with git If you have trouble keeping track of changes you’ve made in projects, then git can help you. It’s a version control program that remembers all the changes you’ve make to a project, and it also helps you collaborate with others on that project. Git’s great for managing websites, computer program source code, and even papers. In this workshop, we’ll learn about how git views the world so that we can use it more effectively, and we’ll get hands-on practice using git to track changes in a small project. Time: 3:00-4:00pm Location: Alderman Library Electronic Classroom (ALD 421) Instructor: Eric Rochester March 26 Web Site Design and Development This workshop will help demystify the process of creating a web site by introducing some basic concepts and methods for web site design and development. By the end of the workshop, students will understand rudimentary HTML for web page markup and Cascading Style Sheets (CSS) for web site presentation and design. Students should walk away from the class with a simple but tasteful “About Me” page they can publish to the web, and use as a foundation for building a larger web site. Comfort with a computer and web browser will make the workshop easier; prior experience with web design or development could be useful, but is not required. Time: 3:00-4:00pm Location: Alderman Library Electronic Classroom (ALD 421) Instructor: Jeremy Boggs April 2 Data Management for Graduate Students I Time: 3:00-4:00pm Location: Alderman Library Electronic Classroom (ALD 421) Instructors: Purdom Lindblad and Sherry Lake April 9 Data Management for Graduate Students II Time: 3:00-4:00pm Location: Alderman Library Electronic Classroom (ALD 421) Instructors: Purdom Lindblad and Sherry Lake April 16 Introduction to Omeka Time: 3:00-4:00pm Location: Alderman Library Electronic Classroom (ALD 421) Instructor: Ronda Grizzle"},{"id":"2014-01-30-test","title":"Test post","author":"ammon-shepherd","date":"2014-01-30 10:32:21 -0500","categories":["Announcements"],"url":"test-post","layout":"post","content":"Test post to see if auto building of site is working installed jekyll and now hopefully it works. and installed bundles and such and it should work now… and one more time… now testing with gunicorn.."},{"id":"2014-02-03-wireframing-fun","title":"Wireframing fun","author":"francesca-tripodi","date":"2014-02-03 08:50:50 -0500","categories":["Grad Student Research"],"url":"wireframing-fun","layout":"post","content":"Over the last week Stephanie, Eliza, and I (with significant help from Jeremy) have been putting together a wireframe of our product. Partaking in this process has been so exciting and I’ve learned a few key concepts: I’ve become familiar with using GitHub . By building a repository we’ve been able to collaborate on the same files and build tickets in the system to help us organize what else we need to create as well as assign people on the team to each task. I feel more confident using HTML, CSS, and Terminal. While I’d still qualify myself as a novice, I am starting to feel a little less lost/scared. Believe it or not I’ve actually had a great deal of fun working on this mini-team. Using HTML provides an instant-gratification that most academic work lacks. While I might never see the returns on my dissertation, I immediately see the fruits of my labor with HTML and could not help but smile when I would refresh my browser and see my changes appear. Jeremy’s suggestion to actually build out the wireframes was a great one. While our end product isn’t pretty, it definitely is a step in the right direction and I’m looking forward to seeing how our programming team ends up building out this blueprint. To see our wire-frames to get a preview of what Ivanhoe will do, visit http://scholarslab.github.io/ivanhoe-mockups/."},{"id":"2014-02-04-neatline-gettysburg-address","title":"The \"Nicolay copy\" of the Gettysburg Address","author":"david-mcclure","date":"2014-02-04 05:13:47 -0500","categories":["Experimental Humanities"],"url":"neatline-gettysburg-address","layout":"post","content":"[Cross-posted from dclure.org ] Launch the Exhibit This is a project that I’ve been hacking away at for some time, but only found the time (and motivation) to get it polished up and out the door over the weekend - a digital edition of the “Nicolay copy” of the Gettysburg Address, with each of the ~250 words in the manuscript individually traced out in Neatline and wired up with a plain-text transcription of the text on the right side of the screen. I’ve tinkered around with similar interfaces in the past, but this time I wanted to play around with different approaches to formalizing the connection between the digitally-typeset words in the text and the handwritten words in the manuscript. Your eyes tend to dart back and forth between the image and the text, and it’s easy to lose your place - how to reduce that cognitive friction? To chip away at this, I wrote a little sub-plugin for Neatline called WordLines, which automatically overlays a little visual guideline (under the hood, a d3 -wrapped SVG element) on top of the page that connects each pair of words in the two viewports when the cursor hovers on either of the instantiations. So, when the mouse passes over words in the transcription, lines are automatically drawn to the corresponding locations on the image; and vice versa. From a technical standpoint, this turns out to be quite easy - just get the pixel offsets for the element in the transcription and the vector annotation on the map (for the latter, OpenLayers does the heavy lifting with helpers like getViewPortPxFromLonLat, which maps spatial coordinates to document-space pixel pairs), and then draw a line connecting the two points. The one hitch, though, is that this involves placing a large SVG element directly on top of the page content, which, by default, will cover all of the underlying elements (shapes on the map, words in the text) and block them from receiving the cursor events that drive the rest of the UI - including, very problematically, the mouseleave event that garbage-collects old lines and prevents them from getting stuck on the screen. The work-around is to put pointer-events: none; on the SVG element, which causes the browser to treat it as a purely visual veneer over the page - cursor events drop through to the underlying content elements, and everything else behaves normally. This is just barely and only very recently cross-browser, but I’m not sure if there’s actually any other way to accomplish this, given the full set of constraints. Modeling intuitions about scale Originally, I had planned to just leave it at that, but, as is almost always the case with these projects, I ended up learning lots of interesting things along the way, and I ended up going back and adding in another set of annotations that make note of some of the more historically noteworthy aspects of the manuscript. Namely, I was interested in the different types of paper used for the two different pages (Lincoln probably wrote the first page in Washington before departing, the second page after arriving in Gettysburg) and the matching fold creases on the pages, which some historians have pointed to as evidence that the Nicolay copy was perhaps the actual “reading copy” that Lincoln used when delivering the speech, since eyewitness accounts describe Lincoln pulling a folded piece of paper out of his coat pocket. The other candidate is the Hay draft, which includes lots of changes and corrections in Lincoln’s hand, giving it the appearance of working draft that was prepared just before the event. One problem with the Hays draft, though, is its size - it’s written on larger paper and has just a single fold down the center, which would seem to make it an unlikely thing to tuck into coat pocket. When I read about this, I realized that I had paid almost no attention to the physical size of the manuscript. On the screen, it’s either extremely small or almost infinitely large - a tiny speck when you zoom far back, and an endless plane of beige-and-black when you zoom in. But, in this case, size turns out to be of great historical significance - the Nicolay copy is smaller than the Hays copy, especially when folded along the set of matching creases clearly visible on the pages. So, how big is it? This involved a bit of guesswork. The resource page for the manuscript on the Library of Congress website doesn’t include dimensions, and direct Google searches didn’t turn up an easy answer, so I started poking around the internet to see if I could find other Lincoln manuscripts written on the “Executive Stationery” used for the first page. I rooted up a couple of documents for sale by rare book sellers, and in both cases the dimensions are listed at about 5 inches in width and 7-8 inches in height, meaning that the Nicolay copy - assuming the stationery was more or less standardized - would have folded down to a roughly 5 x 2.5-inch rectangle, which seems reasonably pocket-sized. (Again, this is amateur historical conjecture - if I’m wrong, please let me know!) I sketched out little ruler annotations labeling the width of the page and the height of the fold segment, but, zooming around the exhibit, I realized that I still didn’t any intuitive sense of the size of the thing. Raw numerical measurements, even when you’re beat across the head with them, become surprisingly abstract in the a-physical, point-of-reference-less netherlands of deeply-zooming digital landscapes. I dug out a ruler and zoomed the exhibit back until the first page occupied five real-world inches, and then held my hand up to the screen, imagining the sheet of paper in my hand. And then I thought - why not just bake some kind of visual reference directly into the exhibit? I hunted down a CC-licensed SVG illustration of a handprint, and, using the size of my own hand as a reference, used Neatline’s import-SVG feature to position the outline in the whitespace to the right of the first page of the manuscript:"},{"id":"2014-02-04-scholars-lab-grads-partner-with-washington-lee-university","title":"Scholars' Lab Grads partner with Washington & Lee University","author":"purdom-lindblad","date":"2014-02-04 10:46:33 -0500","categories":["Announcements","Digital Humanities"],"url":"scholars-lab-grads-partner-with-washington-lee-university","layout":"post","content":"[![quinn_anya_photo](http://static.scholarslab.org/wp-content/uploads/2014/02/quinn_anya_photo-e1391521680507-300x179.jpeg)]( http://www.flickr.com/photos/quinnanya/7744258278/sizes/z/ ) Photo by Anya Quinn The Scholars’ Lab is pleased to partner with Washington and Lee University on a grant from the Associated Colleges of the South. The ASC grant furthers the W&amp;L Digital Humanities Working Group’s efforts to create an Introduction to Digital Humanities course and explore a future Digital Humanities (DH) certificate at Washington and Lee. The Working Group strives to integrate digital tools and methods into humanities pedagogy. W&amp;L; Working Group members will consult with Scholars’ Lab and other UVa Library staff throughout their planning process. The innovative  Praxis Program  at the Scholars’ Lab (also the heart of our international Praxis Network ) is the model to explore what might form the core curriculum for a W&amp;L DH certificate. The Praxis Program fosters a highly collaborative approach to scoping and building of  Digital Humanities tools, engaging in digitally-inflected public humanities work, and advancing humanities scholarship. Partnering with Washington and Lee faculty, UVA graduate students will contribute their experiences and research areas to the introductory course as they gain valuable experience working in a small liberal arts environment. Keep an eye out for future updates on this exciting partnership!"},{"id":"2014-02-05-podcast-adeline-koh-roopika-risam-2","title":"Podcast: Adeline Koh & Roopika Risam","author":"ronda-grizzle","date":"2014-02-05 05:26:07 -0500","categories":["Podcasts"],"url":"podcast-adeline-koh-roopika-risam-2","layout":"post","content":"Digital Humanities Speakers: Adeline Koh and Roopika Risam Theories and Practices of Postcolonial Digital Humanities Both postcolonial studies and the digital humanities have gained currency within the academy but are subject to strident critiques from interlocutors. Postcolonial studies has been criticized for being overly reliant on jargon and apolitical, whereas the digital humanities have been taken to task for failing to interrogate questions of race, power, and identity fully. The Postcolonial Digital Humanities (#dhpoco) intervenes in these gaps through theory and praxis. #dhpoco engages postcolonial studies to address global issues relating to race, gender, class, sexuality, and disability within cultures of technology while bringing the activist praxis of the digital humanities to the work of postcolonial studies. Co-founders Adeline Koh and Roopika Risam discussed the theoretical underpinnings of #dhpoco and outlined the tactics that #dhpoco employs. This event was co-sponsored by The Carter G. Woodson Institute and the Scholars’ Lab. Dr. Koh and Dr. Risam’s presentation slides are available from Slideshare: Theories and Practices of Postcolonial Digital Humanities As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://a270.phobos.apple.com/us/r30/CobaltPublic/v4/9f/e5/5c/9fe55cfd-0579-873a-03d4-87531a950e8c/332-4848115438543626250-risam_koh.mp3”]"},{"id":"2014-02-05-praxis-weekly-digest-1","title":"Praxis Weekly Digest #1","author":"stephanie-kingsley","date":"2014-02-05 04:00:52 -0500","categories":["Grad Student Research"],"url":"praxis-weekly-digest-1","layout":"post","content":"First, if you missed our post last week about redesigning Ivanhoe, we are now in the process of building the game as a WordPress Theme .  If you are a potential Ivanhoe user, help us out by giving some feedback about the way you use WordPress . Congratulations to Zach for passing his orals!  Now he has read even more books and will be that much better qualified to wage a fierce Ivanhoe war.  Way to go, comrade! Francesca and I dusted off our artistic abilities the other day to brainstorm a logo.  Watch out for a post from Francesca on this first step to designing Ivanhoe. And last but certainly not least, many thanks to our Development Team for putting in hours and hours of learning and wielding PHP, led by the tireless and endlessly patient Jeremy.  They now have a page built which allows a user to enter a move and which then displays the moves in a series of posts.  Great job, guys!  (Stay tuned for a post on this from Veronica .) As we plunge headfirst into building and designing the Ivanhoe WordPress Theme, I reflect on the vast change which has occurred in our group dynamics between this semester and last.  In the fall, we spent most of our time deciding what Ivanhoe was about.  Several posts mused on teamwork (see “Forming, Norming, Storming &amp; Performing,” “ More Musings on Tuckman,” “Stephen Covey intervenes in wire-framing Ivanhoe”, and “Sticky Situations: Lessons in Group Cohesion” ), and, indeed, we had difficulties coming together on some points.  In retrospect, several elements contributed to this.  As I mentioned then in “Stephen Covey,” we all have strong personalities and fervent opinions.  Apply that to the attempt to rethink an already abstract idea such as Ivanhoe (which I would then have described as a game where you make moves on a text or concept to intervene in that text and generate criticism), and rabbit holes get deeper and deeper.  Furthermore, as a group we didn’t know how we all fit into the mix.  Once we took our places in Design (Francesca and Zach), Development (Eliza, Scott, and Veronica), and as project manager (me), we each had an individual purpose and set of responsibilities.  Now that the wire-frames are finished ( see Francesca’s post ), we all feel energized and are moving forward. As project manager, I realize how much I enjoy talking with the team and seeing what challenges everyone is taking on.  I enjoy getting updates on people’s progress and being a sounding board for ideas.  I also thrive on creating to-do lists and getting word out about what is going on in Praxis.  That being said, I see it as part of my job to share our progress with you, as well. This will be the first of a series of weekly check-in posts I will be writing as we proceed in building and designing Ivanhoe this spring.  Each post will provide a space to acknowledge what our group has accomplished that week.  Our charter emphasized equal credit.  Something which I believe is critical to all members receiving that equal credit, however, is to acknowledge individually what the teams and members have done from week to week.  I plan for these posts to provide that acknowledgement.  Only in that way will those interested in our activities see how each member’s contribution fits into the whole.  My weekly updates will also uphold our charter’s emphasis on outreach by letting you all know what is going on in our group.  Now that we are actively building, this is more vital than ever. So welcome to my Weekly Praxis Digest.  Keep an eye out for the upcoming posts I mentioned from Francesca and Veronica, don’t forget to fill out our WordPress questionnaire if you’re interested in Ivanhoe, and wish us luck as we move into a new week of building!"},{"id":"2014-02-10-websites-media-buttons-and-logos-oh-my","title":"Websites, Media Buttons, and Logos - oh my! ","author":"francesca-tripodi","date":"2014-02-10 05:00:01 -0500","categories":["Grad Student Research"],"url":"websites-media-buttons-and-logos-oh-my","layout":"post","content":"Last week I took a different approach to design. Instead of wireframing in html, Stephanie and I decided to break out the colored pencils and tap into our creative side. First, we worked on creating an Ivanhoe logo. Inspired by the original website   - Stephanie and I modified the header by focusing in on the “I” and “V” only. We also incorporated a tool (or I suppose the way he’s holding it the pen looks more like a weapon of sorts… after all, “the pen is mightier than the sword”!). We also decided to use the same color scheme as PRISM  and we hope that by doing so it creates a sense of unity among Scholars Lab products. Next, we drew up a modified “Add Media” button. Why you may ask? Well even though WordPress already has an “Add Media” button - I believe the drabness of the feature ends up fostering a text-first space in WordPress rather than working with, manipulating, and linking other forms of media besides text (for more on my logic of why a modified “Add Media” button is necessary - click here ). We hope that by incorporating this feature it will draw students into our game as we intend - fostering the possibility of a pedagogical space that includes all forms of media. Even though in the end the button might just be a modified version of what already exists (think make it red vs grey) it was still fun envisioning how our game might look in the end. Stephanie and I also thought that it would be good to use WordPress for our informational site since we are building a WordPress plugin…. however we were “convinced” that the better way to go would be building the site ourselves. While I now see why it is important to go this route, building a site from the ground up brought with it a wave of anxiety. I still feel so uneasy about my CSS/HTML skills and I was concerned that not relying on WordPress would ultimately mean roadblocks for the team. In addition to the information site, I also need to be working on designing the actual game and was really worried that my skill level would impede progress. Luckily for me we have an awesome support team and Jeremy and Wayne sat down with me (and my son Lev!) to help Zach and I begin building our site. While we still have a great deal of work to go on it, I was once again surprised at my capabilities (and also satisfied/proud to see the fruits of our labor). I think in the end it’s just about making the time to come into SLAB for help and being a bit more confident in myself. [![Wayne teaching me CSS--and Lev too!](http://static.scholarslab.org/wp-content/uploads/2014/02/Lev-300x225.jpg)](http://static.scholarslab.org/wp-content/uploads/2014/02/Lev.jpg) Wayne teaching me CSS--and Lev too! Finally - we are in the process of crowdsourcing ! We’d love to hear from you on how we can make Ivanhoe great.  We’re still interested in how you use WordPress, so fill out our questionnaire if you’d like to help.  Thanks!"},{"id":"2014-02-13-foreign-languages-and-ivanhoe-progress","title":"Foreign Languages and Ivanhoe Progress","author":"veronica-ikeshoji-orlati","date":"2014-02-13 05:00:45 -0500","categories":["Grad Student Research"],"url":"foreign-languages-and-ivanhoe-progress","layout":"post","content":"Eight years ago, I sat staring at my Latin prose composition homework. The assignment was to translate a few sentences and a couple of short, not-particularly-complex paragraphs from English into Latin. In that precise moment, however, it would have been equally effective to ask me to go find and slay a fire-breathing dragon, since the task of translating not just words, but entire thoughts into a language I had spent most of my life just reading seemed absolutely insurmountable. How did Cicero make his speeches so persuasive… and eloquent ? How did Martial write such astringent, sharp-witted satires with so few words… in meter ? Clearly a lot of talent was involved. Or, perhaps, magic. Over the past couple of weeks, we have started to build the core features of our Ivanhoe WordPress Theme . We have created the ‘Games’ and ‘Moves’ custom post types and have a (mostly) functional template page for making new moves with the WordPress WYSIWYG editor hooked in. We’re currently working on how to relate moves to one another and display those relationships for each game, and it seems as though we’re making progress. (Or so Jeremy, Eric, and Wayne insist.) Here is some photographic evidence of a couple of us looking very serious and typing things: And of what we’ve gotten done thus far: Nevertheless, working on Ivanhoe sometimes feels a bit like a Latin prose comp throwback. Some five-line blocks of code have taken hours to get right, and the WordPress Codex seems to be written in an impenetrably-dense, grammar-book style. But in my current attempt to write and think in another language, I feel somewhat less bewildered. I am certainly no more prepared to write PHP functions and read WordPress’s Codex than I was to tackle the English-Latin exercises of Bradley’s Arnold with Gildersleeve’s grammar in hand. But I am not being left to my own devices to figure things out this time, and it has made a huge difference."},{"id":"2014-02-14-praxis-weekly-digest-2","title":"Praxis Weekly Digest #2","author":"stephanie-kingsley","date":"2014-02-14 06:00:02 -0500","categories":["Grad Student Research"],"url":"praxis-weekly-digest-2","layout":"post","content":"This week, Praxis has made some very exciting progress.  Eliza, Scott, and Veronica continue to work on our WordPress Theme .  As Veronica mentioned in her post, “Foreign Languages and Ivanhoe Progress,” the challenge the Development team faces this week is figuring out how to create links between moves which respond to other moves.  For instance, if in the Suffragette Game, a telegram from Lord Asquith is found in Mary Leigh’s trunk, and “Mary Leigh” made this move in response to the newspaper article posted by “Anonymous Suffragette,” then we want those two moves to be linked together in the WordPress database.  Then, when a user views one of those moves on the front end, each move needs to display links to its associated move.  This system of networking moves is a central feature of our Ivanhoe, so Development’s successful execution of this complex task will be a major breakthrough in the programming of the Ivanhoe Theme overall.  Take a look at an extract of the Suffragette Game on our mockups, hosted by GitHub, to see examples of this network. Our Design team, Francesca and Zach, will be working on the logo and overall aesthetic for our Ivanhoe informational website, which will eventually extend to the Theme itself.  We chose to construct Ivanhoe as a WordPress Theme so that we would be able to control the look of the game, and now our designers are working to decide what that look should be.  What should the colors, fonts, and graphics of the website communicate about our interpretation of the Ivanhoe Game ?  Jeremy met with Francesca, Zach, and me this week to instruct us in tackling this step.  He stressed the fact that each design choice should have special significance to Ivanhoe—meaning, we can’t just choose a font or color because we like it.  With Jeremy’s help, Design began making progress in brainstorming a logo.  They want to incorporate the Prism colors and aesthetic to stress the continuity between the projects, while adapting Prism’s design in such a way as to emphasize Ivanhoe’s focus on interactive play.  The thought is yet in its nascent stages, but it compelled me to visit some of the design blogs from previous Praxis cohorts, and a quote from designer Chris Peck struck me as useful as we begin to consider Ivanhoe with respect to Prism: There’s something very compelling about refraction as a metaphor for collective interpretation of text. The crowd is a prism that reveals facets of the text, and the text is a prism that reveals facets of the crowd. The rainbow splayed across the text is an apt image for Prism’s aspirations for bringing text to life through accumulated interpretations. ( “Prism on Spring Break” ) Chris wrote this while playing with the idea of displaying multiple colors of highlighting across text ; he eventually changed this to showing only one at a time.  Still, his comments show his design ideas to be metaphors for how he viewed Prism theoretically, and this thought process is one which I think we should try and emulate in moving forward on Ivanhoe design. On the Project Management front, I continue to meet with the separate teams, see how they’re doing, take pictures, and publicize.  In this vein, I would like to introduce the #Ivanhoe hashtag, because Ivanhoe is a phenomenon unto itself… can become a whole new movement in critical thought… and is being revitalized for pedagogical and scholarly use as we speak!  What better reasons to create a hashtag?  Look for it in tweets from @PraxisProgram and @scholarslab. I would also like to announce a talk which Praxis will be giving on Prism and Ivanhoe.  Members of both this and last year’s cohorts will be speaking on the use of their projects for textual interpretation at the UVa Graduate English Conference, “Reading Then and Now,” to be hosted in Charlottesville on the UVa campus the weekend of April 4-6; our panel will be that Friday.  I will announce this again with further details when the conference schedule is established, but if you’re planning to be in Charlottesville that weekend, mark your calendars.  We’d love to hear your thoughts on Prism and Ivanhoe."},{"id":"2014-02-17-apply-for-praxis-2014-2015","title":"Apply for Praxis 2014-2015!","author":"purdom-lindblad","date":"2014-02-17 07:46:50 -0500","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"apply-for-praxis-2014-2015","layout":"post","content":"UVa grad students!  Apply by March 10th  for a unique, funded opportunity to work with an interdisciplinary group of your peers, learning digital humanities skills from the experts, and collaboratively designing and executing an innovative digital project. Each year, the Scholars’ Lab  Praxis Program  provides six UVa graduate students with hands-on experience in digital humanities collaboration and project creation. Team members work to take a shared project from conception to design and development, and finally to publication, community engagement, and analysis. Our fellows  blog about their experiences  and develop increased facility with project management, collaboration, and the public humanities, even as they tackle (most for the first time, and with the mentorship of our wonderful  faculty and staff ) new programming languages, tools, and digital methods. The  2013-14 Praxis cohor t is in full swing, thanks to a generous support by the  University of Virginia Library .  Recently, the Scholars’ Lab joined with like-minded institutions to create a  Praxis Network, made up of allied but differently-inflected humanities education initiatives from around the world, mainly focused on graduate training, and all engaged in rethinking pedagogy and campus partnerships in relation to the digital humanities. We will welcome six new, competitively-selected Praxis students in late August 2014 . Each will be awarded $8000 in fellowship funds, and will be expected to devote approximately 10 hours per week in the fall and spring semesters, to learning together and building a collaborative digital humanities project in the Scholars’ Lab. Fellows join our vibrant community, have a voice in intellectual programming for the Scholars’ Lab, and can make use of our dedicated grad lounge. All  University of Virginia graduate students  working within or committed to humanities disciplines are eligible to apply to join the 2014-15 cohort. We particularly encourage applications from women, LGBT students, and people of color, and will be working to put together an interdisciplinary and intellectually diverse team. Application deadline: Monday, March 10th. The application process is simple: direct an email to Purdom Lindblad . Please indicate why you’re interested in the Praxis Program, what you think you will gain from it, and what you feel you would bring to a collaborative digital humanities project. A small committee, consisting of Scholars’ Lab faculty and staff and past Praxis Program participants, will evaluate expressions of interest and schedule group interviews with finalists."},{"id":"2014-02-17-call-for-graduate-fellowship-in-the-digital-humanities","title":"Call for Graduate Fellowship in the Digital Humanities","author":"purdom-lindblad","date":"2014-02-17 07:46:16 -0500","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"call-for-graduate-fellowship-in-the-digital-humanities","layout":"post","content":"The Scholars’ Lab is proud to announce that applications for our prestigious Graduate Fellowship in the Digital Humanities are being accepted for the 2014-2015 academic year. The fellowship supports ABD graduate students doing innovative work in the digital humanities at the University of Virginia. The Scholars’ Lab offers Grad Fellows advice and assistance with the creation and analysis of digital content, as well as consultation on intellectual property issues and best practices in digital scholarship and DH software development. Fellows join our vibrant community, have a voice in intellectual programming for the Scholars’ Lab, make use of our dedicated grad lounge, and participate in one formal colloquium at the Library per semester. Supported by the Jeffrey C. Walker Library Fund for Technology in the Humanities, the Matthew &amp; Nancy Walker Library Fund, and a challenge grant from the National Endowment for the Humanities, the Graduate Fellowship in Digital Humanities is designed to advance the humanities and provide emerging digital scholars with an opportunity for growth. Please see the  DH Fellowship page  for more information about the program, eligibility requirements, and application information. Deadline: March 10th! Please contact Purdom Lindblad, Head of Graduate Programs at the Scholars’ Lab, with questions."},{"id":"2014-02-20-software-development-for-the-ma-humanities-student","title":"Software Development for the MA Humanities Student","author":"eric-rochester","date":"2014-02-20 06:27:10 -0500","categories":null,"url":"software-development-for-the-ma-humanities-student","layout":"post","content":"This is not a transcript of a brief panel talk I gave for the UVa Graduate English Student Association Career Panel. It’s based on what I hope to say, but I’m actually writing this before the event so it (and its links) can be available beforehand. About me I’ve been interested in two things for about as long as I can remember: computers and literature. These intersected a little in science fiction and fantasy, but largely, the two obsessions remained strangely separate. I’d spent a lot of time reading, both “literature” and “trash”; but I’d also enjoyed playing computer games and trying to program my own. It wasn’t until half-way through my PhD program at The University of Georgia that my interests started to come together. Initially, I just had a reputation for being able to help people format columns in Word. Then I got involved in digital humanities, then called humanities computing. I also created a website for a professor, and later I started doing web development and systems administration for the Linguistic Atlas Projects . Although these jobs weren’t my primary focus in graduate school, I did take them seriously. I learned best practices, including version control and testing. This was good for the project, but it was also good for me: doing things right up front saved me pain and sweat later. And this was how my two interests finally found common ground. When I graduated I took a job doing a combination of corpus linguistics and software development. This was good, but when I needed to look for another job, I found that there were fewer options for corpus linguistics than for web development. So I made web sites for a few years. I had a lot of fun, and I learned a lot, both about the work itself and about interacting with clients and stakeholders. For the last almost three years, I’ve been senior developer here at the Scholars’ Lab . What does that entail? Software development : True to my title, a lot of what I do involves developing and maintaining computer systems and web sites. Mentoring and education : Our biggest focus is education and mentoring. Sometimes that means helping someone who walks in with a digital project. More often it involves helping one of the Scholars’ Lab’s fellows or one of the students in the Praxis Program . Documentation : An important–but often overlooked–aspect of software projects is their documentation. We don’t spend enough time on this. That’s not all, but those three is probably how I spend most of my time. What Kind of Work are we Talking about? I’m a software developer, so I’ve necessarily focused on that in talking about my personal journey. However, software projects are large, sprawling, complex behemoths, and there are a lot of different tasks that need to be done and a lot of different specialties that are required to contribute. So even if writing code doesn’t appeal to you, other things might. Project management : Keep everyone on track. Community outreach : Publicize the project and be an active member of the project’s community. Design : Make the product usable. Documentation : A different way to make the product usable. Testing : Check that the product works and works correctly. Gina Trapani has an excellent post talking about how crucial–but also how under-valued–many tasks are in a software project, especially in the open source world. You can read about it at Designers, Women, and Hostility in Open Source . What Advantages do you Have? Typically, people expect those in any technical job to have a STEM background. This is false, and in fact, a humanities background can be a great asset in almost any job in software development. Let me count the ways. Communication This point is trite, but it’s true. At a fundamental level programming involves communicating. Your code must communicate to the computer, to other developers, and even to your future self. You’ll also need to communicate effectively to clients, to your boss, and to co-workers. Education and Negotiation An important part of software development involves educating and negotiating with others. For example, adding a feature may involve dropping another one. This doesn’t make clients happy, and you’ll need to explain why and argue your case. Research Learning new technologies as well as using ones you’re already familiar with both involve a lot of research. Knowing how to learn and how to research is an important asset here. Reading and interpretation Most programmers work from specification documents. Being able to interpret them appropriately is crucial. Multi-level focus I’m not sure what to call this point, but it may be the most important one. When you analyze literature you must command details from a variety of texts and sources and synthesize them to make a larger point. This involves paying attention to both the forest and the trees. Writing software involves the same split focus: on the one hand, you spend a lot of time in the weeds thinking about semicolons; on the other hand, you must keep the big picture in mind to stay on track and on schedule. For some of the same points, plus some others, see Shelby Switzer’s post on How my “impractical” humanities degree prepared me for a career in programming . What Can you be Doing Now? Obviously, finish your degree. This is the most important thing you can do. But in your spare time (ha!), there are some other things you can do, both now and in the future. (Again, apologies: this list is for software programmers, especially web developers.) Learn the basics For web development, design, etc., this means learning HTML, CSS, and JavaScript. CodeAcademy, Code School, and tuts+ are all good. The main thing is to type along yourself. Learn a Web Language Essentially, you want something you can use to interact with a database and dynamically generate web pages. This could be JavaScript using NodeJS, Ruby, or Python . Any of these are good. If there’s one available for your language of choice, the Learn Code the Hard Way series is excellent. Learn a Web Framework Find one based on the language you picked in the previous point. For JavaScript, that means Express or Sails . For Ruby, Ruby on Rails or Sinatra . For Python, Django or Flask . If you’re just getting started, don’t worry about getting a broad knowledge of different technologies. All of them are similar. You’ll be better served by going deep into one choice. What you learn will apply to the other systems, and you can learn them later when required. Also, learn the tools you’ll use to work in these languages. Learn them thoroughly and learn them well. You’re going to live in them. A text editor This is probably the single-most important tool for a software developer. Know it inside and out. Know all of its tricks. Sublime Text is a popular choice right now. Version control Programmers use version control systems to track the changes they make to their code. Git is a very popular choice, and Github allows you to share your code and collaborate with others. Online documentation Find the documentation for your programming language, its libraries, and the web framework you’re using. Also StackOverflow is a popular site for asking questions related to software development. Finally, get your work out there . There’s never been a better time for this than right now. You can put your code online for others to see on Github . You can also run web apps quickly and easily using Heroku . Having code up on these makes it easy for potential employers to see your skills. It also lets them know that you’re active and learning and capable. They won’t replace a good portfolio that directs potential employers’ attention and highlights your best work, but they are a good start, and they’ll set up above most other applicants. In general, this is a great time to go into software development and other technical jobs. Hopefully this post tells you what you need to think about and plan for."},{"id":"2014-02-22-digest-3-project-mutability-shifting-identities-and-changing-roles","title":"(Digest #3) Project mutability: shifting identities and changing roles","author":"stephanie-kingsley","date":"2014-02-22 10:18:47 -0500","categories":["Grad Student Research"],"url":"digest-3-project-mutability-shifting-identities-and-changing-roles","layout":"post","content":"This week was full of excitement.  Our Development Team continued working on getting Ivanhoe up and running; they will be presenting the working WP Theme to our Praxis team this Tuesday. The Design Team started thinking more about how we want the name, logo, font, and overall aesthetic to reflect our game.  The name “Ivanhoe,” in particular, has long been a subject of some contention in our group.  To address this concern, Francesca and Zach will be giving two separate identity pitches for the game on March 4.  In the next couple of weeks, they will think through why we would retain the name “Ivanhoe,” why we might call it something else if we were to change the name, and what aesthetics we would use to reflect those decisions.  Stay tuned for a post from Francesca on this turbulent but essential process. In addition to the activities of the two teams, the roles of our individual members have begun to shift, as well.  Eliza started out solely on development, and although she is still primarily a member of that team, she has become the primary wielder of CSS and HTML in the Praxis group.  Eliza is now actively applying the CSS of our wireframes to the Theme our developers have been creating in PHP; she thus has become somewhat of a development-design liaison.  This will prove invaluable when the Design team has given its identity pitches and it becomes time to bring the chosen identity to life in CSS. Francesca will also begin to take on responsibilities beyond her role on Design.  After the identity pitches have been given, Francesca will spend more time writing the content of our informational website.  Part of the goal of the website will be to explain the decisions behind the game’s aesthetic, and Francesca will be well equipped to address them as a designer.  Thus, as Eliza shifts from Development to Design in March, Francesca will move into more of a Support role. I think these developments are exciting.  As members become interested in different parts of the project, they can move into new roles.  This ability to adapt on the part of the entire group is in accord with our charter goal of flexibility.  It also reflects an attention to the ever-changing needs of the project itself.  As we move through the various stages of conceiving, building, and designing Ivanhoe, different types of work will become necessary. I continue to sit in on the separate teams’ meetings and assign issues and milestones in GitHub .  The shifting roles within our team and changing requirements of the project have required me to continually adjust the assignments and reorganize workflow.  I also constantly re-conceive how I myself fit into the group as project manager.  That, however, is meat for another post."},{"id":"2014-02-24-on-community-listening-3","title":"On Community Listening: 3","author":"erik-deluca","date":"2014-02-24 04:30:20 -0500","categories":null,"url":"on-community-listening-3","layout":"post","content":"Check out my previous posts, On Community Listening 1 and 2 for context. The visual aesthetic of the web environment is minimal because I want listeners to focus on the sounds that the interface holds and not on the overstimulation of visual content. Think Sol Lewitt . This is my favorite series by Mr. Lewitt. The form of the interface is inspired by Alexander Calder’s kinetic sculptures and Earl Brown’s “ Open Form ” works. I like these structures because the art is composed of a network of reconfigurable vignettes that can be perceived from many different perspectives. Calder’s sculptures are reconfigured by environmental factors like the wind and light. The conductor and/or the musicians reconfigure most of Brown’s music . The web interface for “Community Listening” will be different from these two types of works. The form will allow listeners the ability to directly reconfigure and interact with the ethnographic composition. In a sense, the listeners will also become the “conductor” of the work. This type of user flexibility confronts issues of access. The form of digital scholarship can “speak” to many different people from many backgrounds. Right? Unrelated: I’m listening to this and this ."},{"id":"2014-02-26-dh-speaker-series-ted-underwood","title":"DH Speaker Series: Ted Underwood on CS and the Humanities","author":"laura-miller","date":"2014-02-26 09:41:11 -0500","categories":["Announcements","Podcasts"],"url":"dh-speaker-series-ted-underwood","layout":"post","content":"March 10 update: Due to technical difficulties, we are not able to post the podcast of this event.  Apologies for any inconvenience! Beyond Tools: The Shared Questions about Interpretation that Link Computer Science to the Humanities Thursday, March 6 at 10:00 am in Alderman Library, Room 421 The phrase “digital humanities” suggests an encounter with digital technology itself – which might seem to involve departments of computer science only insofar as they build tools for us to use. But as collaborations between humanists and computer scientists grow more common, it’s becoming clear that these disciplines are working on shared, surprisingly fundamental questions. For instance, computer scientists want to understand how we learn to generalize about latent categories from limited evidence, which is a good part of what humanists do when we “interpret an archive” or “develop a theory.” Instead of treating CS as a source of tools, some humanists are starting to approach the discipline as a theoretical interlocutor, analogous to linguistics or anthropology. What might this conversation look like concretely? I’ll flesh out some possibilities, briefly describing a collaboration with David Bamman at Carnegie Mellon (we’re attempting to model character in nineteenth-century novels), and reflecting more generally on the humanistic value of model-building. I’ll also acknowledge some of the social divisions that make this conversation risky. Ted Underwood is Associate Professor of English and Liberal Arts and Sciences Centennial Scholar at the University of Illinois, Urbana-Champaign.  He is the author of two books on eighteenth- and nineteenth-century literary history, including Why Literary Periods Mattered (Stanford, 2013). He is currently developing models of genre in eighteenth- and nineteenth-century books, supported by a Digital Humanities Start-Up Grant from the NEH and an ACLS Digital Innovation Fellowship. A collaborative essay with Andrew Goldstone, topic-modeling the history of literary scholarship, is forthcoming in New Literary History. All Scholars’ Lab events are free, open to the public, and require no advance registration. Image courtesy of http://tedunderwood.com/"},{"id":"2014-03-03-on-stemmatics","title":"On Stemmatics","author":"zachary-stone","date":"2014-03-03 05:00:10 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"on-stemmatics","layout":"post","content":"So this is a git network graph. Specifically, it is the network graph for Ivanhoe from c. 20 February to 1 March. The blue line is our Develop branch and the various branches are features, projects, etc. The first little pink dot is my first branch .  While programming development is far from complete, I forked off a branch to begin styling completed features with CSS. The next little pink dot was my introduction to the world of merge conflict. Gross . [![Stemma](http://static.scholarslab.org/wp-content/uploads/2014/03/Stemma-300x111.png)](http://www.digitalmedievalist.org/journal/9/hall/) © Alaric Hall and Katelin Parsons, 2013. Creative Commons Attribution-NonCommercial licence This is a stemma . It is a stemma of _Konráðs saga keisarasonar. [1] _Up at the top we have our putative archetype, hyparchetype, or possibly even authorial holograph.[2] The myriad of branches represent various copies and what have you. I believe the similarity of these images is both obvious and salutary. Both images map out textual iterations created by humans. Both bear witness to merge problems and variations. Both require diverse groups of human beings to work together to iterate a text. While the technological means of iteration have evolved considerably, the problem of reliable iteration across time and space remains. I am not implying that coding is just like manuscript copying. It isn’t, and digital textuality is not exactly analogous to manuscript textuality. I guess what I am saying is a bit more Eliotean. The end of all exploration will be to return home and to know it for the first time.[3] Mediums change, and as a bibliographer and book historian I certainly avow inseparability of medium and message, but it seems problems remain the same. How to meaningfully pass information from one human being to another and not to fail. [1] I’ve never read Konráðs saga keisarasonar ; this is just the best stemma picture I found on the interwebs. Full credit it is from a post by Alaric Hall Katelin Parsons over at Digital Medievalist . Full citation at the foot of this note. If you want to know more about this specific text/stemma or just want to see a really rad discussion of online editing and connect with one of the best online places for manuscript-y folks, hit it up. Hall and Parsons, _Digital Medievalist _9 (2013). ISSN: 1715-0736. [2] If you are George Kane [3] Loosely “Little Gidding: V,” ll. 26-9 ish."},{"id":"2014-03-06-digest-4-on-managing-projects-not-people-reflections-after-a-project-management-crisis","title":"(Digest #4) On managing projects, not people","author":"stephanie-kingsley","date":"2014-03-06 05:00:20 -0500","categories":["Grad Student Research"],"url":"digest-4-on-managing-projects-not-people-reflections-after-a-project-management-crisis","layout":"post","content":"This digest comes a bit late, because in the interim I have been going through a mild project management crisis.  Now that the crisis is past, I see the experience as the perfect opportunity for a post. I mentioned in my last post that the nature of our project—and thus our team—is changing.  Development has a working game up and running.  They now are adding features and troubleshooting bugs in the program.  Design has given the identity pitches and is moving forward with our new identity: Ivanhoe, designed in such a way as to emphasize the web-like network of moves which our game will inspire.  (Watch for more posts on the design plans from Francesca and Zach.) Eliza is learning SASS, which—very roughly speaking—lets her make her design life easier by building templates of CSS .  These will enable her to not have to enter every little bit of CSS for complex but commonly used formatting.  Zach is becoming more familiar with CSS, and the two of them will start applying the design plan to our website and theme within the  week. Francesca is refining design on our logo and will be transitioning to writing web content this week.  Francesca has reflected before on the nature of our tool as a game (see, for instance, “Are we gaming or just simulating?” and an earlier post on gamification ), and I think she is the perfect person to write web content on Ivanhoe for this reason.  She will be writing a brief history of the Ivanhoe Game, so anyone who wishes to see what our game has grown out of may do so. That leaves me.  For a while, I was heavily focused on what everyone was working on.  I knew that my job was to keep up with that and make sure that everything got done.  This quickly became problematic, however.  Development’s understanding of coding tasks far outstripped my own.  They have also been very organized and have had very little need of managing outside of their own team.  Design also established a system of meeting, dividing up tasks—each member taking a separate design plan for the identity pitch—and plunged into the design.  This left me feeling a bit bewildered.  I felt that I had little to do but blog on what everyone else was doing. This particular circumstance is essentially a good thing; a project manager needs the team members to take initiative and plan their own parts of the project.  Once that’s going smoothly, however, a new style of project management is called for.  I was not prepared for this, and in my frustration, I fear I may have teetered on the brink of MICROMANAGEMENT.  This was the very demon I had striven to avoid in my Praxis involvement, and in a climactic meeting with my team, I discovered that I may have begun this downward spiral.  I didn’t want people to feel like I was looming.  But how to do my job? This is where a project manager needs to remember that she is managing a project, not people—a confusing point, as people are responsible for everything that gets done on a project.  Still, with some coaching from Bethany and Purdom, and after putting inquiries to my teammates as to how I could best be helpful, I emerged from my confusion with a very good idea of what my role would involve in the latter half of this semester. I figure primarily in the role of Support now.  I am responsible for publicity—blogging and looking for opportunities to get the word out about Ivanhoe.  I am also the primary steward of The Timeline (all caps because it’s so important).  This quite simply means knowing what our deadlines are and keeping people aware of them so they can plan their own work accordingly.  I am also involved in testing Ivanhoe—a very fun job where I enter games on our Heroku testing site and then tell Development what’s broken.  I create a new issue on GitHub for each bug, give it a big red “Bug” label, and go look for more.  It’s satisfying in the way that taking a red pen to an essay is satisfying.  And I know that when it’s all done, our game will be the better for it. Lastly, my job is to maintain our vision for Ivanhoe.  This means keeping people on track if they start wanting to add features which veer from our essential goals.  This part of my job is a bit amorphous and abstract at this point, but for all that, it is inspiring.  I look forward to seeing where Ivanhoe will go, and to gaining greater project management experience and insight as the semester progresses."},{"id":"2014-03-06-on-community-listening-4","title":"On Community Listening: 4","author":"erik-deluca","date":"2014-03-06 03:19:43 -0500","categories":null,"url":"on-community-listening-4","layout":"post","content":"Check out my previous posts for context: On Community Listening 1 On Community Listening 2 On Community Listening 3 THIS article published in the LA Times today (2/28) is about the researchers I’m working with on Community Listening,  the wolves they study, and global climate change. Lots of great stuff here, including this quote from Michael Nelson: “To preserve a healthy ecosystem with climate change, we at times are going to have to intervene, and that’s a hard thing to wrap our heads around…” This quote may seem trivial but the wolves in question live in a National Park that has a “let nature take its course” directive. My project is not so much about the wolves that live on Isle Royale but on the deep listening network that exists there. However, this network, I believe, is very dependent on the wolves that live in this place (an issue that I will address in my project). If these wolves go extinct, will this deep listening network also vanish? How do environmental fluctuations, like global climate change and its residual effects, dictate how we listen to the world?"},{"id":"2014-03-17-project-gemini-over-baja-california","title":"Project Gemini over Baja California","author":"david-mcclure","date":"2014-03-17 06:32:01 -0400","categories":["Geospatial and Temporal"],"url":"project-gemini-over-baja-california","layout":"post","content":"[Cross-posted from dclure.org ] Launch the Exhibit A couple weeks ago, somewhere in the middle of a long session of free-association link hopping on Wikipedia, I stumbled into a cluster of articles about Project Gemini, NASA’s second manned spaceflight program. Gemini, I quickly discovered, produced some spectacular photographs - many of them pointed downward towards the surface of the earth, capturing a dizzying opposition between the intelligible scale of the foreground (the 20-foot capsule, 100-foot tethering cords, 6-foot human bodies floating in space) and the completely unintelligible scale of the massive geographic entities below (peninsulas, continents, oceans). As I started to click through the pictures, I found myself reflexively alt-tabbing back and forth between Chrome and Google Earth to compare them with the modern satellite imagery of the same geographic locations. Which made me think - why not try to actually combine the two into a single environment? Over the course of the next few days, I sketched out a little Neatline exhibit that plasters two photographs of Baja California Sur - taken about a year apart on Gemini 5 (August 1965) and Gemini 11 (September 1966) - around the Mapbox satellite imagery of the peninsula. Instead of lining up the coastlines to make the images overlay accurately on top of the satellite tiles, I just plunked them down on the map off to the side at a scale and orientation that makes it easy to compare the two. (We’ve played around with this before, and I like to think of it as faux - or just especially humanistic! - georectification.) Then, using the drawing tools in Neatline, I blocked in some simple annotations that visually wire up the two sets of imagery - outlines around the four islands along the eastern coast of the peninsula, and arrows between the different instantiations of La Paz and San José del Cabo. I also wanted to find a way to visually formalize the difference in perspective between the Gemini photographs (oblique, wide-angle, deliberate) and the Mapbox tiles (flat-on, uniform). Using Illustrator, I created a long, ruler-like vector shape to label the ~200-mile distance between La Paz and the approximate positon of the Gemini 5 capsule when the picture was taken, and then used the “Perspective Grid” tool to render the shape in three dimensions and place it on top of the Gemini photograph, as if the same shape were physically positioned in front of the lens. In Illustrator: And placed in the Neatline exhibit, first to match the shallow angle of the Gemini shot: And then to match the perpendicular angle of the Mapbox tiles: I was also fascinated by the surreal opposition in scale between the Agena Target Vehicle (an unmanned spacecraft used for docking practice in orbit) and Isla San José, which sits serenely in the dark blue of the Gulf of California hundreds of miles below, but occupies almost exactly the same amount of space in the photograph as the 7-foot boom antenna on the Agena. In the space between the two, I dragged out two little shapes that map the sizes of things onto recognizable objects - a 6-foot person in the foreground, Manhattan in the background: Perspective and Perspectivelessness These images fascinate me because they roll together two types of imagery - both ubiquitous on the web - that are almost exact opposites of one another. On the one hand, you have regular pictures, taken by regular (non-astronaut) people. These photographs freeze into place one particular perspective on things. In a literal sense, the world recedes from the lens in three dimensions - walls, buildings, bridges, mountains, valleys, clouds. Close things are big, distant things are small. Some are in focus, others aren’t. And unlike other forms of art like painting, poetry, sculpture, or music, which can claim (overconfidently, maybe) to graft completely new material onto the world, photographs innovate at the level of stance and viewpoint, the newness of the perspective on things that already exist. It’s less about what they add, more about what they subtract in order to whittle the world down to one particular frame. Why that angle? Why that moment? Why that, and not anything else? On the other hand you have spatial photography - the satellite imagery used in Google Maps, Mapbox, Bing Maps, etc. - which is almost completely perspectiveless, in just about every sense of the word. The world becomes a flat, depthless plane, photographed from a distance at a perpendicular angle. Instead of trying to find interesting new ways to crop down the world, spatial tiles try to be comprehensive and standardized. Instead of showing one thing, in one way, at one moment in time, they try to show everything, in the exact same way, at the exact same moment - now. The companies that source and assemble the satellite imagery race to keep it as current as possible, right at the threshold of the present. Last year, Google announced that its satellite imagery had been purged of all clouds . No doubt this makes it more functional, but it also does away with those wispy, bright-white threads of cloud that used to hang over the rainforests in Peru and Brazil, which were lovely. What’s gained, of course, is the intoxicating grandeur of it all, the ability to hold in a single view a photograph of the entire world - which, if nothing else, is some sort of crazy affirmation of human willpower. I always imagine Whitman, scratching out “ Salut au Monde ”, panning around Google Maps for inspiration. Photographs taken by astronauts, though, collapse the distinction in interesting ways. They’re literally “satellite” photography, but they’re also drenched in subjectivity and historical stance. The oceans and continents spread out hundreds of miles below, just like on Google Maps or Mapbox - but the pictures were snapped with regular cameras by the hands of actual people, stuffed into little canisters falling around the world at thousands of miles an hour, which were only up there in the first place due to a crazy mix of socio-political ambitions and anxieties that were deeply characteristic of that particular moment in history. The Gemini imagery is haloed with little bits of space-race technology that instantly historicizes the frame - the nose cone of the capsule blocks out a huge swath of desert and ocean, the Agena vehicle hangs just a couple of hundred feet from the camera, tethered by a slight, ribbon-like cord that twists for hundreds of miles across the Gulf of California."},{"id":"2014-03-19-an-ivanhoe-design-idea","title":"An Ivanhoe Design Idea","author":"zachary-stone","date":"2014-03-19 08:54:45 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"an-ivanhoe-design-idea","layout":"post","content":"So last week Francesca and I each pitched design for our informational website. While the bulk of our pitches focused on the look of the website, I formulated my website design  (ps, as this was just a mock up it isn’t cross browser tested, sorry) to be as transferable as possible (or desired).  Had we gone this direction, certain elements of my design–colors, font, iconography–could have been exported to the finished game so as to give an aesthetic continuity to the entire project. Whether or not this is a good idea–i.e. using the aesthetics of the finished game to make an active and explicit intervention into the game itself–is a topic we are still working through (more anon).  In the end, we went more Francesca’s way than mine–for reasons that may become clear below and in consultation with her post on the design pitches–but the exercise in coming up with a design was generative and opened up new ways to think about the game. Below is a verbatim copy of my rough pitch. Please pardon spelling, grammar, gross generalizations, etc. I will say that the major strength of my design was also its weakness. We all thought (myself included) that it overdetermined the nature and use of the game and might discourage potential (non medieval [well what did you expect?]) users. Summary My design consists of three major elements: a color scheme, a set of three icons, and a specific font used for all text.  These design elements, which will be discussed briefly below, all derive from medieval traditions of book making and reading. I turned to medieval book history for inspiration for two major reasons. First, the name Ivanhoe has an inevitably medieval hue to it. Even if people don’t know the novel, it just sounds medieval. As such, a design that embraced the reception of Ivanhoe grew naturally out of my desire to embrace the history of Ivanhoe in my design. Furthermore, the production of meaning in a manuscript culture—in which books were visual and oral as well as textual—has remained in my mind as we have developed the game. Their method of iteration, bespoke copying, requires close engagement with and modulation of meaning by individuals and groups endemic to my Ivanhoe experience. Consider an opening from a copy of Gatian’s Decretals : [![Bologna, s. xiii Cesena, Biblioteca Malatestiana, Pluteo II sin. cod. 1, fol. 2r](http://static.scholarslab.org/wp-content/uploads/2014/03/glossed-199x300.jpg)](http://classes.maxwell.syr.edu/his311/Lecture%20Three/manuscript_of_gratian.htm) Bologna, s. xiii  Cesena, Biblioteca Malatestiana, Pluteo II sin. cod. 1, fol. 2r The commentary wraps the text, which is in turn illuminated by an illustration depicting oral composition.  If they could have put in movies and music, they would have! Color Scheme From this tradition I gleaned a color scheme that both harkens back to medieval books, but more importantly, reflects their hard-earned insights into how to make messy text legible and navigable. Light-grey text contrasts with a light-cream writing support, and muted reds and blues highlight and punctuate the text. Iconography -The pilcrow ¶ (modern sign for paragraph ) evolved out of capitula, or chapter markings, and was used to signify a new train of thought. Throughout my design pilcrows mark out trains of thought regarding our Ivanhoe narrative. What is the game. Where did come from. Why. These sorts of things. -The hedra ❦ is perhaps the oldest form of punctuation and has been used to mark off major sections of texts up to present times. In my system hedras signal moments of transfer in which information is being passed between users. In our website this is the “Install” section. The leafy, organic, vine implies notions of planting, growing, and connectivity while at the same time marking discreet units. -The manicule, ☝ or pointing hand, is a frequent marginal notation deployed by readers to simply say “look here” “important.” I deploy the manicule for similar functions on the website. It points to documentation and demos. Places we want users to LOOK. Font Choice I make exclusive use of Caudex through out my design. I choose Caudex for three reasons: style, legibility, and versatility. In style it echo aspects of various Gothic bookhands, but, crucially it remains legible in both is minuscule and majuscule forms. This dual legibility allows me to deploy majuscule scripts for logos and headers but use the miniscule for content. Thus my logos, rubrication, and content maintain a basic continuity. Applicability for the Game While I formulated my design identity around the info website, I believe its major elements could translate to gameplay with minimal difficulty. The icons were chosen in part for their symbolic valence, but also for their general utility as bullet points, section markers, and possible buttons in the game. All are text ornaments and not images thus they are easier to work with. The basic color scheme is minimal but useful (light grey for most text, red to flag attention, blue to ornament) and the merits of the font discussed above apply to the game as well. Thus I believe this identity allows for a strong aesthetic sense of what our game is that both relates to the history of Ivanhoe and to possible future experiences of it. While the manuscript book might seem over determined, the Book was and remains the western icon for information transfer and the dynamic tension produced by echoing medieval practice in a post-modern digital environment might produce interesting result."},{"id":"2014-03-21-dh-speaker-series-micki-kaufman","title":"DH Speaker Series: Micki Kaufman on Quantifying Kissinger","author":"laura-miller","date":"2014-03-21 08:28:24 -0400","categories":["Announcements","Visualization and Data Mining"],"url":"dh-speaker-series-micki-kaufman","layout":"post","content":"Everything on Paper Will Be Used Against Me: Quantifying Kissinger Thursday, April 3 at 10:00 am in Alderman Library, Room 421 Click for a larger image.  See www.mickikaufman.com/qk for a detailed description.Scarcity  of  information  is  a  common  frustration  for  many  historians.  However,  for  researchers  of  twentieth-­ and  twenty-­first  century  history  the  opposite  problem  is  also  increasingly  common.  In  contrast  to  scholars  of ancient  history,  who  base  much  of  their  analyses  on  rare  and  unique  relics  of  antiquity,  historians  studying the  ‘Age  of  Information’  (and  the  even  more  recent  period  of  ‘Big  Data’)  increasingly  confront  a  deluge  of information,  a  vast  field  of  haystacks  within  which  they  must  locate  the  needles  -­  and  presumably,  use them  to  knit  together  a  valid  historical  interpretation. As  larger  and  larger  volumes  of  human  cultural  output are  accumulated,  historians  will  continue  to  adapt  and  innovate  new  and  existing  tools  and  methods  -­ especially  those  developed  in  other  fields,  including  computational  biology,  literary  studies,  statistics  and psychology  -­  to  overcome  the  ‘information  overload’  and  facilitate  new  historical  interpretations  of challengingly  massive  digital  archives.    The  declassification  of  Former  Secretary  of  State  Henry  Kissinger’s correspondence  by  the  State  Department  and  the  hosting  of  that  material  on  the  Digital  National  Security Archive  (DNSA)’s  Kissinger  Collection  web  site  presents  just  such  a  challenge  and  concomitant  opportunity. Given  the  role  Henry  Kissinger  played  in  first  ‘computerizing’  the  State  Department  in  the  late  1960s  it  is perhaps  not  surprising  that  the  continuing  declassifications  of  large  volumes  of  material  have  made historians  of  the  Kissinger/Nixon  era  dubious  ‘beneficiaries’  of  the  ‘big  data’  era,  inheritors  of  countless government  mainframes’  worth  of  text. While  simply  having  such  a  large  volume  of  information  online  in digital  form  for  researchers  is  valuable,  the  usual  restriction  to  a  web-­based  ‘search’  form  interface  often renders  it  of  limited  use  and  approachability.  As  detailed  on  the  project’s  web  site   my  work  involves  the  application  of  a  host  of  quantitative  text  analysis methods  like  word  frequency/correlation,  topic  modeling  and  sentiment  analysis  (as  well  as  a  variety  of  data visualization  deisgns  and  methods)  to  historical  research  on  the  DNSA’s  Kissinger  Collection,  comprising approximately  17600  meeting  memoranda  (‘memcons’)  and  teleconference  transcripts  (‘telcons’)  detailing the  former  US  National  Security  Advisor  and  Secretary  of  State’s  correspondence  during  the  period  1969-­ 1977.  This  application  of  computational  techniques  to  the  study  of  twentieth-­century  diplomatic  history  has generated  useful  finding  aids  for  researchers,  provided  essential  testing  grounds  for  new  historical methodologies,  and  prompted  new  interpretations  and  questions  about  the  Nixon/Kissinger  era. Micki  Kaufman   is a doctoral  student  in  US  history  at  the  Graduate  Center  of the  City  University  of  New  York  (GC-­CUNY).  She  received  her  B.A.  in  US  History  from  Columbia University  summa  cum  laude,  Phi  Beta  Kappa  in  2011  and  her  M.A.  in  US  History  from  GC-­CUNY  in 2013.  A  GC-­CUNY  Digital  Fellow  and  recipient  of  GC-­CUNY’s  Provost’s  Digital  Innovation  Grant  in 2012–2014,  her  current  research   involves  the  use  of  computational  text  analysis  and  visualization techniques  in  the  study  of  the  DNSA’s  (Digital  National  Security  Archive’s)  Kissinger  Memcon  and Telcon  collections.  She  is  a  co-­author  of  “General,  I  Have  Fought Just  As  Many  Nuclear  Wars  As  You  Have,”  published  in  the  December  2012  American  Historical Review,  has  served  as  a  digital  humanities  consultant  for  a  number  of  institutes  and  projects,  lectures and  leads  workshops  in  Text  Analysis  and  Visualization,  and  has  taught  US  History  Since  the  Civil War  at  Hunter  College  and  the  Digital  Praxis  Seminar  at  the  CUNY  Graduate  Center.  She  is  also  a platinum-­award-­winning  recording  engineer,  and  a  featured  Sundance/ASCAP  film  score  composer. All Scholars’ Lab events are free, open to the public, and require no advance registration."},{"id":"2014-03-24-9770","title":"Praxis Program Panel: \"Reading Digitally with Prism and Ivanhoe\"","author":"stephanie-kingsley","date":"2014-03-24 06:00:41 -0400","categories":["Grad Student Research"],"url":"9770","layout":"post","content":"On April 4 at 4:30 PM in the Scholars’ Lab, members of current and previous Praxis cohorts will give a presentation on how their projects, Prism and Ivanhoe, can inform textual study and reading in a digital environment.  The presentation will also feature demonstrations of both projects. Presenters include: Scott Bailey (Religious Studies, Praxis ‘13-14) Veronica Ikeshoji-Orlati (Classical Archaeology, Praxis ‘13-14) Stephanie Kingsley (English, Praxis ‘13-14) Sarah Storti (English, Praxis ‘11-12) Brandon Walsh (English, Praxis ‘12-13) The panel follows the opening masterclass of the Graduate English Student Association conference “Reading Then and Now,” which will run April 4-6, 2014.  The conference will also feature a masterclass by UVa English faculty member Rita Felski (RSVP only); keynote speaker Andrew Piper, from McGill University; graduate student papers on reading practices across cultures and time periods; and a workshop by the Rare Book School .  For a complete schedule, visit http://graduate.engl.virginia.edu/gesa/conference/schedule/ . The Praxis panel on Friday is open to the public."},{"id":"2014-03-24-whats-in-a-name","title":"What's in a name? ","author":"francesca-tripodi","date":"2014-03-24 06:00:30 -0400","categories":["Grad Student Research"],"url":"whats-in-a-name","layout":"post","content":"The week before spring break Zach and I pitched two different design strategies for Ivanhoe. [See Zach’s post on a Medieval-themed design.] As part of the pitch we each designed an informational website that we could apply to the game,  but I also was trying to convince our team that we should change our name. Why? Well, even though Ivanhoe has positive associations with an existing community, I felt that the name did not convey how our game is designed to function. Also, by changing our name we were engaging in a meta-learning experience - essentially playing Ivanhoe with the name Ivanhoe! By giving Ivanhoe a new name I did not want us to abandon the old, but rather taking the original spirit of Ivanhoe, make new connections and create something new. _\n_ The design team came up with the name “Rhyzome” as a play on the word Rhizome - a biological term used to describe types of vegetation connected by a series of tubes.  The Rhyzome logo (see above) was designed around this same logic - the nodes in the logo symbolize connections users make during the game and the two sizes of circles represent first and second level moves. Green shows the connection between our game and the hard sciences, conveying our goal of cross-discplinary application. After my pitch our team agreed that Rhyzome was not the best alternative. While the name better conveys what Ivanhoe does, Rhyzome is already associated with an existing community and journal  in the digital humanities. So, we decided that what is best for the team is to stay with the name that evokes recognizability within the DH community but apply the Rhyzome aesthetic to our logo/game design. Below is a first iteration of a logo that combines the two ideas. As you can see there are still incorporating two circle sizes to represent different levels of connections as well as connecting all the letters to represent a network of ideas. This logo also leaves lines open inside the A and the O to represent the possibility for new connections as the game progresses. Question to our readers - is this logo legible? Thoughts on this design? Would really love feedback as I continue to modify it. While my “pitch” for a new name was unsuccessful, in the end it was a win for our team. We now have a great design strategy that matches the theoretical logic of Ivanhoe!"},{"id":"2014-03-25-development-design-and-the-distance-in-between","title":"Development, Design, and the Distance in-between","author":"veronica-ikeshoji-orlati","date":"2014-03-24 21:05:14 -0400","categories":["Grad Student Research"],"url":"development-design-and-the-distance-in-between","layout":"post","content":"In the past week, Scott and I have been chipping away at the various issues Stephanie has uncovered in her testing of proto-Ivanhoe. There are a LOT of issues, and there are still a handful of core features we have left to build by our April 1 deadline. Specifically, we need to work through some bugs with the display and assignment of roles to users, give a bit more thought to the role journal (which took a backseat to primary feature development earlier this semester), and set up a couple of other pages to make navigating the theme a bit more manageable. Building things is fun, but the challenge as we push towards a beta version of Ivanhoe is refining what we have built (our ‘something that works’ version) into a properly useable and playable game. Our code is still quite messy, since we sacrificed cleanliness for functionality as we learned to deal with PHP and WordPress, and some of the bugs we need to fix in the next week or so require that we grapple with what we have already written and think more critically about the logic of what we actually want to happen. And this is where things get interesting. Up until about a week or two ago, the tasks of the development and design teams seemed to be fairly well circumscribed. Scott and I would read WordPress documentation, write functions, break some stuff, then have Jeremy/Eric/Wayne point out where our logic failed. Eliza/Zach/Francesca would theorize, design, and write the code to make the look-and-feel of the theme coherent, navigable, and visually appealing to users on all sorts of devices. But as part of the design team’s trouble-shooting, Zach has recently been inputting WordPress test content to make sure that different types of data show up as expected, and it seems that some of the problems he has encountered are not issues with the CSS or Susy grid but, rather, problems with the consistency of the data the underlying code puts out. As graduate students in the humanities and social sciences, it is very easy to throw ourselves wholly into our respective disciplines and revel in the tangible progress we make there. On the development team, we have likewise had the singular focus of getting features working and have made good progress thus far. It seems, however, that we have arrived at a point at which all of us need to regroup a bit to figure out what tasks each team should prioritize to move the entire project along, because a coherent game is the goal, not a glorious laundry-list of features which nobody can access or use."},{"id":"2014-03-26-call-for-ivanhoe-testers","title":"Call for Ivanhoe Testers!","author":"stephanie-kingsley","date":"2014-03-26 09:45:00 -0400","categories":["Announcements","Grad Student Research","Research and Development"],"url":"call-for-ivanhoe-testers","layout":"post","content":"This year’s Praxis fellows are in the last couple weeks of programming before our release of the new Ivanhoe Game (rebuilt as a WordPress Theme ), and we are looking for people to test the program. The Ivanhoe Game is a pedagogical and critical tool which enables scholars or students to generate discussion and criticism on a subject through role-play.  The original Ivanhoe Game was conceived in the early 2000s by Jerome McGann and Johanna Drucker; they were then joined by Bethany Nowviskie, Nick Laiacona, and others to develop the game in UVa’s SpecLab/ARP–one of several precursors to the Scholars’ Lab .  You can learn more at http://www.ivanhoegame.org/?page_id=21 Potential uses of the Ivanhoe Game WP Theme include: Critical back-and-forth game play among scholars interested in applying different theoretical schools of thought to the same work.  For instance, in a game on The Tempest, one scholar might elect to ‘be’ Freud, another a Postcolonialist, and another a feminist critic; and the ‘moves’ might constitute asides written by these players in response to action in the play. Students in a course on textual editing or book history might take on roles as different people involved in the publication of a hypothetical or real book: author, editor, compositor, illustrator, printer, binder, bookseller, etc.! Law students can take on roles in a digital mock trial, making moves for various motions, documentary evidence, arguments, witness statements, testimony, etc. History students in a course on the American Revolution might take on roles as King George II, the Founding Fathers, generals and soldiers in various armies, and others. This is just to name a few; the possibilities are endless!  Ivanhoe’s construction as a WP Theme means that it can be easily incorporated into already existing WP pages, including course pages.  For a completely bare-bones idea of what Ivanhoe will be, visit our testing app Games page, hosted on Heroku. We hope to start testing on April 8.  We are looking for varying levels of commitment.  For a larger number of testers, 20-30 minutes a week would be helpful for us.  For a few testers who have the time and interest, we hope for 2-5 hours a week. If you are interested, please fill out the form below.  I will contact you within a week to make arrangements for testing."},{"id":"2014-03-27-solrsearch-2-0","title":"SolrSearch 2.0","author":"david-mcclure","date":"2014-03-27 06:20:19 -0400","categories":["Announcements"],"url":"solrsearch-2-0","layout":"post","content":"Today we’re pleased to announce version 2.0 of the SolrSearch plugin for Omeka! SolrSearch replaces the default search interface in Omeka with one powered by Solr, a blazing-fast search engine that supports advanced features like hit highlighting and faceting. In most cases, Omeka’s built-in searching capabilities work great, but there are a couple of situations where it might make sense to take a look at Solr: When you have a really large collection - many tens or hundreds of thousands of items - and want something scales a bit better than the default solution. When your metadata contains a lot of text content and you want to take advantage of Solr’s hit highlighting functionality, which makes it possible to display a preview snippet from each of the matching records. When your site makes heavy use of content taxonomies - collections, tags, item types, etc. - and you want to use Solr’s faceting capabilities, which make it possible for users to progressively narrow down search results by adding on filters that crop out records that don’t fall into certain categories. Stuff like - show me all items in “Collection 1”, tagged with “tag 2”, and so forth. To use SolrSearch, you’ll need access to an installation of Solr 4. To make deployment easy, the plugin includes a preconfigured “core” template, which contains all the configuration files necessary to index content from Omeka. Once the plugin is installed, just copy-and-paste the core into your Solr home directory, fill out the configuration forms, and click the button to index your content in Solr. Once everything’s up and running, SolrSearch will automatically intercept search queries that are entered into the regular Omeka “Search” box and redirect them to a custom interface, which exposes all the bells and whistles provided by Solr. Here’s what the end result looks like in the “Seasons” theme, querying against a test collection that contains the last few posts from this blog, which include lots of exciting Ivanhoe-related news: Out of the box, SolrSearch knows how to index three types of content: (1) Omeka items, (2) pages created with the Simple Pages plugin, and (3) exhibits (and exhibit page content) created with the Exhibit Builder plugin. Since regular Omeka items are the most common (and structurally complex) type of content, the plugin includes a point-and-click interface that makes it easy to configure exactly how the items are stored in Solr - which elements are indexed, and which elements should be used as facets: Meanwhile, if you have content housed in database tables controlled by other plugins that you want to vacuum up into the index, SolrSearch ships with an “addons” system (devised by my brilliant partner in crime Eric Rochester), which makes it possible to tell SolrSearch how to index other types of content just by adding little JSON documents that describe the schema. For example, registering Simple Pages is as simple is this: And the system even scales up to handle more complicated data models, like the parent-child relationship between “pages” and “page blocks” in ExhibitBuilder, or between “exhibits” and “records” in Neatline. Anyhow, grab the built package from the Omeka addons repository or clone the repository from GitHub. As always, if you find bugs or think of useful features, be sure to file a ticket on the issue tracker !"},{"id":"2014-03-28-fedoraconnector-2-0","title":"FedoraConnector 2.0","author":"david-mcclure","date":"2014-03-28 08:28:40 -0400","categories":["Announcements"],"url":"fedoraconnector-2-0","layout":"post","content":"Hot on the heels of yesterday’s update to the SolrSearch plugin, today we’re happy to announce version 2.0 of the FedoraConnector plugin, which makes it possible to link items in Omeka with objects in Fedora Commons repositories! The workflow is simple - just register the location of one or more installations of Fedora, and then individual items in the Omeka collection can be associated with a Fedora PID. Once the link is established, any combination of the datastreams associated with the PID can be selected for import. For each of the datastreams, FedoraConnector proceeds in one of two ways: If the datastream contains metadata (e.g., a Dublin Core record), the plugin will check to see if it can find an “importer” that knows how to read the metadata format. Out of the box, the plugin can import Dublin Core and MODS, but also includes a really simple API that makes it easy to add in new importers for other metadata standards. If an importer is found for the datastream, FedoraConnector just copies all of the metadata into the item, mapping the content into the Dublin Core elements according to the rules defined in the importer. This creates a “physical” copy of the metadata that isn’t linked to the source object in Fedora - changes in Omeka aren’t pushed back upstream into Fedora, and changes in Fedora don’t cascade down into Omeka. If the datastream delivers content (e.g., an image), the plugin will check to see if it can find a “renderer” that knows how to display the content. Like the importers, the renderers are structured as an extensible API that ships with a couple of sensible defaults - out of the box, the plugin can display regular images (JPEGs, TIFs, PNGs, etc.) and JPEG2000 images. If a renderer exists for the content type in question, the plugin will display the content directly from Fedora . So, for example, if the datastream is a JPEG image, the plugin will add markup like this to the item show page: Unlike the metadata datastreams, then, which are copied from Fedora, content datastreams pipe in data from Fedora on-the-fly, meaning that a change in Fedora will immediately propagate out to Omeka. (See the image below for a sense of what the final result might look like - in this case, displaying an image from the Holsinger collection at UVa, with both a metadata and content datastream selected.) For now, FedoraConnector is a pretty simple plugin. We’ve gone back and forth over the course of the last couple years about how to model the interaction between Omeka and Fedora. Should it just be a “pull” relationship (Fedora -&gt; Omeka), or also a “push” relationship (Omeka -&gt; Fedora)? Should the imported content in Omeka stay completely synchronized with Fedora, or should it be allowed to diverge for presentational purposes? These are tricky questions. Implementations of Fedora - and the workflows that intersect with it - can vary pretty widely from one institution to the next. The current set of features was built in response to specific needs here at UVa, but we’ve been talking recently with folks at a couple of other institutions who are interested in experimenting with variations on the same basic theme. So, to that end, if you’re use Fedora and Omeka and interested in wiring them together - we’d love to hear from you! Specifically, how exactly do you use Fedora, and what type of relationship between the two services would be most useful? With a more complete picture of what would be useful, I suspect that a handful of pretty surgical interventions would be enough to accommodate most use patterns. In the meantime, be sure to file a ticket on the issue tracker if you find bugs or think of other features that would be useful."},{"id":"2014-03-31-digest-5-managerial-musings-preceding-the-launch","title":"(Digest #5) More Reflections on Project Management","author":"stephanie-kingsley","date":"2014-03-31 06:00:22 -0400","categories":["Grad Student Research"],"url":"digest-5-managerial-musings-preceding-the-launch","layout":"post","content":"It has been a couple weeks since my last digest, but as you can see from other posts, the Praxis goings-on are many! Development scurries to fix the multitude of bugs that seem to keep crawling out of the WordPress woodwork, while Design continues to apply CSS, SASS, and SUSY to bring our vision of Ivanhoe to fruition. Meanwhile, Francesca and I draft, revise, and reorganize content for our informational website, which will go up next week. As Project Manager, I have been busy getting word out about Ivanhoe, preparing for the Praxis panel and demo on April 4, gathering testers, and constantly rethinking how we want to present our new game to the world. I also continue to reflect on my own role as project manager. A few weeks back, I chronicled my project-management crisis, in which I reached a point where I wasn’t entirely sure what my purpose was because my team needed very little actual management. As we rapidly approach our soft launch (April 8), I realize how very much I have to do in terms of promoting Ivanhoe and organizing the next major phase of the project: testing.  In writing my post on testing, sending emails to prospective testers, and preparing Ivanhoe web content, I have to consider what people unfamiliar with Ivanhoe need to know about it. I also find myself revisiting the question which our team debated through the entirety of the first semester: what is Ivanhoe? I am not going to speculate on what Ivanhoe is in this post; for my presentation of the game, check out my “Call for Testers” post . What I do wish to take a moment to marvel at is how having to present Ivanhoe to others has made me see new aspects of it which I hadn’t considered before. While showing the game to a friend just last night in a sort of informal think-tank session, I discovered just how useful it is to be able to view a string of posts containing YouTube videos, image files, text, and web links, one after another. Our Theme also allows users to view a list of all the moves a player has made in a given role, which makes it easy to see how the player has developed that role over the course of the game. Essentially, we have streamlined multimedia game play and greatly facilitated role review. When Ivanhoe comes out, and after we kill all the bugs, of course, it will be easy to use for collaborative multimedia game play. I see even better now how my role as project manager is both coherent and vital.  By being at the forefront of publicity, I constantly revisit the question of what Ivanhoe is, and I can bring those insights back to team meetings and use them to guide us as we continue to make decisions about features and design details. At the end of “On Managing Projects, Not People,” I mentioned that the task of maintaining our vision for Ivanhoe was at that point “a bit amorphous and abstract.”  I now have a much better idea of how very real that job is.  I look forward to presenting that vision to the world, incarnate, in a couple weeks."},{"id":"2014-03-31-neatline-text","title":"NeatlineText: Connect Neatline exhibits to documents","author":"david-mcclure","date":"2014-03-31 08:00:11 -0400","categories":["Announcements"],"url":"neatline-text","layout":"post","content":"Download the plugin Today we’re pleased to announce the first public release of NeatlineText, which makes it possible to create interactive, Neatline-enhanced editions of text documents - literary and historical texts, articles, book chapters, dissertations, blog posts, etc. - by connecting individual paragraphs, sentences, and words with objects in Neatline exhibits. Once the associations are established, the plugin wires up two-way linkages in the user interface between the highlighted sections in the text and the imagery in the exhibit. Click on the text, and the exhibit focuses around the corresponding location or annotation. Or, click on the map, and the text scrolls to show the corresponding sections in the text. We’ve been using some version of this code in internal projects here at the lab for almost two years, and it’s long overdue for a public release. The rationale for NeatlineText is pretty simple - again and again, we’ve found that Neatline projects often go hand-in-hand with some kind of regular text narrative that sets the stage, describes the goals of project, or lays out an expository thesis that would be hard to weave into the more visual, free-form world of the Neatline exhibit proper. This is awesome combination - tools like Neatline are really good at displaying spatial, visual, dimensional, diagrammatic information, but nothing beats plain old text when you need to develop a nuanced, closely-argued narrative or interpretation. The difficulty, though, is that it’s hard to combine the two in a way that doesn’t favor one over the other. We’ve taken quite a few whacks at this problem over the course of the last few years. One option is to present the text narrative as a kind of “front page” of the project that links out to the interactive environment. But this tends to make the Neatline exhibit feel like an add-on, something grafted onto the project as an after-thought. And this can easily become a self-fulfilling prophecy - when you have the click back and forth between different web pages to read the text and explore the exhibit, you tend to write the text as a more or less standalone piece of writing, instead of weaving the narrative in with the conceptual landscape of the exhibit. Another option is to chop the prose narrative up into little chunks and build it directly into the exhibit - like the numbered waypoints we used in the the Hotchkiss projects back in 2012, each waypoint containing a small portion of a longer interpretive essay. But this tends to err in the other direction, dissolving the text into the visual organization of the exhibit instead of presenting it as a first-class piece of content. NeatlineText tries to solves the problem by just plunking the two next to each other and making it easy for the reader (and the writer!) to move back and forth between the two. For example, NeatlineText powers the interactions between the text and imagery in these two exhibits of NASA photograph from the 1960s: (Yes, I know - I’m a space nerd.) NeatlineText is also great for creating interactive editions of primary texts. An earlier version of this code powers the Mapping the Catalog of Ships project by Jenny Strauss Clay, Courtney Evans, and Ben Jasnow (winner of the Fortier Prize prize at DH2013!), which connects the contingents in the Greek army mentioned in Book 2 of the Iliad with locations on the map: And NeatlineText was also used in this interactive edition of the first draft of the Gettysburg Address : Anyway, grab the code from the Omeka add-ons repository and check out the documentation for step-by-step instructions about how to get up and running. And, as always, be sure to file a ticket if you run into problems!"},{"id":"2014-04-01-theming-neatline-exhibits","title":"Creating themes for individual Neatline exhibits","author":"david-mcclure","date":"2014-04-01 09:16:12 -0400","categories":["Geospatial and Temporal"],"url":"theming-neatline-exhibits","layout":"post","content":"tldr: Neatline makes it possible to create separate themes for individual exhibits, which is useful if you want to host a collection of self-contained Neatline projects on a single site. To get started, fork the exhibit starter theme, which abstracts out the style, layout, and UX of the Project Gemini over Baja California exhibit. One of the coolest but most under-documented features in Neatline is the ability to create separate themes for each individual exhibit. Since Neatline exhibits are just one particular type of “view” inside of Omeka, it’s always been possible to customize the styling and layout at the level of the Omeka theme. Changes the the Omeka theme, though, propagate to all the exhibits on the site. In many cases, this is ideal - if you have a collection of closely-related Neatline projects, all part of the same thematic umbrella, it makes sense that they should all look more or less the same. For examples of this, check out Jeremy’s beautiful Astrolabe and Neatscape themes for Omeka, which were designed with Neatline projects in mind. In other cases, though, this can be a real hindrance. Sometimes it can make sense to host a number of self-contained Neatline exhibits in the same installation of Omeka. For example, imagine you’re using Neatline in a big lecture course, and you split the class up into 10-15 groups of students, all working on separate exhibits. As the semester draws to an end, some of the groups want to use the NeatlineText plugin, and need a layout that positions the exhibit narrative on the side of the screen, flush with the edge of the window. But other groups are just threading the text content into the record bodies, and don’t want a big, empty container element taking up space on the screen. How to handle both at once? Or, for a concrete example, take a look at the Neatline Labs site, which I use a sandbox for little Neatline-powered experiments and feature demos. By design, these projects are all totally different - different content, different layouts, different Javascript interactions, etc: It would be annoying to have to spin up a completely new instance of Omeka for each of these projects. To get around this, Neatline implements its own “sub-theming” system, piggybacking on top of the capabilities provided by Omeka, that makes it possible to customize part or all of the appearance, layout, or behavior of each exhibit on an individual basis. This is an opt-in system that can be mixed with the regular, site-wide theming system - if you have 10 Neatline exhibits on your site, you could write exhibit-specific themes for three of them, and leave the other seven unchanged, allowing them to continue to inherit the generic Omeka theme. And, within the three exhibit-specific themes, you have full control over which parts of the theme you override - for one, you could leave the layout unchanged, but modify the CSS; for another, you could leave the CSS the same but change the layout and add some custom Javascript interactions. Exhibit-specific themes are also highly portable - once you’ve built one to your liking, it can be adapted for new exhibits just by copying and renaming the directory. I’ve held off on documenting this publicly because I wanted to be sure that the file structure and Javacsript APIs used in the themes worked well at scale - but at this point it’s all pretty battle tested, and I’m curious to see what other folks can come up with! Getting started: Creating the theme directory Neatline themes are created as directories that sit inside of the Omeka theme. For any given exhibit, Neatline will look for a theme directory that has the same name as the “URL slug” of the exhibit, the unique, plain-text identifier used to form the end of the exhibit’s public-facing URL. So, imagine you’ve got an exhibit called “Test Exhibit,” with a URL slug of test-exhibit . To create a theme for the exhibit, create a directory called test-exhibit at this location relative to the root of your Omeka theme: [omeka-theme]/neatline/exhibits/themes/test-exhibit For example, here’s the layout of my fork of the Neatlight theme, with the theme directories for a handful of exhibits at neatline.dclure.org : Anatomy of a Neatline theme Neatline themes consist of just four files: style.css, script.js, show.php, and item.php . style.css Use style.css to add custom CSS to the exhibit. Neatline loads this as the last stylesheet on the page, after the Omeka CSS and after the CSS provided by the Neatline core (which, if you want, can be omitted from the page by providing a custom show.php template - see below). style.css can be anything from a handful of simple rules to change fonts or colors up to a complete redesign of the page. script.js Use script.js to add custom Javascript interactions to the page. Again, these can be as simple or complex as needed. The Neatline front-end application is a big chunk of code, and it’s a bit beyond the scope of this article to really dive into the API in detail. The gist of it, though, is that Neatline is structured as a bunch of little mini-applications, called “modules,” that communicate with one another using a pub-sub messaging system, powered by the superb EventAggregator component in the Marionette framework . The cool thing about this architecture is that snippets of code in the script.js file can hook directly into this messaging system and interact with Neatline just as if they were included in the core codebase - Neatline literally won’t know the difference. There’s really no limit to what you could do here - the entire Neatline editing environment, for instance, is implemented as a single module (containing lots and lots of nested sub-modules), and could theoretically be grafted onto Neatline completely inside of an exhibit theme. This makes it possible to wrap up a Neatline exhibit in pretty much any kind of interface without having to modify the internals. That said, in most cases you’ll probably just need a few little snippets to add in some visual bells and whistles, or to manage complex layout tasks that are tough to accomplish in CSS. For example, here are a few snippets I used in the Project Gemini over Baja California project: Position the text container on the left side of the screen and fill the height of the window: Add an NProgress -powered loading bar to the page: Implement custom zooming buttons: Add a “Loading Tiles…” spinner that displays when WMS imagery is being loaded from Geoserver: For an example of a fully-fledged module, which follows the file layout conventions of the Neatline core, take a look at the Lines module in the Gemini theme, which intercepts events broadcast by NeatlineText and draws the little yellow lines between the text and the map. show.php By default, all Neatline exhibits are displayed using the show.php that ships with the plugin. If you create a file called show.php in the exhibit theme, though, Neatline will use that file in place of the default. This makes it possible to completely customize the structure of the markup in any way you want. For example, if you look closely at some of the Jacascript examples above, you’ll notice that in a couple of places the code is selecting elements (things like $('#wms-loader') and $('div.narrative') ) that aren’t actually templated anywhere in the default show.php, which looks like this: This works, though, because I’m providing a custom show.php template that provides those elements (e.g., see the #wms-loader element down near the bottom): item.php Last but not least, Neatline makes it possible to override the template that’s used to generate the metadata output for items displayed inside Neatline exhibits. By default, Neatline uses a simple template that pretty much just follows the layout of the regular item “show” pages in Omeka: But, imagine you had an exhibit that was filled with items that represent photographs, and, for the sake of cleanliness and visual economy, you just want to display the item title and the image thumbnail. Just drop in a new item.php template that does exactly what you need: And Neatline will automatically use that template instead of the default. What if you need different templates for different items, though? For example, imagine that you actually have two types of items in the exhibit - the images, which just need the title and thumbnail, but also a set of letters, which are structured as “Text” type items with the transcriptions of the documents entered into the “Text” field. So, how to display both types of items in the exhibit, without resorting to a weird, Frankenstein template that accommodates both? First, add tags to the records in Neatline: Then, just create two template in the exhibit theme - one called item-image.php (the same as above), the other called item-letter.php . In the letter template, just display the title and text: Neatline will automatically use the tag-specific templates for any records tagged with image or letter, and fall back to the unadorned item.php template for records that aren’t tagged. Starter theme So far, we’ve just been entering all of our custom CSS and Javascript directly into the style.css and script.js and  files. This works fine for simple themes, but it can start to get a little clunky as the theme grows more complex - nobody likes to see a big heap of Javascript snippets, all doing different things, crammed into the same file. So, how to decompose style.css and script.js into separate files? One good solution is to use a task runner like Grunt to concatenate multiple source files into the style.css and script.js files, which, instead of being edited directly, become compiled payload files that are updated automatically by the task runner. To make it easy to get started, I’ve created a little starter theme, based on the theme used for the Project Gemini over Baja California project, with all of the configuration and file structure in place to build out themes for exhibits that use the NeatlineText extension. This includes all of the layout, styling, and UX interactions from the Gemini project, like the little yellow lines that wire up the text with the map. https://github.com/scholarslab/neatline-theme-template Just fork the repo, clone it into your Omeka theme, and theme into the sunset!"},{"id":"2014-04-08-all-together-now","title":"All Together Now","author":"veronica-ikeshoji-orlati","date":"2014-04-08 18:36:21 -0400","categories":["Grad Student Research"],"url":"all-together-now","layout":"post","content":"Recently, Bethany shared a twitter conversation about why “Praxis Program Team” has been listed among the authors of publications and presentations on Prism. As a member of the 2013-14 Praxis cohort, I can attest to the fact that we haven’t yet given as much thought to the question of publication and presentation as previous years have. In our charter, we adopted the “Equal Credit” clause from the 2012-13 cohort’s charter, which was itself a thoughtful reiteration of the same clause in the 2011-12 cohort’s charter . But since ratifying our charter in mid-September, we haven’t taken the time to think critically about what the “Equal Credit” clause means in the long run. As the end of our fellowship year together looms on the horizon, it seems prudent to reflect on what ‘equal credit’ means in our re-imagination of the Ivanhoe Game as a WordPress theme . It seems especially important given that collaborative work is a concept not entirely at home in the academic world in which we, as humanities and social sciences graduate students, have been well-steeped. We started the year with a gargantuan task - to redefine, redesign, and build anew the Ivanhoe Game. To accomplish that goal, however, we had the even greater challenge of attempting to become a cohesive, functional group. We struggled (or, as Francesca more thoughtfully wrote, we took our time) to draw upon all of the intellectual and methodological strengths represented in our cohort. Tensions eased as soon as we broke into development, design, and support teams to start building Ivanhoe, but then we had to deal with communication breakdowns along the way . One thing, however, has become patently clear: the whole of what we have accomplished thus far is certainly more than the sum of our individual contributions. Since I appear obsessed with metaphors in writing about Praxis, I offer one from my previous life as a chamber musician. I played Baroque cello and was often asked ‘why would you ever choose to play basso continuo and not just focus on being a solo concert cellist?’ Of course, one can always produce an appropriately snide response to such a question (‘have you seen the number of solo pieces for Baroque cello?’). But the fact is, my motivations for choosing Baroque cello as my instrument were different. I thought it was absolutely fantastic the way that the basso continuo formed the foundation of all (well, most) Baroque music, and the way individual performers melded into a single instrument in Baroque ensembles was breathtakingly powerful. Every member of a musical ensemble is responsible for practicing and developing his or her individual technique and musicality. But it is when those individuals come together as an ensemble that the performance happens, that stories are told. This, to me, reflects the principle underlying our “Equal Credit” clause, and it is why the re-imagined Ivanhoe belongs to the Scholars’ Lab and Praxis Program Team as a whole. And look how much fun it is to work together! When you close an issue, Eric might even draw you a unicorn fighting Pegasos before crossing off the issue from our whiteboard:"},{"id":"2014-04-08-day-of-dh-2014","title":"Day of DH 2014","author":"purdom-lindblad","date":"2014-04-08 16:26:04 -0400","categories":["Digital Humanities","Grad Student Research","Research and Development"],"url":"day-of-dh-2014","layout":"post","content":"[![Gratuitous shot of the laptop stand prototype Jeremy and I printed. ](http://static.scholarslab.org/wp-content/uploads/2014/04/kraken-300x225.jpg)](http://static.scholarslab.org/wp-content/uploads/2014/04/kraken.jpg) Gratuitous shot of the laptop stand prototype Jeremy and I printed. Yesterday morning, I arrived at the Lab early, grabbed a cup of coffee, and began to work on a series of slides for an upcoming data management for humanities and social science graduate students workshop . As I worked, the Lab slowly, then more quickly, filled with my incredible colleagues, students, and one very adorable baby. Over the course of the day, there was an ebb and flow of questions, answers, experiments, consultations, and a few random cat gifs. Today, the official Day of DH mirrored the feel, if not the same events, of yesterday. I realize this post is in danger of sounding overly-sentimental (and extremely privileged), but I have been thinking a great deal about Miriam Posner’s excellent Commit to DH People, not Projects post, particularly as the Praxis team readies for the Ivanhoe launch. So, I thought I would contribute a few of today’s observed moments of the Scholars’ Lab cultivating people. We are very fortunate to have Spandana Bhowmik visiting the Lab from Jadavpur University in Calcutta . Spandana opened the morning with a presentation of her work. As she walked us through her project, several good questions were raised despite the fact none of us were very familiar with the texts she studies. Engaged listening is something I have noticed in my short four months in the Lab. In the afternoon, the Praxis team gathered as they ready for the launch of Ivanhoe . In the midst of talking out strategies to support the launch, the possibility of Ivanhoe crashing after 3 minutes was raised. Immediately, the response: “Maybe, but what a glorious 3 minutes it will be.” A casual moment reflecting failure was possible, but the discussion then turned to what must be done to release the best working game we could—something Miriam points to in her post, sometimes failure is good, but steps can be taken to protect against failure. I am consistently impressed with how the R&amp;D team set our fellows up to succeed, to gain confidence, and to laugh at and then understand the broken bits. The flip side of this is I am also impressed with how hard our fellows work. All of this is not to sidestep the important discussions around the implicit dangers of the imperative to “ do what you love ;” this post is not really about doing what one loves, but more about seeking out people who are committed to cultivating environments of experimental play, collaborative learning, and fellowship."},{"id":"2014-04-09-announcing-the-ivanhoe-information-website-and-beginning-of-testing","title":"(Digest #6) Announcing the Ivanhoe Information Website and Beginning of Testing","author":"stephanie-kingsley","date":"2014-04-09 10:00:31 -0400","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"announcing-the-ivanhoe-information-website-and-beginning-of-testing","layout":"post","content":"Yesterday we celebrated the Day of DH by preparing for internal testing of the Ivanhoe Game WP Theme.  The entire team is now scurrying to make some finishing touches to the theme, info site, and documentation before our testers begin their games.  Development has been working to stabilize roles and the role journal features, and they have reported success.  The following image indicates Development’s glee at this and similar recent successes: Design continues to reflect on and tweak the aesthetics of Ivanhoe, while Support (now consisting of Francesca and myself) work on the website.  Francesca just finished drafting documentation; see her recent post on the process of walking through Ivanhoe and imagining where users will need guidance.  Also see Veronica’s post reflecting on equal credit, a tenet from our charter; now that we have a product we can begin taking credit for, figuring out how to apportion that credit will start to become a bigger deal, so it’s a good time to start thinking about it. In addition to the typical weekly digest material where I give an update on each team’s activities, this week’s digest brings extra-exciting news: we have an informational website.  To learn about Ivanhoe and start thinking about what it will do, visit our website .  You can also visit our testing space (a Heroku staging app) to view game play as testing (which begins tomorrow) proceeds. Although the Ivanhoe Game WordPress Theme is still in its development, we want you all to be aware that it will very soon be available for download.  Once we’ve spent a few weeks working to improve Ivanhoe based on feedback from our testers, then we will officially launch Ivanhoe version 1.0, and you can start customizing it and playing your own games on it.  (Stay tuned; the official launch date will be announced within the week.) So keep an eye on Heroku to see what’s going on with our testers’ games, and start thinking of ideas for your own.  It will not be long before the Ivanhoe Game returns and is available for everyone to enjoy. Happy Belated Day of DH!"},{"id":"2014-04-09-documenting-ivanhoe","title":"Documenting Ivanhoe","author":"francesca-tripodi","date":"2014-04-09 10:00:22 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"documenting-ivanhoe","layout":"post","content":"In preparation for our launch, Stephanie and I spent some time documenting Ivanhoe to help users navigate our site. For inspiration we visited Neatline’s website  because we felt that their documentation process was spectacular! Doing this was much more difficult than anticipated because I was trying to see the game from a person who had never heard of it before. While the technology is fairly intuitive (i.e. filling out fields is rather easy for most people familiar with WordPress)  why they want to fill out the fields gets back to the theoretical underpinnings of our game. For example - Starting a New Game is pretty easy. Click “New Game” and fill in the blanks (very similar to writing a post for those familiar with Word Press). But when you start a new game one of the fields is “Add Media/Content.” Now  what you do with this field is much more complicated than a simple “how to” might entail. Therefore, we felt it was important to add more than just “fill in the fields.” Here is some supplemental information regarding Add Media/Content field that we created. Add Media/Content:  Arguably one of the most important aspects of the game, the Add Media/Add Content feature allows you to shape the structure of your game. In this field we suggest listing the objective of your game (What do you want players to achieve? Do you want to have rules? Is there a way to win?). There are many ways you can play Ivanhoe. You can start with just the objective of the game and see where players take it. You can insert an image, a video, a music recording, or text and have people make new connections. You can start with a suggested list of roles or have players invent them on their own! The sky is the limit in terms of creativity and functionality so be sure to take some time to think about how you want to play the game before completing this section. Hopefully our documentation eases users into Ivanhoe and facilitates an environment that is easy to feel comfortable in! That being said, we plan on providing access to our GitHub account so that users can constantly provide feedback for how our documentation process could improve."},{"id":"2014-04-11-more-better-breaking","title":"More Better Breaking","author":"scott-bailey","date":"2014-04-11 05:00:35 -0400","categories":["Grad Student Research"],"url":"more-better-breaking","layout":"post","content":"Over the course of the last few months, those of us in the development team have been hard at work writing the code to make the Ivanhoe game function. I thought I’d give a (very) brief look into our development workflow. 1.) Each week, sometimes several times a week, check Github for issues assigned to development. ( Current list here )\n2.) Rank said issues on the whiteboard according to priority, difficulty, and whether we want to just do it to get the momentum boost of being successful, even on something easy.\n3.) After choosing an issue, write out what the issue requires to be fulfilled in natural language on the whiteboard.\n4.) Write out the first glimmers of what code will be necessary to achieve each step on the whiteboard.\n5.) Create a new git branch, write said code, and check to see if it works.\n6.) In the incredibly rare situation that it works on the first go, celebrate wildly, or, stoically proclaim what we need to get done next.\n7.) If the screen comes up blank, lint that php.\n8.) If it returns an error with a line number, be thankful for such a magnificent gift. Fix it.\n9.) If there is no error returned, but the feature doesn’t work, write some code to break something so that it gives you some good information. Fix it.\n10.) When that doesn’t work, try to break it better. Fix it.\n11.) When that doesn’t work, go find Wayne, Eric, or Jeremy. With their help: more better breaking. Fix it.\n12.) Commit and push to Github (which you should have been doing throughout anyway), and close the issue, carefully referencing the commit number.\n13.) Celebrate appropriately. This might involve a unicorn and a pegasus."},{"id":"2014-04-18-podcast-micki-kaufman-on-quantifying-kissinger","title":"Podcast: Micki Kaufman on Quantifying Kissinger","author":"laura-miller","date":"2014-04-18 09:28:33 -0400","categories":["Announcements","Podcasts","Visualization and Data Mining"],"url":"podcast-micki-kaufman-on-quantifying-kissinger","layout":"post","content":"Digital Humanities Speaker Micki Kaufman “Everything on Paper Will Be Used Against Me”: Quantifying Kissinger Scarcity  of  information  is  a  common  frustration  for  many  historians.  However,  for  researchers  of  twentieth-­ and  twenty-­first  century  history  the  opposite  problem  is  also  increasingly  common.  In  contrast  to  scholars  of ancient  history,  who  base  much  of  their  analyses  on  rare  and  unique  relics  of  antiquity,  historians  studying the  ‘Age  of  Information’  (and  the  even  more  recent  period  of  ‘Big  Data’)  increasingly  confront  a  deluge  of information,  a  vast  field  of  haystacks  within  which  they  must  locate  the  needles  -­  and  presumably,  use them  to  knit  together  a  valid  historical  interpretation. While  simply  having  such  a  large  volume  of  information  online  in digital  form  for  researchers  is  valuable,  the  usual  restriction  to  a  web-­based  ‘search’  form  interface  often renders  it  of  limited  use  and  approachability.  As  detailed  on  the  project’s  web  site, Ms. Kaufman’s  work  involves  the  application  of  a  host  of  quantitative  text  analysis methods  like  word  frequency/correlation,  topic  modeling  and  sentiment  analysis  (as  well  as  a  variety  of  data visualization  deisgns  and  methods)  to  historical  research  on  the  DNSA’s  Kissinger  Collection,  comprising approximately  17600  meeting  memoranda  (‘memcons’)  and  teleconference  transcripts  (‘telcons’)  detailing the  former  US  National  Security  Advisor  and  Secretary  of  State’s  correspondence  during  the  period  1969-­ 1977.  This  application  of  computational  techniques  to  the  study  of  twentieth-­century  diplomatic  history  has generated  useful  finding  aids  for  researchers,  provided  essential  testing  grounds  for  new  historical methodologies,  and  prompted  new  interpretations  and  questions  about  the  Nixon/Kissinger  era. Micki  Kaufman   is a doctoral  student  in  US  history  at  the  Graduate  Center  of the  City  University  of  New  York, a  GC-­CUNY  Digital  Fellow,  and a recipient  of  GC-­CUNY’s  Provost’s  Digital  Innovation  Grant  in 2012–2014. Click below to stream the podcast, and you can view the accompanying slides on the Quantifying Kissinger website . As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”http://a1322.phobos.apple.com/us/r30/CobaltPublic/v4/76/c2/ee/76c2ee48-51c6-ac29-fda4-9c60724ec1eb/336-283879797946496247-2014.04.04_kaufman2.mp3”]"},{"id":"2014-04-21-dhf_panel","title":"Criminal Women, Misdirection, and Learning to Listen: A Conversation about the Digital Humanities","author":"purdom-lindblad","date":"2014-04-21 03:50:09 -0400","categories":null,"url":"dhf_panel","layout":"post","content":"Please join us Tuesday, April 22, at 10 AM for the Digital Humanities Graduate Fellows Brunch. Alderman Room 421 Fellows Erik DeLuca, Gwen Nally, and Tamika Richeson share their projects as well as engage in a larger conversation about collaborating around digital projects. Erik investigates the listening networks with “Community Listening in Isle Royale National Park.” His work this year has focused on digital tools to allow listeners to interact with his ethnographic composition. Gwen seeks new approaches to understanding philosophical texts through the use of language processing, topic modeling, and Naive Bayesian Analysis. Her work has dived into markers within the texts that indicate hedging or misdirection with “Processing the Dialogues.” Tamika explores black women’s lawbreaking in Civil War era Washington, D.C. to understand the racial and gendered context in which American criminal law took shape with her project, “Black Women in Civil War D.C.” Tamika seeks to use spatial narrative tools, such as Neatline, to visualize black women’s lives within the city. Join us for brunch. [gallery ids=”10169,10170,10172”] Erik DeLuca : Making music and sound art of all sorts, that entangle algorithmic and intuitive modes of composing, excites me. A major element of my dissertation, “Field(art)works: Paths to Composing,” is an ethnographic composition that explores a deep listening network between a biologist and community of wolf-listening park visitors. I volunteer for both Sensate: A Journal for Experiments in Critical Media Practice and for The Bridge PAI in Charlottesville. Gwen Nally is a graduate student in the philosophy department. She studies Plato and teaches Bioethics. She was a Praxis Fellow in 2013, loves design, and does calligraphy.  Tamika Richeson is a doctoral candidate in History at the University of Virginia. Her work explores black women’s lawbreaking in Civil War era Washington, D.C. to understand the racial and gendered context in which American criminal law took shape. Her interests include nineteenth century social and cultural history, African American Studies, women and gender studies, digital humanities, criminal law, and public history. She currently serves on the Prince William County Historic Preservation Foundation executive board, and the President’s Commission on Slavery and the University. Tamika Richeson has recently been awarded the Woodrow Wilson Dissertation Fellowship in Women’s Studies."},{"id":"2014-04-28-plane-table-mapping-aka-instant-gratification-mapping","title":"Plane Table Mapping aka Instant Gratification Mapping","author":"kelly-johnston","date":"2014-04-28 06:06:53 -0400","categories":["Geospatial and Temporal","Visualization and Data Mining"],"url":"plane-table-mapping-aka-instant-gratification-mapping","layout":"post","content":"“Plane table mapping is the most interesting of all to do.  One can hardly browse through an account of its various operations without wishing to go directly into the field and do them.” - Down To Earth : Mapping for Everybody, 1944 Humans love maps.  Every day in the Scholars’ Lab we help aspiring cartographers navigate the complexities of geographic information systems software.  But software is expensive and requires electricity. What if we could make a cartographic masterpiece appear on an ordinary sheet of paper without installing any software or downloading any data or loading any batteries in our GPS? That’s instant gratification mapping via plane table methods.  So let’s “ go directly into the field and do them ”! We’re talking about old-school cartography.  As early as the 16th-century, writers described the plane table method in detail.  Long before computers and software, early 2oth-century school boys in knickers were taught to map distant church steeples by “screwing your drawing board to a camera tripod”. ![](http://celebrating200years.noaa.gov/foundations/mapping/image2_650.jpg) Plane table mapping from [NOAA](http://celebrating200years.noaa.gov/foundations/mapping/image2.html) Historic resources like Low’s Plane Table Mapping inform our methodology.  Modern interpretations of plane table mapping as art expand the boundaries beyond the two dimensional plane.  A new crop of 21st-century plane table mappers are in the field, among them geography students at Texas A &amp; M University . We developed our Scholars’ Lab plane table mapping workshop with a low-cost do-it-yourself mindset.  For our work surface (or plane table) my Scholars’ Lab colleague Chris Gist attached a plywood table top to a camera tripod.  Rather than invest in an expensive alidade for sighting distant objects, we used the edge of a triangular architect’s scale.  A second-hand compass pointed to magnetic north.  A smartphone app helped level the table, as will a marble.  A set of keys on a string served as a plumb line.  A pencil and eraser completed our mapping toolkit. In our workshop introduction we covered the basics of establishing a baseline, then sighting from multiple stations toward each visible landscape feature to create intersecting lines.   The map began to emerge on the paper without taking a single distance measurement.  Lines and angles were drawn using only visual methods.  If we could see it, we could map it from afar even if the feature was behind a locked gate, across a busy street, or otherwise inaccessible. We’re beginners, so in our one-hour workshop we had our hands full making a simple map on level ground.  We didn’t expand into the complexities of plane table mapping on uneven terrain. Even on flat ground we made mistakes.  We struggled with leveling, sighting, slipping scales, and tilting tables.  We learned that when something is wrong, it’s quickly apparent while still on-site and can be corrected.  Mistakes are discovered and corrected in real-time, not later back in the office. This new cohort of plane tablers commented on their experience: This was fun.  I had no idea how maps were made before GPS. I’ll use this to illustrate to my students how errors can easily creep into any data collection exercise. I’m trying this with my kids this weekend.  It will get us outside and they’ll love it. I’m seeing the world with new eyes. Happy mapping! Thanks to Schaeffer Somers, University of Virginia School of Architecture, for the equipment loan."},{"id":"2014-04-29-connect-create-inspire-the-ivanhoe-game-returns-2","title":"Connect, Create, Inspire: the Ivanhoe Game returns!","author":"stephanie-kingsley","date":"2014-04-29 08:18:53 -0400","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"connect-create-inspire-the-ivanhoe-game-returns-2","layout":"post","content":"The Scholars’ Lab Praxis Fellows are thrilled to announce the beta release of the Ivanhoe Game !  Ivanhoe is a collaborative role-playing game in which players make critical interventions in a text, cultural object, or topic to help them learn.  Ivanhoe is about connecting ideas, crafting new interpretations, and inspiring creative scholarship. The Ivanhoe Game is now available as a WordPress Theme!  It is the practicum project of this year’s Praxis Fellowship, an intensive year-long program in the UVa Scholars’ Lab that trains humanities and social-sciences graduate students in digital-project management, design, development, troubleshooting, and distribution.  Fellows work together to compose a project charter, wire-frame ideas, give dynamic conceptual pitches, and write code to build a digital humanities tool, all the while being mentored by the brilliant and ever-patient Scholars’ Lab faculty.  Fellows blog throughout the year, reflecting on their triumphs and learning experiences and sharing them with the wider DH community.  See posts from Veronica and Scott about the perplexities of coding with PHP, alternative design ideas from Zach, reactions from Eliza at first learning version control in GitHub, thoughts from Francesca about the nature of game play and collaboration, and my own project-management musings as I discovered my role within the group.  Praxis is an innovative approach to graduate training which emphasizes collaboration and iterative learning: Fellows work together in a trial-and-error process to discover how to take a project from being a mere idea to a useable product ready to be introduced to the world. Our project, the Ivanhoe Game WordPress Theme, is now ready to take its first steps into the world of digital scholarship and pedagogy.  This tool is a vibrant reimagining of a game originally developed in the UVa SpecLab .  (For more on Ivanhoe’s history, see “ Designing Ivanhoe,” by Johanna Drucker; “ IVANHOE: Education in a New Key,” by Jerome McGann; and “ Subjectivity in the Ivanhoe Game: Visual and Computational Strategies,” by Bethany Nowviskie. ) The Ivanhoe Game can be played on any type of cultural object or topic.  In Ivanhoe, players assume roles and generate criticism by pretending to be characters or voices relevant to their topic and making moves from those perspectives.  We think of these moves as interventions—a text or work is not stable but, rather, dynamic and ever subject to interpretation by its readers.  Furthermore, these interventions are reflective and deliberate: they are “self-conscious acts of interpretation,” as Scott so concisely and perfectly puts it.  Ivanhoe thus provides a way of delving into a subject while also maintaining a firm focus on the players themselves. A few features of our Ivanhoe distinguish it from the original.  Whereas the SpecLab Ivanhoe Game was largely text based, we have designed Ivanhoe to accommodate subjects from any discipline and moves with all manner of media.  Another element of Ivanhoe that the Praxis Fellows have found particularly provocative and have given special attention is the network of moves that these games generate.  From one move can spring forth a multitude of other moves.  We have programmed ways of linking moves together as “Source” posts and “Responses” to emphasize that network.  Lastly, we have always seen Ivanhoe as something which should above all be accessible, so that the concept of Ivanhoe may be freely adapted by as wide a user base as possible.  We have built it as a WordPress Theme, which anyone with a WordPress page may easily apply and use for whatever purpose they like. Now that you have heard just enough about Ivanhoe to be a little baffled but exhilarated nonetheless, we invite you to try it out and explore its possibilities.   To download the Theme itself, visit our informational site .  There, you will find out more about us and Ivanhoe, as well as documentation to assist you in using Ivanhoe.  If you wish to try Ivanhoe but do not have a WordPress page, visit our testing app, hosted on Heroku and available for a limited time for Ivanhoe trial.  As Ivanhoe is a work in progress, we would love your feedback, so feel free to post any issues you find to our GitHub page . Thanks, and we hope you enjoy connecting, creating, and inspiring with Ivanhoe. Stephanie Kingsley, for the 2013-2014 Praxis Fellows"},{"id":"2014-04-30-a-few-uses-for-ivanhoe","title":"An Ivanhoe example and guidelines for getting started","author":"stephanie-kingsley","date":"2014-04-30 10:00:30 -0400","categories":["Grad Student Research"],"url":"a-few-uses-for-ivanhoe","layout":"post","content":"As I mentioned previously, any Ivanhoe game can be played on any topic.  Being interdisciplinary ourselves (Classical Archaeology, English, Religious Studies, and Sociology), our group has naturally tended toward interdisciplinary games: the suffragette journalism game, the Elgin Marbles debate game, a sci-fi game, etc.  We all had a great time on these games, and I think it was because we could all find something to relate to in them within our respective fields.  However, this intense interdisciplinarity often made finding topics for games difficult, and we all agreed that a shared knowledge or experience base would help players in conceiving and playing an Ivanhoe game.  This seems to have been borne out in Ivanhoe testing ; we had many testers volunteer, but few ended up playing extended games.  I think the reason for this is that our volunteers were individuals from different disciplines who signed up over the internet and had no face-to-face time.  Given this, I suspect that testers had a difficult time connecting with one another and having shared interests. If you’re interested in playing Ivanhoe, I would suggest gathering either a group of colleagues (shared knowledge base) or friends (shared experience) and selecting a topic which you all find exciting.  Personally, my primary field of interest is book history and textual studies.  A game which I recently came up with was a game on Poe’s “The Raven.”   In this game, one person would be Poe, one person his editor, another his compositor (type-setter), etc.  Each week, “Poe” would make a move which would contain only the text of one stanza from “The Raven.”  Then, the editor would copy the text out of Poe’s move, change it as he saw fit, and that would be his move.  This would continue, until all roles had messed with Poe’s text, and the final versions is what the published result would be. In order to play this game, players would need to learn about Poe’s publishing practices, the publication history of “The Raven” specifically, the publisher’s house style, etc.  Or, players might want to play such a game from a more theoretical standpoint: the editor might decide to take a Freudian stance and edit Poe’s text from that perspective. The multimedia possibilities for this game are marvelous.  Players could post videos demonstrating typesetting, images of later editions and illustrations, a sound clip of an aria the editor might have heard at the opera the evening before and which may have impacted his work on Poe’s poem, etc.  The sky’s the limit with Ivanhoe! This is just one example, but keep an eye on our blog this week and hear from my fellow Praxers about their ideas!"},{"id":"2014-04-30-all-systems-go-we-think","title":"All systems go! (we think...) ","author":"francesca-tripodi","date":"2014-04-30 06:00:32 -0400","categories":["Announcements","Grad Student Research"],"url":"all-systems-go-we-think","layout":"post","content":"Yesterday at 1PM we launched Ivanhoe 1.0. We are excited about our progress and hope you can take some time to download the theme to your own WordPress accounts. We have made documentation readily available for those new to Ivanhoe and we hope users will add to this space ( via GitHub ) so that we can continue to create a better product. As I reflect back on the last year I wanted to share a few things…. My favorite moment: playing Apples-to-Apples during a holiday get together. I feel like the experience bonded our team in a cool way and renewed my enthusiasm for integrating games at work. One can never predict the unexpected outcomes of play. What I learned about myself:  I can’t do it all, but that’s ok… especially when you have a team of amazing people ready to step in and help you out. How I would integrate Ivanhoe into the classroom:  A theoretical concept in Media Studies (among other disciplines) is the idea of “ intertextuality ” - or how we make meaning of one type of media (a text, movie, television show, etc) based on our already existing knowledge of another form of media. I created a game in Ivanhoe using this concept. I uploaded a film from YouTube and inserted the following directions: 1. Watch one of the videos (original or move) available on the game. 2. Create a role based on the video you watched. 3. Make a move (or respond to a move) by inserting in another video from YouTube (preferably under 3 minutes) whose meaning is tied to your role AND the video you watched. Use the rationale section to justify the connection.  4. The game ends when no more connections can be made. I look forward to seeing how others incorporate the tool and the games users end up concocting along the way…."},{"id":"2014-05-01-dialogical-code-and-the-adventure-of-pair-programming","title":"Dialogical Code and the Adventure of Pair Programming","author":"scott-bailey","date":"2014-05-01 05:25:28 -0400","categories":["Grad Student Research"],"url":"dialogical-code-and-the-adventure-of-pair-programming","layout":"post","content":"Near the end of last semester, as the developers of SLAB taught us Praxers to write code (PHP in this case), pushing us to learn different conditional loops and such through repeated problem solving exercises, they also encouraged us to work in pairs or in even larger groups. Coding is not something you do alone, they said. I’d say, rather, that really good, efficient, and fun coding is not something you do alone. Throughout this spring semester, as Veronica and I have written much of the PHP that drives the Ivanhoe WordPress Theme, we have pair programmed. We have sat together in the grad lounge of the Scholars’ Lab and we’ve scrawled our ideas across three whiteboards (front and back), planning our next steps, prioritizing development GitHub issues, sketching out the logic of the next piece of code we’re going to write. And then we’ve sat together, or sometimes stood or paced, and written the code, one of us typing, the other pouring through documentation, checking syntax, or suggesting a different approach. It’s been a heavily dialogical process. A substantial amount of the back-end code of Ivanhoe is the surface hiding hours and hours of conversations. It’s been brilliant and fun and one of the best parts of my year. As we’re getting close to the end of Praxis (and we’re still working, still coding, still trying to get at least the foundation of some of the more advanced features in), I’m thinking a lot about how much of our personalities or styles of thinking have been embedded in the code itself. The code is a literary object, produced iteratively by a group of people. Could you tell which of us is which just looking at it? Could you read the style of the comments, the organization of the conditionals, or the placement of the variable definitions and know something about us? About me? I suspect you can, but I also think it probably gets harder as the code develops and gets closer to release, as it is iteratively picked over, cleaned up, made to fit standards and requirements. All of which are good - clean, efficient code is beautiful. But so is the archive of our work on GitHub, which is messy and bears the impressions of our thinking (including all the commented-out code I refused to delete until the end, just in case we ever needed it again - silliness, given what GitHub does). Our commit history there is the record of our conversations, between all of us in the Scholars’ Lab and Praxis who have put time and effort and caring into this work, not just those of us coding."},{"id":"2014-05-02-fluffy-tree-the-future-of-ivanhoe","title":"Fluffy Tree: The Future of Ivanhoe","author":"veronica-ikeshoji-orlati","date":"2014-05-01 22:30:30 -0400","categories":["Grad Student Research"],"url":"fluffy-tree-the-future-of-ivanhoe","layout":"post","content":"On Tuesday, we launched Ivanhoe 1.0 . As Scott noted yesterday, much of the PHP which drives our Ivanhoe Wordpress theme is the product of our many hours spent in the Scholars’ Lab pair-programming. Now that Ivanhoe 1.0 is out in the world, Scott and I have settled down to tackle the next feature: fluffy tree. (Minotaur Scott &amp; Ammit Veronica tackle Fluffy Tree Bunny) Fluffy tree will enable users to respond to multiple moves at once, thereby creating a broader network of connections within each game. The feature name arose from a conversation we and the Scholars’ Lab developers had early in the semester about how to conceptualize the Ivanhoe game within the blog-based structure of WordPress - we didn’t want a ‘vine,’ with one move linking to the next in a linear/hierarchical fashion, but a ‘fluffy tree,’ in which moves intertwined with one another in an organic fashion. To write the code for fluffy tree, we have had to take a jumbled mess of potential connections and turn it into a clear, linear progression of events. This has required a lot of white-board time and adding JavaScript to our programming repertoire. While we do not yet have fluffy tree working, we have definitely figured out how to work on it - and really, knowing how to work collaboratively is a goal in itself."},{"id":"2014-05-02-go-litel-boke","title":"\"Go litel boke!\"","author":"zachary-stone","date":"2014-05-02 07:05:02 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"go-litel-boke","layout":"post","content":"“Go, litel boke, go, litel myn tragedye,” cries Chaucer at the close of his Troilus and Criseyde . As he hands his book off to the vicissitudes of manuscript production—memorably evoked in his poem to Adam Scriveyn[1]—the persistent fear that scribes may “miswrite the” or “mysmetre” the work troubles the poet’s mind. Iteration is a problem. As is influence. Chaucer exhorts his book to be humble, not to envy Virgil, Homer, Ovid, Lucan, or Statius; rather, this little book is to kiss their steps. And yet, Chaucer, we know, surely did not “kis the steppes” of Boccaccio, his putative source. Rather, he audaciously re-writes the Trojan War as a romantic tragedy of Boethian proportions. Whereas his erstwhile follower John Lydgate loves the politics of Troy in his monumental Troy Book, Chaucer seems enamored with the politics of love. Questions of who “miswrite” what abound and increase. Writing in response to Chaucer’s seemingly unflattering portrait of Criseyde, Robert Hennryson’s Testement of Cresseid claims to pick up where Chaucer leaves off. Troy endlessly enchants English writers, and in the Trojan tradition we “so gret diuersite / In Englissh and in writyng in oure tonge.” Troynovant—whether London or not—rises endlessly in English. “But ȝet to purpos of my rather speche:” what has this to do with Ivanhoe? Well. As we’ve worked on Ivanhoe, I’ve commented about the ways in which DH development reminds me of manuscript production.[2]  What strikes me about Ivanhoe is the way in which it embraces the most unruly, anxiety-inducing aspects of medieval textuality—the possibility of endless revision, perpetually unbound books. Ivanhoe is a broad concept, and we have discussed many definitions of it over the past year.  For me, Ivanhoe has become an alternate way of making meaning, a mode of communication that exploits ambiguities and varieties of meaning to create an open landscape in which “sentence” and “solas”—to reach back to the Chaucerian well—exist in dynamic tension and continually generate unexpected readings. Perhaps Chaucer isn’t that worried at the end of Troilus. Perhaps he’s actually in on the joke. He exhorts his book to “subgit be to alle Poyesye,” a game Chaucer clearly understands to consist of adaptation and expansion. His prayer is simply “That thow be vnderstonde” but he leaves that understanding open. The book, once sent out into the world, takes on a life of its own. Tuesday we launched Ivanhoe, our own little “go litel program, go, oure tragedye” moment. We did so with the understanding that were giving up absolute control over Ivanhoe, and I think we share Chaucer’s prayer—to be understood. So. How should you use this little thing we launched into the world? Honestly, part of Ivanhoe is figuring that out yourself. There are “so gret diuersite” of ways to do Ivanhoe—as many as there are to do Troy—that really it’s up to each user. But maybe, it might be fun to return to Troynovant, or rather, to rebuild it. Go, litel boke, go, litel myn tragedye, Ther god thi makere ȝet, er that he dye, So sende myght to make in some comedye; But litel book, no makyng thow nenvie, But subgit be to alle Poyesye, And kis the steppes where as thow seest space Uirgile, Ouide, Omer, Lucan and Stace. And for ther is so gret diuersite In Englissh and in writyng of oure tonge, So prey I god that non myswrite the, Ne the mysmetre for defaute of tonge. And red wher-so thow [MS ȝow] or elles songe, That thow be vnderstonde, god I biseche. But ȝet to purpos of my rather speche –[3] [1] Geoffrey Chaucer, “Chaucers Wordes unto Adam,” &lt; http://www.bartleby.com/258/61.html &gt; [2]  http://www.scholarslab.org/digital-humanities/on-stemmatics/ [3] Geoffrey Chaucer, Troilus and Criseyde, ed. Barry Windeatt (London: Longman, 1984),  &lt; http://quod.lib.umich.edu/c/cme/Troilus/1:5.28?rgn=div2;view=toc &gt;, V.1786-99."},{"id":"2014-05-05-ivanhoe-and-imaginative-analysis","title":"Ivanhoe and Imaginative Analysis","author":"elizabeth-fox","date":"2014-05-05 06:00:14 -0400","categories":["Grad Student Research"],"url":"ivanhoe-and-imaginative-analysis","layout":"post","content":"When I wasn’t working as a Praxis fellow this year, I taught first-year writing.  In the class, we tackled the course subject—ghost stories—through a variety of topical lenses, looking at horror stories and films, web comics, and ghost hunting TV shows.  Although the course focused on argumentative writing, at the students’ request, I ended up adding a creative component: students could, for extra credit, write and submit their own ghost story.  I assumed the assignment would be simple and fun, allowing students to let off some creative energy at the end of the semester.  What I didn’t realize at first is how helpful these stories would be to students’ engagement with the material.  Most students did not just write ghost stories; they wrote works that responded to a semester’s-worth of texts, drawing on the styles and formats that we had considered during the past few months.  They incorporated the common tropes of ghost stories, nodding at—and poking fun at—the clichés of the genre.  They creatively explored the difference between a ghost story (acknowledged fiction) and a haunted encounter (believed to be true), allowing the conventions of each type of story to dictate the structure. Although not framed by digital media or the rules of gameplay, the experience couldn’t help but remind me of Ivanhoe .  Here, once again, I saw the importance of creative intervention, allowing students to engage with texts in ways that enable imaginative analysis.  Such an approach, although seemingly outside the realm of argumentative writing, enabled students to synthesize topics and to showcase their understanding of course materials in ways that felt fun and dynamic.  Projects like Ivanhoe—which allow for a sustained creative endeavor and for interactions among multiple students—pointedly foster this type of creative dialogue.  In doing so, Ivanhoe allows for imaginative analysis on a grand scale, inviting students to join in a conversation within, rather than about, a text.  The result, as I discovered, is not just an increased understanding of the material, but also a greater sense of involvement with it. It’s one thing to assess a collection of texts; it’s quite another to feel yourself a part of them. The experience also reminded me of the close connection between my efforts on the Praxis team and my work as a teacher and a student at the university.  At times during the year, it’s been a challenge for me to see my various roles functioning symbiotically, especially when work on Ivanhoe came into competition with lesson planning or preparing for my oral exams.  It’s fortunate that this end-of-the-semester experience has reminded me of the bond among all of these scholarly pursuits.  My teaching, my research, and my work on Ivanhoe all inform each other, suggesting new ideas and approaches that (like any good Ivanhoe project) make key interventions in my thinking.  Just as my research and teaching framed my ideas for Ivanhoe, so my understanding of Ivanhoe’s values ultimately informed my teaching.  So now, as the semester draws to a close, I can consider with pleasure all the things that I’ve learned as a Praxis fellow, and all the ways that my time here will continue to influence by work in the years ahead."},{"id":"2014-05-05-praxis-is-about-people-reflections-after-the-launch","title":"Praxis is about people: reflections after the launch","author":"stephanie-kingsley","date":"2014-05-05 10:00:21 -0400","categories":["Grad Student Research"],"url":"praxis-is-about-people-reflections-after-the-launch","layout":"post","content":"When I initially drafted the Ivanhoe launch announcement, my goal was to make it communicate in as concise a fashion as possible what Ivanhoe was and where people could download it and learn more.  I completely forgot that Ivanhoe has been as much about us, the Praxis Fellows, and our learning, as about software development.  It wasn’t until Bethany pointed out that everyone might not know who we were and I might want to include a paragraph describing the program.  It then occurred to me: just as Ivanhoe is ultimately about the players–encouraging self awareness through role playing–Praxis, too, is about the players. I am dazzled when I think about how our group has evolved over the course of the year.  Those of you who have been following our blog will have seen countless posts about teamwork.  Francesca’s post “ Forming, Norming, Storming, and Performing,” a reflection upon Bruce Tuckman’s theory of group development, was in many ways a response to our experience drafting our charter.  That was a turbulent time for our team, as it required making decisions about the somewhat nebulous question, what did we want to get out of Praxis?  Then, the next difficult question: what did we want our Ivanhoe to be?  A series of heated debates ensued, until we began drawing out our ideas, which as I discussed in “ Stephen Covey intervenes in wire-framing Ivanhoe,” greatly assisted our communications.  The second we began putting ideas on paper, conversation flowed better, and we started being able to make decisions more quickly.  Shortly after our concept pitches in November, Veronica mused on our group dynamics, highlight several problems in her post “ Sticky Situations: Lessons in Group Cohesion .”  She targeted not having defined roles within the team and not knowing how to articulate our ideas to one another as major problems. Once we all had our roles, teamwork went a lot more smoothly.  Everyone had an individual purpose, and beginning in December our posts became much more skills oriented.  Several posts by Veronica and Scott, our developers, focused on coding and breaking things (clearly, this self-reflection was helpful, as the breakages were fixed and we now have a working game), and posts by our designers soon focused on learning CSS. As project manager, I too had more specialized tasks, although it would take a bit longer for me to get a clear picture of what exactly my job entailed.  This process entailed learning important lessons, like how not to create more work for the team than was necessary, and how not to micromanage .  I was confused about my job: I thought I was supposed to be managing the team, when really, I needed to be managing the project, which at that point required me to be chief publicity person, keeper of the timeline, and stewardess of the Ivanhoe vision. Our division of labor and assumption of roles certainly increased productivity, but it was most important for how it helped us get along as a group.  Being busy graduate students, of course we were still stressed, but personal group stress was much lower once we all knew what we were doing.  We even got to the point where we rearranged tasks to fit better with individual members’ personal lives, and workflow and communication continued to improve.  We knew we had to continue getting things done, but we would move forward only in the best way possible for our members. My conclusion from all this is three-fold: 1) that human beings always function better when they have defined tasks before them, 2) that the development and happiness of those human beings are more important than any project itself, and 3) that the story of our Ivanhoe is also the story of our Praxis cohort.  In Ivanhoe, players keep role journals to reflect upon the evolution of their roles.  This blog is our Praxis role journal, and our learning experience this year has been our Ivanhoe game.  It thus seems appropriate to closely follow the Ivanhoe launch announcement with a photo gallery representing the great times we’ve had together this year: [gallery ids=”9422,9592,9573,9586,9581,9588,9593,9560,10253,10252,10255,10254,10256,10261,10258”] Thanks to the entire Praxis team, Fellows and SLab mentors, for a wonderful year!"},{"id":"2014-05-08-welcoming-ammon-and-scott","title":"Welcoming Ammon Shepherd and Scott Bailey","author":"wayne-graham","date":"2014-05-08 04:39:34 -0400","categories":["Announcements"],"url":"welcoming-ammon-and-scott","layout":"post","content":"We are thrilled to announce two new additions to the Scholars’ Lab team. Scott Bailey will join our R&amp;D; group later this month and Ammon Shepherd will join us in July, both in the roles of Digital Humanities Software Developer. Ammon Shepherd joins us from the fabulous Roy Rosenzweig Center for History and New Media where he is currently  Associate Director of Technology, responsible for overseeing CHNM’s vast server infrastructure. He is  finishing his PhD in history at George Mason University and has taught courses in Western Civ and Public History at Mason and Arizona State. Ammon’s dissertation is entitled “ Nazi Tunnels: German Factory Dispersal Projects of World War II .” In his new role at the Scholars’ Lab, Ammon will be working at the nexus of software development and server infrastructure. He will be contributing not only to the numerous open source projects the R&amp;D; group maintains, but also helping with our ongoing work on GIS data discovery, the development of workflows for a geospatial “head” for the Hydra Project, in collaboration with Stanford University, and helping to support the server infrastructure that houses these projects and our collaborations with UVa faculty. We’re equally excited that Scott Bailey will be joining us shortly. Scott was one of our 2013-2014 Praxis Program Fellows, who pushed a lot of excellent code to the source of the program’s recently released Ivanhoe WordPress theme, meant to enable collaborative criticism through role play. Scott is finishing his Ph.D. in Philosophical Theology at UVa with a dissertation that examines “vulnerability as a locus of dogmatic reflection” through lenses of continental philosophy and neuroscience. Scott has taught in the Engineering School and been heavily involved in instructional technology support at UVa. In Scott’s new role as a DH Developer, he will be helping maintain and develop our numerous plugins for Omeka (many Neatline -related) as well as collaborating with scholars and Library staff on maintaining and updating numerous UVa faculty projects. Both Ammon and Scott will be contributing to the other missions of the Scholars’ Lab, including graduate student mentoring in our DH Fellows and Praxis programs, as well as participating in other experimental humanities initiatives and pursuing independent research. We are really excited to welcome Ammon Shepherd, his wife Jessica, and their entire family, and Scott Bailey and his partner Karen to the Scholars’ Lab family."},{"id":"2014-05-09-washington-and-lee-trip","title":"Washington and Lee Trip","author":"brandon-walsh","date":"2014-05-09 05:52:22 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"washington-and-lee-trip","layout":"post","content":"Cross-posted at  my personal website . Last week Sarah  and I drove to Washington and Lee University as part of a new collaboration  enabled by a grant from the Associated Colleges of the South. As part of the endeavor, Scholars’ Lab fellows are guest teaching pieces of an Introduction to Digital Humanities course. Our task, in particular, was to co-teach for a day on the topics of project management and software development. While we each took part and taught in both conversations, Sarah took the lead on the former topic and I took the latter. I can’t rave enough about the experience enough, so I’ve organized my thoughts into three sections below. Undergraduates + Digital Humanities = Dynamite I am endlessly delighted by the reactions of undergraduates when they get introduced to the digital humanities. In virtually every case, I have encountered students hungry to learn the material. The W&amp;L students were no exceptions. We found students ready to learn, eager to participate, and wiling to ask hard questions about the affordances and limitations of the field. You can find reflections by the students on their course blog . What’s more, the Washington and Lee students stand poised to make real contributions to digital scholarship. They have worked up some really interesting projects on the history of coeducation at W&amp;L  and on the changing vision and reality for Robert E. Lee’s Chapel on the university grounds . Co-Teaching Sarah and I work well together, and we have presented together in the past. But we had not taught together before the Washington and Lee trip. Full disclosure: I adore everything about co-teaching. It immediately disrupts the one-way transmission of information from the instructors to the students and forces the conversation to be more collaborative; co-teaching allows you to occupy simultaneously and more obviously the dual roles of student and teacher. It takes the pressure off any one person to keep the ship sailing smoothly, which empowers and enlivens the conversation. Co-teaching seems especially well-suited to the digital humanities, which value collaboration and play. Seminar discussions and workshops are different from working on teams to build projects, but co-instructors can make the experience a bit more lab-like, a bit more collaborative. Teaching DH! It is one thing to learn and practice digital humanities. It is another thing entirely to turn around and help others do the same. I have only really been hacking away for two years now, so I felt a bit unqualified to talk down software development as an invited speaker. I tend to assume that the Scholars’ Lab has a better sense of my own abilities than I do in most cases, though, and the invitation to W&amp;L was no exception. The practice of putting together presentations on project management and software development was incredibly empowering. It helped me to have more confidence in myself as a digital humanist. No longer does the prospect of teaching an introduction to digital humanities course appear to be a vague and nebulous question mark. I now know that I could do it, because I have already done so in part. I also have a better sense of my own developing pedagogy of digital humanities. Opportunities to teach digital humanities like this, to perform with no net, are rare. You teach to learn, and this is as true in the digital humanities as it is anywhere else. I learned a great deal from the bright undergraduates at W&amp;L."},{"id":"2014-05-13-on-co-teaching-and-gratitude","title":"On co-teaching and gratitude","author":"sarah-storti","date":"2014-05-13 08:06:32 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"on-co-teaching-and-gratitude","layout":"post","content":"This post developed out of a response to Claire Maiers’s comment on Brandon’s post from last week about our co-teaching venture at Washington and Lee University. She asked us to go into a little more detail about how co-teaching actually worked for us: how we planned class time and decided who would lead what and when. I do try to answer that below, but I invite Brandon and any other practitioners of co-teaching to weigh in via comments. As I wrote this, I also found myself reflecting on how lucky and privileged we were to be afforded this opportunity in the first place. And so I’d like to take this opportunity to publicly thank Purdom Lindblad, all the fantastic people in the Scholars’ Lab, Sara Sprenkle and Paul Youngman at W&amp;L, and Brandon, too, for taking a chance on me. Thanks also to the amazing students at W&amp;L for their responsiveness and enthusiasm. This co-teaching venture was one of the highlights of my year. Before we did anything else, Brandon and I met with Purdom to discuss the kinds of things we wanted the students to get out of discussions about project management and software development. Because Brandon and I are products of the Praxis Program, I think we both (more or less consciously) modeled these desired results on the things we had created during similar Praxis lessons: we wanted the students to make things, not just to talk about things. After that initial conversation, we decided to split primary responsibility for the topics: Brandon worked up a PowerPoint slideshow that conveyed his basic lesson outline for software development (including prompts for activities and discussions) and I did the same for project management. I should note that for me at least the conversation we had beforehand was very helpful here: I always like to backwards plan lessons, beginning with where I want the class to end up, so discussing “deliverables” for a lesson on project management with Brandon and Purdom before I started planning helped me shape my lesson outline. We emailed the slideshows to each other the day before our class so that we could internalize the basic narratives each of us wanted to develop over the course of the day. Finally, in all honesty, we just encouraged each other to interrupt, to add on, to redirect discussion, and to otherwise productively contribute to the lesson that was not “ours.” Sometimes that meant one of us wrote student responses on the whiteboard while the other talked, and sometimes that meant one of us contributed an example or a question to the other’s talking points, or pushed back against student comments. We both spent time in working groups with the students, helping them draw wireframes and design charter drafts. We kind of played it by ear. We had four hours of class time to fill, which did seem kind of terrifying to both of us at first, and partly as a result of this we planned more activities than we actually had time for. But I think the most important element of our plan was flexibility: we knew what our goals were, and we each had a map of sorts to get us there, but we were both ready and willing to throw things out the window on the way—and we did. Teaching has always seemed much like improv comedy to me: you have to run with what’s working in the moment. It’s possible that not every teacher feels this way, but I think the fact that Brandon and I were both prepared to diverge from The Plan was critically important to our success. I’ll close this by quoting the last sentence of Claire’s comment: “I imagine [planning for co-teaching] is easier when the teachers already have a rapport.” This seems exactly, precisely right to me. As Brandon mentioned last week, while we had never taught together before this trip, we had recently planned and delivered a two-person presentation. But additionally, and importantly, I have observed Brandon’s teaching, and he has observed mine. We also regularly talk about teaching and about how simultaneously challenging and awesome it is. We knew what the other person was likely to feel okay with throughout the day, which made everything nearly stress-free. This is all to say that I think rule #1 of co-teaching is: know thy partner(s) in crime. Rule #2 is: come to an agreement about what, as a team, you want to deliver to your students at the end of the day. Brandon and I had excellent co-teachers for models in this regard (here’s looking at you, Bethany, Wayne, Jeremy, Eric R., Eric J., David) and I hope they and others feel welcome to add to the conversation. Thanks for the question, Claire!"},{"id":"2014-05-15-one-teach-one-drift","title":"One Teach, One Drift","author":"ed-triplett","date":"2014-05-15 11:53:02 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"one-teach-one-drift","layout":"post","content":"Like Sarah, Brandon and Gwen, I also drove over Afton Mountain to teach at Washington &amp; Lee a couple of weeks ago. As I played peek-a-boo with the trucks on I-64 on that rainy, foggy morning I must admit I gained a bit more respect for Wayne Graham’s daily commute – but that is a story for another time. In this post, I want to hop on to Brandon and Sarah’s discussion about team teaching and add some reflections on my experience at W&amp;L. I flew solo on this teaching mission, but as I expected would happen, toward the end of the day-long lesson I envied Brandon and Sarah’s ability to support each other as co-teachers. The tools &amp; processes I was introducing to the students were things I have taught before – Photogrammetry &amp; 3D modeling in Google SketchUp – but in the past it has been a team-teaching experience. Before I talk about teaching at W&amp;L, I want to describe two team-teaching experiences that I had fluttering in the back of my mind before and (especially) after I trekked over Afton Mountain. Last year I worked with Wayne Graham from the Scholars’ Lab and Will Rourk from the Digital Media Lab to guide Prof. Louis Nelson’s graduate and undergraduate students through a semester-long photogrammetry project . Wayne and I essentially “discovered” photogrammetry together a couple of years ago, and we worked our way through several different software packages and capture methods by running experiments and – more importantly – talking regularly about the results. Consequently, when it was time to instruct others, we both knew the pitfalls, and we always had backup if a student needed individual attention during a class-wide demonstration. I have heard this method described as “ One Teach, One Drift ” and I believe it is the optimal method for teaching software. Last Summer, as part of a 3-week long NEH Summer Institute on 3D Visualization of Humanities Heritage I was able to “drift” for a good friend and former IATH colleague Chad Keller in his “Intro to Trimble SketchUp” and his “Intro to 3D Studio Max” courses. My role as the “drifter” was completely spontaneous. The SketchUp course in particular was a daunting task for Chad, because his students were all college instructors, and at this early stage of the institute, most of them had never navigated in a 3D space before, let alone attempted to model a scale building. For the record, SketchUp is probably my least favorite 3D modeling software. I am far from an expert in it, but as I watched Chad’s demonstration slow down as different students missed a step in the complex process I decided to just stand up and “catch up” the stragglers so that Chad could continue without stopping every 30 seconds. By itself, Chad’s step-by-step tutorial was so good, that I unabashedly adopted it for my second demonstration at W&amp;L. Still, I knew that unlike Chad’s experience at the NEH institute, I would not have a drifter, so I decided to model a “finished” version of the tutorial the day before. There is no substitute for experience when it comes to staying calm through a potentially chaotic demonstration. Knowing how people will react when they feel they are getting left behind, and being aware that it is impossible to assemble a room full of people with the same visual &amp; technological acuity becomes a well of patience when hands shoot up, inarticulate groans break out, or students restlessly move ahead while you are helping individual strugglers. After witnessing some of these completely normal and somewhat unavoidable issues in the past, I expected to see it again at W&amp;L – especially given that I was “alone” on this mission. As it turned out, the W&amp;L students had been so well prepared by their professors before I arrived that they displayed a truly rare level of patience, maturity and humor. It sounds simple, but assembling a class full of students who “get” that they are being introduced to something new – that they will not emerge from the end of a demonstration with expertise that can only come from practice – is exceptional. Mad props to Paul Youngman and Sarah Sprenkle for nurturing an experimental frame of mind in their students, and kudos to the students themselves for rolling with me as we checked out 3D modeling &amp; photogrammetry. In retrospect, I can’t believe that things went as smoothly as they did at W&amp;L. Without a “drifter” to help keep the tutorial moving, I had no delusions that all the students would end the day with a completed model of a church – but I was pleasantly surprised by our progress. I’m also not going to say I was as serene as a yoga instructor when we noticed that the toolkit interface for SketchUp on Mac – which all of the students had – was completely different than the one I have always used on PC. Fortunately, the IQ Center at W&amp;L was quick on the draw. In a swift and decisive move, they switched out the students’ personal Mac laptops with a bank of PCs and we forged ahead confidently. The only thing they could not fix was the fact I woke up with a cold and had to croak my way through the last 30 minutes of class. I am proud to say that there was an audible cheer when both sides of the room successfully cut out windows for their scaled 3D church at the end of the day. It bears repeating that there is no substitution for experience, and I learned a lot from my third time teaching Trimble SketchUp. As I walked to the garage and spoke to Sarah Sprenkle about how the day had gone, I was a little worried that I had set one of the groups of students on a very difficult path. The week before I had worked with this group at the Scholars’ Lab on refining their idea for a DH project, and they seemed excited by the idea of building a 3D model of Lee Chapel. After that meeting, I decided to cut the photogrammetry lesson I planned in half and run with Chad’s SketchUp tutorial. Yesterday I saw the model of Lee Chapel the students created on their project page here, and I was floored by their execution in such a short time. 3D modeling software has a notoriously steep learning curve, and I have seen professional educators thrash through it with what can best be described as “Bug Rage” – but the students at W&amp;L were never intimidated by it. My final thought may seem ambiguous given my praise of the Lee Chapel group’s SketchUp model, but I think my future courses will not use Trimble SketchUp as the applied software for 3D modeling of heritage sites. As I told the students at the time, SketchUp is “simple” in comparison to more powerful software packages like 3D Studio Max, but in its attempt to anticipate a user’s workflow, it fails to introduce new users to the basic units of 3d modeling – polygons. The result is a process that is not particularly intuitive, and creates a system where the only sure way to correct a mistake is by reversing via the “undo” button. My advisor here at UVa, Prof. Lisa Reilly and I are constructing a Fall 2014 course on Digital Humanities methods for art and architectural historians that will include 3D modeling, so the lessons I have picked up from teaching and observing the students at W&amp;L will have immediate impact on our syllabus. With additional class time, I may use SketchUp as a brief intro to the process of “rough-sketching” a scaled, 3D building, but after witnessing the success of the Lee Chapel group at W&amp;L, I am less hesitant to offer students a more powerful/complex software solution. The optimal arrangement would include a 3D Studio Max drifter, but with a bit of preparation and students with the right attitude, I think I can handle the lessons solo again. Thanks for reading, and thanks to W&amp;L for a great experience."},{"id":"2014-05-19-welcoming-our-new-scholars-lab-fellows","title":"Welcoming our new Scholars’ Lab Fellows!","author":"purdom-lindblad","date":"2014-05-19 09:20:32 -0400","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"welcoming-our-new-scholars-lab-fellows","layout":"post","content":"We are thrilled to announce our partnership with 9 new graduate fellows for the 2014-2015 academic year! Joining an illustrious community of past recipients of Scholars’ Lab Fellowships, the new cohort hail from 5 disciplines in the humanities and social sciences at the University of Virginia. James Ambuske, Jennifer Foy  , and Emily Senefeld  join us as our three Digital Humanities Graduate Fellows . Throughout the year, they will have the opportunity to collaborate closely with Scholars’ Lab staff to integrate digital tools and methods to their dissertation research. James Ambuske’s dissertation is titled “Scotland’s American Revolution: Emigration and Imperial Crisis, 1763-1803.” Jennifer Foy’s dissertation is titled “Mapping Sympathy: Sensibility, Stigma, and Space in the Long Eighteenth Century” Emily Senefeld project is titled ”The Cultural Programs of the Highlander Folk School, 1932-1964.” James, Jennifer, and Emily’s projects converge around questions of social networks and mapping. We are looking forward to seeing their projects develop. This year marks the 4th year of our Praxis Program! Past Praxis teams developed Prism, a web application for crowd-based interpretations of texts, and re-imagined Ivanhoe, a platform for playfully (and collaboratively) interpreting texts and artifacts. This year’s cohort will refine Ivanhoe as well as explore our new Maker Space.  2014-2015 Praxis Fellows are: Amy Boyd (English)\nSwati Chawla (History)\nAndrew Ferguson (English)\nJoris Gjata (Sociology)\nJennifer Grayburn (Art and Architecture)\nand Steven Lewis (Music) Look for more information about our fellows in the early fall on the Scholars’ Lab blog."},{"id":"2014-06-05-come-work-with-us-makerspace-student-consultant","title":"Come Work With Us in Our New Makerspace","author":"laura-miller","date":"2014-06-05 10:44:40 -0400","categories":["Announcements","Experimental Humanities"],"url":"come-work-with-us-makerspace-student-consultant","layout":"post","content":"Are you a UVA graduate student or upper-level undergraduate in the humanities?  Interested working in our new Makerspace? [![133799283_8e9ab4cd6e_b](http://static.scholarslab.org/wp-content/uploads/2014/06/133799283_8e9ab4cd6e_b-300x232.jpg)](http://static.scholarslab.org/wp-content/uploads/2014/06/133799283_8e9ab4cd6e_b.jpg) photo courtesy of flicker user marymactavish The Scholars’ Lab in Alderman Library is opening a new Makerspace in Fall 2014.  It will allow user experimentation with 3D modeling and printing, physical computing (e.g. Arduino, wearables) and more.  We are seeking part-time student assistants to help maintain the public space, field users’ basic maker and general computing (both Mac and PC) questions, and connect researchers to Scholars’ Lab staff when necessary.  When not actively engaged with users, assistants will be asked to pursue their own research, use the equipment, and publish their processes and observations on the Scholars’ Lab blog. Experience with 3D modeling and printing, electronics, GIS, and/or programming preferred, but can be learned on the job.  The successful candidate will be able to work up to 10 hours per week. An important aspect of the maker culture is apprenticeship and supporting makers in their pursuit of professional experience. We are looking for motivated individuals who are capable of working independently and value the opportunity to engage with and support a growing community.  Benefits of the job may include: access to expertise and mentoring in your field of interest, use of equipment and tools, and ability to shape Scholars’ Lab workshops and programming. Candidates should include a cover letter discussing their interest in working in the Scholars’ Lab, detailing any experience or interest in participating in a maker space, and outlining any previous experience with public service or assisting others in using technology. If you would like to apply, please fill out an application in CavLink ."},{"id":"2014-06-09-check-out-copyipsum","title":"Check Out Copyipsum","author":"jeremy-boggs","date":"2014-06-09 06:00:37 -0400","categories":["Research and Development"],"url":"check-out-copyipsum","layout":"post","content":"Of the bazillion lorem ipsum generators out there, the one I use most often for day to day work is Loripsum . It has a few great features, like choosing length of paragraph, adding other HTML elements, and only getting plain text. You can also get all this stuff through a simple API. Friday morning, while bouncing between working on some redesign ideas for this site and messing with the Marvel API, I decided that I really wanted to be able to get some lorem ipsum text from the Loripsum.net API using a command line tool. Thankfully, Eric was around to hold my hand as we put together Copyipsum . To get going with Copyipsum, you can install it using pip : pip install copyipsum You can also clone the public repository: git clone git://github.com/clioweb/copyipsum.git Or download the tarball: curl -OL https://github.com/clioweb/copyipsum/tarball/master Once you have a copy, you can run the setup yourself. Move into the copyipsum directory, then do: python setup.py install Once you have it installed, you can now use the copyipsum command to save some lorem ipsum to your clipboard and use wherever you want. All the options available on the Loremipsum.net API are available as arguments to the copyipsum command, and are documented in the project’s README, but here are a few examples to illstrate some of the things you can do: Get 10 Paragraphs copyipsum -p 10 Get 10 Paragraphs with Headings copyipsum -H -p 10 Get 10 Paragraphs with Headings and decorators like bold, italic, and mark copyipsum -d -H -p 10 Get 10 Long Paragraphs copyipsum -p 10 -s long As usual, bug reports and feature requests are welcome! Feel free to add though through the project’s issue tracker ."},{"id":"2014-06-20-learning-ruby-opening-moves","title":"Learning Ruby: Opening Moves","author":"purdom-lindblad","date":"2014-06-20 05:48:53 -0400","categories":["Digital Humanities","Research and Development"],"url":"learning-ruby-opening-moves","layout":"post","content":"As the Praxis Fellows wrapped up Ivanhoe, I turned my attention to the Praxis Network . The Praxis Network, which showcases eight like-minded, but differently enacted programs all exploring new ways of teaching humanities students, began with the goals of creating an easy way to compare a variety of programs and to provide a model for others. The success of the website, prompted new goals of better networking students within the Network as well as allied programs. After a few conversations with Bethany, Wayne, and Jeremy, we had a basic outline for an open directory of like-minded programs and another directory of Praxis Network students. I realized pretty quickly it is easy to talk through an idea, but much more difficult for me to break the larger idea into a series of small steps. Often breaking down the big into the small makes a great deal of sense; my challenge was to realize small, in this case, means exceptionally tiny and extremely specific. Creating the open directory of ‘fellow traveler’ programs began with a series of drawings defining (and redefining) directory. How would people contribute information? How would I collect the data and publish to the web? A sketch: create Google Form pull data from form and write Markdown Files edit if needed add to git push to gh-pages and voila!, published Simple, right? Now, the reality–I am new to Ruby and to using git. After creating the Google Form, I installed Ruby and the gems google_drive, dotenv, and rake . We, then, created a git repository and a .gitignore file. Next, Wayne and I created a .env file to keep passwords and form keys private. After the initial set up, I was ready to tackle the Rakefile . [gallery ids=”10398,10399,10400”] I am still trying to wrap my head around the Rakefile . At the moment, I am working with metaphors, which is not best practice when thinking about the highly specific process of laying out a series of steps for the computer to follow. Even so, I think of the Rakefile as a scratchpad to experiment with the series of instructions. First, I defined the requirements (the gems I previously installed– ruby-gems, dotenv, and google_drive ) and defined the necessary tasks (import data from Google Drive) and methods (loop over this data and make a Markdown file for each row passed to it). I am still working on defining the content to display online. The next step is to investigate using Jekyll categories as a way to filter programs. My biggest challenge, so far, has been that I have not devoted regularly scheduled time to working. Chunking an hour or so when it was convient means I have not internalized basic commands. Worse, each time I return to the script, I need to re-familiarize myself with each element within it. To address this, I have scheduled daily time on my calendar (with reminders) and have begun Ruby the Hard Way and Ruby Koans ."},{"id":"2014-07-02-a-digital-declaration-of-independence-with-text-painting-and-map","title":"A (Digital) Declaration of Independence","author":"david-mcclure","date":"2014-07-02 05:56:04 -0400","categories":["Geospatial and Temporal"],"url":"a-digital-declaration-of-independence-with-text-painting-and-map","layout":"post","content":"[Cross-posted from dclure.org ] Launch the Exhibit Way back in the spring of 2012, a couple months before we released the first version of Neatline, I drove up to Washington to give a little demo of the project to the folks at the Library of Congress. I had put together a couple of example exhibits for the presentation, but, the night before, I was bored and found myself brainstorming about Washington-themed projects. On a lark, I downloaded a scan of the 1823 facsimile of the Declaration of Independence from the National Archives website, and spent a couple hours tracing polygons around each one of the signatures at the bottom of the document. I showed the exhibit the next day, and had big plans to flesh it out and turn it into a real, showable project. But then I got swept up in the race to get the first release of Neatline out the door before DH2012 in Hamburg, and then sucked into the craziness of the summer conference season, and the project slipped down into the towering stack of things that I could never quite find time to work on. For some reason, though, the idea popped back into my head a couple months ago - maybe because Menlo Park is submerged in a kind of permanent summer, and it pretty much always feels like a good time to eat ice cream and shoot off fireworks. After mulling it over for a couple weeks, I decided to resurrect it from the dead, spruce it up, and post it in time for the 4th of July. So, with two days to spare, here we go - an interactive edition of the Declaration of Independence, tightly coupled with three other “views” in an effort to add dimension to the original document: A full-text, two-way-linked transcription of the manuscript and the signatures at the bottom. Click on sentences in the transcription to focus on the corresponding region of the scanned image, or click on annotated blocks on the image to scroll the text. An interactive edition of Trumbull’s “Declaration of Independence” painting, with each of the faces outlined and interactively linked with the corresponding signature on the document. All of which is plastered on top of a map that plots out each of the signers’ hometowns on a custom Mapbox layer, which makes it easy to see how the layout of  the signatures maps on to the geographic layout of the colonies. Which, by extension, tracks the future division between Union and Confederate states in the Civil War - Georgia and the Carolinas look awful lonely over on the far left side of the document. Once I positioned the layers, annotated the signatures and faces, and plotted out the hometowns, I realized that I had painted myself into an interesting little corner from an information design standpoint - it was difficult to quickly move back and forth between the three main sections of the exhibit. In a sense, this is an inherent characteristic of deeply-zoomed interfaces. The ability to focus really closely on any one of the three visual grids - which is what makes it possible to mix them all together into a single environment - has the side effect of making the other two increasingly distant and inaccessible, more and more so the further down you go. For example, once you’ve focused in on Thomas Jefferson’s face in the Trumbull painting, it’s quite a chore to manually navigate to the corresponding signature on the document - you have to zoom back, pan the map up towards the scanned image, find the signature (often no easy task), and then zoom back down. This is especially annoying in this case, since this potential for comparison is a big part of what’s interesting about the content. What I really wanted, I realized, was to be able to switch back and forth in a really simple, fluid way among the different instantiations of any individual person on the document, painting, and map - I wanted to be able to flip through them like a slideshow, to round up all the little partial representations of the person and hold them side-by-side in my head. So, as an experiment, I whipped up a little batch of custom UI components (built with the excellent React library, which fits in like a dream with Neatline’s Javascript API) that provide a “toggling” interface for each individual signer, and the exhibit as a whole. By default, when you hit the page, three top-level buttons in the right corner of the window link to the the three main sections of the exhibit - the hometowns plotted along the eastern seaboard, the declaration over the midwest, and the painting over the southeast. In addition to the three individual buttons, there’s also a little “rotate” button that automatically cycles through the three regions, which makes it easy to toggle around without looking away from the map to move the cursor: More useful, though, it’s possible to bind any of the individual signers to the widget by clicking on the annotations. For example, if I click on Thomas Jefferson’s face in the painting, the name locks into place next to the buttons, which now point to the representations of that specific person in the exhibit - “Text” links to Jefferson’s signature, “Painting” to his face, and “Map” to Monticello: Once you’ve activated one of the signers, click on the name to show an overlay with a picture and biography, pulled from a public domain book published by the National Park Service called Signers of the Declaration: This is pretty straightforward on the map and document, where there’s always a one-to-one correspondence between an annotation and one of the signers. Things get more complicated on the map, though, where it’s possible for a single location to be associted with more than one signer. Philadelphia, for example, was home to Robert Morris, Benjamin Rush, Benjamin Franklin, John Morton, and George Clymer, so I had to write a little widget to make it possible to hone in on just one of the five after clicking the dot: Last but not least, each sentence in the document itself is annotated and wired up with the corresponding text transcription on the left - click on the image to scroll the text, or click on the text to focus the image: Happy fourth!"},{"id":"2014-07-24-codespeak-kit","title":"Announcing the #Codespeak Kit!","author":"bethany-nowviskie","date":"2014-07-24 05:20:14 -0400","categories":["Announcements","Research and Development"],"url":"codespeak-kit","layout":"post","content":"Today, the Scholars’ Lab is pleased to make a few modest contributions toward the broadening of a conversation we opened last fall, in a summit for digital humanities software developers called Speaking in Code . The summit, generously funded by the National Endowment for the Humanities and UVa Library, brought together 32 advanced developers working on humanities data, tools, and systems, to discuss ways to create inclusive, welcoming developer communities and to address the social and intellectual implications of tacit knowledge exchange in their craft. On the codespeak site today, you’ll find links to a customizable kit that anyone can use to host their own Speaking in Code gathering. It includes our starter bibliography, advice for welcoming a diverse group of participants and planning an event, and the framework of a Jekyll website (complete with instructions), ready for you to modify and publish easily, using GitHub Pages. You’ll also find the beginnings of a set of DH developer advice posts and “origin stories,” to which you are invited to contribute, and an invitation to continue the conversation on Twitter (hashtag #codespeak ) and IRC (Freenode channel #speakingincode ).  A white paper on the outcomes of the inaugural summit will also be available soon, and summit particpants may have other documents to share. Read a little more about the purpose and outcomes of Speaking in Code at CLIR, in the following posts: How We Learned to Start/Stop “Speaking in Code” and A Kit for Hosting “Speaking in Code”"},{"id":"2014-07-28-neatline-2-3","title":"Neatline 2.3","author":"david-mcclure","date":"2014-07-28 07:01:55 -0400","categories":["Announcements"],"url":"neatline-2-3","layout":"post","content":"Today we’re happy to announce Neatline 2.3 ! This release includes a couple of nifty new features and, under the hood, a pretty big stack of bug fixes, performance tweaks, and improvements to the development workflow. The coolest new feature in 2.3 is a simple little addition that we’ve gotten a number of requests for in the last few months - the ability to “hard link” to individual records inside of an exhibit. In the new version, when you select a record in an exhibit, a little fragment gets tacked on to the end of the URL that points back to that record. For example, if the record has an ID of 16, the URL will change to something like: www.omeka-site.org/neatline/show/exhibit**#records/16** Then, if someone goes directly to this URL, the exhibit will automatically select that record when the page loads, just as if the reader had manually clicked on it - the map will focus and zoom around the record, the popup bubble will appear, the timeline will scroll, and any other custom event bindings added by the exhibit’s theme will fire. This is nice because it makes it easier to use Neatline as a kind of geospatial “footnoting” system that can be referred to from external resources - sort of like the Neatline Text extension, except the text doesn’t have to be housed inside of Omeka. Imagine you’re working on an article that makes reference to some geographic locations, and you want to plot them out in Neatline. This way you could put the text of the article anywhere on the web (a Wordpress blog, an online journal, etc.) and just link to the relevant parts of the Neatline exhibit using plain old anchor tags. For example, check out this simple little Neatline exhibit, which just plots out the locations of eight US cities. Then, click on these links to open up the same exhibit, this time focused on the individual cities: New York, San Francisco, Chicago, Los Angeles, Seattle, Denver, Atlanta, and (but of course) Charlottesville . Check out the change log for the full list of updates in 2.3, and grab the new production package from the Omeka addons repository . Thanks Jenifer Bartle, Jacki Musacchio, Rachel King, Lincoln Mullen, and Miriam Posner for helping us find bugs and brainstorm about features! As always, drop a note on the GitHub issue tracker if you run into problems or have ideas for new features."},{"id":"2014-08-18-omeka-neatline-mac-development-oh-my","title":"Omeka, Neatline, Mac, development, oh my!","author":"eric-rochester","date":"2014-08-18 05:41:08 -0400","categories":["Research and Development"],"url":"omeka-neatline-mac-development-oh-my","layout":"post","content":"At the Scholars’ Lab, we’re big big advocates of Open Source. All of our projects are available freely and openly on Github, and we’re always more than happy to accept pull requests. We’d like to be able to empower everyone to contribute to our projects as much as they’re able to and comfortable with. Unfortunately, one of our flagship projects, Neatline, isn’t easy to contribute to. There are a number of reasons for this, but one is that the development environment is not trivial to get set up. In order to address this and make it easier for others to contribute, we’ve developed an Ansible playbook that takes a not-quite-stock Mac and sets up an instance of Omeka with the Neatline plugin available, as well as all the tools necessary for working on Neatline. Ansible is a system for setting up and configuring systems. It’s often used to set up multiple servers—for instance, a database server and a static web server, both working with a dynamic web applications deployed on several computers. If you’re familiar with Chef or Puppet, Ansible solves the same problems. In this case, we’ll use it to configure our local development workstation. We’ve published these playbooks on Github in the neatline.dev repository, on the mac-ansible branch . You can get this by cloning it to your local machine. (Since this is for getting started developing Neatline, I assume that you’re already comfortable with git . If not, there are lots of great tutorials .) $ git clone --branch mac-ansible https://github.com/erochest/neatline.dev.git Requirements In creating this, I’ve aimed for starting from a stock Mac. And I missed pretty badly. However, the necessary prerequisites are minimal. You’ll just need to have these things installed. XCode Homebrew Once those two are on your machine, you can install the other two dependencies. These are available through Homebrew . So open Terminal and type these lines: $ brew install python\n$ brew install ansible That’s all. You should be ready to go. Settings This project includes a number settings that you can change to customize your installation. Those are found in the file playbook.yaml . The relevant section is labelled vars, and it allows you to set information about the Omeka database ( omeka_db_user, omeka_db_password, and omeka_db_name ), which version of Omeka you wish to use ( omeka_version ), where you wish to install it ( omeka_dir ), and where you want to point your browser to ( dev_hostname ) as you’re working on the site. The defaults are: vars:\n  db_user: root\n  db_password:\n  omeka_db_user: omeka\n  omeka_db_password: omeka\n  omeka_db_name: omeka\n  dev_hostname: omeka-neatline.dev\n  omeka_dir: \"{{ ansible_env.HOME }}/omeka/neatlinedev\"\n  omeka_version: stable-2.1\n  debug: true\n  neatline_repo: git@github.com:scholarslab/Neatline.git\n  php_version: 55 Change these to reflect what you’d like your personal Omeka/Neatline installation to look like. One option that I’ll call out in particular is neatline_repo . This is the git repository that you’ll be working with. If you’re using github to host your project, you can fork the primary Neatline repository (from the URL given above). And when you’ve completed your work, if you’d like to contribute back, you can send us a pull request through the Github site. Setting Up Finally, we’re ready to actually create the system. This is quite easy. In the Terminal, from the neatline.dev directory, run the neatline-dev script. $ cd neatline.dev\n$ ./neatline-dev Now wait. After your computer whirs away for a while, you’ll get your prompt back. When that happens, you should be able to point your browser to http://omeka-neatline.dev (in the example above). There you’ll see the Omeka installation form. What Just Happened? The Ansible playbook does a number of tasks. It installs all the dependencies that you’ll need, including PHP, NodeJS, and MySQL . It sets MySQL to start automatically when you log in, and it creates the Omeka MySQL user and database. It configures Apache to work with PHP and to find your Omeka directory. It downloads and configures Omeka and turns on debugging. It clones Neatline into Omeka’s plugin directory. It initializes git flow for working in Neatline and leaves you on the develop branch. And it installs the necessary JavaScript and PHP tools, including Grunt, Bower, Composer, and PHPUnit . After all that, it really needs a break. You probably do too. Future Unfortunately, that’s only the first step that we need to take to make the Neatline code-base approachable. Some more things that we have planned include: Documentation on all the moving parts. Documentation on the overall architecture of Neatline. Documentation on the code. What’s where? If you wish to change something, where would you find it? As we get those parts in place, we’ll keep you posted."},{"id":"2014-08-20-prism-news-heroku-and-llc","title":"Prism News - Heroku and LLC","author":"brandon-walsh","date":"2014-08-20 04:57:54 -0400","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"prism-news-heroku-and-llc","layout":"post","content":"Cross-posted on _ my personal site_ This past year the Scholars’ Lab has implemented many performance upgrades and bug fixes for Prism . The most recent upgrade is particularly exciting: users can now deploy their own personal Prism installations to Heroku with the click of a button. Well - it will take the click of a button and a few other commands. I’ve added a section detailing just how to do so under the “ Deploy to Heroku ” section of the Prism Github’s readme. It was already possible to implement private user communities by marking uploaded prism games as “unlisted” and then distributing the links to your group of participants. The Heroku deploy function makes this process a bit easier by allowing to users to host all of their games in one place. The process also sets you up well to tinker with the Prism codebase using a live app, as Heroku provides instructions for cloning the app to your desktop. All of this on the heels of another exciting announcement: the Praxis Program has a short article on Prism appearing in the Digital Humanities 2013 special conference issue of Literary and Linguistic Computing . In the piece, we summarize Prism’s and interventions into conversations on crowdsourcing with special reference to its user interface. It’s a good day to e-highlight!"},{"id":"2014-08-21-taking-the-alt-ac-route","title":"Taking the Alt-Ac Route","author":"scott-bailey","date":"2014-08-21 05:05:01 -0400","categories":["Digital Humanities","Research and Development"],"url":"taking-the-alt-ac-route","layout":"post","content":"I am not where I thought I’d be. A year ago I’d have told you that right about now I’d be frantically writing my dissertation, trying to get an article out, and desperately looking for professorial jobs in contemporary theology and/or continental philosophy. Instead, I’m still working on my dissertation (not quite as frantically as I perhaps should), but I’m mostly being a digital humanities developer in the Scholars’ Lab at U.Va. Instead of reading book after book of theology on a daily basis, I read a lot of documentation for programming languages. Instead of hours each day writing theology deeply conditioned by years of reading Heidegger, Barth, and Jüngel, I’m writing, at the moment, rather a few lines of Javascript. Within the next few weeks I’m hoping to make that Coffeescript instead. Why write var repeatedly when you don’t need to? Instead of days and nights spent moving between coffeeshops, library tables, and my own apartment, I sit (or stand) at my desk in the Scholars’ Lab offices for about eight hours a day, taking an occasional break from the space to work somewhere else in the library or go to a meeting in Clemons. It’s a different pattern of life and work, one I had not expected. And it has been quite a transition since I started working in May. Those first few weeks settling in and working eight hours days, with few breaks, were exhausting. But it’s also been rather a lot of fun, learning new languages, working collaboratively on projects, and simply being around brilliant and engaging people every day. How or why am I here though? Why not continue on the path I spent seven years on, aimed at the traditional professoriate? The quick and easy answer is the job market and the changing character and organization of universities, both of which are frequent topics in the news these days. The more difficult answer is about a changing sense I have of my own broad interests and what type of work can be satisfying. As a Teaching + Technology Support Partner at U.Va., then more significantly as a Praxis Fellow here in the Scholars’ Lab, I began working with technology first within the pedagogical sphere and then more broadly within the academic world. I began to see the possibilities of use of different technologies within academic research, teaching, and scholarly communication. As a Praxis Fellow, I then realized that I really enjoy working with the technology itself, specifically, coding. The type of systematic and problem oriented thinking that you engage in when coding is remarkably similar to the type of thinking one engages in with dogmatic or systematic theology. Then, in the process of building Ivanhoe, writing the code began to be the thing I most wanted to do out of all the things I was doing (dissertating, TAing, and so forth), and it was fulfilling each day to do narrow, concrete work and see the results. Now, a little bit of success and enjoyment with coding is not necessarily enough to spur a whole-sale career change. It is enough, though, to raise questions and possibilities about shifting the path just a bit. In the Scholars’ Lab, I am a digital humanities developer. That means I code, but it also means I think about what it means to do research in the humanities and will, over time, work with faculty and graduate students to help them with their own research. Because I am lucky enough to be in the Scholars’ Lab, it also means I do research of my own and I speak with the excellent people here in the lab, all with their own interests and specialties. Together, we compose a cadre of scholarly practitioners deeply interested in our own fields, in the larger state of humanistic inquiry, in the development of innovative research methods and tools, and in graduate education. From that standpoint, while my daily routine might look quite different from expected, the place where I am is not so far off the path of the traditional academic. In that case, while I am not where I thought I’d be, I am also not not where I thought I’d be."},{"id":"2014-08-24-announcing-the-fall-2014-scholars-lab-gis-workshop-series","title":"Announcing the Fall 2014 Scholars’ Lab GIS Workshop Series","author":"laura-miller","date":"2014-08-24 13:15:51 -0400","categories":["Announcements","Geospatial and Temporal"],"url":"announcing-the-fall-2014-scholars-lab-gis-workshop-series","layout":"post","content":"All sessions are one hour and assume attendees have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials with expert assistance. All sessions will be taught by our GIS Specialist, Chris Gist, on Thursdays from 2:00-3:00 PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community. September 4th\nMaking your First Map Getting started with new software can be intimidating. This workshop introduces the skills you need to work with spatial goodness. Along the way you’ll get a taste of Earth’s most popular geographic software and a gentle introduction to map making. You’ll leave with your own cartographic masterpiece and tips for learning more in your pursuit of mappiness at UVa. September 11th\nGetting Your Data on a Map Do you have GPS points or a list of latitude and longitude you would like to show as points on a map? This session will show you how to turn your data into map layers and how to connect them to make lines and polygons as well. September 18th\nPoints on Your Map: Street Addresses and More Spatial Things Do you have a list of street addresses crying out to be mapped? Have a list of zip codes or census tracts you wish to associate with other data? We’ll start with addresses and other things spatial and end with points on a map, ready for visualization and analysis. September 25th\nGeoreferencing – Putting Old maps and Aerial Photos on Your Map Have an old map or an aerial photograph that you would like to use as a spatial layer? This session will teach you techniques to properly place your data and make it useable in GIS software. We will also demo similar techniques for Google Earth. October 2nd\nTaking Control of Your Spatial Data: Editing in ArcGIS Until we perfect that magic “extract all those lines from this paper map” button we’re stuck using editor tools to get that job done. If you’re lucky, someone else has done the work to create your points, lines, and polygons but maybe they need your magic touch to make them better. This session shows you how to create and modify vector features in ArcMap, the world’s most popular geographic information systems software. We’ll explore tools to create new points, lines, and polygons and to edit existing datasets. At version 10, ArcMap’s editor was revamped introducing new templates, but we’ll keep calm and carry on. October 9th\nCollecting Your Own Spatial Data Research projects often rely on fieldwork to build new datasets. In this workshop we’ll focus on tools for spatial data collection. First we’ll take a quick look behind the curtain to see how GPS really works and how to use that knowledge to our advantage. Then we’ll evaluate free or low-cost options to gather locations and associated attributes using handheld GPS devices, smartphones, and apps. This workshop will introduce you to a range of devices and methods for mobile spatial data collection.  "},{"id":"2014-08-27-fall-2014-scholars-lab-gis-workshop-series","title":"Fall 2014 Scholars’ Lab GIS Workshop Series","author":"chris-gist","date":"2014-08-27 04:49:42 -0400","categories":["Announcements","Geospatial and Temporal","Research and Development"],"url":"fall-2014-scholars-lab-gis-workshop-series","layout":"post","content":"All sessions are one hour and assume attendees have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials with expert assistance.  All sessions will be taught on Thursdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community. September 4th Making your First Map Getting started with new software can be intimidating.  This workshop introduces the skills you need to work with spatial goodness.  Along the way you’ll get a taste of Earth’s most popular geographic software and a gentle introduction to map making.  You’ll leave with your own cartographic masterpiece and tips for learning more in your pursuit of mappiness at UVa. September 11th Getting Your Data on a Map Do you have GPS points or a list of latitude and longitude you would like to show as points on a map?  This session will show you how to turn your data into map layers and how to connect them to make lines and polygons as well. September 18th Points on Your Map: Street Addresses and More Spatial Things Do you have a list of street addresses crying out to be mapped?  Have a list of zip codes or census tracts you wish to associate with other data?  We’ll start with addresses and other things spatial and end with points on a map, ready for visualization and analysis. September 25th Georeferencing – Putting Old maps and Aerial Photos on Your Map Have an old map or an aerial photograph that you would like to use as a spatial layer?  This session will teach you techniques to properly place your data and make it useable in GIS software.  We will also demo similar techniques for Google Earth. October 2nd Taking Control of Your Spatial Data:  Editing in ArcGIS Until we perfect that magic “extract all those lines from this paper map” button we’re stuck using editor tools to get that job done.  If you’re lucky, someone else has done the work to create your points, lines, and polygons but maybe they need your magic touch to make them better.  This session shows you how to create and modify vector features in ArcMap, the world’s most popular geographic information systems software.  We’ll explore tools to create new points, lines, and polygons and to edit existing datasets.  At version 10, ArcMap’s editor was revamped introducing new templates, but we’ll keep calm and carry on. October 9th **Collecting Your Own Spatial Data ** Research projects often rely on fieldwork to build new datasets.  In this workshop we’ll focus on tools for spatial data collection. First we’ll take a quick look behind the curtain to see how GPS really works and how to use that knowledge to our advantage.  Then we’ll evaluate free or low-cost options to gather locations and associated attributes using handheld GPS devices, smartphones, and apps.  This workshop will introduce you to a range of devices and methods for mobile spatial data collection."},{"id":"2014-09-08-fellows","title":"The Fellows are Coming! The Fellows are Coming!","author":"purdom-lindblad","date":"2014-09-08 06:36:45 -0400","categories":["Announcements","Events","Grad Student Research"],"url":"fellows","layout":"post","content":"As the fall term begins, we are delighted to welcome our 9 (!) SLab Fellows. Ranging across 5 disciplines from the arts, humanities, and social sciences . Our Graduate Fellows in the Digital Humanities and Praxis Program Fellows join a distinguished community of past fellows . Graduate Fellows in the Digital Humanities We are excited about the opportunity to work closely with our Graduate Fellows in the Digital Humanities, James, Jenny, and Emily as they develop their projects. Please join us at their welcome lunch and panel September 12th, at 12 PM in the Scholars’ Lab Commons. As a previous post noted, James, Jennifer, and Emily’s projects converge around questions of social networks and mapping. James Ambuske’s  (History) dissertation is titled “Scotland’s American Revolution: Emigration and Imperial Crisis, 1763-1803.” Jennifer Foy’s   (English) dissertation is titled “Mapping Sympathy: Sensibility, Stigma, and Space in the Long Eighteenth Century” Emily Senefeld’s  (History) project is titled “The Cultural Programs of the Highlander Folk School, 1932-1964.” Praxis Program Fellows Marking our 4th year of the Praxis Program, we warmly welcome: Amy Boyd (English) Swati Chawla (History) Andrew Ferguson (English) Joris Gjata  (Sociology) Jennifer Grayburn  (Art and Architecture) and Steven Lewis (Music) Past Praxis teams developed Prism, a web application for crowd-based interpretations of texts, and re-imagined Ivanhoe, a platform for playfully (and collaboratively) interpreting texts and artifacts. This year’s cohort will refine Ivanhoe as well as explore our new Makerspace . With a talented and disciplinarily diverse team, we are looking forward to their dive into Ivanhoe. Keep track of their experiments on our blog !"},{"id":"2014-09-08-realignment","title":"Realignment","author":"jeremy-boggs","date":"2014-09-08 12:05:01 -0400","categories":["Announcements"],"url":"realignment","layout":"post","content":"I’m happy (and relieved…and nervous) to share a redesigned and realigned web site for the Scholars’ Lab !  Realignment is an apt word for these changes, I think, because they were done in an effort to better showcase the work the Scholars’ Lab has been doing and the manner in which we do that work. The realignment has been a few months in the making, and there are lots of great updates to the site, so this will be a modest summary of those updates, with more posts to come detailing some of the motivations behind new features and content. We’ve taken care to highlight the work and focus areas of the Scholars’ Lab throughout the site. Additionally, we’ve added some content features for better communication and sharing of our work and events. In particular: We clearly delineate the SLab’s areas of focus—Project Incubation, Graduate Training, Experimental Humanities, and Geospatial Scholarship—on the home page. We have new section of the site for our Makerspace ! We’re featuring splendid photographs of all our current staff and graduate fellows. (Many thanks to Shane Lin for the photographs.) a complete list of all the people who are or have been associated with the Scholars’ Lab on the People page. We maintain an archive of events on our Events page, with permanent pages for each event, collecting related posts, comments, and social media conversations about particular events. We provide a persistent but not-intrusive way to sign up for our newsletter. The part I’m most proud of: We decided this summer to do what we ask our Praxis Program fellows to do each year, and devise a charter that summarizes the things we care about as scholars, teachers, and colleagues and spells out the ways in which we expect to conduct ourselves and our collaborations. The design itself leans toward a more modern aesthetic than the previous design. You can read more about the technical details of the site on our Colophon page, but here’s a rundown of some technical and design changes: We spent some much needed, ongoing attention to accessibility in the site’s design and markup. We include space for a decorative header image, one that authors can change on individual posts and pages. We use bigger, more readable typography throughout. (As with the previous design, I wanted to make sure that the content we share on the site, is the primary focus, with other elements serving a support role.) We have a main navigation that works much better across devices and screen widths. There are still a few wrinkles to iron out, and if I know me half as well as I think I do, I’ll be ironing those out for a while. If you see a problem with the site, feel free to leave a comment here or add a ticket to our Github repo for our WordPress theme . Complaints and issues should be sent to me. Candy and compliments should all go to my incredible colleagues at the Scholars’ Lab for content, feedback on design, terrific feature ideas, and support."},{"id":"2014-09-11-ivanhoe-considerations-for-the-next-cohort","title":"Ivanhoe considerations for the next cohort","author":"stephanie-kingsley","date":"2014-09-11 11:17:41 -0400","categories":["Grad Student Research"],"url":"ivanhoe-considerations-for-the-next-cohort","layout":"post","content":"Welcome to the next Praxis cohort! As Purdom mentioned yesterday in her post, the 2014-2015 Praxis Fellows will continue working on the Ivanhoe Game .  As last year’s Praxis project manager, I frequently found myself in the position of spokesperson for Ivanhoe. People would ask me, “What is Ivanhoe?” and I would deliver an answer like “Ivanhoe is a platform for making collaborative interventions in a text via role play.” While a pretty good description of the game, this response nonetheless invites many questions—questions which my dazzled but bewildered auditors would inevitably ask: “How do you play?” “Can you win?” “What do you mean by a ‘role’?” “What do you mean by ‘intervention’?” “Do you have to have read Ivanhoe to play?” Well, the last question is straightforward enough (you do not have to have read the novel of that name by Sir Walter Scott to play), but the others are a bit more complex. Ivanhoe originators Johanna Drucker and Jerome McGann and SpecLab pioneers Bethany Nowviskie, Geoffrey Rockwell, and Chad Sansing debated these very questions in a special issue of TextTechnology (12:2) in 2003, when Ivanhoe development was in full swing. Eleven years later, our Praxis cohort took up these questions while building Ivanhoe as a WordPress Theme. We designed our Ivanhoe to be as open-ended and non-prescriptive as possible, not wanting to predetermine game play in any way. But what would be wrong with exploring the possibilities of Ivanhoe, showing users what we found, and inviting users to join that conversation? Perhaps Praxers’ own experimentation and transparency regarding their experiences might better guide users toward getting into Ivanhoe and trying it out themselves. I’m excited to see where the next cohort takes Ivanhoe, how they interpret it, and what kinds of games they play. But at this crucial moment before the games begin, I feel it necessary to impart a few thoughts on what our own cohort considered to be the essential elements of Ivanhoe. I will begin by listing them: The Text Moves Role Play Collaboration The Text Any Ivanhoe game begins with a text. But what constitutes a ‘text’? On the original Ivanhoe informational website, the SpecLab researchers termed it a “‘discourse field,’ the documentary manifestation of a set of ideas that people want to investigate collaboratively.” Our cohort considered an Ivanhoe text to be merely the focus of the interpretation. This could be a literary text, a work of visual art, a piece of music, a film, or even a concept—such as a historical time period. One example of the last would be the Suffragette Journalism Game, which took as its ‘text’ the 19th-century women’s suffrage protests in England. To some extent, the Ivanhoe game we played on the Elgin Marbles was also a concept game, since we didn’t have the marbles themselves on hand to study and interpret but rather found ourselves interpreting the whole historical legacy of the marbles. So a ‘concept text’ can work well for Ivanhoe, but is a concept–which may come without “documentary manifestation”–a legitimate text? Are there any constraints on what can be a ‘concept text’? Is there a point in an Ivanhoe game at which a concept, loosely interpreted, can break down and the game drift into meaninglessness? Do you need a sense of textual integrity for the game to have coherence and meaning? The Move The second integral feature of an Ivanhoe game is the move, which our developer Scott Bailey succinctly defined as a “self-conscious act of interpretation.” These acts constitute what we like to refer to as “textual interventions.” The first Ivanhoe games played by Drucker and McGann, as well as the SpecLab researchers, involved actual changes made directly into the texts: Players added text, changed it, or deleted it from works such as Sir Walter Scott’s Ivanhoe _and Henry James’ _The Turn of the Screw. Our cohort began with a game on Edgar Allen Poe’s “The Tell-tale Heart,” which, following its predecessors, involved changes to the text itself. As we found ourselves becoming more and more interdisciplinary in our aims, however, the typical move became more a commentary on the text than changes to it. Does mere interpretation actually intervene in a text?  Which term is more characteristic of an Ivanhoe move?  What are we actually doing when we make an Ivanhoe move, and how do moves make the game itself progress? Role Play [Actors refused to participate] Role play has always been the cornerstone of the Ivanhoe Game: each player assumes a role—a particular voice or critical stance from which to make moves. In the SpecLab’s A Turn of the Screw game, players even chose aliases so that other players couldn’t guess their roles; arguably, these aliases add another level of role play. But when we interpret a text, don’t we automatically assume a role of some sort? When I read, I approach a text from the role of “Stephanie reading for fun” or “Stephanie reading to have brilliant ideas,” for instance—and my reading styles in these roles, alas, are very different. Thus, it is important to keep in mind the self-conscious _in the definition of a role. These are “_self-conscious acts of interpretation”: your role must be deliberate and specific, and you must constantly be reconsidering your role and striving to make moves which develop that role. It is a more directed manner of analysis than simply trying to come up with points of interest for class discussion, as the Stephanie-bent-on-brilliance attempts to do. The element of self consciousness, therefore, is absolutely integral to the Ivanhoe Game. Directly related to self consciousness, what about the idea of the role journal? The SpecLab Turn of the Screw game included role journals in which players commented on why they made certain moves; these journals were kept secret from other players. Additionally, players also could comment on other players’ moves; these comments were public. This game had two ways players could be self-conscious of game play. I particularly want to pose this question to the next cohort: is the role journal a core feature of Ivanhoe? Our own cohort spent hours debating this question (see Scott’s excellent post on the issue), but after we decided to keep the role journal, it quickly was pushed to the back-burner during development and became a secondary feature in terms of importance. Must an Ivanhoe game have an actual role journal, or is the role journal merely an emblem of the self-conscious attitude of the players?  Is its function simply to keep players adhering to their roles, or does it serve other purposes?  This is a question which I would love to see this year’s cohort test. Collaboration Lastly, Ivanhoe must have players; for me, this means it is collaborative. Even if players are competing, the result is still collaboration because one player’s ideas necessarily inform and impact those of the other players. This collaboration is one feature which we considered to be very important to Ivanhoe; it’s what makes it a game—or as Francesca wrote in her post last year, gamification, an idea and debate worth consideration for the next cohort but one which I will not attempt to discuss here.  Can you have a one-person Ivanhoe game?  I do not think so, but then that is really an opinion guided by my firm belief in the final–and perhaps most essential–Ivanhoe feature: FUN!  Grab some friends and have fun with collaborative criticism in an Ivanhoe game of your own! Conclusion…? What I realize as I go down the list of core features is 1) what I assumed were core features are perhaps not so self evident as I had thought, and 2) in attempting to create a list I have hovered dangerously—and somewhat tantalizingly—close to deconstructing Ivanhoe altogether. Why? What is so elusive about Ivanhoe? My neat-and-tidy list ended up melting into a series of questions and the important but nonetheless vague mantra of “fun.” Perhaps, then, my contribution to the next cohort is not a list of Ivanhoe essentials, but a few questions which they can consider as they start their play-testing. I will be fascinated to watch their progress on the Praxis blog, and I encourage readers to follow it as well. Watch for their reflections on Ivanhoe, and even join in the discussion by leaving comments. Play an Ivanhoe game, and let us know what you think are the core features. Ivanhoe, after all, is about getting conversation started. Good luck, Praxers, and let’s get gaming!"},{"id":"2014-09-15-about-whom-i-have-become","title":"About Whom I Have Become... ","author":"joris-gjata","date":"2014-09-15 11:33:18 -0400","categories":["Grad Student Research"],"url":"about-whom-i-have-become","layout":"post","content":"Hi everybody! It is not easy to talk about oneself, especially when you are not sure who your audience is. Still, I will not let the amorphous ambiguous unidentifiable audience scare and silence me. For this post, I want the main audience to be the 2014-2015 Praxis Fellows. We are on the way to becoming a team and for this crucial achievement to become reality, time is not the only one that has to do the work. I consider introductions like this an important step towards team building. So, let me tell you what I think you need to know about me. I was born in Korca, Albania but moved to Tirana, the capital, for my high school years at the ‘Mehmet Akif’ Turkish college. I completed my undergraduate studies in International Relations and International Economics at the Middle East Technical University (METU) in Ankara, Turkey. Afterwards, I pursued a Masters degree in International Political Economy at the London School of Economics and Political Science (LSE) in London, UK. Another important experience for me was being a Visiting Scholar for nine months at the Central Asia-Caucasus Institute of SAIS Johns Hopkins University in Washington, DC. I got interested in transition economies and the role state-society relationships played in shaping or influencing the outcomes of economic policies and reforms, plus acquired skills on organizing events that involved both academics and policy makers. Now, I am in the fifth year of the Sociology PhD program, preparing a dissertation proposal on the emergence of a new form of regulation in the United States - ratings, in two fields - healthcare and finance. This project is part of my broader interest and fascination with innovation as a process of knowledge formation and manipulation. Having worked for some time with the literature on the diffusion and implementation of innovations among organizations, I have developed the conviction that we need to better conceptualize innovation, recognizing its multidimensionality and its transformation through time. I view the Praxis Program as an opportunity to engage more actively with my ideas on innovation in an environment that embodies and ‘lives’ some of the most essential practices for creative production: collaboration, interdisciplinarity, vision and passion. I am curious and eager to learn about project-based teams in action; especially the processes through which collaboration becomes possible when anchored around an idea that is developed collectively - though in the case of Ivanhoe the idea is given to us from another collectivity and other collaborative efforts. I am looking forward to getting involved and reflecting systematically on the processes through which innovation emerges and is transformed by bringing into a community of practice people of different backgrounds like me and you. I hope this experience will help solidify my identity as a researcher that is ‘in’ not ‘out of the world’. In the Praxis Program, I want to learn about the tools that can help researchers make their ideas grow and matter. I expect to broaden my knowledge and enhance my skills on project management, team work coordination, data visualization, and effective communication of ideas to multiple audiences. The most important thing I would bring to a collaborative digital humanities project - and a team like ours - is my rich experience with managing differences and change. As I mentioned above, I have passed through four different systems of education (Albania, Turkey, UK, US), wandered intellectually among three different fields of research (politics, economics, and sociology), and acquired valuable insights comparing different cultural configurations. My education has been in English most of the time but I have good knowledge of several other foreign languages: Italian, Turkish, French and basic German, Greek, and Russian. I hope who I have become till now can be a modest contribution to what we want us to become in the future - an effective team with a collaborative digital humanities project that addresses a diverse audience with global not only local concerns. Furthermore, having read some literature on collaboration and knowledge-sharing/transfer among teams and organizations, I believe I can bring to the project and the team more awareness of the processes that enable collaborations to succeed and be productive. Who I have become till now is due to several communities and beings that supported me - my families and friends, my husband Ali, our cats Sheqeri and Piperi, and our birds Dielli dhe Qielli, among others. I am grateful that I have the chance to continue becoming and grow in conversations with you dear 2014-1015 Praxis fellows supported by a supportive community like the Scholars’ Lab."},{"id":"2014-09-15-hello-world-3","title":"Hello world","author":"amy-boyd","date":"2014-09-15 06:15:58 -0400","categories":["Grad Student Research"],"url":"hello-world-3","layout":"post","content":"I’m Amy R. Boyd, a third-year Ph.D. student in English. I am interested in the British nineteenth century, especially the intersections between literature, science, and gender, as well as theories of the novel. I took enough computer science classes as an undergraduate to complete a minor in CS, so I’m excited to have the opportunity to put my long-dormant skills to use once again. Other than my half-finished digital edition of a portion of Lewis Carroll’s Game of Logic, completed for Professor David Vander Meulen’s course on editing and textual criticism, I have not had much of a chance to marry my work in literary studies with my more tech-y interests. I’m happy to be a part of this, even though I’m still not entirely certain what the Ivanhoe Game is or does or is could do in the future. I look forward to honing my coding skills on a humanties-centric project and finding my way as a digital humanist. And, of course, I’m happy to be a part of anything that gives me a chance to work with the other brilliant Praxis fellows."},{"id":"2014-09-16-lets-play","title":"Let’s Play","author":"andrew-ferguson","date":"2014-09-16 10:10:59 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"lets-play","layout":"post","content":"Hi. I’m Andrew Ferguson. Three years ago, while fighting my way through English Lit exam lists full of doorstop novels and deep-end theory, I decompressed with a hobby that seemed as far from literary scholarship as could be: watching other people play videogames online, in archived or livestreamed forms called Let’s Plays, or LPs. And, somewhere along the eighth time I saw Final Fantasy VI utterly broken by players with skill levels I found previously unimaginable, I realized my two preoccupations weren’t all that dissimilar, with each having much to learn from the others—and, what’s more, that it was the literary that had much more to learn from the videogames than the other way around. All that has now fed into my dissertation, “Let’s Play: Narrative Strategies and the Playerly Text,” attempts to read a variety of 20th/21st century novels through modes of videogame play. The authors covered range from Kathy Acker to Flann O’Brien to Colson Whitehead; the games from Portal to Pac-Man to The Binding of Isaac . There’s a few chunks out in the wild: on Vladimir Nabokov’s Pale Fire and Super Mario Bros. _; _and on James Joyce’s Finnegans Wake and Metroid ; the rest (knock wood) will be finished by the end of the year. Upshot being, I spend a lot of time watching and thinking about different ways to play things; hence, I am inordinately excited to be part of this particular year’s Praxis cohort as we think through all the different ways we can build on last year’s superb _Ivanhoe _WordPress plug-in . But it’s not so much the case of leaving my own impression on a game with a distinguished lineage; rather it’s the chance to take part in something that mirrors the best of what I’ve seen in the LP community: a collective practice of building, critiquing, rebuilding, and gradually broadening the range of responses within texts and projects, as well as the practice itself and the composition of the community. It’s a mode of criticism that, even prior to joining this particular cohort, I have come to think of as “cohortative”—at once insistent, desirous, encouraging, and passionate. Very much looking forward to getting to know everyone here, especially my colleagues—and discovering along with them new modes of play."},{"id":"2014-09-16-prism-in-the-classroom-questions-to-frame-discussion","title":"Prism in the Classroom: Questions to Frame Discussion","author":"brandon-walsh","date":"2014-09-16 11:53:31 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"prism-in-the-classroom-questions-to-frame-discussion","layout":"post","content":"Cross-posted on  my personal site . I have been touting the use of Prism in the university classroom for some time now, but a recent exchange with Annie Swafford suggested to me that it might be worth explicitly outlining how I would go about doing so. With that in mind, I’ve composed the following set of questions for how I might frame discussion of Prism in the classroom. I’ve admittedly only had very brief chances to implement the tool in the classroom myself, so the thoughts come largely out of speculation and conversation. It should be noted as well that I assume below that you have already chosen a text and categories along which it should be marked (I may write on ways to approach such choices at a later date). In what follows, I move from general questions that I think would be helpful in framing any discussion of the tool to a particular use-case in James Joyce’s A Portrait of the Artist as a Young Man . The former questions inform and engage my latter use-case. I prepare for class discussion by assembling a list of questions to be explored, and I would organize a Prism discussion around two lines of inquiry: tool-specific and visualization-specific. Some of these questions can be helpful for framing your own thoughts. Others could usefully be posed to the class as a whole as a means of framing discussion. Tool-Specific Questions How do the tool and our framing of it affect how we read the text? How is Prism’s mode of reading different from what we normally do? Is it the same that we’ve always been doing – close reading in a different form? What are the problems with the form? Can we really boil interpretation down to a series of numbers, visualize it, and move forward? Or is there more to interpretation than that? How do individual interpretations join in with the group reading? How much is the interpretive process encapsulated in the marking of a text? The visualization? The conversation that follows? How do the terms you choose for highlighting (the facets) guide the experience of reading the text? How do the explanations you provide for those terms affect the marking experience? When do the terms break down? If the terms propose a binary, what happens to that opposition over the course of the experience? Visualization-Specific Questions Which passages were marked the least for a particular category? The most? Why in either case? Which passages were particularly contentious, marked in many different ways? Where do particular categories cluster? How does the visualization show a relationship between the categories? How does your own interpretation link up to the collected visualization produced by the tool? Do the two visualizations tell us anything meaningful? Would we be able to find these meanings on our own? How does the visualization reflect the interpretive process? Why might we care more about a particular visualization for a particular reading? How is the quantified version of interpretation that Prism generates distinct from what we might learn from a discussion on our own? Can we imagine limits to this approach? The primary job of an instructor using Prism is to help the students connect the results of the tool to the larger discussions encapsulated by the marking categories. Look at the results with a skeptical eye and ask how they can be meaningfully related to the ideas and provocations of the marking categories. My favorite early use of Prism asked users to mark James Joyce’s A Portrait of the Artist as a Young Man along the categories of “modernism” and “realism.” In a class, I would intersperse observations based on the visualizations with a discussion of the passage and the two marking categories. What do we mean by modernism? By realism? How is each expressed at the level of the text? What do we mean by literary experiment? By fragment? By realist details? What different genres does the text move through? Does the text construct a coherent narrative? Putting realism and modernism alongside one another in Prism forces students to reconsider the binary, which quickly breaks down in practice. We can talk about whole novels or poems as belonging to one or another category, but can we do the same for individual sentences? For words? 80% of users at the time of this writing believe that the first word of the excerpt, “once,” is modernist. But why? If you look at the winning facet visualization, people seem primarily to be marking whole passages as one category – they are interpreting realism and modernism in chunks, not in terms of individual words. Readers tend to mark as modernist those generic changes where the excerpt suddenly adopts the form of nursery rhyme or of a fairy tale, suggesting that it is not any one genre but the shift between several in rapid succession that readers find to be modernist. The font size visualization suggests that those passages referencing physical actions by people are more likely to be associated with realist: “His father told him that story” and “When you wet the bed first it is warm then it gets cold” are marked as being especially realist. With this observation in hand, why these details? Why are the body and the bodily detail markers of a realism? Why might an association with the family suggest realism? How do they come under pressure in the face of aesthetic experiment? Obviously these suggestions are just beginnings for how to approach Prism in the classroom. Many other fascinating examples have already surfaced, particularly those that use the tool to teach basic reading and foreign language skills. Get in touch if you have used the tool in your classroom! I would love to hear how you did so."},{"id":"2014-09-16-the-digital-sea-of-exchange","title":"The Digital Sea of Exchange ","author":"jennifer-grayburn","date":"2014-09-16 05:59:03 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"the-digital-sea-of-exchange","layout":"post","content":"Greetings! My name is Jennifer Grayburn and I’m a sixth-year PhD Candidate in the History of Art and Architecture Department at the University of Virginia. My research focuses on medieval Northern architecture and its intersection with Old Icelandic texts. Using cultural memory theory and sea-basin frameworks, my dissertation explores the spread of architectural ideas, especially across the North Sea and North Atlantic. Teaching is my main passion and, over the past six years, I have worked as a library supervisor, teaching assistant, and instructor at the University of Virginia for courses like Architectural Survey I and Viking Art and Archaeology. In the Middle Ages, the sea was the main venue for organic and fluid exchange. For us, however, this process of exchange has been accelerated by the internet and is no longer limited to physical travel. Praxis is my opportunity both to satisfy my own personal curiosity about the technology we use on a daily basis and to consider more critically how technology affects opportunities to disseminate information. Exchanging information is the foundation of the learning process and digital tools allow us to do so in increasingly interactive and decentralized ways. Ivanhoe is the ideal project to consider how digital tools and creative play can complement existing educational strategies. While Ivanhoe is text-based at this point, I’m looking forward to jumping in and seeing how we might incorporate images and other creative expressions to expand its relevance to multiple disciplines."},{"id":"2014-09-17-individuality-and-collective-effort","title":"Individuality and Collective Effort","author":"steven-lewis","date":"2014-09-17 07:46:39 -0400","categories":["Grad Student Research"],"url":"individuality-and-collective-effort","layout":"post","content":"Hey everyone! My name is Steven Lewis. I’m a second-year Ph.D. candidate in Music. My research interests include late 20th century jazz neoclassicism, early jazz, and 19th century African-American secular music. In my most recent project, I explored jazz performance as a means of constructing counternarratives of black American music history. I earned a BA in Jazz Saxophone, and I see my practice as an improviser as an important extension of my research. Jazz is a music built on a balance between individuality and collective effort. The musicians in a good jazz band bring their personal styles into conversation with each other to create something new. I see Praxis as an opportunity to engage in this sort of improvisatory dialogue with my colleagues, each of whom is coming into the program with a unique set of skills and interests. The Ivanhoe project, with its emphasis on creativity and collective play, strikes me as an exciting way to engage my interests as a jazz musician while learning about digital technologies and their implications for my own work as a humanist."},{"id":"2014-09-22-a-fox-among-others","title":"A Fox... Among Others","author":"swati-chawla","date":"2014-09-22 04:19:36 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"a-fox-among-others","layout":"post","content":"The fox knows many things, but the hedgehog knows one big thing. Greetings! I am Swati Chawla, a second year PhD student at the department of history. The question, “Can the masters’ tools dismantle the masters’ house?” posed at a dhpoco  talk last year resonated with my work on Tibetan exile in India, and was the reason I applied for the Praxis fellowship. I am interested in better understanding technologies that help a homeless and stateless population create virtual homes in physical and digital spaces. As someone who worked at state museums and archives in Delhi, I am aware of the limitations of government-regulated brick-and-mortar institutions, and optimistic about the potential of digital spaces as collaborative, democratic and voluntary. But what was that about foxes and hedgehogs, you might ask… The fragment quoted above is attributed to the Greek poet Archilochus. Isaiah Berlin uses it to distinguish between hedgehog-y thinkers who relate everything to a single organizing principle, and fox-y ones who pursue random, unrelated, and often contradictory ends, or seek a variety of experiences solely for what they are in themselves.  In an academic environment that organizes scholars around narrowly defined specializations (by geographical, temporal, and thematic focus in my discipline), I believe the Scholars’ Lab is one of those rare spaces that values fox-y skills. I am excited about working in the multi-disciplinary Praxis cohort, even as I am finding my way in the discipline of history after seven years of training in literary studies. And finally, to quote the inimitable Andrew Ferguson –  Let’s Play !"},{"id":"2014-09-22-visualizing-early-america-through-mapscholar-and-beyond","title":"Visualizing Early America through MapScholar and Beyond","author":"james-ambuske","date":"2014-09-22 07:25:40 -0400","categories":["Geospatial and Temporal","Grad Student Research","Research and Development","Visualization and Data Mining"],"url":"visualizing-early-america-through-mapscholar-and-beyond","layout":"post","content":"Hello, DH World! As this is my first official post as a DH Grad Fellow in the Scholars’ Lab, I’d like to start it by thanking the folks in the Lab for the opportunity to join the team for this academic year. I feel really fortunate that I have the chance to hang out with bright and fun people for the next several months. Now on to the topic at hand. I’d like to talk briefly about a project developed independently of the Scholars’ Lab in which I played a role, before moving on to muse how that experience will bear on my work as a new DH Grad Fellow in the SLab. In the introduction to his remarkable work, In the Eye of All Trade: Bermuda, Bermudians, and the Maritime Atlantic World, 1680-1783, the historian Michael J. Jarvis asks, “what did early America look like from the deck of a ship, and how might this perspective change the ways we understand it?” This provocative question challenges scholars of early America to rethink how historical actors in a variety of contexts interpreted the world around them in spatial and geographical terms. A sailor traversing trade routes connecting London, Bermuda, and mainland colonial ports like Philadelphia or New York had a very different sense of the world in comparison to Thomas Jefferson atop Monticello or the Catawba in colonial South Carolina. What role then can the digital humanities play in our efforts to reconstruct these historical perspectives? One solution is a new tool called “ MapScholar .” MapScholar is a simple, yet dynamic interactive visualization platform that enables anyone to tell stories through the creation of digital map galleries. The program works with the Google Earth online plugin in web browsers. It gives users the ability to georeference multiple historical maps on the Google Earth globe. Archives and libraries have made a prolific number of maps available online in the last few years. This has created new opportunities for users of programs like MapScholar or Neatline to bring together different kinds of sources in new and innovative ways. In MapScholar, a number of tools permit users to annotate maps with text, shapes, images, data, and even video. Different modes allow curators to display maps as an “Atlas” or as a “Book” depending on the particular goals of the project. This ongoing initiative was conceived and developed by professors S. Max Edelson and Bill Ferster with support from the National Endowment for the Humanities, the American Council of Learned Societies, and UVA. I’ve been extremely fortunate to work with Max, Bill, and the rest of the team over the last two years to build this program and test the limits of its possibilities. We’ve created a site, Visualizing Early America: Three Maps that Reveal the New World, to demonstrate MapScholar’s capabilities. It tells the story of three key moments in early American history. The featured maps reflect European and American perceptions of colonial North America. The site also highlights some of the tools that one can use in creating and interpreting these digital galleries. I encourage you to take a look! My work on MapScholar has informed the project I’d like to pursue during my time as a DH Fellow. My dissertation centers on the massive emigration of Scots to North America in the era of the American Revolution. Im’ interested in how that migration informed Scots’ perception of the British Empire. Between 1763 and 1775 roughly 40,000 Scots left home for the colonies, and as farmers and tradesmen from both the Highlands and Lowlands removed to places like New York or North Carolina, leading figures in Scotland debated what the loss of those people meant for Scotland and the stability of the British Empire at a time when American colonists increasingly questioned their own attachment to Great Britain. I want to visualize part of this emigration phenomenon using Neatline in an attempt to understand how the local origin of the emigrants, their professions, and their stated reasons for leaving Scotland influenced the kind of discussions politicians and commentators had in trying to assess the potential consequences of this migration. In other words, I want to recreate their collective mental map and show how the changes in that map altered the arguments for or against emigration over time. Developing this project will help me to write one of the key chapters of my dissertation. The story I am telling is transatlantic in scale, and using a digital tool like the Scholars’ Lab ’s Neatline to organize the geography of Scottish emigration more effectively will enable me to clarify my dissertation’s argument. And that, I think, points to the larger potential of the digital humanities. Tools like MapScholar and Neatline can inform the direction of our scholarship by bringing to “life” historical sources in new and compelling ways. I’m excited to begin my time in the Scholars’ Lab and look forward to pushing the digital envelope, especially with my Fellows in (digital) crime, Jennifer Foy and Emily Senefeld ."},{"id":"2014-09-26-upgrading-neatline-and-omeka","title":"Upgrading Neatline and Omeka","author":"ammon-shepherd","date":"2014-09-26 07:53:08 -0400","categories":["Research and Development"],"url":"upgrading-neatline-and-omeka","layout":"post","content":"One of my first projects here at the Scholars’ Lab was to help update some Omeka/Neatline sites. These are sites we keep around as examples of our Neatline plugin for Omeka, and they were a few versions behind. While a pretty easy process to do by hand, having a script to take care of it makes it even easier for future upgrades, too. I call the script ONUS (Omeka Neatline Upgrade Script). While the script was written specifically for our purposes, it may be useful if you have an Omeka install using the Neatline plugin. It can be found here on github.com, feel free to use at your own risk. https://github.com/scholarslab/onus I perhaps went a little overboard and made the script pretty robust. I was going to take the opportunity to learn some Ruby, but ended up writing it in Bash. What is Bash (or ksh or zsh or tcsh)? Bash is a shell. When you open a terminal, or command line, what you type in is called a shell. It takes your commands and does stuff with them. They are all basically the same, but have various things they do better than others, hence the great number of them. A shell script is basically a document that has a list of commands that the shell can run. Most shells allow for some logic, like if/while/case/for statements, variables, functions, etc. Usually very basic programming. (A good tutorial for learning the command line in general is found here: http://cli.learncodethehardway.org/book/ ) I thought I could write the script very quickly in Bash since I know that language relatively well (much better than Ruby), but one thing I learned is that Bash does not handle comparing floating point numbers. Floating point numbers is a fancy way of saying numbers with decimal points, like version numbers, (ex. 2.2.1 and 1.3.1). Bash does not have an easy, default way to compare these numbers like most programming languages (like Ruby, Python, and Perl), and even other shells (like ksh and zsh). In retrospect it probably would have been better to write this in Ruby or some “real” programming language from the get go, but alas I didn’t have the need for comparing floating point numbers until most of the logic was already figured out and coded in Bash. I’ll run through how to use the script as well as go through some of the logic for working with floating point numbers in Bash. Running the Script The script runs the commands needed to upgrade Omeka from 1.5.x to the latest version, and Neatline from 1.x.x to the latest version. You would run this script on the server/computer where the Omeka installation is found. You can pass the path to the Omeka install to the script, or it will prompt you for it. The script can take four flags/switches/options: -L : Upgrade Omeka and Neatline to the latest and greatest versions. (Note: “Pre-2.0 versions of Neatline can’t be upgraded directly to version 2.2. Upgrade to version 2.0 first!”)\n-n [number]  :  Where [number] is a valid Neatline tag from https://github.com/scholarslab/Neatline. This will upgrade Neatline to the specified version number. Note, all Neatline version have three digits x.x.x.\n-o [number]  :  Where [number] is a valid Omeka tag from https://github.com/omeka/Omeka. This will upgrade Omeka to the specified version number.\n-s  :  Do not upgrade Omeka\n-t  :  Do not upgrade Neatline Basic Usage: Download the onus.sh file from the github repo. It doesn’t really matter where you put this script, but your home directory is a good location. On the command line, enter the directory where the file is located and type the following command. This will allow you to execute the script. chmod u+x onus.sh If needed, change any default variables at the top of the file, ex. paths to MySQL, PHP, and git. The default is to use your account’s system default. This may be different based on your system, if for example you are testing with MAMP  on your Mac computer. MYSQL=”/path/to/bin/mysql”\nMDUMP=”/path/to/bin/mysqldump”\nMADMIN=”/path/to/bin/mysqladmin”\nPHP=”/path/to/bin/php” Change one ‘sed’ line if needed, to work with GNU/Linux. Remove the empty double quotes after -i sed -i “” “80s/.*/${migrate}/” ${path}/plugins/Neatline/migrations/2.0.0/Neatline_Migration_200.php Run the script by typing the following at the command promp; make sure to type the period ( . ) before the forward slash ( / ) and then the script name. ./onus.sh /path/to/omeka/install This will upgrade Omeka and Neatline to the next available major release. Run the script as many times as needed to get to the latest version. For more examples, please visit the GitHub page at  https://github.com/scholarslab/onus NOTE: One important thing to be aware of is when you  upgrade from 1.5.x versions of Omeka and 1.x.x versions of Neatline the Neatline functionality will be broken until Omeka and Neatline are upgraded to the very latest stable versions. So if you plan on using this script, then plan on upgrading to the very latest versions! ## Getting Back Neatline Exhibits After Upgrading to 2.0.0 We noticed one big problem when upgrading sites with versions of Omeka and Neatline previous to 2.0.0 by hand. This section will detail the steps to fix it by hand for demonstration purposes, but they are included in the script. Omeka and Neatline both go through some significant database (and code) changes from 1.5.x to 2.x.x. The biggest seemed to be that the upgrade script for Neatline didn’t “take” and needed to be done manually. For these sites, the Neatline Exhibits did not get transferred from the old database table to the new table. The first step is always to make a backup copy of the database and files. That way if anything goes awry, you can easily put things back together. To back up the database, simply take a MySQL dump. mysqldump -uUserName -pPassword databasename &gt; databasename.sql Do this in the main directory of Omeka. Then make a zip file of the entire Omeka directory. zip -r omeka-backup.zip /path/to/omeka/ Next, deactivate any plugins, including Neatline and NeatlineMaps. One of the big changes with the 2.x.x version is that NeatlineMaps is rolled into Neatline. Grab a 2.0.x version of Omeka. Either do this with github git clone https://github.com/omeka/Omeka NewOmeka or with a zip file. wget http://omeka.org/files/omeka-2.0.4.zip\nunzip omeka-2.0.4.zip Add the 2.0.0 version of Neatline plugin into the NewOmeka/plugins directory, along with any other upgraded plugins you may need. NeatlineText, NeatlineSimilie and NeatlineWaypoints may be needed if you used that functionality in the previous version. Copy the db.ini file from the old installation to the NewOmeka/ directory. Now load the admin page for NewOmeka/ in the browser: http://domain/NewOMeka/admin/. Upgrade the database and login to upgrade and reactivate the Neatline plugin and other plugins as needed. You may notice things go smoothly, except the existing Neatline exhibits may not transfer. To get them into the new database tables, add the following two lines at line 80 in the NewOmeka/plugins/Neatline/migrations/2.0.0/Neatline_Migration_200.php file: $fc = Zend_Registry::get(\"bootstrap\")-&gt;getPluginResource(\"FrontController\")-&gt;getFrontController();\n$fc-&gt;getRouter()-&gt;addDefaultRoutes(); Run the following database command to allow the background process to run: mysql -uuser -ppassword database --execute=\"UPDATE prefix_processes SET status='starting' WHERE id=1;\" Run the following php command to get the processes started. /path/to/bin/php /path/to/NewOmeka/application/scripts/background.php -p 1 Finally, if everything in the new version looks good, you can remove the old and replace it with the new. mv /path/to/omeka/ /path/to/old-omeka/\nmv /path/to/NewOmeka /path/to/omeka Some Script Logic Initially, I used the script to upgrade both Omeka and Neatline to the next higher version, going through every single minor version incrementally. When upgrading from Omeka 1.5.1 and Neatline 1.0.0 to the latest versions (2.2.2 for Omeka and 2.3.0 for Neatline), I had to run the script over 20 times! That was way too labor intensive and time consuming, so next I added some logic to just skip to the next major release. That dropped the times needed to run the script down to four. This is how the script behaves if run without any options. But I could do better than that! I added in some command line options/flags that allow you to upgrade to any Omeka or Neatline version you specify. Now you can upgrade from Omeka 1.5.x and Neatline 1.x.x directly to Omeka 2.0.4 and Neatline 2.0.0, then right to Omeka 2.2.2 and Neatline 2.3.0. Two steps! Bash and floating points As mentioned above, Bash does not work with floating points, so I had to create a function to deal with that. Dealing with version numbers, especially with minor version numbers kind of requires the need to compare floating point numbers… In the script I use two different functions: # Compare two floating point numbers.\n# Usage:\n# if $( compare_floats number1 number 2 ); then\n#     echo 'number1 is less'\n# else\n#     echo 'number2 is less'\n# fi\n#\n# result  : the string 'true' or 'false'\n# number1 : the first number to compare\n# number2 : the second number to compare\n# Read it as: is number1 less than number2? It returns 'true' if number1 is\n# less, and 'false' if number1 is greater.\nfunction compare_floats() {\n    echo | awk -v n1=$1 -v n2=$2 '{if (n1&lt;n2) printf (\"true\"); else printf (\"false\");}'\n} This function basically compares two numbers. It outputs true if the first number is less than the second number, and false if the first number is greater than the second number. Another way to think about it would be to ask the question, is the first number less than the second number? It returns ‘true’ if that is true, and ‘false’ if that is false. The function is basically echoing the result of the awk command, so let’s look at what it does a bit more closely. awk -v n1=$1 -v n2=$2 '{if (n1&lt;n2) printf (\"false\"); else printf (\"true\");}' First we call the awk command with two ‘variable’ flags. The -v says that the next expression sets a value to a variable, so n1 is the variable and $1 is the value. The $1 and $2 are actually variables themselves. When you call this function later in the script, you pass it two numbers. These numbers are automatically assigned to variables, the first number to $1 and the second to $2. The next part of the awk command processes an if/else statement; the part within the single quotes and curly braces. The single quotes are required by Bash, and the curly braces tell awk to process the action. The part within the braces is the basic if/else statement. If the comparison is true, then do the first step; else/otherwise do the next step. So, in our case, if the first number (n1) is less than the second number (n2), then print “true”, otherwise print “false”. See here if you are interested in learning more about the ways of awk http://www.grymoire.com/Unix/Awk.html#uh-1 . This function is used in two ways in the script. First, it just does a basic check in an if statement. Check if this number is less than that number: if $( compare_floats $n_upgrade 2.0.0 ); then This checks if the next Neatline version is less than 2.0.0. If that is true, it runs some commands. Second, we can use the function as part of a multi conditional check: if [[ -d $path/archive/ &amp;&amp; $( compare_floats $o_upgrade 2.0.0 ) == \"true\" ]]; then Here we check if the /archive/ directory exists (used with Omeka versions less than 2.0) and we also check if the next version to upgrade Omeka to is less than 2.0.0. If both of those conditions are met, then we can run some code. Otherwise we do some more conditional checking and different code running. And, the next function… # Pass the current version first, then the array\n# the function echoes the version just greater than the current version,\n# i.e., the next version to upgrade to.\n#\n# Usage:\n# variable=$( get_next_version $num array[@] )\n#\n# variable : the next version greater than $num\n# $num : the current version\n# array[@] : an array of all possible versions\nfunction get_next_version() {\n    num=$1\n    declare -a ARRAY=(\"${!2}\")\n    for i in ${ARRAY[@]}\n    do\n        if awk -v n1=$num -v n2=$i 'BEGIN{ if (n1&lt;n2) exit 0; exit 1}'; then\n            echo $i\n            break\n        fi\n    done\n} With this function we are doing a similar thing, comparing two numbers, but we are comparing one number against a list of numbers. To run this function you pass the current version and a list of possible version numbers. The function will compare the number you pass it with the list, and echo the next highest number from the list. Let’s break down this function as well. num=$1\ndeclare -a ARRAY=(\"${!2}\") These first two lines are simply getting the input from calling the function and turning them into an internal variable and an internal array. You may remember the ‘$1’ is the first number passed to the function. But where is the ‘$2’? It’s expanded and changed a bit because it is actually an array, or a list of numbers, rather than a single number. In Bash, you can write a variable with a dollar sign (ex. $myNumber) or with curly braces (ex. ${myNumber}). The second option allows you to string multiple variables together and do basic string or array manipulation. For example we could put two variables together: ${myNumber}${anotherNumber} In this case we are declaring a new array named ‘ARRAY’ and pre-populating it with the values of the passed array. We do some indirect expansion to get the values of the passed array, that’s the “${!2}” part. Basically, this part says set the values of the array passed as the values of the new array (in this case we call it ‘ARRAY’), rather than setting the name of the array as the value of the new array. See here for more explanation on Bash arrays and indirection http://wiki.bash-hackers.org/syntax/arrays#indirection Next we do a standard for loop to go through every element or value of the array. for i in ${ARRAY[@]} Next we check the current version number against each number in the list of version numbers, using the same awk command as before. if awk -v n1=$num -v n2=$i 'BEGIN{ if (n1&lt;n2) exit 0; exit 1}'; then In this case, though, instead of printing ‘true’ or ‘false’ the command exits without errors (exit 0) or exits with an error (exit 1). The if statement that includes the awk command will execute the code within the if statement only if the statement returns true, that is, if the awk command exits without errors. So, if the first number is less than the second number, then exit without errors (true) and echo the second number. This becomes the next version number. Basically you can think of this as getting the current version number, then comparing this to a list of all possible version numbers, starting with the lowest number. When the current version is greater than the possible version number, do nothing. When the current version is not less than the possible version number, then the possible version number is greater, and therefore should be the next version to upgrade to. The break statement within the for loop tells the for loop to stop looping when it has found the next version number. Conclusion Few, that was a long winded explanation of the script. The two above functions, and accompanying explanation, could have been avoided by using a programming language (like Ruby, Python, or Perl) instead of a shell script because they handle floating point comparisons naturally. So, just to summarize, if you have Omeka and Neatline installed already, and would like to upgrade to the latest version, then you can run this script on the server where Omeka is installed. It requires that you have git installed. All of the other programs it depends on are pretty standard."},{"id":"2014-09-26-what-the-junk","title":"What. The. Junk.","author":"jeremy-boggs","date":"2014-09-26 10:07:28 -0400","categories":["Experimental Humanities"],"url":"what-the-junk","layout":"post","content":"So I had to take a sick day yesterday. Stuffy nose, scratchy throat, headache, grouchiness. Attempting to brighten my day, Wayne sent me a nice text message: ![Extra parts](http://static.scholarslab.org/wp-content/uploads/2014/09/extra-parts.jpg) Extra parts Turns out he was joking. He actually said “totally” when I asked, but that turned out to be a lie. Today, here’s what I came back to: So OK, maybe it’s not as bad as it seems. Most of the time when we’ve clogged up the extruder, it’s because we’ve leveled the build platform too close and caused it to scrape against the extruder, essentially mashing up the filament on the nozzle. Unloading and reloading the filament usually solves this, followed by running the leveling utility again. I tried all that this time, but to no avail. What. The. Junk. So as is typical of my kin, I feared the worse. I expected to spend a ton of time taking the extruder apart, soaking the nozzle in acetone or some other crazy substance to dissolve the filament that had built up in the nozzle. (This instructable has a good breakdown of the steps to unclog a 3D printer nozzle .) With a heavy sigh that is also typical of my kin, I proceeded to disassemble the extruder. First thing you need to do, of course, is turn off and unplug the printer. You’ll touch wires and things, and its better to be safe than sorry. Then you have to take off the fan assembly in front of the extruder, and pull out the drive assembly: ![Printer carriage with fan and stepper motor removed.](http://static.scholarslab.org/wp-content/uploads/2014/09/disassembled-1024x768.jpg) Printer carriage with fan and stepper motor removed. At this point, I took a look at the drive assembly, which in our case includes the fabulous spring-loaded arm replacement for the drive. I noticed something a bit odd: ![Stepper motor with spring-loaded arm](http://static.scholarslab.org/wp-content/uploads/2014/09/stepper-motor-assembly-1024x768.jpg) Stepper motor with spring-loaded arm ![Close up of drive block.](http://static.scholarslab.org/wp-content/uploads/2014/09/stepper-motor-assembly-detail-1024x768.jpg) Close up of drive block showing obstruction Turns out there was junk in it. What the junk indeed. I pushed the arm down to take some tension off the drive, and used a knife to wiggle the bit of plastic that had gotten lodged in there. Now it’s working like new. Or at least like it did before it got clogged. Not sure how this little piece of plastic got stuck in here, but my guess is that it broke off the end of the filament when unloading it. The nozzle wasn’t clogged at all, but this was preventing new filament from going completely through the drive assembly into the nozzle."},{"id":"2014-10-03-bonjour-je-mappelle-julia","title":"Bonjour! Je m'appelle Julia.","author":"julia-schrank","date":"2014-10-03 09:20:39 -0400","categories":["Experimental Humanities"],"url":"bonjour-je-mappelle-julia","layout":"post","content":"Bonjour, Laboratoire des Savants! Hello! I’m Julia, one of the new Makerspace student consultants. When I’m not being a smiling face at the SLab desk, I am a first year M.A.-Ph.D. student in French. I am brand-new to Charlottesville and to UVa, and so far I’m loving everything, particularly this miniature Tour Eiffel, which was my first ever 3D print! [![Julia holding a tiny, black 3D-printed Eiffel Tower.](http://static.scholarslab.org/wp-content/uploads/2014/09/photo-e1411154953963-225x300.jpg)](http://static.scholarslab.org/wp-content/uploads/2014/09/photo-e1411154953963.jpg) Just look at it. Seriously. 3D printing is definitely what brought me to the Makerspace, since I’m a self-professed Material Girl. This moniker describes me in my everyday life but also me as a researcher: I am obsessed with objects. The period I hope to study is centered around 1900 in Paris, which as you may know was the year the city of light hosted the World’s Fair. French Studies scholars call this period the Belle Époque, which translates literally to “the Beautiful Period.” The Belle Époque was given its name for many reasons, but my own personal favorite has to do with the glut of extremely beautiful, refined objects created in this era. However, since I am not yet considered cool enough by Paris museums to touch and examine the extant objects from this period, I have to find another way to gain the tactile information I feel that I need to appropriately assess the value of these items. This is where 3D printing can meet my research needs in a spectacular, unprecedented way. In my time at the Makerspace, I hope to make the untouchable touchable, and invite friends, colleagues, students, and Scholars’ Lab visitors to hold pieces of Paris’s past in the format of the future. I also hope to bring to life design ideas from my favorite artists of the period and recreate fantastical objects that were lost to history. It will be a long journey for me, as I am starting at square one with this technology, but I hope it is one I can share with all of you to help you come closer to solving your own research questions with the help of our Makerbots. To start this mutual journey, I’d like to leave you all with a stanza from a song by Regina Spektor that addresses the consciousness of objects in a museum: “First there’s lights out, then there’s lock up Masterpieces serving maximum sentences It’s their own fault for being timeless There’s a price to pay and a consequence All the galleries, the museums Here’s your ticket, welcome to the tombs They’re just public mausoleums The living dead fill every room But the most special are the most lonely God, I pity the violins In glass coffins they keep coughing They’ve forgotten, forgotten how to sing” Let’s free the objects from their cages and let them sing again in glorious 3D! Allons-y!"},{"id":"2014-10-11-minard-napoleon-neatline","title":"Minard + Napoleon + Neatline","author":"david-mcclure","date":"2014-10-11 05:15:56 -0400","categories":["Geospatial and Temporal"],"url":"minard-napoleon-neatline","layout":"post","content":"[Cross-posted from dclure.org ] Open the Exhibit Yesterday I made the hop across the country to Boston for the NEH Workshop on Digital Methods for Military History at  Northeastern University, where I’ll be giving a couple of workshops about Neatline and soaking up lots of interesting new projects from old friends and new friends alike. Beautiful fall foliage aside, I’m very excited about this! Ever since we worked on the Hotchkiss projects back in 2012, Neatline and military history have been a great pairing. Which is no accident - military history is often about how things change over time on maps, which is basically the tag line for Neatline. In fact, I’ve always assumed that military history played a pretty big role in inventing the whole vocabulary of visual concepts and techniques that have been picked up by digital tools like Neatline - the flowcharts, the arrows, the diagrammatic ways of representing how things move from one place to another. Hotchkiss was using colored pencils to scratch out annotations onto his battle maps back in the 1870s, which is more or less exactly what Neatline is, minus the computer screen. Anyway, when I started putting together the workshop, I decided to use this as an excuse to build out a little Neatline exhibit that I’ve been rolling around in my head about for about three years - an interactive version of Charles Minard’s classic flow diagram of Napoleon’s 1812 invasion of Russia . This is not an original idea. Minard’s map is sort of like the “Stairway to Heaven” of digital mapping and information visualization, and it’s been remade digitally dozens of times. But, I decided to give it try, and see if I could find some kind of interesting riff. I started out by georeferencing a scan of Minard’s diagram, and then traced out vector annotations on top of each of the individual segments that represent the deteriorating size of the Grande Armée over the course of the invasion: Once the basic structure in place, I realized that I didn’t really have an intuitive sense of the scale of the whole thing - how far was it from the Neman River to Moscow? How long did it take? It turns out that the march was about 540 miles in each direction. I fired up Illustrator, blocked in a little ruler-like shape, and dragged out the annotation along the top of the map: Then, the question of time. This seemed like a good excuse for some d3. I made a little chart on the right side of the screen that plots out the size of the French army over the course of the ~5-month interval of time between when Napoleon crossed the Neman on June 24 and when that last little bit of the army stumbled back out of Russia in December 14. Then, in the exhibit theme, I wrote a little bit of Javascript that wires up the graph with the vector annotations on the map - hover on the graph to highlight the corresponding blocks in Minard’s diagram, and click to zoom to that location: Anyway - simple but fun. I’m looking forward to spending the next two days learning from people who actually know something about military history!"},{"id":"2014-10-21-come-explore-the-makerspace","title":"Come explore the Makerspace!","author":"ethan-reed","date":"2014-10-21 13:01:11 -0400","categories":["Experimental Humanities"],"url":"come-explore-the-makerspace","layout":"post","content":"Hello! My name is Ethan Reed. I’m a second-year PhD candidate in UVa’s Department of English, and one of the Student Assistants in the Makerspace at the Scholars’ Lab. For me, maker technology represents a powerful opportunity to change the ways we teach and the ways think. This may seem obvious, but for me the Makerspace is also a learning space as well as a thinking space. I think that even as people perform research there and investigate problems, that process is itself a kind of critical work. Fortunately there is a word for this lovely concept: critical making. In  “Critical Making: Conceptual and Material Studies in Technology and Social Life,” an article by Matt Ratto (that I found through this thoughtful and informative post by Jon Johnson at UVic’s Maker Lab), Ratto takes a minute to look at how something like critical making might address the “disconnect between deterministic, conceptual understandings of the role of technology in social life, and the more material and nuanced understanding of how one relates to them.” He then puts the goal of it as follows: “Our goal is therefore to use material forms of engagement with technologies to supplement and extend critical reflection and, in doing so, to reconnect our lived experiences with technologies to social and conceptual critique.” This is awesome! I couldn’t agree more, and find Ratto’s idea to be one of the main ways I think about our Makerspace at the Scholars’ Lab. Having an end product is great, but that making process can be just as, if not more important. As I connect this to my own research on things like global literary networks, systems of cultural value, and how they connect to the individual authors and texts that interact through them, knowing how real examples of networks and systems operate seems crucial. More generally, I think that leaving technology “black-boxed” and not knowing how it works under the hood has material consequences in how authors write about technology and how we read, think, and talk about what they’ve written. I also think it would be in bad faith not to mention simply how much fun it is to tinker with these technologies. There is a specific kind of joy in hearing and smelling a Makerbot hard at work printing something found on Thingiverse . Watching it offers that familiar kind of meditative hypnosis inspired by campfires burning wood or washing machines spinning clothes. Of course, the best part is then learning how and why it all works the way it does, getting your hands on the thing to tinker with it for a while and think about the results."},{"id":"2014-11-06-rules-and-flexibility-learning-from-games-for-ivanhoe","title":"Rules and Flexibility: Learning from Games for Ivanhoe","author":"joris-gjata","date":"2014-11-06 09:02:21 -0500","categories":["Grad Student Research"],"url":"rules-and-flexibility-learning-from-games-for-ivanhoe","layout":"post","content":"Do we want well-defined rules and roles? Do we want them to be fluid? Can rules and roles provide creative fluidity and playful flexibility? These questions have been a recurring theme in our conversations with the Praxis team as well as in our meetings with some of the Scholars Lab members. For me, questions about the importance of rules and roles emerged after our first introduction with the Ivanhoe project - when we learned about its history and vision. My immediate reaction to Ivanhoe was that it aims to do too much as it is meant to be anything you want. Thus, I thought the freedom it offered to users was theoretically attractive - as it allowed them to choose having rules or not, and put no limits to their expression of creativity - but not useful in helping them act and keep the game going. This ‘freedom through design’ unfortunately seemed to unintentionally limit players as it did not offer a clear understanding of what the purpose of the game was. Players risked isolating themselves and others with their moves: less interaction with others also made the game feel less of a game and more of another-thing-on-my-to-do-list. Too few rules, too much fluidity and ambiguity! This was my first thought on why I was confused and suspicious about Ivanhoe as a game. I was given too many options and were left with so many decisions to make. As a user/player I was not even asked the questions - e.g. do you want to play a game with rules or without rules? - I had to think of them myself. The challenges in playing Ivanhoe as a game for me seemed to come out of too much complexity, a lot of fluidity, unlimited flexibility and not clearly defined purposes of use. These feelings and thoughts could be just specific to my experience or because of my culture. As some researchers have noticed, the ability to perceive choices and the understanding of freedom changes from culture to culture (watch this fascinating TedTalk on The Art of Choosing ) As the 2014-15 Praxis team, we discussed this issue briefly during the process of drafting our charter. Previous charters had referred to the fluidity of roles and rules. While we understood the purpose of this statement - to allow for a processual definition of roles and rules and not assume that you know from the start who you are and what you do best - i.e. acknowledging individuals’ becoming rather than being - we were skeptical about the assumption it implied, that clearly defined rules and roles are more detrimental to creative work. This week we have to decide in what direction we want to go with the Ivanhoe project and what roles we are interested in taking to realize our goals within the Praxis program. The question of defining rules and roles has reemerged recently in our discussion about what aspects of design and development we want to concentrate on in order to make Ivanhoe simpler and more accessible. Ivanhoe seems to require some kind of shared interest that makes individual players feel part of a group, a team or a greater collectivity brought together by the game. In its actual development and conceptualization, this game is different from Kari Kraus and her collaborator’s game DUST, as it lacks a clearly defined purpose, a particular audience and a narrative. It resembles much more a game like Minecraft in the flexibility of use it gives players. Nevertheless, even when compared to Minecraft, it lacks some design features that could make it an engaging game, one that keeps you in for a considerable time: visualization of the flow of the game i.e. a story line and extensive documentation that offers different scenarios for play. In this context, if I would have to choose between enhancing the playfulness and game-ness of Ivanhoe through new design features within the game - e.g. through turn-taking and visualization of the path/progression of a game through a map - and motivating audience through the provision of extensive documentation of different games and options available in this platform, I would prefer the former. I think it is important to acknowledge the power of well-designed rules and their ability to enable creativity and learning. Rules can be seen as channeling flexibility and fluidity into useful streams and productive avenues. We cannot assume the playfulness will break in and make Ivanhoe a game that keeps you involved and excited. Definitely, rules cannot be perfect but they can be a good starting point for action. As Joseph Schumpeter and Karl Polanyi note in their works - Capitalism, Socialism, and Democracy and The Great Transformation respectively -in advanced capitalist societies like the United States, regulation tends to be seen with suspicion because it is viewed as in opposition to freedom and democracy - much valued principles in American culture. They pointed out to the problems of propagating this myth that tended to silence any debate on the design of rules and different ways of organizing life. I see their point also highlighted by Abbott and Snidal (2000) in their article theorizing the use of hard law and soft law in the international context. Among others, they argue that less formalized rules i.e. soft law are a temporary mechanism of gaining time needed to negotiate large differences among parties and that in international negotiations, developing countries preferred hard law i.e. more formalized rules because soft law tended to benefit much more those already in power. In short, don’t fear the rules! Let’s talk more about the KIND of rules and regulation we want and need to best realize our objectives. Regulation is a necessary enabling mechanism. Maybe to further develop Ivanhoe as a game, we need to reflect on our own game experiences: what kept you engaged in the game? Was there a particular element of that game that made you stay active as a player? Were there rules that enabled a sense of community or shared purpose to drive the game? What kind of rules make a game good or bad for you?"},{"id":"2014-11-07-on-not-knowing-what-im-doing","title":"On Not Knowing What I'm Doing","author":"steven-lewis","date":"2014-11-07 06:50:00 -0500","categories":null,"url":"on-not-knowing-what-im-doing","layout":"post","content":"A couple of weeks ago, Jeremy and the rest of the Scholars’ Lab staff helped us learn the basics of HTML, CSS, and Git. Then I promptly forgot them. Then, with wonderfully patient help from the staff, I remembered them, only to forget them again. After putting my limited HTML and CSS knowledge to good use working on my web page, I’m cautiously optimistic that I won’t take all semester to figure out what I’m doing.\nLike many people, I spend most of my time doing things that I already know how to do. Doing things that I’m already good at doing makes me feel good about myself. The culture of graduate school only reinforces this habit. I’m sure that I’m not alone in admitting that, when a professor or colleague in one of my seminars mentions a scholar whose work I don’t know, I often mumble that I’ve “read a little bit of” their work or respond with a sage nod of affirmation, or a knowing grunt. There’s an overwhelming pressure to know everything about everything, or to fake it until you do.\nAll of this has made the past few touch-and-go weeks of basic HTML, CSS, and Git alternately terrifying and liberating. It’s been a very long time since I’ve learned a completely new skill and been totally unsure of what I’m doing. I can’t remember the last time I approached someone after a lesson, looked them in the eye, smiled sheepishly, and asked them to re-explain everything that they just covered, but more slowly, please. At the same time, being in an environment where I’m encouraged to ask questions, make mistakes, and play around with concepts makes me feel like a child in the best way possible. I just hope that my completed web page looks like it was made by a grown person and not an eleven year old who audited a computer science class."},{"id":"2014-11-11-steps-taken","title":"Steps Taken","author":"andrew-ferguson","date":"2014-11-11 11:02:47 -0500","categories":["Grad Student Research"],"url":"steps-taken","layout":"post","content":"It’s a busy time around the Praxis Lab. At the moment our attention is divided between conceptual thought on the future of Ivanhoe, and practical education in the basic tools we’re going to need to carry out any of our concepts—plus, of course, the little external distractions of classes, comprehensive exams, job applications, etc. As a result, everything feels very baby-steppy at this point, such that I think it’s worth considering and celebrating the steps we have taken and realizing that they may not be so small as they seem. First, we have now delivered our charter . This document showcases the collaboration and camaraderie we have developed in these opening months, between movie viewings, foodie chat, and coffee machine queues. While the charter is important of itself, it’s also something we know that we can revisit as needed; it’s the interpersonal bonds we will be relying on to carry us through the project development. Second, we have all established a web presence, including rudimentary personal websites (my own of which I’ll link once I’m a little more confident about it…). This shows off an initial development of skills in HTML, CSS, and Git that will strengthen with use over the Praxis year and beyond. Third, we are already making plans for presentations about aspects of Ivanhoe, especially for next summer—while many deadlines are still several months off, DH2015 ’s just hit a few days back, and we put in a poster-presentation abstract in hopes both of having it approved, and also of figuring out over the next 6 months what we’ll actually have to show in Sidney. Finally, we’re all coming to grips with our thoughts about what Ivanhoe is, and what it could be—between lots of talks about the nature of play, the differences between games and platforms, and the pitfalls of gamification (that may just be me, more in a future post)."},{"id":"2014-11-14-podcast-kari-kraus","title":"Podcast: Kari Kraus on Humanistic Design","author":"laura-miller","date":"2014-11-14 07:36:51 -0500","categories":["Events","Podcasts"],"url":"podcast-kari-kraus","layout":"post","content":"F inding Faultlines: An Approach to Humanistic Design Historically we know that many new technologies have inherited parts from prior technologies. The skateboard remediated the surfboard; the camera pilfered from the gun; the telephone harvested batteries and wires from the telegraph; and early models of the phonograph borrowed the treadle mechanism of the sewing machine. In each of these instances, the logic of part-whole relationships governs the design. “Many of a technology’s parts are shared by other technologies,” notes Brian Arthur in The Nature of Technology, “so a great deal of development happens automatically as components improve in other uses ‘outside’ the host technology.” [1] Drawing inspiration from this assemblage model of design, I’ll report on research I recently conducted at the University of Maryland investigating how individuals locate the fault lines in objects and analyze them into their component parts. I’ll discuss several potential application domains, including the design of fragmented or non-finito products: intentionally unfinished things [2], such as a sketch, the torso of a statue, or (in the case of my own work) an alternate reality game that incorporates mobile and web apps. Kari Kraus is an associate professor in the College of Information Studies and the Department of English at the University of Maryland. Her research and teaching interests focus on new media and the digital humanities, digital preservation, game studies and transmedia storytelling, and speculative design. She is writing  a book  about how artists, designers, and humanities researchers think about, model, and design possible futures.  In addition, she is collaborating on DUST, a multiplayer collaborative online adventure for students in middle and high school, created through a partnership between Brigham Young University, University of Maryland, College Park, and NASA, with funding from the National Science Foundation. [1] Qtd. in Kevin Kelly, What Technology Wants (New York: Viking, 2010) 45. [2] Jin-min Seok, et al., “Non-Finito Products: A New Design Space of User Creativity for Personal User Experience,” CHI 2014, Toronto Canada. This talk was recorded in Alderman Library, Rm 421 on October 20, 2014.  If you encounter problems with the audio, please email scholarslab@virginia.edu ."},{"id":"2014-11-18-playing-with-toast-our-first-ivanhoe-game","title":"Playing with toast: Our first Ivanhoe game","author":"jennifer-grayburn","date":"2014-11-18 07:00:44 -0500","categories":null,"url":"playing-with-toast-our-first-ivanhoe-game","layout":"post","content":"We were pleased—and perhaps a bit surprised—that we completed our Praxis Charter so painlessly. It was both a test run to see how effectively we could work together as a new team and an opportunity to synthesize our many divergent ideas and goals for this experience. In terms of our work dynamic, we were quickly introduced to what will likely be a major reoccurring obstacle: our schedules simply do not overlap. We eventually did find one precious hour during which all six of us are available and we tentatively reserved it each week in case we need it. Generally, though, we developed a system that seems to work well for us: balancing critical, individual contributions with short, effective meetings. By mauling over drafts and comments independently before our meetings, we were able to make the most of our meeting time and plowed through our major revisions and concerns as a team. The ease of writing our charter was likely assisted by our shared goals for the Ivanhoe platform. While acknowledging that we all want to gain something different from the Praxis Program, we generally agreed that our ultimate product —the redesign of Ivanhoe—should be open, engaging, accessible, and flexible in order to appeal to as many players and communities as possible. We do not want Ivanhoe to be a neglected platform or a mere requirement for a classroom assignment; our goal is to capture the original sense of play so clearly embodied in the early Ivanhoe games played by Jerome McGann, Johanna Drucker, and Bethany Nowviskie. In our charter, we agreed that, “while committed to accessibility, the team will be wary of diffusing our attention among too many goals, platforms, or disciplines. We will do our best to focus on a smaller set of outcomes to be developed to completion.” Yet, this is easier said than done! After playing a few Ivanhoe games as a cohort, we began to consider how exactly we could redesign Ivanhoe to inspire the sense of play we desire. In our first Ivanhoe game,“ Toast Sandwich,” which was selected for our shared love of all things food and cooking, we responded as various characters to Mrs. Beeton’s infamous toast sandwich recipe. We agreed to make this an open game with no rules or restrictions regarding roles and moves. While fun, the game itself petered out fairly quickly and we recognized a few obstacles that inhibited creative, interactive, and extended play. First, when making a move, we could only respond to one other previous move. While this did not alter the game directly (we could still write our move in a way that incorporated previous moves), it did limit how those moves were tracked and ordered by the game. Second, we were not able to see the previous moves while making our own move without opening another tab. This made it difficult to incorporate or refer directly to past moves as the game commenced. Finally, with no notification function, it was just far too easy to forget about the game. If no one makes a move and no one remembers to check for other moves, the game ends with little opportunity to resume or follow-up. While not the best game, our play was nevertheless instructive! Already, we felt that multiple functions needed to be altered: multiple responds, a split interface of some kind, and a notification system are necessary to help facilitate intuitive and fun play. Yet, on the other hand, we also discussed our encounter with the formal design of the website and Ivanhoe interface, which seemed clunky and dated. Could we also incorporate a formal redesign to help foster excitement and desire to play the game? Could we design or display the games in a way that would make them easily accessible to those not actively playing? On a related note, there seems to be a need to build up a collection of documentation or example games showing  how to play. One obstacle that we encountered was perhaps too much flexibility. Recently, Praxis Fellow Jordis Gjata grappled with this issue in her blog “ Rules and Flexibility,” asking if total flexibility or more stringent game-like rules were needed to enhance players’ experiences. So much for our focus on smaller outcomes! We quickly amassed a workload impossible to complete in our short time frame. To me, the question about which aspects we need to focus on is tied directly to the needs of our players. But, who are our potential players?! We talked a lot about the use of the Ivanhoe platform within pedagogical frameworks, targeted for classroom use and skill development. Yet, fellow Praxis Fellow Andrew Ferguson, in his blog “ Steps Taken,” brought up a significant concern about the arbitrary ‘gamification’ of required assignments and asked if this forced play would ultimately diminish the playful atmosphere that we desired. As a result, we considered how a platform like Ivanhoe could also be useful for people already engaging in such role play and collaborative writing—not for a grade, but rather for fun. There is no clear path forward, as our next step seems to be enmeshed in the question of our players’ motivation: will we be creating a platform that helps to excite and motivate students required to participate for a course grade or that tries to entice writers and players already motivated to play to use this particular platform? In an attempt to better understand the implications of this question, I spent the last few weeks researching what we hoped would the focus of the Ivanhoe experience: that is, creative play. Specifically, what is it? Can play be used for education purposes? Is there a way that we can inspire it with the structure or design of Ivanhoe? I plan to answer these questions in my next blog post."},{"id":"2014-11-19-No_Content_Found","title":"Old Wine in New Wineskins","author":"zachary-stone","date":"2014-11-19 06:55:22 -0500","categories":null,"url":"No Content Found","layout":"post","content":"Last week the Scholar’s Lab and The Medieval Colloquium @ UVA had the pleasure of hosting Professor Kathleen Kennedy . Kathleen came to us from Penn State-Brandywine where she teaches all things Medieval. Recently, though, Kathleen’s interest in medieval materiality—with particular regard to the book—has lead her in some new directions. Over the course of a workshop and a talk, and a lot of food, Kathleen encouraged us to think about resonances of between modern and medieval media technology and presented two related ideas. First, drawing on her experience with the few medieval texts surviving in large (200+) numbers—such as the Wycliffite Bible, Books of Hours, and late medieval statutes—she articulated the need for a “prosopography of the book.” As she walked a group of graduate students through a recent publication on the topic she demonstrated how large data sets might influence our understanding of individual codices. In her model Book History and Codicology—on discipline focused on social milieu the other on individual objects—are brought together in composite readings made possible, in part, by technological advances. In her talk she expanded on these thoughts by situating “Old Media Studies” in relation to both New Media Studies and DH. In a provocative talk that brought together a diverse array of scholars and students she made the case for thinking about medieval texts and books in Media studies terms. Her talk explored both the intellectual opportunities such an approach provides as well as the pragmatic work the label might accomplish. Moreover, she argued that the intellectual and pragmatic work of “Old Media Studies” might actually be the same thing. In this, the allusion of her title is apt. Unlike the new wine shoved unto old wine skins that ruins both, the new wine skin preserves aged wine and protects it for the future."},{"id":"2014-11-19-troubleshooting-acceptance-testing-in-rspec-and-capybara","title":"Troubleshooting Acceptance Testing in RSpec and Capybara","author":"scott-bailey","date":"2014-11-19 06:06:30 -0500","categories":["Research and Development"],"url":"troubleshooting-acceptance-testing-in-rspec-and-capybara","layout":"post","content":"Over the past two weeks, I’ve been pair programming with Eric . Together, we’ve been building out a suite of acceptance tests for Ivanhoe to provide a basic check on critical, user-facing functionality as we refactor parts of the codebase . While learning to write basic tests has been relatively straightfoward for me, due to the natural language character of acceptance tests with RSpec and Capybara, we have run into a couple of hang-ups in getting the test suite functional. I want to briefly describe one of these problems and how we resolved it. Database Transactions and WordPress Problems After getting a number of tests working, Eric and I decided to straighten out the database transactions occuring before running the suite and between each test. Before the suite runs, we set up a testing database from fixtures, and between each test we want to reset the database. This reset ensures that any given test is unaffected by database events from previous tests. Initially, this was to be handled with two Ruby gems: database_cleaner and sequel . The database reset wasn’t working when we attempted to run it however. Eric realized that, due to the testing running in a different process than the application, we would not be able to use the default transaction strategy. Given that WordPress uses a MySQL database, the deletion strategy was not available, leaving us only with truncation. Truncation, however, without some fine tooling, would have cut off more of the database than necessary, requiring a more extensive rebuild from the fixtures. We decided that it would make more sense to write more restrained database transactions ourselves in the spec_helper.rb file. While this provided an intial solution, it led to lengthy test times, and lengthy test times lead to a distinct lack of excitement for running tests regularly, substantially hindering the effectiveness of testing at all. We set out to refactor the database transactions to reduce testing time, but in doing so created a perhaps more significant problem. Running our tests led to two significant problems in WordPress: TinyMCE, the WYSIWYG editor used by default in the WordPress editor, was no longer working, with relevant failing tests indicating that TinyMCE was not defined on pages with the WordPress editor an admin user, with full privileges, was able to login, but not able to access the back-end Dashboard at all, being given instead a message that the user did not have the necessary permissions to access the page. Tracing Errors The first issue we discovered due to the failure message returned by RSpec and the screenshots at point of failure provided by capybara-screenshot . The second we only discovered in the course of attempting to access the backend of Wordpress to create a post to test whether TinyMCE was being loaded correctly (Ivanhoe uses front-end forms for creating posts of various custom post-types). Given that the first issue was clear and explicit immediately, I began investigating why TinyMCE wasn’t defined. Using the developer tools in Chrome, I discovered that rather a number of links to Javascript files were in fact not being generated in the footer, among those TinyMCE and its component pieces. Thus began a day of tracing out how default Javascript files are queued in the WordPress core. While illuminating as to how WordPress generates and gathers such links, this ultimately led nowhere due to a lack of PHP errors, failure to generate new and helpful PHP errors, and inability to locate any point where the testing script might interfere with the generation of these default links. As part of this work, though, I tried to create a post through the WordPress Dashboard to check whether TinyMCE was being loaded along with the editor when called through WordPress core (vs. when called through the front-end forms in Ivanhoe). Doing so revealed the second issue, that even as an admin user I was not able to access the Dashboard due to permissions. Checking the database tables against those of a working installation of WordPress and Ivanhoe, I discovered that the wp_usermeta table was being overwritten with key-value pairs from a different table, thereby preventing WordPress from recognizing the user’s permissions. Without knowing precisely why, Eric and I thought that fixing this might fix the Javascript loading problems. During our weekly pair programming time, we set out to figure out how and why the testing script was causing this mis-copy of the database table. Through a repeated cycle of starting with a clean database, running the script with different system actions commented out, and checking the database, we eventually discovered that the database fixtures themselves, loaded during the test script, were creating the erroneous key-value pairs in the wp_usermeta table. After generating a clean set of fixtures, our tests worked, passing as expected. Both errors in WordPress were resolved. To ensure that we don’t run into this problem again, we used a rake task to create backup copies of the clean database tables and then used those in spec_helper.rb to ensure clean data between tests. We still don’t know when the error in the fixtures was introduced, and I’ll be spending at least a bit of time trying to figure that out, but, for now the testing suite for Ivanhoe is working. We’ll continue building that out for better coverage and begin refactoring the Ivanhoe codebase. When I mentioned to Wayne and Ammon that we had figured out the error, and it turned out not to be the script at all, neither was surprised. They were, I suspect, not suprised by two things: that it took several days of work to debug a problem in WordPress and that it wasn’t the script at all, just bad fixtures."},{"id":"2014-12-05-podcast-jentery-sayers-on-remaking-the-past","title":"Podcast: Jentery Sayers on Remaking the Past","author":"laura-miller","date":"2014-12-05 08:11:14 -0500","categories":["Events","Experimental Humanities","Podcasts"],"url":"podcast-jentery-sayers-on-remaking-the-past","layout":"post","content":"Remaking Victorian Miniatures: The Speculative Stitches between 2D and 3D In both digital humanities and popular culture, there is a rapidly growing interest in big data. How not to read a million books? How to wrangle petabytes of data? How to discover and express patterns across thousands of images? Frequently, this research is framed as a response to an historically unprecedented abundance of information. For instance, over 40 million photos are posted to Instagram each day, and roughly 6,000 tweets appear on Twitter each second. But in this talk I shift the focus away from amounts of data and toward ways of seeing with computers. In this sense, contemporary computing is more about mediation than media, or more about vision than the visible. As a case in point, I draw material from the Kits for Cultural History project at the University of Victoria’s Maker Lab in the Humanities in order to outline how digital humanities methods also help scholars think small. More specifically, I explain how the Kits use computer vision, photogrammetry, and fabrication techniques to remake Victorian miniatures (such as Gustave Trouvé’s electric jewelry) in 3D, based on extant 2D drawings, diagrams, photographs, and research notes from the second half of the nineteenth century. These miniatures pose a number of curious problems for humanities scholars: How to historicize the minutiae of their manufacture? How to understand their undocumented uses and reception? And how to reconstruct them when they no longer exist, or parts of them are simply not accessible? Comparable to big data projects, these acts of remaking involve some serious speculation where archival gaps are concerned. They also rely quite heavily on computation to stitch together evidence in the presence of absences. In short, they are matters not of “how many” but of “how this becomes that.” Jentery Sayers is Assistant Professor of English and Director of the Maker Lab in the Humanities at the University of Victoria. His research interests include comparative media studies, critical theories of technology, and cultural histories of dead devices. He is currently working on a book about the “digging condition” of digital humanities, or how new media, data, and computing were embedded in materialist metaphors and methods during the 2000s. With William J. Turkel (Western University), he is the PI of the SSHRC-supported Kits for Cultural History project, which is reconstructing pre-digital technologies using physical computing and fabrication techniques. This talk was recorded in Alderman Library, Rm 421 on November 20, 2014.  You can follow Jentery’s accompanying slides and read his further thoughts on the relevance of remaking . If you encounter problems with the playback, please email scholarslab@virginia.edu ."},{"id":"2015-01-08-spring-2015-scholars-lab-gis-workshop-series","title":"Spring 2015 Scholars’ Lab GIS Workshop Series","author":"chris-gist","date":"2015-01-08 09:29:41 -0500","categories":["Announcements","Geospatial and Temporal"],"url":"spring-2015-scholars-lab-gis-workshop-series","layout":"post","content":"All sessions are one hour and assume attendees have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials with expert assistance.  All sessions, except where noted, will be taught on Wednesdays from 1PM to 2PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community. January 21st\n**Making Your First Map with ArcGIS\n**Here’s your chance to get started with geographic information systems software in a friendly, jargon-free environment.  This workshop introduces the skills you need to make your own maps.  Along the way you’ll get a taste of Earth’s most popular GIS software (ArcGIS) and a gentle introduction to cartography. You’ll leave with your own cartographic masterpieces and tips for learning more in your pursuit of mappiness at UVa. January 28th\n**Getting Your Data on a Map\n**Do you have a list of Lat/Lon coordinates or addresses you would like to see on a map?  We will show you how to do just that.  Through ArcGIS’s Add XY data tool and Geocoding (address matching), it is easy to take your tabular lists and generate points on a map. February 4th\n**Georeferencing a Map\n**Would you like to see historic map overlaid on modern aerial photography?  Do you need to extract features of a map for use in GIS?  Georeferencing is the first step.  We will show you how to take a scan of a paper map and align in it in ArcGIS. February 18th\n**Easy Demographics\n**Need to make a quick demographic map or religious adherence?  This workshop will show you how easily navigate Social Explorer.  This powerful online application makes it easy to create maps with contemporary and historic census data and religious information. February 25th\n**Historic Census Data\n**Would you like to map the poverty in Philadelphia around the turn of the 20th Century?  How about a racial breakdown by state in the 1860s?  This workshop will focus on how to download historic census boundary and tabular data to make historic demographic maps. March 4th\n**Learning Old-School Mapping Techniques\n**How did folks make maps before GPS and satellite imagery?  In this workshop we’ll focus on plane table mapping.  Using just a flat surface, a sheet of paper, a straight edge, and a pencil we’ll learn techniques to create accurate maps for large geographic areas.   With plane table mapping, if you can see it, you can map it.  This technique can be particularly helpful to researchers such a biologists working in small areas under heavy tree canopy where GPS fails. March 18th\n**DIY Aerial Photography\n**Want to do your own aerial photography?  We will discuss techniques, equipment and issues of capturing your own data. March 25th\n**DIY Aerial Photography\n**In this special session, we will spend a couple hours outside collecting aerial data from various platforms."},{"id":"2015-01-19-getting-under-the-hood-with-arduino","title":"Getting “under the hood” with Arduino","author":"ethan-reed","date":"2015-01-19 03:23:00 -0500","categories":["Digital Humanities","Experimental Humanities"],"url":"getting-under-the-hood-with-arduino","layout":"post","content":"Circuit boards, breadboards, digital inputs/output pins, analog outputs – “physical computing” can be an intimidating prospect for people with no experience.  As a person with almost no experience, I know these apprehensions first-hand.  Learning a new vocabulary, basic electronics, even basic programming? Ah! I’m writing from my own experience with Arduino to say this: getting started with physical computing and even basic programming is easier than it sounds.  Taking a page from the pedagogical playbook of Learn Code The Hard Way, it worked best for me when I tried to do something even if I didn’t understand everything when I got started.  I followed a set of instructions through to a given goal, and then learned about what I’d done and how I’d done it after the fact.  Arduino kits like the ones we have at the Makerspace are the perfect way to go about doing this. So what is an Arduino?  As their website explains, these are miniature computers that “can sense and control more of the physical world than your desktop computer.”  They can take inputs from all sorts of sensors in the physical world and produce all sorts of outputs back into the physical world.  This ranges from making lights blink, a buzzer ring, taking temperatures, or all sorts of crazy stuff that is much, much more complicated. And people are finding new things to make every day . But when I first got an Arduino it was just a box. Inside the box was an Arduino board, a breadboard, various wires and other things to plug in, and a USB to connect to my computer. Most importantly, there was an instruction manual.  Inside the manual I found a series of snappy pictures and diagrams with user-friendly (and often funny) explanations of what was going on.  I was using the manual from SparkFun Inventor’s Kit - V3 . It suggested I get started trying to make an LED plugged into my breadboard blink at regular intervals. The surprise was this: going through this thing was not only not-scary, it was incredibly fun. It was like building from a Lego manual, but for a circuit board – the bright and strangely shaped objects fitting together, the clearly diagrammed instructions, and the tangible end-product that gives not only a satisfied feeling of having built something, but also the slight amazement at what has been built (I did that? – really?). My light was blinking in a matter of minutes. I disassembled and moved onto the next chapter, building a potentiometer (a dial instead of a switch) to change the rate at which an LED blinked. But where’s the programming ? Something has to be making all these electronics do what you want. Indeed, there is some programming in programs called sketches .  But the programming was as simple as downloading the sketch files from the SparkFun tutorial into the Arduino program. Tada! Programming accomplished. And there are comments throughout the files explaining how the code works, and where you might try playing around with it. Okay, so Arduinos are easy-to-use, inspire child-like joy a la Legos, and teach us some basics about physical computing and programming.  What about the bigger picture – why would I want to learn how to use them? First, I consider it a form of what Matt Ratto calls “ critical making,” something I talk about in my first post here.  Second, it’s a portal into understanding some of the other amazing work digital scholars are doing with physical computing. I’m thinking of a recent article on “circuit bending” by Nina Belojevic on “ Circuit Bending Videogame Consoles as a Form of Applied Media Studies,” in which she takes apart an NES console, rebuilds it purposefully as a “glitch console,” and reflects on the social implications of her material engagement with the physical hardware.  “By actively taking apart, breaking, remaking, and tinkering with these material devices,” she says, “hobbyists, hackers, artists, and scholars can engage with, study, highlight, and challenge social justice issues” – encouraging people to ask not just about developers, but about “who soldered the VRAM onto their console’s circuit board.”  Jon Jonson at UVic’s Maker Lab has posted about a related project on “ Building an SNES ‘Glitch Controller’ ” – work that, he writes, is “contingent upon haptic exploration and depends highly on trial and error,” a kind of creative play to which I think Arduinos can be a great introduction.  This is not to mention projects that use actual Arduino’s as an interface with the physical world, of which there are many . So come in and try them out!"},{"id":"2015-01-21-spring-2015-makerspace-workshops","title":"Spring 2015 Makerspace Workshops","author":"laura-miller","date":"2015-01-21 09:20:41 -0500","categories":["Experimental Humanities"],"url":"spring-2015-makerspace-workshops","layout":"post","content":"Introduction to Omeka\nWednesday, January 28 10:00 am–11:30 am · Alderman Library, Room 421 Omeka is a simple, free, web publishing system developed at the Roy Rosensweig Center for History and New Media at George Mason University. It was specifically built to enable scholars, archives, libraries, museums, and independent researchers to create online exhibits of their work without having to know HTML or CSS. If you have a collections of digital resources that you want to show in a scholarly way, Omeka could be a great tool to have in your toolkit. Instructor: Ronda Grizzle Introduction to 3D Printing\nThursday, January 29 2:00 pm–3:30 pm · Alderman Library, Room 421 This workshop will introduce participants to the exciting world of desktop fabrication. We’ll provide a brief overview of current trends and tools for 3D modeling and printing. We’ll also go over the basics of model creation with photogrammetry, and discuss how 3D printing works, including a live demonstration with one of our Makerbots. Instructor: Jeremy Boggs Working with Arduino I\nThursday, February 5 2:00 pm–3:30 pm · Alderman Library, Room 421 Do you want to hack your personal items with switches or sensors? Arduino is a tool for making microcomputers that can sense and control the physical world. This workshop will introduce participants to the basics of physical computing programming through a series of hands-on exercises using our Arduino kits. No electronics experience required! Instructor: Jeremy Boggs Introduction to Neatline\nWednesday, February 11 10:00 am - 11:30 am · Alderman Library, Room 421 Using Neatline, anyone can create beautiful, interactive maps, timelines, and narrative sequences from collections of archives and artifacts, telling scholarly stories in a whole new way. Join us for this hands-on introduction. See http://neatline.org/ for more information. Instructor: Ronda Grizzle Working with Arduino II\nThursday, February 12 2:00 pm–3:30 pm · Alderman Library, Room 421 New to microcontrollers? Or used an Arduino before and want more time to play in a supportive environment? Come on by!  Arduino is a tool for making microcomputers that can sense and control the physical world. This workshop will introduce participants to the basics of physical computing and programming through a series of hands-on exercises using our Arduino kits. This workshop builds on the Working with Arduino I workshop, but it’s not required to attend this one. Instructor: Jeremy Boggs HTML for Beginners\nThursday, February 19 2:00-3:30 pm · Alderman Library, Room 421 Wonder how websites work? Want to get started creating web content of your own, but have no idea how to do that? This is the class for you. We’ll cover everything from how URLs work to basic HTML coding skills to general netiquette. This workshop is intended for absolute beginners with no knowledge of HTML. Instructor: Ronda Grizzle **\nIntro to Wearables and Soft Circuits Wednesday, February 25  [THIS EVENT HAS BEEN RESCHEDULED FOR **MARCH 18 at 10:00 AM ]\n_ 10:00-11:30 am · Alderman Library, Room 421_ Have ideas to make your life simpler with hacks for your outerwear or accessories?  This beginner workshop will introduce the basics of circuity and give an overview of current trends in wearable computing. Participants will make their own circuit using LED’s and conductive thread. Materials will be provided and no experience with sewing or electronics is necessary.\nInstructors: Jeremy Boggs and Purdom Lindblad Working with Arduino III\nThursday, February 26 2:00 pm–3:30 pm · Alderman Library, Room 421 New to microcontrollers? Or used an Arduino before and want more time to play in a supportive environment? Come on by!  Arduino is a tool for making microcomputers that can sense and control the physical world. This workshop will introduce participants to the basics of physical computing and programming through a series of hands-on exercises using our Arduino kits. This workshop builds on the Working with Arduino I and II workshops, but they’re not required to attend this one. Instructor: Jeremy Boggs Introduction to 3D Printing\nThursday, March 5 2:00 pm–3:30 pm · Alderman Library, Room 421 This workshop will introduce participants to the exciting world of desktop fabrication. We’ll provide a brief overview of current trends and tools for 3D modeling and printing. We’ll also go over the basics of model creation with photogrammetry, and discuss how 3D printing works, including a live demonstration with one of our Makerbots. This course is a repeat of the Jan. 29 session. Instructor: Shane Lin Scholars’ Lab workshops assume attendees have no previous experience. They will be hands-on with with expert assistance.  All are free to attend, and they are open to the UVa and larger Charlottesville community. If you can’t make a session but would like to learn more about any of the above topics, please visit the student consultants in our Makerspace or email scholarslab@virginia.edu to set up an individual appointment."},{"id":"2015-01-23-can-ivanhoe-facilitate-playful-learning-both-in-and-out-of-the-classroom","title":"Can Ivanhoe facilitate playful learning both in and out of the classroom?","author":"jennifer-grayburn","date":"2015-01-23 06:52:52 -0500","categories":["Grad Student Research"],"url":"can-ivanhoe-facilitate-playful-learning-both-in-and-out-of-the-classroom","layout":"post","content":"You can discover more about a person in an hour of play than in a year of conversation –Plato From the beginning of the year, the Praxis cohort recognized ‘play’ as one of the key aspects of the Ivanhoe experience. Yet, how does play shape Ivanhoe? What are the effects of this play? In previous years, the focus has been on the role of play in the classroom, but we considered from the beginning, as expressed in our Praxis Charter, that Ivanhoe might be useful, fun, and even educational for other communities as well. Is Ivanhoe relevant–educationally or otherwise–for established, self-motivated textual play, like fan fiction and collaborative creative writing? Is the original educational conception and identity of Ivanhoe lost if such communities do use it? Or does ’education’ expand to include informal opportunities to learn and grow outside of the classroom?  As we still struggle with the identity of our players, it is important to consider what exactly Ivanhoe does, how it does it, and for whom it is relevant. In this blog, I will start at the beginning: What is play? Can it be educational? How is it educational? Is it something that we can foster or produce within a formal setting? And finally, how does Ivanhoe provide a unique experience that incorporates aspects of both play and education? “Play” is a term that seems relatively straightforward; it is something we have all done and something that we all believe that we can recognize. But, what does it really mean? In its most casual definition, play is defined by Webster Dictionary  as a “recreational activity.” This definition implies that these are activities done for fun, for enjoyment–activities that are_ not work._ This dichotomy between work and play, however, is ambiguous at best and misleading at worst. First, this opposition no longer reflects the realities of many people’s livelihoods, as the formal line between work and play is blurring  increasingly in our age of ubiquitous connectivity, flex time schedules, and home-based employment. Second, by setting work and play in opposition, we imply that play is the antithesis of work and potentially classifiable as superficial, unproductive, and without purpose. The concept of play, however, has been the subject of much scholarly discussion, especially within studies on childhood development. While scholars admit that play is a difficult concept to pin down in a concise definition, most acknowledge that it is often easily recognizable by those experiencing it. Yet, play should not be conflated with ‘fun’ and ‘enjoyment.’ Researchers have argued that play can, in fact, be a way for people to explore not only happy things, but also things that cause insecurity, fear, and anxiety. Rather, it seems that play–whether scary or fun–is something that needs to be self-motivated, something that provides a sense of control through the choice to participate. In Play, Creativity and Digital Cultures, Jackie Marsh cites S. Millar in order to argue that play is not actually a  thing, not a noun. Rather, “play is best used as an adverb; not as a name of a class of activities, nor as distinguished by the accompanying mood, but to describe how and under what considerations an action is performed”(211). Sometimes the choice to engage in play is determined by social protocol or relationship expectations, but it is, regardless, something that lets the player select his or her own level of engagement and participation. Scholars who study children’s play focus largely on self-motivation as a way for children to explore places, activities, relationships, and physical phenomena within a safe, low-risk environment. In other words, play is a self-initiated, low-stakes opportunity to work through new information, recognize patterns, establish networks, and otherwise  learn  about issues and processes that concern the player.  _While it can manifest in many forms, play not only  can  be educational, it _is  educational;  this form of education, however, is typically subconscious and based on experience, practice, adjustment, and even failure rather than memorization and formal instruction. Since players (rather than instructors) are in control of these experiences, it allows them to be spontaneous, flexible, and adaptable in order to account for new players or their own shifting interests and questions. That is, one advantage of play over formal instruction is that students can intuitively adjust the form of their play to account for what they have already learned and to explore new areas of inquiry. With this in mind, it is easy to see how established, self-motivated textual play–such as fan fiction and other forms of creative writing–can offer important educational opportunities. Reading and writing encourage skill practice and familiarity with different ways to communicate and create meaning. Collaborative writing, moreover, requires writers to negotiate relationships, provide and implement feedback, and integrate critically new information into contexts provided by other contributors. Similarly, fan fiction requires critical analysis of a text and the self-conscious creativity to work within or push the boundaries of a provided context. Players can experience a broad range of awareness during these activities and many might not recognize that they are performing and practising such skills. Yet, the flexible time commitment, pursuit of a topic of interest, opportunity to experiment and create without potentially lasting consequences (like grades), freedom to change directions when needed, and choice to include (or exclude) other participants means that players maintain control of their own engagement throughout the process. When Jerome McGann theorized the educational value of Ivanhoe with Johanna Drucker and Bethany Nowviskie in “ IVANHOE: Education in a New Key,” he argued that Ivanhoe works “to promote rigorous as well as imaginative thinking; to develop habits of thoroughness and flexibility when we investigate our cultural inheritance and try to exploit its sources and resources; and to expose and promote the collaborative dynamics of all humane studies.” These are similarly the skills already being honed by those engaged in self-motivated textual play. The necessary component of Ivanhoe, however, is the active and self conscious  role- play, for it not only requires students to interpret or reinterpret a cultural text, it requires them to embody it. This must be done self-consciously as players both internalize and synthesize (rather than summarize or describe) aspects of a text and as they negotiate and respond to the moves of their collaborators. While this performance can complement and even enhance established, social, and self-motivated textual play, our inclusion of a private ’role journal’ in Ivanhoe to record motivations and justifications for moves challenges even causal writers to be more self-conscious and self-reflexive. As Sandra Wills, et. al., notes in her discussion of classroom role-play, such reflection is a key component to foster a deeper learning opportunity, not only to consider and recognize why and how a player does something, but also to practice communicating it. It is clear, then, that play does provide advantages for learner-controlled education. Similar skills can, of course, be taught in a formal setting in a more straightforward manner so that the participants are fully aware of their status as ‘student’ or ‘learner.’ The question now is if such playful activity (and its self-motivating benefits) can be replicated in a classroom. Over the past few decades, a large corpus of scholarship and guides have been produced to tap into the educational opportunities of play for more formal educational experiences. Yet, if students are forced to participate in play or lack the control to alter the play to meet their own interests, does play loose its very essence and efficacy? In other words, does a playful game-like activity transform back into work when the student looses control? When it is required, formal, and potentially high-stakes in terms of grading? In a previous blog post, Andrew expressed this very  skepticism of ‘gamification’ in the classroom. According to Alexandra Ludewig and Iris Ludewig-Rohwer at the University of Western Australia, required and graded game play might, in fact, be limited in its efficacy. In “ Does web-based role-play establish a high quality learning environment? Design versus evaluation,” Ludewig and Ludewig-Rohwer test the educational results of an established role-playing experience designed for language students. Using Bloom’s Taxonomy of Educational Objectives, the role-play experience was designed to “encourage students to engage in self-directed and peer-assisted learning, involve experiential and real-world learning, incorporate resource-based and problem-based learning, include reflective practice and critical self-awareness, utilize open learning and alternative delivery mechanisms and also allow for freedom of choice and individual learning style preferences” (165). Yet, when the students were surveyed and tested following their experience, Ludewig and Ludewig-Rohwer discovered a great disjunction between what the role-play  should have done and what it actually did (or at least what it did according to students’ expectations). While some students enjoyed the experience, many reported anxiety about the unfamiliar assignment format, uncertainty about grading, and no increase in language abilities. While such an example seems to indicate that Ivanhoe might not be the playful pedagogical tool that we assumed, all is not lost!  I suspect that the high-stake consequences of grading in the example above increased the students’ anxiety, while the discrepancies between the flexible application of skill and the more formal testing format made evaluation difficult. The sense of play was not absent in the experience itself, but rather in the uncertainty as to if and how it would be evaluated, especially for students so accustomed to an established format of graded assignments. We, as the designers of Ivanhoe, cannot control how players and instructors experience and use the game; however, we continue to emphasize performance in order to encourage playful, experimental encounters and reflection in order to facilitate self-aware educational opportunities. In itself, the activity of role-play  does allow students to control their level of participation and interaction in a low-stakes setting (that is, students cannot ‘fail’ at the game; moves build upon each other to form the burgeoning interpretation). Even if students are required to participate for a grade, they do control their role, analysis, engagement, and response to their fellow collaborators. Again, a student’s experience is dependent upon their own expectations and how an instructor introduces, structures, and evaluates a classroom game. The role journal will encourage students to make moves critically, justify their choices, and reflect upon their development. It, moreover, can help to initiate classroom discussion and to encourage evaluation on students’ insights, connections, and applications, rather than innate creativity. If this is done successfully, the students may not have ‘fun’ with an assignment, but will still be able to apply playfully the same skills as self-motivated players and better communicate the process of their own learning. For more detailed examples of on-line role-play assignments and tips for evaluation, see Sandra Wills, et. al.,  The Power of Role-Based E-Learning. Consulted Sources: McGann, Jerome, with Johanna Drucker and Bethany Nowviskie. “ IVANHOE: Education in a New Key .” Romantic Circles (Dec. 2014). Ludewig, Alexandra, and Iris Ludewig-Rohwer. “ Does web-based role-play establish a high quality learning environment? Design versus evaluation .”  Issues in Educational Research 23, no. 2 (2013): 164-179. Smidt, Sandra. Playing to Learn: The Role of Play in the Early Years. New York: Routledge, 2011. Willett, Rebekah, Muriel Robinson, and Jackie March, eds. Play, Creativity and Digital Cultures. New York: Routledge, 2009. Wills, Sandra, Elyssebeth Leigh, and Albert Ip. The Power of Role-Based E-Learning. New York: Routledge, 2011. (includes detailed example of online role-playing assignments and ideas for evaluations)"},{"id":"2015-01-31-what-could-make-ivanhoe-special","title":"What Could Make Ivanhoe 'Special'? ","author":"joris-gjata","date":"2015-01-31 10:06:30 -0500","categories":["Grad Student Research"],"url":"what-could-make-ivanhoe-special","layout":"post","content":"Last semester was not bad for our Praxis team. Our achievements were modest but considerable: a charter, a team with a common interest in any activity concerning food, plus a vibrant debate on redesigning the website for our common project - Ivanhoe. While we failed in meeting certain deadlines and charter objectives, i.e. having tangible visible results, I want to emphasize how productive the process has been for us in my view. We had an exemplary meeting as a team to decide on a tagline for Ivanhoe. It was a successful meeting because we had a limited amount of time, a very specific well-defined task and an agreed process of making decisions. The most significant element of this meeting for me was free brainstorming and discussion of the logic behind our preferences and choice of words. The task in front of us seemed small -what is a tagline after all: put five-six words together and you’re done. But we did not underestimate the importance of this task. We considered carefully each and every word of the tagline, making sure that it reflected the ideas that gave rise to Ivanhoe at the first place as well as our vision as a team about its future. Look at our notepad! Within an hour of conversation, we decided to call Ivanhoe: ‘A Gateway to Collaborative Textual Play’ . The key concepts we built this phrase on were: fluidity, interpretation, reflection, performativity, and collaboration. Our tagline draws a vision of Ivanhoe not just as a space with pre-established boundaries - a platform or a place of some kind - but a space that enables the emergence of boundaries, actors and places through play. That is why we chose the term ‘gateway’ and followed it with the preposition ‘to’: to indicate its initial purpose as an enabling environment for learning. As Jeniffer’s last post seems to conclude, Ivanhoe can enable learning through play. In the current version, Ivanhoe enables learning through the ‘role’ and ‘role journal’ features. These features require players to reflect on their moves/interpretations - before and after they complete them. While I agree that Ivanhoe is a learning environment, I want to argue that this is not enough to define it and that we need to specify further the kind of learning this environment enables that other environments do not. The existing version of Ivanhoe seems to emphasize one kind of learning - learning that results from reflection on one’s individual actions and thoughts. One might argue that this kind of learning through such self-reflection can happen everywhere and does not need a facilitating enabling environment like Ivanhoe. For example, commenting to a Facebook status or a WordPress blogpost also requires self-reflection - you have to think about yourself, your words and interpretation of a particular text - a photo, or music video, or quote from somebody’s book, etc. This is similar to what you can do in Ivanhoe. I want to suggest that one way to make Ivanhoe ‘special’ i.e. to define its use as a learning environment, could be to create a feature that makes it enable another kind of learning - learning from others through reflection on the trajectory of play and the relationships among players’ interpretations. This feature could be incorporated within the existing ‘role’ and ‘role journal’ features. It would require players not only to reflect on their own individual moves before and during play, but also to reflect on the moves of others and the way in which the relationships between players’ moves shape the process of play and its end-result - an ‘Ivanhoe game’. The aim of such feature in Ivanhoe would be to enable a kind of learning that is difficult to achieve in other environments - WordPress, Facebook, a museum, or even a classroom: learning about others and how one’s interactions with others shape cultural objects or texts of common interest. This kind of learning could be achieved by requiring reflection in a journal - before, during and/or after play - about the set of relationships that contribute to the emergence of the text of common interest and define its form. I argue that Ivanhoe can become special if it is able to promote reflection about other players’ interpretations of a text and the interaction between interpretations that give a particular form to that text. I am looking forward to our team’s discussion of this option and the implementation of such idea into a well-designed Ivanhoe feature. We have a tagline and we agreed that for us it is not just a statement about what Ivanhoe does in its current state, but more of an expression of our aspirations about what Ivanhoe can do. The conversation that led to the tagline creation helped us as a team consolidate our ideas about Ivanhoe and what we wanted it to be. However, we need to continue the debate on what Ivanhoe does and what can make it special as it will shape our future work this spring semester. I hope that the time we take learning new skills- html, css, php, etc.- will not be seen separate from the time we give to talking and reaching an agreement about the use and purpose of Ivanhoe - or how to make it special. I am looking foreword to more successful meetings this new year."},{"id":"2015-02-02-moving-peoplelinking-lives-dh-symposium","title":"Moving People/Linking Lives DH Symposium","author":"brandon-walsh","date":"2015-02-02 09:16:35 -0500","categories":["Digital Humanities","Events","Geospatial and Temporal","Grad Student Research"],"url":"moving-peoplelinking-lives-dh-symposium","layout":"post","content":"I am pleased to announce that “ Moving People, Linking Lives: An Interdisciplinary Symposium ” will take place March 20-21, 2015 at the University of Virginia. Friday, March 20 events will take place in the Kaleidoscope Room. Saturday, March 21 events will take place in Alderman 421 except for an evening reception, location to be determined. Presentations and workshops will open dialogue across different fields, periods, and methods, from textual interpretation to digital research. Invited participants include specialists on narrative theory and life writing, prosopography or comparative studies of life narratives in groups, and the diverse field of digital humanities or computer-assisted research on cultural materials, from ancient texts to Colonial archives, from printed books to social media. Invited participants include: Elton Barker, Jason Boyd, James Phelan, Susan Brown, Margaret Cormack, Courtney Evans, Will Hanley, Ben Jasnow, Ruth Page, Sue Perdue, Sidonie Smith. We hope to have lots of locals involved with digital work participate as well, and we particularly encourage graduate students to join in for the weekend! Our symposium will bridge the gaps among our fields; share the innovations of several digital projects; and welcome the skeptical or the uninitiated, whether in our historical fields or in the applications of technology in the humanities. Booth, Clay, and Ogden have each led digital projects with some common themes and aims: locating, identifying, and interpreting the narratives—or very often, the lack of discursive records—about individuals in groups or documents, in Homer or other ancient text, Medieval French hagiography, and nineteenth-century printed collections of biographies in English. We want to open discussion of many potential methods including our own—data mining and digital editions of texts; relational databases and historical timelines and maps—for research on groups of interlinked persons, narratives or data about their lives, and documents or other records, and synthesizing and visualizing this research in accessible ways that reach students and the public. Digital innovation, however, should be informed by traditions of scholarly interpretation and advanced theoretical insights and commitments. Narrative theory and Theory generally, ideological critique including studies of gender and race, textual and book history studies, transnational and social historiography, philology and language studies, archeology, cultural geography and critical cartography, are all gaining influence on digital projects. Invited participants will be posting about their research to our blog  in the weeks leading up to the symposium, anyone is free to comment on the posts. In addition, our participants will be building a Zotero-powered bibliography in the weeks leading up to the symposium full of rich materials related to the event’s discussion. Organized and hosted by Alison Booth, Jenny Strauss Clay, and Amy Odgen and sponsored by the Page Barbour Committee, the departments of English, French, and Art, the Institute for Humanities and Global Cultures, the Scholars’ Lab and Institute for Advanced Technology in the Humanities, and other entities at UVa,  all events are free and open to the public . More information can be found on the blog as planning progresses, and you can follow us on twitter at @livesdh. Join in the conversation on the blog at movingpeoplelinkinglives.org, and we hope to see many come out for fruitful interchange in March!"},{"id":"2015-02-06-refactoring-ivanhoe","title":"Refactoring Ivanhoe","author":"scott-bailey","date":"2015-02-06 06:12:29 -0500","categories":["Research and Development"],"url":"refactoring-ivanhoe","layout":"post","content":"As one of last year’s Praxis Fellows, I helped build Ivanhoe, a “WordPress Theme enabling collaborative criticism through roleplay - for scholars, students, and cultural enthusiasts.” While Ivanhoe was perfectly functional when released, one could not say that it exemplified orderly, well-formed code. It was, and is, after all, code written by novice developers. That it was still released as such is, I think, a testament to the Praxis Program, which legitimately treats Fellows, graduate students typically learning about digital humanities, project management, and coding for the first time, as fellow workers and collaborators of the Scholars’ Lab staff, not children whose mess must be cleaned up after the fact. Mentorship entails developing skills and capacities over time through partnership, not paternalistic oversight. And yet, this year we have a new cohort of Praxis Fellows, most of them learning to code for the first time, tasked with improving Ivanhoe in terms of features and design. In order to facilitate their ability to understand the Ivanhoe code base to a sufficient degree to add to it without too much time lost overcoming the wall of unorganized code, we decided to refactor Ivanhoe, part of that effort being the introduction of a set of acceptance tests, about which I’ve already written . That’s the official line at least, and a true one, but not the whole story for me personally. As part of the development team in last year’s Praxis cohort, I contributed substantially to Ivanhoe. Throughout my time in Praxis, I advanced significantly in my ability to code, and have learned far more about best practices, efficient code, and application organization since becoming a full developer in the Scholars’ Lab. However, there is still code from the very earliest commits in Ivanhoe still in the codebase, from before I (or my fellows) had really clear ideas about how to structure files, comment code clearly, or craft elegant functions. Doing a refactor of Ivanhoe was a chance for me to look back at a lot of code I had written early on, and implement improvements based on everything I’ve learned since. I want to share a few highlights from that refactor, now current in the develop branch of the Ivanhoe repo, which I carried out in frequent consultation with Eric Rochester, our senior developer. Tests Are Awesome, Type Errors Are Not The refactor process began with implementing the test suite about which I’ve already written, which would provide a way to make sure that the refactored code generally worked as expected. Once the test suite was written, I created a new branch containing the tests, merged in the develop branch of the repo, and ran the tests. And found a serious fail immediately. The develop branch of Ivanhoe implements a feature that allows the user to respond to more than one source move at a time. Since this feature was merged into develop back in late April, early May, it was, apparently and unbeknownst to us, broken due to a simple error: at times an integer was passed to the function generating the response form, while at other times, with the multiple sources, an array was passed. This simple type error was easily fixed (and fixed again differently after using PHP classes), and I became immediately enamored of testing. Rest assured, every step of the way through the refactor, I ran my tests, always quietly elated when line after line of green passes appeared in my terminal. Function Reorganization One of the biggest problems with the code base as we left it at the end of the Spring semester was the functions.php file, a 1000+ lines of unorganized, sometimes commented functions addressing numerous aspects of the theme. In order to make these functions more accessible to those just jumping into Ivanhoe development, I sorted through these functions, identifying major types of functions and then breaking those sets of functions out into separate files in the includes directory. These files are then pulled into the functions.php file, necessary in WordPress, with require_once statements. Broken up and reorganized, functions are immediately more easily accessible for development. For the initial breakup of functions, see here . Cleaning Up Messy Templates Some of the view templates in Ivanhoe are fairly straightforward, but others in the released version were quite messy, with multiple custom WordPress queries sometimes nested within each other, variable definitions scattered throughout the page, inconsistent formatting, and often very little commenting to indicate what’s happening throughout the template. While the fixes for these issues were relatively minor, they make a difference to the legibility of the code. When possible, I moved variable definitions and logic to the top of each template file. I added comments noting the beginning and end of presentation sections for each custom query, especially when nested. Some of the templates are still complex, but now more accessible for someone coming fresh to the project. For some of these changes, see here. Presentation vs Data-related Logic Lacking a true templating language and with a substantial API, WordPress can make a hash of the distinction between presentation logic and data-related logic. That’s a problem, but probably not an excuse for our own failure to differentiate the two in Ivanhoe. For instance, in the released version of Ivanhoe, we have a function called ivanhoe_get_move_responses, which uses a get_posts query to get any responses to a move, iterates over the array of responses to build an unordered list with the post titles of the responses as list items, and then echoes that chunk of HTML. In the template using this function, we simply call the function. This one function does more than it needs to or should. It builds a usable chunk of data in the array of responses, which has nothing necessarily presentational about it, builds a presentational element (the unordered list), and then actually implements that presentation. The refactor breaks this up into two functions, one data-related and the other presentational. It makes a further change for best practice and versatility, replacing the immediate echo at the end of the presentational function with a return. The echo is moved to the view template, keeping the actual presentation where it belongs. Both of these changes - breaking up functions based on type, and replacing echo with return in presentational functions - were implemented multiple times throughout the refactor. PHP Classes - Unncessary But Fun(?) In the released version, a single template, with quite a bit of procedural logic, generated the various new move, game, and role forms. I mentioned earlier in this post that I worked with Eric on this refactor; it was the primary subject of our pair programming time for the last while. Our conversation on this particular file went roughly like this: Eric: When I look at this file, I think about PHP classes. Scott: Will PHP classes make this file easier to deal with? Eric: Maybe, maybe not. Scott: Alright, let’s do it. Now, this refactor was largely about making the Ivanhoe theme more accessible for novice developers to engage. It was secondarily an opportunity for me to practice and implement some of what I have learned on one of my own previous projects. It was also, though, thirdly, an opportunity for me to learn more about refactoring and coding patterns with Eric’s help. Re-working the post form template into a single base class with three sub-classes is an example of this. It wasn’t necessary, and it didn’t shorten the code or boost application performance in a truly substantial manner. It did, however, provide me a clearer idea of how recognizing patterns within code written according to one paradigm can lead you to achieve the same results through an entirely different paradigm. Briefly, we had one file that was generating effectively the same form three times, with only minor changes in labels and data depending on whether the form needed to generate a new game, role, or move. These changes were controlled by a series of variable definitions and if statements following a top-down order in the file. Given that all three instances of the form shared a basic composition, it made sense to create a single base class for the post form with all of the shared functionality as methods on the class. The differences between forms were handled by sub-classes for each different post type (game, role, move), either adding sub-class specific methods or, if necessary, overwriting a method from the base class. The original file generating the forms then became much simpler - it basically just checks for post type, then concretely instantiates the appropriate sub-class. To see these changes, check here . (Un)Finished The major aspects of the refactor are now done, and it’s been merged down into the develop branch of the repository, available to be hacked on by this year’s Praxis cohort. I look forward to seeing what they do with it, and to then, somewhere down the road, coming back to refactor again with all that I will have learned by that time."},{"id":"2015-02-09-call-for-applications-praxis-fellowship","title":"Call for Applications, Praxis Fellowship","author":"purdom-lindblad","date":"2015-02-09 09:02:32 -0500","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"call-for-applications-praxis-fellowship","layout":"post","content":"UVa grad students! Apply by March 2 for a unique, funded opportunity to work with an interdisciplinary group of your peers, learning digital humanities skills from the experts, and collaboratively design and execute an innovative digital project. The 2015-2016 Praxis cohort will get underway in September, thanks to generous support by the UVa Library and the endorsement of GSAS . Each year, the Scholars’ Lab Praxis Program provides six UVa graduate students with hands-on experience in digital humanities collaboration and project creation. Team members work to take a shared project from conception to design and development, and finally to publication, community engagement, and analysis. Praxis is a unique and well-known training program in the international digital humanities community. Our fellows blog about their experiences and develop increased facility with project management, collaboration, and the public humanities, even as they tackle (most for the first time, and with the mentorship of Scholars’ Lab faculty and staff) new programming languages, tools, and digital methods. The program prepares fellows with digital skills they can apply to the Praxis fellowship project as well as to their future research. In 2012, the Scholars’ Lab inspired like-minded institutions to create the Praxis Network, made up of allied but differently-inflected humanities education initiatives from around the world, mainly focused on graduate training, and all engaged in rethinking pedagogy and campus partnerships in relation to the digital humanities. The Praxis Network Student Directory showcases how Praxis Program alumni have traveled diverse career paths, including tenure-track teaching and distinguished digital humanities positions within academic libraries and research centers. We will welcome six new, competitively-selected students to the UVa Praxis Program in late August 2015. This fellowship replaces recipients’ teaching responsibilities for the academic year. Fellows are expected to devote approximately 10 hours per week in the fall and spring semesters to learning together and building a collaborative digital humanities project in the Scholars’ Lab. Fellows join our vibrant community, have a voice in intellectual programming for the Scholars’ Lab, and can make use of a dedicated grad lounge. All University of Virginia graduate students working within or committed to the humanities are eligible to apply to join the 2015-16 cohort. We particularly encourage applications from women, LGBT students, and people of color, and will be working to put together an interdisciplinary and intellectually diverse team. Apply by emailing an expression of interest to Purdom Lindblad, indicating the applicant’s research interests, summary of plan for use of digital technologies in the applicant’s research, summary of skills, interests, and methods the applicant will bring to the Praxis Program, and a statement of what the applicant hopes to gain as a Praxis Fellow. Applicants must be available for in-person interviews on Grounds between March 16th and 27th. For questions about the Praxis Program or the application process, please email Purdom Lindblad . Deadline: March 2, 2015\nNotifications: April 10th, 2015"},{"id":"2015-02-11-new-developments-for-the-praxis-network","title":"New Developments for the Praxis Network","author":"purdom-lindblad","date":"2015-02-11 10:36:38 -0500","categories":["Announcements"],"url":"new-developments-for-the-praxis-network","layout":"post","content":"The Praxis Network was developed as part of a 2012-2013 Scholarly Communication Institute focused on graduate education. Its goal is to share model programs that are engaged in methodological training and collaborative research in the humanities. Deeply invested in demystifying collaborative, iterative, and public work, Praxis Network programs prep students to have a broader view of the humanities. The first iteration of the Praxis Network was simply about sharing the model of these aligned, but differently inflected programs. This next phase is about deepening the connections among programs, exploring ways to facilitate networking our students, and to create a space for other interesting, similarly oriented programs to share their missions. Two new directories have been added as a first step towards identifying and strengthening networks among students and programs. The Student Directory highlights the research interests of Praxis Network students. The Directory of Related Programs showcases like-minded programs invested in rethinking humanities methodological training and fellowships in the digital humanities. Do you run a similar effort? Add your program!"},{"id":"2015-02-16-podcast-thorny-staples-on-managing-smithsonian-research-data","title":"Podcast: Thorny Staples on Managing Smithsonian Research Data","author":"laura-miller","date":"2015-02-16 08:38:37 -0500","categories":["Events","Podcasts"],"url":"podcast-thorny-staples-on-managing-smithsonian-research-data","layout":"post","content":"Managing the Record of Research at the Smithsonian Can institutions effectively manage cross-team digital research data in real time? Can it preserve that data so that it can be seamlessly presented in conjunction with publications? To answer those questions, the Smithsonian Institution has built a first pilot system, called SIdora, designed to be used by Smithsonian researchers to capture and organize digital “evidence” as they create it in their research process, and use it directly in their analysis and dissemination activities. The goal is to actively support the research process as it unfolds, leaving behind a coherent expression of the digital content for a complete research project that can permanently stand alongside related publications. Sidora, a general information architecture and software environment based on Islandora and Fedora, is designed to manage research output as if it were part of a network of information. Staples will present the architecture and demo the software, using research data from a complete excavation of an archaeological site in Panama, and an international study of mammal populations. Thorny Staples is currently the Director of the Office of Research Information Services at the Smithsonian Institution. He has previously been Director of the Fedora Project; Director of Community Strategy and Alliances for DuraSpace; CIO of the National Museum of American Art at the Smithsonian Institute; Director of Digital Library Research and Development at the University of Virginia; and Project Director at the Institute for Advanced Technology in the Humanities at the University of Virginia. This talk was recorded in Alderman Library, Rm 421 on February 9, 2015.  Click below to stream the podcast.  If you encounter problems with the playback, please email scholarslab@virginia.edu . As always, you can listen to our podcasts on the Scholars’ Lab blog, or subscribe on iTunesU . [podloveaudio src=”http://a1611.phobos.apple.com/us/r30/CobaltPublic1/v4/cc/b6/a9/ccb6a98c-8360-32d4-0022-4a0647686fc1/335-5035744038474421641-staples.mp3”]"},{"id":"2015-02-24-call-for-applications-graduate-fellowship-in-the-digital-humanities","title":"Call for Applications, Graduate Fellowship in the Digital Humanities","author":"purdom-lindblad","date":"2015-02-24 04:28:21 -0500","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"call-for-applications-graduate-fellowship-in-the-digital-humanities","layout":"post","content":"Applications for the  Scholars’ Lab ’s prestigious Graduate Fellowships in the Digital Humanities are now being accepted for the 2015-2016 academic year. Applications are due March 23, 2015. The fellowship supports ABD graduate students doing innovative, dissertation-related work in the digital humanities at the University of Virginia. The Scholars’ Lab offers Grad Fellows advice and assistance with the creation and analysis of digital content, as well as consultation on intellectual property issues and best practices in digital scholarship and DH software development. Fellows join our vibrant DH community, have a voice in intellectual programming for the Scholars’ Lab, can make use of a dedicated grad lounge, and participate in one formal colloquium at the Library per semester. Supported by the Jeffrey C. Walker Library Fund for Technology in the Humanities, the Matthew &amp; Nancy Walker Library Fund, and a challenge grant from the National Endowment for the Humanities, the Graduate Fellowship in Digital Humanities is designed to advance humanities research and provide emerging digital scholars with an opportunity for growth. Eligibility Applicants must be ABD, having completed all course requirements and been admitted to candidacy for the doctorate in the humanities, social sciences or the arts at the University of Virginia. Applicants must be enrolled full time in the year for which they are applying. A faculty advisor must review and approve the scholarly content of the proposal. Applicants are strongly encouraged to demonstrate prior experience in digital scholarship. Experience can include work on a collaborative project, comfort with programing, design, and code management, experience with public scholarship, and critical engagement with digital tools. Applicants with  Praxis Program or equivalent experience will have a competitive edge, but all are welcome to apply. A complete application package will include the following materials: a cover letter, addressed to the selection committee; a Graduate Fellowship Application Form; a dissertation abstract; a summary of the applicant’s plan for use of digital technologies in his or her dissertation research; a summary of the applicant’s experience with digital research and/or public humanities projects; a description of UVa library digital resources (content or expertise) that are relevant to the proposed project; and 2-3 letters of nomination and support, at least one being from the applicant’s dissertation director. Questions about Grad Fellowships and the application process should be directed to: Purdom Lindblad Deadline: March 23, 2015\nNotifications: April 17, 2015"},{"id":"2015-02-24-on-the-shelf","title":"On the Shelf","author":"andrew-ferguson","date":"2015-02-24 09:04:50 -0500","categories":["Grad Student Research"],"url":"on-the-shelf","layout":"post","content":"The past couple weeks, we have been crash-coursing PHP via Wayne Graham’s surely famous slide decks. We’ve been doing this despite not (yet) delivering our task for the fall semester—the redesign of the Ivanhoe webpage. It’s a necessary shift. While we’re fortunate to be assisted by the Scholars’ Lab dev team—including Scott Bailey, a holdover from last year’s Praxis cohort—if we’re going to improve on the game’s workings, we have to be able to get in and make those changes, and we also need lots of practice in making those changes. Start any later, and there just won’t be enough time to develop skills or features. But, as is often the case in academic study (as also, I suspect, in DH), this problem isn’t so much disappearing or even receding as it is mutating: taking on a new and more urgent form. In this case, that’s the design of the Ivanhoe game itself. Our hope is to take the Ivanhoe that presently exists and implement some of the features that almost-but-didn’t-quite make it in last year, while also streamlining the frontend with the aim of making players return to the game at more regular intervals. In all of this, we’re holding onto the principles we had established for the Ivanhoe site, but couldn’t quite yet put into practice there: leaving as much as possible open to the players themselves to customize, so they can experience it in the ways most amenable to their own textual plays. We hoped to symbolize this on the site by incorporating an array of styles that would allow users to approach the website in, if not the infinite ways that we hope Ivanhoe will afford them, then at least enough that they can choose their own stripe of Ivanhoe. Of course, this means implementing these styles across multiple pages, rather than the landing page alone—this was the hurdle we were at when it became imperative to switch tasks. We are at the point now of signing off on our prototype game design, which has been heavily informed by the conversations we had about the site itself. And perhaps it was necessary, if roundabout, to approach things this way. After all, the experience of a text is rarely smooth and never instantaneous. Why should a project devoted to furthering our textual experiences prove any different?"},{"id":"2015-02-26-monkey-mind","title":"Monkey Mind","author":"purdom-lindblad","date":"2015-02-26 10:20:35 -0500","categories":["Digital Humanities","Research and Development"],"url":"monkey-mind","layout":"post","content":"A monkey is easily flummoxed by a coconut. A hole is cut into the coconut and filled with sweet food (or something shiny). The monkey slips her hand into the coconut, grasps the treat, and is trapped. She can readily unhand the treat (or shiny object), but is unwilling to let go. Letting go, for monkeys as well as humans, is harder than it sounds. In my case, letting go means releasing the need to share what I’ve been thinking about lately in a well-formed thesis. What I have to say here is all a bit tangled still. Humanities requires attention to the world and should be applicable to carving out the ways and means of peace. Peace means shelter, access to resources, work that does not harm people or the planet as much as meaning safety, non-violence, and calm. I often return to a conversation with Veronica in which she said academic study carves out a space to reflect on both the object of study and our present moment. I also keep returning to Bethany’s DH2014 keynote . What do we work on, what are the priorities, at a moment of mass extinction? Mixed in are bits of floating inspiration, such as designing for care rather than against ourselves . (Why aren’t Vancouver’s bench shelters in every city?) Further, I have been thinking a great deal about connections and modes of sharing. The new  Praxis Network directories are laying the groundwork for more meaningful connections among the students, alumni, and programs of our partnering groups. I think virtual conferences, workshops, and casual meet-ups can go a long way to personalizing the Praxis Network and extending it beyond one’s home program. My colleague, Jeremy, reminds me that much more than the platform for sharing, it is our attention to others, our empathy and willingness to share that forge meaningful connections. Adding to this mental stew, I recently attended two powerful trainings on bystander intervention at UVa, offered in the wake of our difficult autumn. Jeremy and I, along with 2 other UVa librarians Melinda Baumann and Matthew Vest, attended a 4-day Green Dot training. The premise of Green Dot is that all of us (bystanders) can reduce inter-personal violence (harassment, stalking, sexual violence) by becoming more aware of what is happening around us and checking in on a situation that feels like it could lead to violence. This idea of checking in was also strongly echoed in a recent Suicide Awareness training offered by CAPS, UVa’s Counseling and Psychological Services . The goals of the Suicide Awareness training were to help people recognize others who are in distress and to provide examples of how to help. Much like Green Dot, the advice was simple–ask. One of the counselors said simply noticing and asking about another’s emotional state could be enough for that person to seek help, even if that person brushes you off. Both Green Dot and the Suicide Awareness training ask for a cultivation of awareness and empathy for the people around us. I keep mapping the notion of being aware of what’s happening, not ignoring it even when it is awkward or hard, and stepping in to redirect choices that lead to violence back to the bench-shelter, and in less-able-to-articulate it way to Bethany’s keynote, the Praxis Network, and my own research plan for use of Scholars’ Lab R&amp;D time. Bethany asked what are the things we, as a community, attend to. I am trying to better connect my work in the digital humanities to an active cultivation of empathy and care of people and our planet. I am not advocating for a radical humanities, though that’d be awesome. I am attempting to know the landscape of my own monkey-mind, to better understand what to release as well as what to nurture; when to hold on, and when to let go."},{"id":"2015-03-02-adventures-in-converting-subversion-to-git","title":"Adventures in Converting Subversion to Git","author":"wayne-graham","date":"2015-03-02 04:37:03 -0500","categories":["Research and Development"],"url":"adventures-in-converting-subversion-to-git","layout":"post","content":"While the Scholars’ Lab was founded in 2006, we manage a lot of projects that had their roots in the eText Center in the late 1990s. These projects have lived through the numerous ”best practices” of the various eras, many still bearing the marks of those bygone eras (you see a lot of projects that used FTP clients to manage the project as evidenced by numerous  WS_FTP.log files for those who remember that). Most of the legacy projects we work with were migrated to Subversion by the the 2000s, but if you’re one of the cool kids, you’ll know that everyone uses git these days (thanks in large part to GitHub ). Recently we’ve been working on one of these projects ( Salem Witch Trials ) to help get it ready for a forthcoming book and we found that Subvesion kept getting in the way of actually doing work. Little and big issues kept nagging collaborators like flaky user permissions and issues with adding numerous files had made the Subversion repository to nearly 4Gb. Ben Ray came by my office one day and asked if there was a “better” way to do this. I suggested an experiment with git and GitHub and seeing if that would help ease some of the pain points he was having with Subversion. At first, I naively thought this was be pretty straight forward. We’ve migrated other Subversion repositories to git and they’ve been relatively painless. However, in dealing with projects that started in the 1990s, you always expect a little (ok, a lot) of weirdness. I started out this process using the git-svn utility which converts repos from an SVN-style (directories branches, tags, and trunk ) to git-style repo ( trunk becomes master branch, and converts branches and tags ). Part of this step is to map the authors in the repository to how git addresses its authors. I ran a bit of bash off the svn log to create a list of the authors: $ cd path/to/svn_repo\n$ svn log -q | awk -F '|' '/^r/ {sub(\"^ \", \"\", $2); sub(\" $\", \"\", $2); print $2\" = \"$2\" &lt;\"$2\"&gt;\"}' | sort -u &gt; authors.txt This just generates a text file ( authors.txt ) with unique authors and I had to expand the mappings to read like so: wsg4w = Wayne Graham &lt;wsg4w@uva.edu&gt; After getting the authors, I made a clone of the repository and ran in to the first issue. The default convention in SVN is to create directories for your stuff in branches, tags, and trunk. The trunk directory is where your stuff typically is, but this is more of a convention than an enforced policy. In my case the repo was structured like trunk/branch/stuff . This meant I needed to pass another flag to get the actual source files out. $ git svn clone https://subversion.lib.virginia.edu/repos/salem -T trunk/branch -A authors.txt --no-metadata This would start and get some way through and start throwing 500 errors. Ok, no big deal as I think to myself, “I’ll just mirror the SVN repo locally and then I can run this again on my own machine without any crazy network stuff in the way.” $ cd mkdir -p /tmp/salem\n$ svnadmin create /tmp/salem\n$ echo \"exit 0;\" &gt; /tmp/salem/hooks/pre-revprop-change\n$ chmod +x /tmp/salem/hooks/pre-revprop-change\n$ svnsync init file:///tmp/salem https://subversion.lib.virginia.edu/repos/salem\n$ svnsync sync file:///tmp/salem This too got through some of the download process and started throwing 500 errors . After submitting a trouble ticket, it turned out the Subversion server was running out of memory trying to check out the nearly 4Gb repository. After the server admins increased the amount of memory on the virtual server running Subversion, I ran the svnsync again. After a good long while, I had a local copy of the entire repo and could on it without any network latency. So I reran the git svn utility, but instead of an https connection, I change that to the file URI: $ git svn clone file:///tmp/salem -T trunk/branch -A authors.txt --no-metadata After a bit of churning, this process finished and I had a newly minted git repository with all the history from the SVN repo. I then start looking at things and notice that the repo is really big, even with all of the compression that occurs with git . I start looking around and notice that at some point in the project’s history, all of the images that were being used were added to the project history. Not only that, there were also copies of all of those images in a tarball that was being tracked. Someone had realized this wasn’t good and had removed it, but because of the way in which SCM systems work, we would continue to track these files. To figure this out, I first counted the objects git was tracking. $ git count-objects -v\ncount: 5414\nsize: 41548\nin-pack: 40222\npacks: 1\nsize-pack: 2164015\nprune-packable: 0\ngarbage: 0\nsize-garbage: 0 That size-pack told me there was over 2Gb of data that it knew about. What’s in there? I took a look at the git index and pulled the largest blobs out. $ git verify-pack -v .git/objects/pack/pack-*.idx | sort -k 3 -n | tail -5\n5b7e8c63a0bacd3dc2ab92db2d1d1cbc2359e69c blob   4715942 4715522 2077929726\nf3e135fd90caa6a05a1da13a2afc60c8a0af1063 blob   4743461 1703778 6751124\n6f9cbe6fa3fd702a70d666160329ef1176dd4a07 blob   8042973 7227900 1043663492\nd18b98c09c0dcbf9edc2f6ccf91672a399c8a79d blob   9662999 2747934 8477062\n17ccd45824bb4cb1e1c8b03e5780fa31175c18ab blob   48199680 47913744 93724263 This gave me references for the blobs, but I also needed to figure out what file was taking up so much space. I used the hash of the really big file as a good candidate for removal ( 17ccd45824bb4cb1e1c8b03e5780fa31175c18ab or 17ccd45 with its shorthand). $ git rev-list --objects --all | grep 17ccd45\n17ccd45824bb4cb1e1c8b03e5780fa31175c18ab trunk/branch/cocoon/html/Essex/vol2/gifs/gifs.tar So now I know the path, let’s see were this was introduced. $ git log --oneline --branches -- trunk/branch/cocoon/html/Essex/vol2/gifs/gifs.tar\n...\nddb3b1e Second commit I then rewrote the git history to yank references to this blob out since revision ddb3b1e . $ git filter-branch --index-filter 'git rm --ignore-unmatch --cached trunk/branch/cocoon/html/Essex/vol2/gifs/gifs.tar' -- ddb3b1e^.. Now I need to delete the objects and prune and reindex the git database. $ git clone --no-hardlinks file:///Users/yourUser/your/full/repo/salem salem-smaller This took a really long time, and after checking the repo size, it was still the same size. Time for some more drastic measures. After some poking around on StackOverflow (where you go when you need to figure out something like this), I came across this question Which commmit has this blob? which had some promising information about finding large files (read the entire thread; lots of really good advice). However, none of it seemed to be helping decrease the size of the repository packfile . In fact, when I would run the command to see what blobs were taking up the most room, I always found the same files, even if I yanked them out and rewrote the history. After several more hours of research I came across Ted Naleid’s approach at Finding and Purging Big Files From Git History . One of the parts I was missing was actually cloning the local directory. So, I cloned the local directory and removed the hard links: $ git clone --no-hardlinks file:///Users/yourUser/your/full/repo/salem salem-smaller After checking again the objects were much smaller: $ git count-objects -v\ncount: 0\nsize: 0\nin-pack: 32347\npacks: 1\nsize-pack: 1978880\nprune-packable: 0\ngarbage: 0\nsize-garbage: 0 Smaller, but there’s a lot more I can yank out that should have never been there. Using Ted’s approach, I wrote a bash script to generate file paths for all the blobs. ```\n#! /usb/bin/env bash clear echo “Finding all objects in the repo…”\ngit rev-list –objects –all | sort -k 2 &gt; allfileshas.txt git rev-list –objects –all | sort -k 2 | cut -f 2 -d\\ | uniq echo “Generating the SHA hashes and sorting them biggest to smallest…”\ngit gc &amp;&amp; git verify-pack -v .git/objects/pack/pack-*.idx | egrep “^\\w+ blob\\W+[0-9]+ [0-9]+ [0-9]+$” | sort -k 3 -n -r &gt; bigobjects.txt echo “Generate object SHAs”\nfor SHA in cut -f 1 -d\\  &lt; bigobjects.txt ; do\n  echo “Looking up $SHA…”\n  echo $(grep $SHA bigobjects.txt) $(grep $SHA allfileshas.txt) | awk ‘{print $1,$3,$7}’ » bigtosmall.txt\ndone; echo “Done.”\necho “Look at the bigtosmall.txt file for large files.\\n”\necho “You can remove any large files from your repo history with:\\n”\necho “\\t git filter-branch –prune-empty –index-filter ‘git rm -rf –cached –ignore-unmatch MY-BIG-DIRECTORY-OR-FILE’ –tag-name-filter cat – –all”\necho “\\nYou can then compress it by cloning the repo without hard links:”\necho “\\t git clone –no-hardlinks file:///Users/yourUser/your/full/repo/path repo-clone-name”\n``` [gist id=ed6d074267e60d7fef07 file=gistfile12.sh] This generates several text files, but I was concerned with the bigtosmall.txt . This has the file paths of the large files in the repo. 17ccd45824bb4cb1e1c8b03e5780fa31175c18ab 48199680 trunk/branch/cocoon/html/Essex/vol2/gifs/gifs.tar\nd726f0a0cab047838e3405ad59d3c5399f42db87 12300550 trunk/branch/cocoon/html/maps/DHS/danvers_hist_soc/put_hse2.tif\n06a4076cac85350be52261a8f11df0ecb42d6696 10610964 trunk/branch/cocoon/html/maps/images/musick_nurse.tif\n6f9cbe6fa3fd702a70d666160329ef1176dd4a07 8042973 trunk/branch/cocoon/images/small/casey.tif\n5b7e8c63a0bacd3dc2ab92db2d1d1cbc2359e69c 4715942 trunk/branch/cocoon/html/archives/essex/eia/large/eia22r.jpg\n20ea6bb7b466cd4ba4716834bae7507989ff88b7 3861655 trunk/branch/cocoon/html/archives/essex/eia/large/eia06r.jpg\n8a37fabb82418c6e6b07abf08821a053b2dc4b11 3770686 trunk/branch/cocoon/html/archives/essex/eia/large/eia13r.jpg\n5fecd828115d3909cbe70de0be3936f96fb61868 3708386 trunk/branch/cocoon/html/maps/DHS/danvers_hist_soc/Summerhouse Looking through this, what I found was that most of these large files were in an archives directory, or images directory. What I did was move these more static files to a shared location on the server (most of which are a child of the html directory), so I ran the following to remove the html directory from the history: $ git filter-branch --prune-empty --index-filter 'git rm -rf --cached --ignore-unmatch trunk/branch/cocoon/html' --tag-name-filter cat -- --all This forces git to go through all of the commits, removing references to these blobs (whose history can be managed as a separate entity). This took a while, and after the history rewrite was finished, I recloned the repo and the pack size was quite a bit smaller. After a lot of this, it got a lot smaller: $ git count-objects -v\ncount: 0\nsize: 0\nin-pack: 24334\npacks: 1\nsize-pack: 966726\nprune-packable: 0\ngarbage: 0\nsize-garbage: 0 Now that it’s under a single Gigabyte, I’m happy. So is GitHub with it’s “suggestion” that repos should be under 1Gb in size . There’s probably more that could be cleaned up, but this is more finding odd files here and there. Hopefully this saves someone else some digging (including my future self for the next migration from Subversion to git). Dealing with these legacy projects, particularly on performance issues related to decisions made 10 - 15 years ago in the workflow for source management can be difficult to track down and figure out; not to mention obscure and and somewhat impenetrable. However, there does come a point where people working on a long-running project will start to feel real pain in working on the system and being able to improve this performance helps ensure people continue to want to work on a project rather than abandoning it due to frustration with the tooling. After a lot of really high-CPU spiking git rewrites, I discovered BFG which is an alternative to the git-filter-branch strategy. It claims it’s up to 720x faster. While I didn’t do a timed comparison (I really wanted to stop messing with this), I can say that it was orders of magnitude faster. So much so that if I need to do this again, I’ll probably use that tool first."},{"id":"2015-03-03-something-about-php","title":"Something about PHP","author":"steven-lewis","date":"2015-03-03 06:28:29 -0500","categories":null,"url":"something-about-php","layout":"post","content":"We’ve spent the past month and a half learning PHP. It’s an arduous task only complicated by our own busy schedules. Trying to learn a new language becomes much more difficult when also trying to wrangle sixty undergraduates every week, or finish a dissertation, or find a job. And yet progress continues steadily. The decision to sideline the redesign of the Ivanhoe information page was a good one in that it’s given us more time to work on the fundamentals of programming.   Our development team has produced multiple promising wireframes of the redesigned Ivanhoe game, and the group is close to approving a prototype design of the game. The rest of us are in the process of researching Wordpress plugins, hoping to find one that will provide email and social media notifications to players when someone makes a new move in a game that they’re playing. The goal of all of this is to make the Ivanhoe game responsive and appealing, something that people want to play. Of course, all of this leads inevitably back to grasping the fundamentals of PHP coding. The PHP homework has been many of our single greatest struggle, lurking unfinished in some corner of our minds even as we made progress on other areas of our larger project. PHP is difficult to internalize not because it’s radically unfamiliar but because it’s similar enough to written English to cause repeated problems. For example, to express “and” in PHP, one would write “&amp;&amp;” rather than “&amp;.” A typical line of PHP code is almost intuitive enough to write unaided, and is simultaneously just complicated enough to make an aspiring programmer throw up their hands in frustration. Mistakes will be—and are currently being—made, and it’s at times like this that we’re most lucky to have such a patient and generous Scholar’s Lab staff. P.S.: Does anyone have ideas for great uses of Wordpress plugins as teaching tools? If so, send them our way!"},{"id":"2015-03-09-neatline-2-4-0","title":"Neatline 2.4.0","author":"wayne-graham","date":"2015-03-09 10:52:52 -0400","categories":["Announcements","Experimental Humanities","Research and Development"],"url":"neatline-2-4-0","layout":"post","content":"We’re happy to announce a new version of Neatline which adds a couple new features along with resolving a few small issues. The two main features in this release  were implemented based on community feedback. First, it’s now possible to set the opacity of a WMS layer when its selected using the “selected” opacity setting. Previously this setting only pertained to drawn geometries on a Neatline record. Second is the ability for custom themes to provide containers for Neatline widgets. This gives theme developers more control over where elements of a Neatline exhibit are displayed on the page. There were also some issues that are resolved. An optimization was introduced in Neatline 2.3 which caused Neatline to not render WMS maps created using MapWarper. This has been corrected and maps created using the NYPL MapWarper and Harvard WorldMap WARP tools properly render in Neatline. By the way, these two resources provide a great number of maps for you to use in Neatline without needing to run your own instance of GeoServer or other service. We also fixed an issue where styles with an underscore (“_”) in them would not render properly, and we fixed an issue that would move SVG layers at specific zoom levels. You can check out the Changelog for more detail on these changes. As always, you can download the latest release from the Omeka Add-Ons  Repository . If you run into any issues, you can always ask a question on the Omeka Forums or submit an issue or feature request on our issue tracker ."},{"id":"2015-03-10-watermarking-and-ocr-ing-your-images","title":"Watermarking and OCR-ing Your Images","author":"ammon-shepherd","date":"2015-03-10 06:34:00 -0400","categories":["Research and Development"],"url":"watermarking-and-ocr-ing-your-images","layout":"post","content":"In the process of my dissertation research I have accumulated over 2,000 images, nearly all scans of documents. One goal of my dissertation is to make these documents open and available (where appropriate) in an Omeka repository. In order to more correctly attribute these documents to the archives where I got them, I need to place a watermark on each image. I also need the content of the documents in a format to make it easy to search the text. The tools to do each of those steps are readily available, and easy to use, but I needed a script to put them together so I can run them on a handful of images at a time, or even hundreds at a time. I’ll walk through the problem and show the steps I used to solve it. When at the Neuengamme Concentration Camp Memorial Archive near Hamburg in the summer of 2013, I found about 25 testimonials of former inmates. In most cases I took a picture of the written testimonial (the next day I realized I could use their copier/scanner and make nicer copies). So I ended up with quite a number of folders, each containing a number of images. So the goal became to watermark each image, and then run an OCR program on each image to get the contents into plain text. Watermark There are many options for water marking images. I chose to use the incredibly powerful ImageMagick tool. The ImageMagick website has a pretty good tutorial on adding watermarks to single images. I chose to add a smoky gray rectangle to the bottom of the image with the copyright text in white. The image watermark command by itself goes like this: width=$(identify -format %w \"/path/to/copies/filename.png\"); \\\ns=$((width/2)); \\\nconvert -background '#00000080' -fill white -size \"$s\" \\\nlabel:\"Copyright ©2014 Ammon\" miff:- | \\\ncomposite -gravity south -geometry +0+3 - \\\n\"/path/to/copies/filename.png\" \"/path/to/marked/filename.png\" This command can actually be run on the command line as is (replacing the copyright text and paths to files of course). The command is actually three commands and should be written on one line, but for ease of reading, the backslash () denotes where I split the commands onto the next line. I’ll explain the command below. The first line gets the width of the image to be watermarked and sets it to the variable “width”. width=$(identify -format %w \"/path/to/copies/filename.png\"); \\ The second line gets half the value of the width, and sets it to the variable “s”. s=$((width/2)); \\ The third line starts the ImageMagick command (and is broken onto several lines using the \\ to denote that the command continues on the next line). The code from convert to the pipe | creates the watermark, a dark grey rectangle with white text at the bottom of the image. convert -background '#00000080' -fill white -size \"$s\" label:\"Copyright ©2014 Ammon\" miff:- | \\ The rest of the command tells ImageMagick where to put the watermark, the original image to use, and where to put the image with a watermark and what to call it. composite -gravity south -geometry +0+3 - \\\n\"/path/to/copies/filename.png\" \"/path/to/marked/filename.png\" The results can be seen on the following image. OCR Most of the images I have are pictures of typed up documents so they are good candidates for OCR (Optical Character Recognition), or grabbing the text out of the image. OCR can be done using a program called tesseract . The tesseract command is relatively simple. Give it an input file name, an output file name, and an optional language. tesseract \"/path/to/input/file.png\" \"/path/to/output/file\" -l deu This will OCR file.png and create a file named file.txt. The -l (lowercase letter L) option sets the language to German (deu[tsch]). Below are two examples. Note that the translation is not letter-for-letter perfect, but the software does a good job. The benefits of OCR’ing documents is apparent when needing to search for specific details. Now that I have each page OCR’ed, I can do searches on these files, where otherwise I had to read through the entire PDF page by page or look at every single image. For example, today I’m looking through a 50+ page PDF transcript of a survivor interview to find the parts where she talks about her experiences at the Porta Westfalica camp. While I will read through each page individually, I can get a quick sense of where I should be looking by doing a search on the OCR’ed pages to find out where the term ‘Porta’ is found. Now I know that at least on pages 47 and 48 is where I’ll find some description of her time in Porta Westfalica. The Script Imagine typing in those commands for every single image that I want to OCR and watermark. That would take way too long, and computers are really good at doing repetitive tasks, so I’ll let the computer take care of that. That’s where writing a script comes in very handy. A script is basically a file that tells the computer a bunch of commands to execute. In this case the commands are the ImageMagick and tesseract commands; with some logic thrown in to find the right files and put the results in the right place. The mascot is a crop from this image of monks (found in a 1911 book about characters of the Middle Ages, SCENES &amp; CHARACTERS OF THE MIDDLE AGES, By the Rev. EDWARD L. CUTTS, b.a. LATE HON. SEC. OF THE ESSEX ARCHÆOLOCICAL SOCIETY, http://www.gutenberg.org/files/42824/42824-h/42824-h.htm#Page_39 ). The name ‘cowl’ (Copy, OCR, Watermark, Language) is inspired by monks, whose work it was to copy and transcribe documents (similar to what this script does). Originally, I wrote a script using BASH, a shell scripting language specific to Unix based computers. That script is available at my GitHub repo:  https://github.com/mossiso/ocr-watermark A nice write up on how to use this script in its original format (and the basis for the content of this post) is found on my dissertation blog:  http://nazitunnels.org/2014/11/watermarking-and-ocring-your-images.html  The most up to date steps will be at the GitHub repo linked above. I plan to update the BASH script to behave the same way as the below-detailed script, so things will definitely change in the future. I was able to rewrite the script in a more universally available language; Ruby. That script is available here:  https://github.com/mossiso/cowl Here is how to use the Ruby script. The most up to date version of these steps is at the GitHub repo linked above. Set up This assumes you already have ruby, git and bundler installed. Installing Ruby:  https://www.ruby-lang.org/en/documentation/installation/ Installing Git:  http://git-scm.com/book/en/v2/Getting-Started-Installing-Git Installing bundler:  http://bundler.io/#getting-started NOTE: At the moment, you should also have tesseract and GhostScript installed. There are ruby gems to handle these, but they are not playing nicely yet, so these commands are called from the command line for now. Instructions for installing tesseract: https://code.google.com/p/tesseract-ocr/wiki/ReadMe Instructions for installing GhostScript: http://www.ghostscript.com/doc/9.15/Install.htm If you’re on a Mac, it is highly recommended that you use homebrew ( http://brew.sh/ ) or some such thing for installing programs Download the repo in your home directory: &lt;code&gt;git clone https://github.com/mossiso/cowl\n&lt;/code&gt; This creates a folder called cowl and puts four files into it. Now change directories into the cowl directory. &lt;code&gt;cd cowl\n&lt;/code&gt; Get the required gems by running bundler. &lt;code&gt;bundle\n&lt;/code&gt; Edit the cowl file to change the default copyright text. Change the line (line 21 in the image below) that looks like this: &lt;code&gt;options.mark_text = \"Copyright ©2014 The Marvellous and awesome Me\"\n&lt;a href=\"http://static.scholarslab.org/wp-content/uploads/2015/02/edit-copyright1.png\"&gt;&lt;img src=\"http://static.scholarslab.org/wp-content/uploads/2015/02/edit-copyright1.png\" alt=\"edit-copyright\" height=\"418\" class=\"aligncenter size-full wp-image-11562\" width=\"886\"&gt;&lt;/img&gt;&lt;/a&gt;                                                  \n&lt;/code&gt; Examples In your terminal program, enter into the directory where you have images. &lt;code&gt;cd /path/to/images/\n&lt;/code&gt; Run the cowl command (if you ran the git command in your home directory, and your home directory is /home/billy) &lt;code&gt;ruby /home/billy/cowl/cowl\n&lt;/code&gt; Running without any options will create a copy of the images, OCR them, put a watermark on the copies, and combine them all into one PDF. The default text for the watermark is hard coded in the cowl script. You can change it there, or use the -t option. &lt;code&gt;ruby /home/billy/cowl/cowl -t \"Copyright ©2015 Billy Jorgenson Photography\"\n&lt;/code&gt; To change the language used in the OCR to English (since most of my documents are in German, I hard coded it to use German :) ). &lt;code&gt;ruby /home/billy/cowl/cowl -l eng\n&lt;/code&gt; If you have a PDF file to start with, run the command with the -g option. This will break the PDF into PNG images, then make copies, run the OCR, put on the watermarks, and finally create a new PDF with the watermark on each page. &lt;code&gt;ruby /home/billy/cowl/cowl -g\n&lt;/code&gt; You can also use the tool as a way to make copies of your images (with or without a watermark). &lt;code&gt;ruby /home/billy/cowl/cowl -nop # no watermark, ocr, or PDF\n\nruby /home/billy/cowl/cowl -op # no ocr or PDF\n&lt;/code&gt; If you run into any issues or have an idea for an upgrade, feel free to add an issue to the GitHub repo, or even send me an email. Happy cowl’ing!"},{"id":"2015-03-11-printing-things-that-print-a-miniature-hand-press-project","title":"Printing Things That Print: A Miniature Hand-Press Project","author":"ethan-reed","date":"2015-03-11 10:24:19 -0400","categories":["Experimental Humanities"],"url":"printing-things-that-print-a-miniature-hand-press-project","layout":"post","content":"For the past few months, fellow English PhD candidate James Ascher and I have had a small side-project going on in the Makerspace: printing a small, desktop-sized hand press, and getting it to work consistently.  The model we found calls it a “ Pocket Gutenberg,” but the idea of a small, hand-powered, personal “hobby” printing press is an historic one going back to Ben Lieberman and the “Liberty Press” – a history James will hopefully be posting about soon. I want to talk about our experience making this thing and trying to get it to work.  By putting it up here, I’m hoping it will be an example of the kinds of small exploratory projects that often become gateways into the growing scholarly field of remaking old technologies. On my end, I was also inspired by Jentery Sayers in his recent visit to the Scholars’ Lab (all the way from the Maker Lab at UVic ) in which he discussed exactly this kind of project, though on a much larger and more rigorous scale (we have his talk in podcast form - check it out !). So what did we make? As linked above, we started by printing a desktop hand press. James then printed a plate with the motto, “Collaborate -&gt; Iterate -&gt; Discuss (and Print).” Using some ink we had meant for rubber stamps, we inked the plastic plate and gave it a shot – as you can see, what came out is readable (sort of) but not very pretty. We jerry-rigged a few modifications (i.e. taped-on cardboard to raise and even out the paper) and then I printed out a single set of type from another model on Thingiverse, and sized it so that it would fit with our press. Printing with this type was a real pain as there was no way to keep them in place – a problem which, again, involved another series of jerry-rigged semi-solutions. Until we figured out a way to make this setting and pressing of type easier, our new press was going to be very limited in its applications. [gallery link=”file” columns=”2” ids=”11586,11621” size=”medium”] As you can see, our goal in re-creation here wasn’t so much fidelity to a specific historical object as it was to explore the capabilities of an older technology (the hand-press) shrunk down in a new way to an individual desktop size that anyone could use on their own. Clearly, even this more modest goal led to a series of joyful problems.  It turns out stamp ink is, well, mainly meant for stamps, and that not all paper is created equal.  These may seem like obvious insights (and I’m sure they were to James, who knows much more about books as physical objects than I do) but for someone only recently beginning to learn about bibliography, textual studies, and the history of printing, these were practical concerns I had heard of but never had to really deal with before.  And they are all things we are still working on with making our press work. So why does this matter? In an earlier post titled “ The Relevance of Remaking,” Sayers offers some questions that I found to be helpful in making meaning out of experiences with old technology. Two in particular – “From what materials was it [the old technology] made?” and “Through what measures was it deemed a success?” – were particularly revealing in our case.  I was faced with problems like: what kind of paper works best when you are using plastic type? There’s a big difference between printer scrap paper we used originally and the softer paper, like from a cheap paperback, that we ultimately decided would work best.  For that matter, I wondered what kind of paper works best using metal type – and what about differences in kinds of metals? What about different kinds of ink? What’s in “gel-based” stamp ink, anyways (as it reads on the back of the box)?  When we tried it again with a small sample of the oil-based kind of ink used in relief printing (courtesy of Josef Beery ), and the prints came out much clearer, even after multiple presses (despite being limited by the granularity of the Replicator 2 model and print). These aren’t new problems – they’ve been around for as long as people have been trying to print things. And despite being surrounded by scholars interested in these fields here at UVa, it’s been very easy for me to take them for granted on a day-to-day basis – something I didn’t realize until trying to deal with them first-hand. To summarize: What James and I are up to here is very small-scale. But while piecing together something from a 3D printer is just the tip of the iceberg in terms of remaking old technologies, it’s been a really good way for me to get my foot in the door – although the technology we’re building is not strictly historical, the problems associated with it are historic ones.  Used in conjunction with the kinds of resources we have at the Rare Book School or Special Collections, this was a useful supplement to the study of books and book-making that I had the power to explore on my own. It certainly opened my eyes to a whole set of new questions in a physical, immediate way. Regarding the bigger picture: we haven’t thought through every potential use of this press, or what scholastic infrastructure it will fit into as a project.  In that sense it is surely subject to the “speculative condition” of purely exploratory research Bethany identifies and discusses in this enlightening post about speculative computing ; but I would maybe liken our project even more to the kinds of projects she has described in another post as “too small to fail.” In this sense, even if our press ended up being useless, this particular failure could serve as “a concrete experiment” our peers can talk to us about and learn from. But this smallness and speculative-ness have their payoffs – most of what I ended up learning about remaking old technologies I came across because we had already started working with the desktop hand press. It was only after diving in and having problems that I had the impetus to see what others in the Maker community had to say about similar projects.  What I found was really encouraging and left me with a whole bunch of new questions regarding our mini-project: when Sayers asks in another post, “Why Fabricate?”, that we consider “for which methodologies and research areas does material depth especially matter,” and when Devon Elliott et al. ask in “New Old Things” (an article cited in Sayers’ piece) for us to seek out “projects that allow us to imaginatively remake past technological artifacts and to experiment with past technological worlds,” the project acquired a new level of meaning for me. I’m not sure if “make first, ask questions later” is a policy I would always encourage, but in this small-scale situation, given the time, resources, and encouragement to explore, following my interests ended up leading to not only to real, living problems, but also real opportunities to combine my work at the Makerspace with my work in literary study. Success!  In this instance, exploration resulted in the surprise of discovering a new set of important questions already being explored by others."},{"id":"2015-03-13-novice-struggles-and-expert-blindness-how-my-discomfort-with-php-will-make-me-a-better-instructor","title":"Novice struggles and expert blindness: How my discomfort with PHP will make me a better instructor","author":"jennifer-grayburn","date":"2015-03-13 09:38:45 -0400","categories":null,"url":"novice-struggles-and-expert-blindness-how-my-discomfort-with-php-will-make-me-a-better-instructor","layout":"post","content":"It was my third blank screen. I switched back over to my text editor and tried again. I looked at the format of my PHP, looked at my functions. I looked up the PHP documentation, deleted elements that did not seem right, and saved again. Back in my browser, the screen was still blank. Last week, my computer at least gave me an error warning to let me know I had incorrect code. Today, there was nothing—even the static html components failed to appear. At this point, I had no idea what to do. I had exhausted my knowledge of PHP, exhausted my understanding of what to look for or how to fix it. After two straight hours, I was frustrated, overwhelmed, and tired, yet had nothing to show for the time and effort I put into it. It is the first time I remember feeling like I would and could never build the competency I needed to complete our remodel of Ivanhoe.  I felt inadequate. I felt stupid. I made excuses that maybe I just was not ‘meant’ to work with computers. Last week, Steven Lewis blogged about his own difficulties with our PHP homework.   For him, the challenge is not that the PHP is completely unfamiliar, but rather than it incorporates elements of English that are familiar, though not used in a familiar or intuitive way. This is a key observation, for it shows how we try to make sense of the PHP based on our own experiences and cognitive framework. We see symbols that look familiar and try to use them and read them in a way that already makes sense to us. I have used this type of association in the past when learning new languages; in order to make sense of an unfamiliar language, I look for cognates or contexts and grammatical elements that look familiar based on my experiences with English. Of course, this process works better for some languages than others (for examples, there are less similarities between English and highly inflected languages or languages that use different scripts), but it nevertheless reveals how we learn by association and then application based on our established knowledge set. So on one hand, I understand how this comparison of code to language can make PHP look deceptively intuitive, especially when symbols work in a way that is unfamiliar to our native language. For me, however, the challenge is less the code form as it is the logic behind it. The logic of PHP is more mathematical than grammatical, more literal and precise than our everyday language. This difference has been my biggest obstacle, for while I can memorize and regurgitate the system of code, I still struggle to grasp how all of the pieces intersect and how to solve problems when they arise.  I still cannot apply my knowledge to new contexts, cannot see how one set of tasks are similar to another, cannot anticipate how literally the computer will process my command. Even the resources available to consult are new and unfamiliar; traditional experts and peer reviewed publications are replaced by flexible online forums with an overwhelming number comments suggesting alternative approaches and opposing advice. Last week was a low point. I was frustrated and depressed; I did not know how to start or even which questions to ask when given the chance. It felt hopeless and I had the strongest urge to give up. I am familiar with the concept of “expert blindness,” but had always associated it with content, assumed that an expert knew so much information that they were unable to unpack it and introduce it in an order that made sense to someone new to the field. My PHP-exhaustion last week, however, expanded my understanding of expert blindness to incorporate not just information, but also skills. It has been years since I have been truly stumped, completely unsure how to even proceed, let alone succeed. Over the past 10 years as an undergraduate and graduate student, I have not only expanded my knowledge of my field, but also developed proficiency in the tools and skills necessary to analyze and present that knowledge. If I run into an issue with my research, I know who to consult, which publications to scan, how to read the plans and diagrams, what professional expectations to meet, and how to integrate that information with the vast compilation of knowledge I already have. In other words, I can apply my knowledge to new contexts and problem solve so efficiently that I cannot remember a time when I could not. My own expert blindness crept up on me, affecting not only how I learn, but also how I expect others to learn. As an instructor, I have always tried to established clear learning goals and to execute them in a way that engages the students actively. I have constructed assignments to help them learn new vocabulary, develop their writing, and analyze the formal qualities of an unknown artifact. Looking back, however, I recognize moments when I took students’ knowledge and skills for granted because they are things I do without thinking. Most recently, I remember asking my students to analyze a building plan in order to identify different housing forms and, therefore, different eras of construction. While I lectured on these structure forms in class, the students’ papers revealed that some did not understand how to read the plans I introduced. When a plan shows building walls with varying thicknesses of lines, it is not at all obvious that other line variations showing stages of construction do not in fact display real striations or stipulations in the landscape. While I was able to revisit the topic in class after recognizing my lapse of instruction, it made me realize how much we gloss over–especially in lecture courses–and how easy it is for students to repeat that information correctly on an exam even if they do not fully grasp it. I wonder if my students ever felt as I did during my PHP exercises—confronted with a task or certain methods that frustrated them, throwing everything they had at it with no particular order or insight. I wonder if I then interpreted their regurgitation of my own words as deep and meaningful learning. How many students drop classes or majors because they were frustrated and felt they could never understand? Is there a better way to present course content, not to get all of the content in, but rather to ensure that the students come away enlightened and enriched by what they confronted, confident that they can continue to develop those skills not only as students, but also as life-long learners? How do we encourage them to ask questions, even if they seem too broad or simple? While I came to the Praxis Program hoping to gain concrete digital skills, I will come away with a renewed affinity with my students as learners confronted with something so new that it challenges them. As academics, it is often very easy to stay within the established bounds of our disciplines, very difficult to get out of our comfort zones in order to learn something (anything) that requires completely different skill sets than our own. I wonder if we should perhaps make this discomfort a bigger priority within the graduate curriculum in order to broaden our thinking as researchers and think critically about our methods as instructors. My frustration with PHP is not gone, but my realization that this frustration is a natural part of the learning process–something I expect my own students to experience–helps me push through it. We are now working directly with the Ivanhoe code and the practical application of more abstract concepts allows me to see concrete results. By creating my own localized git branch, I have been able to play with the code on my own terms, to change something and see how it affects the site in a low stakes setting. But I never would have made it this far without the support and encouragement of the dedicated Scholars’ Lab staff. They have a willingness to repeat concepts, ability to recontextualize and reword information in personalized ways, and openness to my never-ending stream of questions. Most importantly, they have encouraged us to jump in, to make mistakes, to learn from them. While they have their own expertise in the field, their own “content knowledge” of how to do these tasks themselves, they have also developed what Mitchell J. Nathan et. al in “ Expert Blind Spot ” describes as “pedagogical content knowledge.” The former refers to their own competence in the field, while the latter underscores their anticipation of the learning obstacles of novices and their ability to structure and teach in a way that meets the needs of learners rather than the expectations of experts (Nathan 7). They do not distinguish themselves as experts from us as students; rather, there is the acknowledgement that they, too, were once where we are (including their own humanities background), that they have merely had more time to learn and master these skills. This self-awareness allows them to recognize learning opportunities and (dare I say?) enjoy them. At our first Praxis meeting months ago,  Jeremy Boggs  showed us the xkcd comic,  xkcd: Ten Thousands . While I thought it was a nice mindset then, I saw myself exclusively as the character with knowledge to share. I recall this comic now and it resonates more deeply with my recent experiences; I am both characters at once as long as I can embrace unrestrictedly the expertise others offer me and recognize that I can offer expertise in return. Perhaps it is impossible to do either successfully without grasping the other. Regardless, there is no doubt that I will embrace both as I move forward."},{"id":"2015-03-16-podcast-rob-nelson-on-topic-modeling-in-the-humanities","title":"Podcast: Rob Nelson on Topic Modeling in the Humanities","author":"laura-miller","date":"2015-03-16 12:13:21 -0400","categories":["Events","Podcasts"],"url":"podcast-rob-nelson-on-topic-modeling-in-the-humanities","layout":"post","content":"The Potential and Pitfalls of Topic Modeling for Humanities Research This talk will introduce the text-mining technique called topic modeling, briefly explaining what it is and how it’s done. It will then turn to more substantial questions: what does this technique offer humanities researchers and what are its methodological limitations and problems? Both the potential and the pitfalls of topic modeling will be illustrated through research that uses topic models of newspapers to explore Civil War nationalism. Dr. Robert K. Nelson is the director of the Digital Scholarship Lab at the University of Richmond. His current research uses a text-mining technique called topic modeling to uncover themes and reveal historical patterns in massive amounts of text from the Civil War era.  He is currently completing two projects from this research.  One is a digital project that will publish and analyze multiple topic models of Civil War-era archives including the Richmond Daily Dispatch and the New York Times .  The other is an essay that analyzes these models to produce a comparative analysis of Union and Confederate nationalism and patriotism. This event is co-sponsored by the Institute for Advanced Technology in the Humanities, the Data Science Institute ’s Center for the Study of Data and Knowledge, and the Scholars’ Lab. This talk was recorded in Alderman Library, Rm 421 on February 25, 2015.  Click below to stream the podcast, and follow along with Rob’s slides .  If you’d like to hear more from the Scholars’ Lab, subscribe to our podcast series on iTunesU . [podloveaudio src=”http://a643.phobos.apple.com/us/r30/CobaltPublic3/v4/02/76/7d/02767d82-95d8-2f30-6639-b9acaaa3f16a/304-7485002225613836972-nelson.mp3”]"},{"id":"2015-03-25-hearing-silent-woolf","title":"Hearing Silent Woolf","author":"brandon-walsh","date":"2015-03-25 05:12:43 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"hearing-silent-woolf","layout":"post","content":"[This week I presented at the 2015 Huskey Research Exhibition  at UVA. The talk was delivered from very schematic notes, but below is a rough recreation of what I discussed. The talk I gave is a crash course in a new project I’ve started working on with the generous help of the Scholars’ Lab that thinks about sound in Virginia Woolf’s career using computational methods. Eric Rochester, especially, has been endlessly giving of his time and expertise, helping me think through and prototype work on this material. The talk wound up receiving first prize for the digital humanities panel of which I was a part. The project is still very much inchoate, and I’d welcome thoughts on it. Cross-posted on my own blog .] When I talk to you, you make certain assumptions about me as a person based on what you’re hearing. You decide whether or not I might be worth paying attention to, and you develop a sense of our social relations based around the sound of my voice. The voice conveys and generates assumptions about the body and about power: am I making myself heard? Am I registering as a speaking voice? Am I worth listening to? The human microphone, made famous by Occupy Wall Street, nicely encapsulates the social dimensions of sound that interest me: one person speaks, and the people around her repeat what she says more loudly, again and again, amplifying the human voice without technology. Sound literally moves through multiple bodies and structures the social relations between people, and the whole movement is an attempt to make a group of people heard by those who would rather not listen. As a literary scholar, I am interested in how texts can speak in similar ways. The texts we read frequently contain large amounts of speech within them: conversations, monologues, poetic voice, etc. We talk about sound in texts all the time, and the same social and political dimensions of sound still remain even if a text appears silent on the page. If who can be heard and who gets to speak are both contested questions in the real world, they continue to structure our experiences of printed universes. All of this brings me to the quotation mark. The humble piece of punctuation does a lot of work for us every day, and I want to think more closely about how it can help us understand how texts speak. The quotation mark is the most obvious point at which sound meets text. Computational methods tend to focus on the vocabulary of a text as the building blocks of meaning, but they can also help us turn quotation marks into objects of inquiry. Quotation marks can tell us a lot about how texts engage with the human voice, but there are lots of them in texts. Digital methods can help us make sense of the scale. I examine Virginia Woolf’s quotation marks, in particular, for a number of reasons. Aesthetically, we can see her bridging the Victorian and modernist literary periods, though she tends to fall in with the latter of the two. Politically, she lived through periods of intense social and political upheaval at the beginning of the twentieth century. Very few recordings of Woolf remain, but she nonetheless thought deeply about sound recording. The worldwide market for gramophones exploded during her lifetime, and her texts frequently featured technologies of sound reproduction. Woolf’s gramophones frequently malfunction in her novels, and I’m interested in seeing how her quotation marks might analogously be irregular or broken intentionally. Woolf is especially good for thinking about punctuation marks in this way: she owned a printing press, and she often set type herself. The following series of histograms gives a rough estimation of how Woolf’s use of quotation changes over the course of her career. On GitHub  you can find the script I’ve been working on with Eric to generate these results. The number of quotations is plotted on the y-axis against their position in the novel on the x-axis, so each histogram represents more quoted speech with higher bars and more concentrated darknesses. If you have an especially good understanding of a particular novel, Mrs. Dalloway, say, you could pick out moments of intense conversation based on sudden spikes in the number of quotations. The histograms are organized in such a way that to read chronologically through Woolf’s career you would read left to right line by line, as you would the text of a book. The top-left histogram is Woolf’s earliest novel, the bottom-right corner her last. To my eye, the output suggests high concentrations of conversation in the novels at the beginning and ending of Woolf’s career. We can see that her middle period, especially, appears to have a significant decrease in the amount of quoted speech. In one sense, this might make sense to someone familiar with Woolf’s career. Her first two novels feel more typically Victorian in their aesthetics, and she really gets into the thick of modernist experiment with her third novel. One way we often describe the shift from Victorian to the modernist period is as a shift inward, away from society and towards the psychology of the self. So it makes sense that we might see the amount of conversation between multiple speaking bodies significantly fall away over the course of those novels. The seventh histogram is especially interesting, because it suggests the least amount of speech of anything in her corpus. But if we visualize things a different way, we see that this novel, The Waves, actually shows a huge spike in punctuated speech. This graph represents the percentage of each text that is contained within quotation marks, the amount of text represented as punctuated speech. This might look like a problem with the data: how could the text with the fewest number of quotations also have the highest percentage of quoted speech? But the script is actually giving me exactly what I asked for: The Waves  is a series of monologues by six disembodied voices, and the amount of non-speech text is extremely small. More generally, charting the percentage of quoted speech in the corpus appears to support my general readings of the original nine histograms: roughly three times as much punctuated speech in the early novels as in the middle period, with a slight leveling off in the end of her career. We could think of The Waves as an anomaly, but I think it more clearly calls for a revision of such a reading of speech in Woolf’s career. The spike in quoted speech is a hint that there is something else going on in Woolf’s work. Perhaps we can use the example of The Waves to propose that there might be a range of discourses, of types of speech in Woolf’s corpus. Before I suggested that speech diminished in the middle of Woolf’s career, but that’s not exactly true. My suspicion is that it just enters a different mode. Consider these two passages, both quoted from Mrs. Dalloway : Mrs. Dalloway said she would buy the flowers herself. Times without number Clarissa had visited Evelyn Whitbread in a nursing home. Was Evelyn ill again? Evelyn was a good deal out of sorts, said Hugh, intimating by a kind of pout or swell of his very well-covered, manly, extremely handsome, perfectly upholstered body (he was almost too well dressed always, but presumably had to be, with his little job at Court) that his wife had some internal ailment, nothing serious, which, as an old friend, Clarissa Dalloway would quite understand without requiring him to specify. In each case, the text implies speech by Mrs. Dalloway and by Hugh without marking it as such with punctuation marks. Discourse becomes submerged in the texture of the narrative, but it doesn’t disappear entirely. Moments like these suggest a range of discourses in Woolf’s corpus: dialogue, monologue, conversation, punctuated, implied, etc. All of these speech types have different implications, but it’s difficult to get a handle on them because of their scale. I began the project by simply trying to mark down moments of implied speech in Mrs. Dalloway by hand. Once I got to about two hundred, it seemed like it was time to ask the computer for help. The current plan moving forward is to build a corpus of test passages containing both quoted speech and implied speech, train a python script against this set of passages, and then use this same script to search for instances of implied speech throughout Woolf’s corpus. Theoretically, at least, the script will search for a series of words that flag text as implied speech to a human reader - said, recalled, exclaimed, etc. Using this lexicon as a basis, the script would then pull out the context surrounding these words to produce a database of sentences meant to serve as speech. At Eric’s suggestion, I’m currently exploring the Natural Language Toolkit  to take a stab at all of this. My own hypothesis is that there will be an inverse relationship between quoted speech and implied speech in her corpus, that the amount of speech left unflagged by quotation marks will increase in the middle of Woolf’s career. Once I have all this material, I’ll be able to subject the results to further analysis and to think more deeply about speech in Woolf’s work. Who speaks? What about? What counts as a voice, and what is left in an ambiguous, unsounded state? The project is very much in its beginning stages, but it’s already opening up the way that I think about speech in Woolf’s text. It tries to untangle the relationship between our print record and our sonic record, and further work will help show how discourse is unfolding over time in the modernist period."},{"id":"2015-04-01-on-sharing-credit-and-courting-trolls","title":"On Sharing Credit and Courting Trolls","author":"swati-chawla","date":"2015-04-01 07:19:55 -0400","categories":null,"url":"on-sharing-credit-and-courting-trolls","layout":"post","content":"The Praxis team was invited to two presentations the last week. The first at the Moving People/ Linking Lives Page-Barbour Symposium, and then at UVa’s Huskey Research Exhibition . With a lot of help from Praxis teammate Jennifer Grayburn, I prepared a slideshow showcasing a brief history of Ivanhoe and the work of the two Praxis cohorts . My talk for the “Lighting Round” of Moving People/ Linking Lives  and the presentation (with Steven Lewis ) for the Huskey exhibition was modified only slightly from the following write-up (co-authored with Steven Lewis and Jennifer Grayburn ): [gallery columns=”2” link=”none” size=”medium” ids=”11840,11841”] The work that Steven and I presenting today is not our work alone: Ivanhoe was first developed by Jerome McGann, Johanna Drucker, and Bethany Nowviskie in the 2000s, and in its most recent avatar, it was updated by the Praxis cohort of 2013-14, our immediate predecessor. We stand before you representing— hopefully faithfully— our teammates Amy R Boyd, Andrew Ferguson, Joris Gjata, and Jennifer Grayburn . Our discussion today is focused on Ivanhoe’s use as a pedagogical aid, and we will be using examples from the game played by participants of the Mellon Graduate Seminar on Composing the Humanities in a Digital Age . Ivanhoe grew out of a dissatisfaction: it was a dissatisfaction with limitations inherent in existing forms of interpreting texts, where readers felt the need for “a more imaginative form wanted to develop a more imaginative form of critical methodology.” We have chosen to describe Ivanhoe as a gateway for textual play. We define text in the broadest sense possible, and although the Ivanhoe game developed through the collaborative analysis of literature, we’ve expanded its scope to be inclusive of a variety of media. A text can be anything: a painting, a piece of music, a passage of fiction or nonfiction prose. We define “play” as the sort of playful collaborative interpretation of a text that occurs during the course of an Ivanhoe game. Collaboration, Reflection, Performativity: Behind this layout is a commitment to three principles: Collaboration, Reflection, and Performativity. Collaboration is fundamental to the mechanics of Ivanhoe gameplay since every game involves multiple players, and is composed of responses to the central textual object and responses to fellow players. Players work together to weave a discursive web around the central text of the game, and in doing so, they draw meaning out of the subject text in relation to each others’ responses. Like collaboration, reflection too is written into the mechanics of Ivanhoe gameplay, and it is this component that best distinguishes Ivanhoe from other forms of collaborative writing (fan fiction, for example). Not only are players making “moves,” they are required to reflect on these moves through the vantage point of their role vis-a-vis the text. The primary means through which we encourage reflection during games is the requirement that all players maintain a role journal. The journal is something that is separate from the gameplay itself and allows players to reflect on the choices they made in making their moves. It provides players a space dedicated to writing about the ways that their moves relate to the character they created, and think about the ways that their moves relate to the moves of the other players. Performativiy is what sets Ivanhoe apart from other modes of methods of solitary reflection, or forums such as “comments” sections on blogs. In short, Ivanhoe requires players to have a stake in the game as their respective “role”. When players create this role for themselves at the beginning of the game, they are expected to “perform” in that role for the duration of the game. This forces them to delve deeply into the implications of a given critical perspective, a particular location and point of view. It is also what makes the Ivanhoe game fun! Next Steps: The question that animates Ivanhoe is: how do we make a program that students want to engage with by choice, and not by force. Towards that end, we are working on three features this year: Email notifications Email notifications are an important addition in this version of the Ivanhoe game because players know immediately when a new move is made. This allows them to respond to each other quickly and keeps the game moving at a relatively fast pace. In our own experience playing the game, pacing–specifically knowing when moves have been made–is extremely important to keeping players engaged with the game currently being played. Responding to multiple moves This is another important functional addition to the first version of Ivanhoe, in which players could respond to only one move at a time responding to more than one move at once allows players to “stretch” the ways in which they perform their character. Responding to multiple moves will also open up different analytical possibilities and allow for drawing connections between larger groups of ideas. Intuitive and streamlined interface Our new interface is designed to allow more personalization by the game administrator and to increase functionality. We eliminated unnecessary visual clutter, reorganized information for more intuitive reading, and implemented options to change color schemes/game images. In closing, we mentioned that Ivanhoe was being used in a class on John Milton’s Paradise Lost in Washington and Lee University, and that we were hoping to get some feedback from the players in a couple of weeks. In fact, I fortuitously met the instructor for this course at the Moving Peoples/ Linking Lives symposium. The Questions: Here are some of the questions that were posed to us after the two presentations. I hope to address some of them in future blogposts. Thoughts and comments welcome! Has/ how does Ivanhoe been folded into classroom pedagogy? Does/ how does Ivanhoe in its current for account for the difference between fiction and non-fiction? You say so much about reflection and collaboration. But these assume responsible playing. How would a game of Ivanhoe deal with trolls? Can you include your own performance as a response to a move? The full powerpoint slides are here: Ivanhoe at Moving People/ Linking Lives Ivanhoe at Huskey Research Exhibition My Learnings: Presenting our collective work was an honor for me, but also brought with up some questions of sharing credit and acknowledging the contributions of others. Luckily, all the thought (and love!) we had put into our charter at the beginning of the year came in handy. We had said: “All members of the Praxis Program team deserve equal credit for their contribution…. Our shared web site should always list general credit in alphabetical order to emphasize the non-hierarchical ethos of the program. Whenever a member of the Praxis Program references the work to the outside community, the project as a whole should be credited to all members.” Since one of these presentations involved a competitive event, we were told that we could not compete as a group of six. We decided as a team that it was okay for Steven and me to formally compete for the prize, as long as the work was credited to the whole team in the presentation and conference publicity materials. While working on a team project with a think tank in Delhi, I remember my mentor saying that a lot more gets done when everyone is generous with sharing credit. I am very glad we have lived up to the ethos of our charter and done exactly that!"},{"id":"2015-04-06-adventures-in-3d-printer-maintenance","title":"Adventures in 3D Printer Maintenance","author":"shane-lin","date":"2015-04-06 11:38:42 -0400","categories":["Experimental Humanities"],"url":"adventures-in-3d-printer-maintenance","layout":"post","content":"Recently, the fan on the bottom of our Makerbot Replicator Dual Extruder began to make some unhealthy noises. Per SLab desk protocol, the first attempt at a remedy was to smack the thing near where it was making the noise, which made it go away for a little while. Eventually, we got tired of hitting our printer every time we started it up, so I took off the bottom panel to oil the cheap 40mm case fan within. Here’s what I found. Amazingly, like an Alaskan infrastructure project, this fan didn’t actually lead anywhere. The only inlets were the small holes in the corner formed by the chamfer of the horizontal plates, so it mostly served to move hot air around the Mightyboard chamber. This lead me to wonder whether or not a fan was actually necessary, especially since the Replicator 2 has no bottom fan at all for its Rev. G Mightyboard. This line of inquiry lead to a surprising discovery. Original Replicators with Rev. E boards use very inexpensive  linear voltage regulators  to bring the 24V power supply down to the 5V that the Mightyboard requires. The regulator is the thing right next to the plug in that picture. Linear regulators function by bleeding off excess energy as waste heat. Hence the fan. A 19V voltage drop at a ballpark 300mA for the board means about 5.7W that needs to be dissipated. That’s actually a lot of power! It also seems that Mightyboards from the Rev. E era fail at a pretty alarming rate and the main culprit is the voltage regulator. The Internet is a bit divided on the exact mechanism of this failure and whether or not improved cooling will significantly alleviate the issue, but most people seem pretty certain that it’s a nigh-inevitable and invariably fatal problem. When the regulator blows (with a sharp ”pop!”), the whole board goes with it. So, great. In the past, Makerbot has reportedly been pretty good about replacing failed Mightyboards, but not about actually correcting the fatal flaw. With the original Replicator now several generations behind and the delays with our recent Replicator 2 replacement parts order, I am not at all confident that we will be able to source a spare in the future. Replicator 2s do not have this issue because Makerbot upgraded the Rev. G boards to switching regulators (costing upward of ten of dollars!) that produce far less heat and do not require any active cooling at all. Short of just waiting for the board to blow, there are two approaches to this problem. The first and best long-term approach is to replace the linear regulator with a switching one. But without the certainty that we can get the board replaced should we screw up the procedure, I was less keen on this plan to start. The second approach is to improve the cooling, which seemed much more straightforward. There is a Thingiverse compilation of various parts useful to achieving this goal. The easiest first step for us was to simply drill holes in the side of the Replicator over where the fan is positioned, so that we can draw in cool air from outside to blow over the regulator. We also printed out a 40mm duct from the Thingiverse collection to use instead of the original offsets to minimize hot air re-circulation. Here’s the result (the original screws were a few millimeters too short, so we had to pick up longer ones - they’re M3, but 4/32’’ works in a pinch since longer M3s are hard to find). The airflow is much improved now. The next step is to pick up better quality fans so we don’t have to oil them every few weeks and to install a second, output fan on the other side of the board. Our Replicator has lasted close to three years in its original form, which seems much better than most people have reported (some of those complaining about failures were on their 3rd board) . Hopefully, it will function for some time yet, even if we decide not to do the regulator swap."},{"id":"2015-04-08-everything-is-not-always-awesome-when-youre-part-of-a-team","title":"Everything Is (Not Always) AWESOME When You're Part of a Team!","author":"joris-gjata","date":"2015-04-08 05:35:47 -0400","categories":["Grad Student Research"],"url":"everything-is-not-always-awesome-when-youre-part-of-a-team","layout":"post","content":"You have watched The Lego Movie I hope. It is a story about an individual who seems to be happy alone with his own routine as a ‘normal’ person, but one day finds out he is ‘the chosen one’ meant to save people. He does not believe in himself and his ability to be the hero. Nevertheless, he ends up being victorious against evil because of his great team. The movie ends with a song whose refrain summarized its conclusion: “everything is awesome, everything is cool when you’re part of a team…” Yes, I really appreciate the idea that individuals are social beings and heroes are the result of collective action. I value the emphasis on team work in this movie, but I would like to draw attention also to the hard work that goes into working within a team and as a team. Getting involved in the Ivanhoe project as part of an interdisciplinary team has made me more aware of the challenges of team work. Working with my Praxis 2014-15 fellows and Scholars’ Lab staff/members on Ivanhoe has taught me several_ important lessons about being part of a team_ that I will definitely keep in mind for future reference wherever I go, especially if working on collaborative projects. First, I have understood that what makes being part of a team totally awesome is people that are genuinely interested in helping you learn and grow as a whole person. My teammates and the Scholars’ Lab staff have been very understanding and flexible during my pregnancy and also with the arrival of my child. They can recognize that researchers are humans and life has to it more than research. They have made me feel supported and also helped me overcome what I would call ‘expert stubbornness’ (not only ‘expert blindness’ as my teammate Jennifer says in her recent blogpost). Expert stubbornness is when you try to do your own deep thinking and hard work to find the answer to a question before asking help from others. I tend to have a lot of this stubbornness, but the rule ‘if you cannot figure out in 5-10 minutes, ask somebody’ has helped me a lot in overcoming this ‘vice’. It made me realize the implicit assumption underlying my behavior: that experts know it all themselves and being an expert means addressing whatever issue without help, alone like a hero. I have learned that time is the most valuable resource and getting help is good for everybody. Thank you Scholars’ Lab for being a safe place where there is no such thing as ‘a stupid question’! You are always there to help us with questions about php, programming and the implementation of our ideas about Ivanhoe. And thank you dear teammates! I would agree with Swati’s recent blogpost : it is awesome to be part of a team that shares responsibility generously for failures and successes. Second, I have learned how challenging it is to create a common vision for the project, given team members with diverse backgrounds and viewpoints . Our discussions about the Praxis 2014-15 charter and the conversations about the conceptualization of Ivanhoe as a gateway to collaborative textual play helped us in this direction. Nevertheless, we still have unresolved conceptual issues regarding what could make Ivanhoe ‘special’ . Having no clear resolution on several important conceptual issues, made being part of a team not always awesome, at least for me. Most importantly, I have realized how challenging it is to make project management a shared responsibility and develop shared leadership within a team. Last semester I supported the vision that management and organization was a common responsibility of the whole team and it would have to emerge from our interactions as team members and human beings. However, when it came to practice, I felt frustrated waiting for such coordination to emerge. It took time to establish good accountability measures for ourselves and each others like delineating simple tasks, specifying deadlines, sending checking-in emails, etc. Grasping the whole purpose of Github and how to use it to facilitate team communication also took time. As a result, last semester we were not able to turn words into deeds and reflect philosophies into practices with a new Ivanhoe info page. This taught me to be more patient and generous with others and myself as work is not always fast and not always without tension in a team of experts. Working within the 2014-2015 Praxis team last semester, made me understand the importance of learning more about project management and organization in multicultural teams. It is hard, but I think it is one of those things that can make being part of a team and working on a collaborative project pretty awesome. Managing a multicultural team and coordinating members’ actions towards the completion of a collaborative project involves special skills that also need to be taught. We think being a researcher by definition means that we are skilled in data management. However, we forget that our research generally as grad students and scholars is solitary work - the academy tends to evaluate individuals not teams. Furthermore, we may be good at managing data but maybe not so good at managing people. I started reading about shared leadership in teams - where “the source of leadership influence is distributed among team members rather than concentrated or focused in a single individual” (Carson, Tesluk and Marrone 2007, 1220). Research shows that the development of shared leadership depends on two interrelated factors: “internal team environment, including a shared purpose, social support, and voice, and level of external coaching support.” (Carson et al. 2007, 1218). As I make sense of my team experience to this point, I think that the conditions for developing shared leadership were there. Yes, maybe we needed to develop voice more inside our team, and maybe a crash course in project management at the beginning of Praxis team work could have been useful. However, overall I believe we were able to develop high level of shared leadership. Our roles and responsibilities have changed over time, and it is in teams with high level of shared leadership that you tend to see such shifts and/or rotations in leadership, “in such a way that different members provide leadership at different points in the team’s life cycle and development” (Carson et al. 2007, 1220). As Andrew mentioned in his latest blogpost, our team has truly experienced the iterative nature of any collaborative digital humanities project. It is a challenge to know that it takes time to become a team, and still act as a team within a limited timeframe. To conclude, it is not always awesome to be part of a team, but it is definitely rewarding. As sociologists Martin Ruef (2010) shows with his book The Entrepreneurial Group, entrepreneurship and innovation are much more successful when they involve collective effort and collaboration in the form of teams. Teamwork is hard but worth committing too. Let’s fight the ideology of individual heroes, geniuses, and lone researchers, but also let’s prepare for the challenges of being part of team. Knowing and learning about how to face those challenges will help keep collaborative projects alive and make working with diverse teams truly awesome. References: Carson, Jay B., Paul E. Tesluk, and Jennifer A. Marrone. 2007. “Shared Leadership in Teams: An Investigation of Antecedent Conditions and Performance.” Academy of management Journal 50(5):1217–34. Ruef, Martin. 2010. The Entrepreneurial Group: Social Identities, Relations, and Collective Action . Princeton University Press."},{"id":"2015-04-10-linking-out","title":"Linking Out","author":"jeremy-boggs","date":"2015-04-10 10:17:18 -0400","categories":["Experimental Humanities"],"url":"linking-out","layout":"post","content":"I’m in the Accessible Future workshop in Atlanta, and we’d had a interesting conversation about how, and how often, web content should “link out” to pages beyond ones own web site. I feel like we’ve talked about this topic every workshop, and there are lots of interesting issues that keep coming up that I never feel are really resolved or explored in depth. I’ve not fully resolved them myself, and since I haven’t written a post here in ages I figured, in the spirit of being flummoxed by a coconut I’d write something up here to see if I can’t start working through this. Approaches to external links usually comes up when we get to talking about whether links to external pages should be opened in a new tab or new window. There seems to be some fear—and I’ve had this conversation at work during consultations and workshops all the time—that if you don’t open external links in a new tab, people will never come back to your web page. And I’ve always wondered what this fear is founded in; it seems to me that if your content is compelling and useful, people will come back to it. If we’re worried about people not returning to our pages, it makes me think the bigger problem is that the content on our pages isn’t as useful as is could be. I’m sure this isn’t universally true, and certainly don’t mean to say that anyone who has this concern is writing crap content or over-worrying. But I just wonder if its speaking to a different issue with the confidence one has in the content they’re sharing. It also seems to say something about what expectations we have of people and their competency with browsers. There are lots of features in browsers people could use, if they knew those features existing and they knew how to use them. You can save your browsing history and review it; I do this  all the time.  You can actually choose to open links in new tabs; I also do this  all the time . So I think that, when we start making choices for the reader, we’re forcing a specific user experience they may not want, and we’re also making it less possible for people to become more savvy users of their browser. My own stances on external links, as a developer and an author (ha!) are: I do not open external links in new tabs or windows.  In fact, I have a browser plugin that automatically disables that. I personally think this is forcing a specific choice on my reader instead of giving them that choice. I open things in new tabs all the time, but I deliberately do it, using my right-click or keyboard shortcut to open a new tab. I know how to do this . This is an option in, as far as I know, every modern browser, though I’m sure its not a feature more. So the question here for me is, should we be thinking more about how to show people to take more control of their browser settings and use, or do we impose specific? Right now, I’m in the camp that favors reader choice, though I worry that’s not the right way of looking at it. I link out to external pages rather liberally.  I understand arguments that too many links can be harmful to understanding. If you have whole paragraphs filled with linked text are really hard to read, especially depending on your link styles. (I’ve occasionally wondered whether it’d be worthwhile having some sort of setting to turn off link styles just to facilitate reading. Not sure what this would look like, though.) But I also like to link to specific things I think are useful to read, instead of just assuming that if a reader wants more information they can search for particular terms. They can still do that, too. I want to have a more nuanced approach to this. I’m not confident my stances are correct, or that I’ve considered enough. If you have any thoughts about this, please share!"},{"id":"2015-04-20-validating-data-with-types","title":"Validating Data with Types","author":"eric-rochester","date":"2015-04-20 09:47:06 -0400","categories":null,"url":"validating-data-with-types","layout":"post","content":"Recently, I had to opportunity to help J. Warren York, a graduate student in the Department of Politics here at UVa. He’s looking at how tax law affects political contributions and advocacy, so this was an interesting project that may tell us something useful about how the US government works [insert your favorite broken-government joke here]. To do this, he needed to download data from a number of different sources in different formats (JSON, YAML, and CSV), pull it all apart, and put some of it back together in a couple of new data files. One of those sources is the Database on Ideology, Money in Politics, and Elections (DIME) . The data from them tells how much people and organizations have contributed to various candidates, PAC, and other groups. And while I’ve seen worse, it wasn’t the cleanest data file out there. (To get an idea of what the data looks like, you can see a sample of 100 rows from this data file in this Google Sheet .) For most projects that I’m reasonably sure that I’ll be the only developer on, I use Haskell . This is a functional, statically typed programming language with a (partially deserved) reputation for being difficult. However, I find that it gives me a nice balance of safety and flexibility, of power and expressiveness. Given Haskell’s reputation, the previous sentence probably seems to border on insanity. Hopefully this post will prove this at least partially correct and will highlight some of the nicer aspects of working in Haskell. It leverages types to provide some assurances that the data is well-formed and consistent. This means I can perform data validation quickly and easily, and that helps everyone. This post is actually runnable Haskell. If you have the GHC compiler installed you can copy and paste this post into a file, say Validating.lhs, and run it from the command line: &lt;code class=\"sourceCode bash\"&gt;$ &lt;span class=\"kw\"&gt;runhaskell&lt;/span&gt; Validating.lhs contribDB_1982.csv&lt;/code&gt; However, to follow this post, you don’t need to know Haskell. I’ll try to explain enough of the concepts and syntax that matter as they come up, so that anyone familiar with computer programming should be able to follow along without getting into the weeds of exactly what’s happening on each line. So first some pre-amble and boilerplate. This just makes available the libraries that we’ll use. &lt;code class=\"sourceCode haskell\"&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"ot\"&gt;{-# LANGUAGE OverloadedStrings #-}&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- If you want more details about the code, including brief&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- explanations of the syntax, you've come to the right place.&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- Pay attention to the comments. This still isn't a tutorial&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- on Haskell, but hopefully you'll have a more detailed&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- understanding of what's happening.&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- First, Haskell code is kept in modules. Executable files are&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- in the `Main` module.&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"kw\"&gt;module&lt;/span&gt; &lt;span class=\"dt\"&gt;Main&lt;/span&gt; &lt;span class=\"kw\"&gt;where&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- Import statements make the code from these modules available&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- in this module. Qualified imports make the code available&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- under an alias (e.g., Data.ByteString.Lazy is aliased to B).&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"kw\"&gt;import qualified&lt;/span&gt; &lt;span class=\"dt\"&gt;Data.ByteString.Lazy&lt;/span&gt; &lt;span class=\"kw\"&gt;as&lt;/span&gt; &lt;span class=\"dt\"&gt;B&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"kw\"&gt;import           &lt;/span&gt;&lt;span class=\"dt\"&gt;Data.Csv&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"kw\"&gt;import qualified&lt;/span&gt; &lt;span class=\"dt\"&gt;Data.Text&lt;/span&gt;            &lt;span class=\"kw\"&gt;as&lt;/span&gt; &lt;span class=\"dt\"&gt;T&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"kw\"&gt;import qualified&lt;/span&gt; &lt;span class=\"dt\"&gt;Data.Vector&lt;/span&gt;          &lt;span class=\"kw\"&gt;as&lt;/span&gt; &lt;span class=\"dt\"&gt;V&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"kw\"&gt;import           &lt;/span&gt;&lt;span class=\"dt\"&gt;System.Environment&lt;/span&gt;&lt;/code&gt; To validate the data, we just need to follow the same steps that we would to load it. Those steps are: Define the data that you want to use; Define how to read it from a row of CSV data; and Read the input. Profit! That’s it. In fact, the last item is so inconsequential that we’ll skip it. But let’s see how the rest of it works. Defining the Data First we need to define the data. We do this using types . If you only know languages like Ruby, JavaScript, or Python, you may be unfamiliar with types. Basically, they specify what your data will look like. For example, they might specify that a Person data instance has a name string field and an age integer field. If you come from Java or C#, you know what types are, but Haskell uses them very differently. In Haskell, types are used to express, encode, and enforce the requirements of your program as much as possible. The guideline is that invalid program states should not be expressible in the types you define. To help with that, some of the loopholes in Java’s type system have been closed (looking at you, null ): this makes these specifications more meaningful. And because Haskell employs type inference, you also don’t need to actually declare the type of every little thing, so you get more benefit for less work. In short, types are how we specify what data we’re interested in. At this point in the process, programming in Haskell is a typical data modeling exercise. But it’s also the foundation for the rest of this post, so we’ll linger here. Before we define the data types, we’ll first define some aliases. These aren’t really enforced, but they make the data types that use these more clear. &lt;code class=\"sourceCode haskell\"&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"kw\"&gt;type&lt;/span&gt; &lt;span class=\"dt\"&gt;OrgName&lt;/span&gt; &lt;span class=\"fu\"&gt;=&lt;/span&gt; &lt;span class=\"dt\"&gt;T.Text&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"kw\"&gt;type&lt;/span&gt; &lt;span class=\"dt\"&gt;Year&lt;/span&gt;    &lt;span class=\"fu\"&gt;=&lt;/span&gt; &lt;span class=\"dt\"&gt;Int&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"kw\"&gt;type&lt;/span&gt; &lt;span class=\"dt\"&gt;Amount&lt;/span&gt;  &lt;span class=\"fu\"&gt;=&lt;/span&gt; &lt;span class=\"dt\"&gt;Double&lt;/span&gt;&lt;/code&gt; The first data type that we’ll create is Party . This will be similar to enumerations in other languages, but in Haskell they’re just regular data types. A Party can be either a Dem (Democrat), GOP (Republican), Independent, or Unknown . &lt;code class=\"sourceCode haskell\"&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- This statement says that you can make a value of type Party &lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- using any of the constructors listed (separated by pipes).&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- In this case, none of the constructors take extra data, so&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- the semantics comes soley from which constructor is chosen.&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"kw\"&gt;data&lt;/span&gt; &lt;span class=\"dt\"&gt;Party&lt;/span&gt; &lt;span class=\"fu\"&gt;=&lt;/span&gt; &lt;span class=\"dt\"&gt;Dem&lt;/span&gt; &lt;span class=\"fu\"&gt;|&lt;/span&gt; &lt;span class=\"dt\"&gt;GOP&lt;/span&gt; &lt;span class=\"fu\"&gt;|&lt;/span&gt; &lt;span class=\"dt\"&gt;Independent&lt;/span&gt; &lt;span class=\"fu\"&gt;|&lt;/span&gt; &lt;span class=\"dt\"&gt;Unknown&lt;/span&gt;&lt;/code&gt; We want to know what kind of entity is receiving the contribution. However, we don’t actually care about who the recipient was: we just want to distinguish between candidates, committees, and state-level elections. We’ll use the ContribEntry data type for this information. The following declaration states that a ContribEntry can be either a Candidate, which must have year information and party information; a Committee, which must have only a year; or a StateLevel, which must have a year and a state code. &lt;code class=\"sourceCode haskell\"&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- This shows how values are given types. `contribYear ::&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- !Year`, says that the `contribYear` field must contain&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- values of type `Year`. The exclamation mark tells the&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- Haskell compiler to execute this value immediately. Unlike&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- most other languages, Haskell will normally wait to&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- evaluate expressions until absolutely necessary.&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"kw\"&gt;data&lt;/span&gt; &lt;span class=\"dt\"&gt;ContribEntry&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;         &lt;span class=\"fu\"&gt;=&lt;/span&gt; &lt;span class=\"dt\"&gt;Candidate&lt;/span&gt;  {&lt;span class=\"ot\"&gt; contribYear ::&lt;/span&gt; &lt;span class=\"fu\"&gt;!&lt;/span&gt;&lt;span class=\"dt\"&gt;Year&lt;/span&gt;,&lt;span class=\"ot\"&gt; contribParty ::&lt;/span&gt; &lt;span class=\"fu\"&gt;!&lt;/span&gt;&lt;span class=\"dt\"&gt;Party&lt;/span&gt; }\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;         &lt;span class=\"fu\"&gt;|&lt;/span&gt; &lt;span class=\"dt\"&gt;Committee&lt;/span&gt;  {&lt;span class=\"ot\"&gt; contribYear ::&lt;/span&gt; &lt;span class=\"fu\"&gt;!&lt;/span&gt;&lt;span class=\"dt\"&gt;Year&lt;/span&gt; }\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;         &lt;span class=\"fu\"&gt;|&lt;/span&gt; &lt;span class=\"dt\"&gt;StateLevel&lt;/span&gt; {&lt;span class=\"ot\"&gt; contribYear ::&lt;/span&gt; &lt;span class=\"fu\"&gt;!&lt;/span&gt;&lt;span class=\"dt\"&gt;Year&lt;/span&gt;,&lt;span class=\"ot\"&gt; stateCode ::&lt;/span&gt; &lt;span class=\"fu\"&gt;!&lt;/span&gt;&lt;span class=\"dt\"&gt;T.Text&lt;/span&gt; }&lt;/code&gt; Each row of the data file will have information about a single contribution made by an individual or organization. Because we’re primarily interested in the data from organizations, this will be collected in an OrgContrib data type. It will hold the organization’s name ( orgContribName ), its district ( orgDistrict10s ), the contribution information ( orgContribEntry ), and the amount of the contribution ( orgContribAmount ). &lt;code class=\"sourceCode haskell\"&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"kw\"&gt;data&lt;/span&gt; &lt;span class=\"dt\"&gt;OrgContrib&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;          &lt;span class=\"fu\"&gt;=&lt;/span&gt; &lt;span class=\"dt\"&gt;OrgContrib&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;          {&lt;span class=\"ot\"&gt; orgContribName   ::&lt;/span&gt; &lt;span class=\"fu\"&gt;!&lt;/span&gt;&lt;span class=\"dt\"&gt;OrgName&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;         ,&lt;span class=\"ot\"&gt; orgDistrict10s   ::&lt;/span&gt; &lt;span class=\"fu\"&gt;!&lt;/span&gt;&lt;span class=\"dt\"&gt;T.Text&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;         ,&lt;span class=\"ot\"&gt; orgContribEntry  ::&lt;/span&gt; &lt;span class=\"fu\"&gt;!&lt;/span&gt;&lt;span class=\"dt\"&gt;ContribEntry&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;         ,&lt;span class=\"ot\"&gt; orgContribAmount ::&lt;/span&gt; &lt;span class=\"fu\"&gt;!&lt;/span&gt;&lt;span class=\"dt\"&gt;Amount&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;          }&lt;/code&gt; That’s it. We’ve now defined the data we’re interested in. On top of the guarantees that types allow the programming language to enforce, this exercise is also helpful because it clarifies what we want from the data and helps us better understand the domain that we’re working in. Data from CSV However, we haven’t connected this data with the CSV file yet. Let’s do that now. To make this happen, we’ll need to take the data types that we just defined and define instances of FromField for ones that are populated from a single field, like Party, and FromNamedRecord for others, which are built from an entire row. FromField and FromNamedRecord are type classes . In object-oriented terms, these are similar to small interfaces, some only declaring one or two methods. Data types can implement the type classes that make sense, but omit the ones that do not. In this case these type classes define what data types can be read from a row of CSV and how that should happen. Party is the first data type we’ll tackle. It only reads a single field, so we’ll define FromField . In the CSV file, the data is encoded with numeric codes, which we’ll change into Party values. &lt;code class=\"sourceCode haskell\"&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- This defines a instance of `FromField` for `Party`.&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- `parseField` is the only method. Multiple listings for this&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- function, combined with the string literals in place of the&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- parameter, means that the method acts as a big case&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- statement on its one parameter. When the function is passed&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- the string \"100\", the first definition will be used. The&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- last clause, with the underscore, is a catch-all, in which&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- the parameter's value will be ignored.&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"kw\"&gt;instance&lt;/span&gt; &lt;span class=\"dt\"&gt;FromField&lt;/span&gt; &lt;span class=\"dt\"&gt;Party&lt;/span&gt; &lt;span class=\"kw\"&gt;where&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;     parseField &lt;span class=\"st\"&gt;\"100\"&lt;/span&gt; &lt;span class=\"fu\"&gt;=&lt;/span&gt; return &lt;span class=\"dt\"&gt;Dem&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;     parseField &lt;span class=\"st\"&gt;\"200\"&lt;/span&gt; &lt;span class=\"fu\"&gt;=&lt;/span&gt; return &lt;span class=\"dt\"&gt;GOP&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;     parseField &lt;span class=\"st\"&gt;\"328\"&lt;/span&gt; &lt;span class=\"fu\"&gt;=&lt;/span&gt; return &lt;span class=\"dt\"&gt;Independent&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;     &lt;span class=\"co\"&gt;-- This catch-all is probably a bad idea....&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;     parseField _     &lt;span class=\"fu\"&gt;=&lt;/span&gt; return &lt;span class=\"dt\"&gt;Unknown&lt;/span&gt;&lt;/code&gt; Notice my comment on the next to last line. Having a catch-all field like this introduces some code smell, and it weakens the type-safety of the field. A better practice would be to define a Party constructor for every numeric code and throw an error when we find something unexpected. Since we’re only interested here in two parties, that would be overkill, so in this case we’ll be more flexible. Now we can define how to read ContribEntry data. This is complicated because we have to look at the value of the recipient_type field in order to figure out which constructor to use. We’ll also define a utility function, defaulting, that defaults empty strings to a given value. &lt;code class=\"sourceCode haskell\"&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- This defines the function defaulting. The first line is the&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- type value. The definition of `defaulting` is a more&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- complicated case statement that first tests `T.null v`&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- (i.e., that it's empty), and `otherwise` is the \"else\" part&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"co\"&gt;-- of the statement.&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;&lt;span class=\"ot\"&gt; defaulting ::&lt;/span&gt; &lt;span class=\"dt\"&gt;T.Text&lt;/span&gt; &lt;span class=\"ot\"&gt;-&gt;&lt;/span&gt; &lt;span class=\"dt\"&gt;T.Text&lt;/span&gt; &lt;span class=\"ot\"&gt;-&gt;&lt;/span&gt; &lt;span class=\"dt\"&gt;T.Text&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; defaulting d v &lt;span class=\"fu\"&gt;|&lt;/span&gt; T.null v  &lt;span class=\"fu\"&gt;=&lt;/span&gt; d\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;                &lt;span class=\"fu\"&gt;|&lt;/span&gt; otherwise &lt;span class=\"fu\"&gt;=&lt;/span&gt; v\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"kw\"&gt;instance&lt;/span&gt; &lt;span class=\"dt\"&gt;FromNamedRecord&lt;/span&gt; &lt;span class=\"dt\"&gt;ContribEntry&lt;/span&gt; &lt;span class=\"kw\"&gt;where&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;     parseNamedRecord m &lt;span class=\"fu\"&gt;=&lt;/span&gt; &lt;span class=\"kw\"&gt;do&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;         &lt;span class=\"co\"&gt;-- Read the recipient_type field. The `.:` operator&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;         &lt;span class=\"co\"&gt;-- reads a specific field from the CSV row.&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;         rtype &lt;span class=\"ot\"&gt;&lt;-&lt;/span&gt; m &lt;span class=\"fu\"&gt;.:&lt;/span&gt; &lt;span class=\"st\"&gt;\"recipient_type\"&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;         &lt;span class=\"co\"&gt;-- If recipient_type is empty, give it a default value&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;         &lt;span class=\"co\"&gt;-- of \"CAND\", and then branch on that.&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;         &lt;span class=\"kw\"&gt;case&lt;/span&gt; defaulting &lt;span class=\"st\"&gt;\"CAND\"&lt;/span&gt; rtype &lt;span class=\"kw\"&gt;of&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;             &lt;span class=\"st\"&gt;\"CAND\"&lt;/span&gt; &lt;span class=\"ot\"&gt;-&gt;&lt;/span&gt; &lt;span class=\"kw\"&gt;do&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;                 &lt;span class=\"co\"&gt;-- Read the cycle (year) and recipient_party fields&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;                 cycle &lt;span class=\"ot\"&gt;&lt;-&lt;/span&gt; m &lt;span class=\"fu\"&gt;.:&lt;/span&gt; &lt;span class=\"st\"&gt;\"cycle\"&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;                 party &lt;span class=\"ot\"&gt;&lt;-&lt;/span&gt; m &lt;span class=\"fu\"&gt;.:&lt;/span&gt; &lt;span class=\"st\"&gt;\"recipient_party\"&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;                 &lt;span class=\"co\"&gt;-- Create a Candidate&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;                 return (&lt;span class=\"dt\"&gt;Candidate&lt;/span&gt; cycle party)\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;             &lt;span class=\"st\"&gt;\"COMM\"&lt;/span&gt; &lt;span class=\"ot\"&gt;-&gt;&lt;/span&gt; &lt;span class=\"kw\"&gt;do&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;                 &lt;span class=\"co\"&gt;-- Read the cycle and return a Committe&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;                 cycle &lt;span class=\"ot\"&gt;&lt;-&lt;/span&gt; m &lt;span class=\"fu\"&gt;.:&lt;/span&gt; &lt;span class=\"st\"&gt;\"cycle\"&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;                 return (&lt;span class=\"dt\"&gt;Committee&lt;/span&gt; cycle)\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;             r &lt;span class=\"ot\"&gt;-&gt;&lt;/span&gt; &lt;span class=\"kw\"&gt;do&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;                 &lt;span class=\"co\"&gt;-- Everything else is a state-level contribution.&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;                 &lt;span class=\"co\"&gt;-- Get the cycle and return that.&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;                 cycle &lt;span class=\"ot\"&gt;&lt;-&lt;/span&gt; m &lt;span class=\"fu\"&gt;.:&lt;/span&gt; &lt;span class=\"st\"&gt;\"cycle\"&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;                 return (&lt;span class=\"dt\"&gt;StateLevel&lt;/span&gt; cycle r)&lt;/code&gt; (You might be wondering why I haven’t needed to define a FromField for Year for the “cycle” fields. Remember that Year is just an alias for Int, and the CSV library already defines FromField for the Int type.) We can finally define the instance for OrgContrib . After the complexity of ContribEntry, this one will be much simpler. We’ll extract the values for a few fields, parse the ContribEntry, and then create and return the OrgContrib value. &lt;code class=\"sourceCode haskell\"&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; &lt;span class=\"kw\"&gt;instance&lt;/span&gt; &lt;span class=\"dt\"&gt;FromNamedRecord&lt;/span&gt; &lt;span class=\"dt\"&gt;OrgContrib&lt;/span&gt; &lt;span class=\"kw\"&gt;where&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;     parseNamedRecord m &lt;span class=\"fu\"&gt;=&lt;/span&gt; &lt;span class=\"kw\"&gt;do&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;         name     &lt;span class=\"ot\"&gt;&lt;-&lt;/span&gt; m &lt;span class=\"fu\"&gt;.:&lt;/span&gt; &lt;span class=\"st\"&gt;\"contributor_name\"&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;         district &lt;span class=\"ot\"&gt;&lt;-&lt;/span&gt; m &lt;span class=\"fu\"&gt;.:&lt;/span&gt; &lt;span class=\"st\"&gt;\"contributor_district_10s\"&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;         contrib  &lt;span class=\"ot\"&gt;&lt;-&lt;/span&gt; parseNamedRecord m\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;         amount   &lt;span class=\"ot\"&gt;&lt;-&lt;/span&gt; m &lt;span class=\"fu\"&gt;.:&lt;/span&gt; &lt;span class=\"st\"&gt;\"amount\"&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;         return (&lt;span class=\"dt\"&gt;OrgContrib&lt;/span&gt; name district contrib amount)&lt;/code&gt; With these in place, we can read the data and have it verified at the same time. For example, if the file reads correctly, I know that the Year data are integers and that Party fields contain valid data. And that’s really all there is to it. Below the end of the article, I’ve included a function to read the CSV data from a file and the main function, which controls the whole process. However, reading and validating the data has already been taken care of. Of course, while these types provide reasonable validation, you could get much better, depending on how you define your types and how you parse the incoming data. (For example, you could only allow valid state codes for StateLevel or limit years to a given range.) If you’re wondering about tests, the implementations of FromField and FromNamedRecord would be good to have tests for. However, the parts of the program’s requirements that are enforced in the types don’t really need testing; for example, I wouldn’t test that party fields will always be parsed as a Party . Types also come in handy in other circumstances: when you’ve left the code for a while and need to get back into it, they provide a minimum amount of guidance; and when you need to refactor, they act as a base-line set of regression tests, to tell you when you’ve broken something. Overall, I find that this small program shows how Haskell can provide a lot of power and expressivity for relatively little code. But the immediate benefit in this case is that I was able to provide John more assurances about his data, and to provide them more quickly. It’s a nice example of leveraging types to write better programs that provide real-world benefits. The full code for this project is in my popvox-scrape repository. Feel free to check it out. &lt;code class=\"sourceCode haskell\"&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;&lt;span class=\"ot\"&gt; readData ::&lt;/span&gt; FilePath &lt;span class=\"ot\"&gt;-&gt;&lt;/span&gt; &lt;span class=\"dt\"&gt;IO&lt;/span&gt; (&lt;span class=\"dt\"&gt;Either&lt;/span&gt; &lt;span class=\"dt\"&gt;String&lt;/span&gt; (&lt;span class=\"dt\"&gt;Header&lt;/span&gt;, &lt;span class=\"dt\"&gt;V.Vector&lt;/span&gt; &lt;span class=\"dt\"&gt;OrgContrib&lt;/span&gt;))\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; readData filename &lt;span class=\"fu\"&gt;=&lt;/span&gt; &lt;span class=\"kw\"&gt;do&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;     rawData &lt;span class=\"ot\"&gt;&lt;-&lt;/span&gt; B.readFile filename\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;     return (decodeByName rawData)\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;&lt;span class=\"ot\"&gt; main ::&lt;/span&gt; &lt;span class=\"dt\"&gt;IO&lt;/span&gt; ()\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt; main &lt;span class=\"fu\"&gt;=&lt;/span&gt; &lt;span class=\"kw\"&gt;do&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;     args &lt;span class=\"ot\"&gt;&lt;-&lt;/span&gt; getArgs\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;     &lt;span class=\"kw\"&gt;case&lt;/span&gt; args &lt;span class=\"kw\"&gt;of&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;         [filename] &lt;span class=\"ot\"&gt;-&gt;&lt;/span&gt; &lt;span class=\"kw\"&gt;do&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;             dataRows &lt;span class=\"ot\"&gt;&lt;-&lt;/span&gt; readData filename\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;             &lt;span class=\"kw\"&gt;case&lt;/span&gt; dataRows &lt;span class=\"kw\"&gt;of&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;                 &lt;span class=\"dt\"&gt;Left&lt;/span&gt; err &lt;span class=\"ot\"&gt;-&gt;&lt;/span&gt; putStrLn (&lt;span class=\"st\"&gt;\"ERROR: \"&lt;/span&gt; &lt;span class=\"fu\"&gt;++&lt;/span&gt; err)\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;                 &lt;span class=\"dt\"&gt;Right&lt;/span&gt; (_, rows) &lt;span class=\"ot\"&gt;-&gt;&lt;/span&gt; putStrLn (  &lt;span class=\"st\"&gt;\"SUCCESS: \"&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;                                             &lt;span class=\"fu\"&gt;++&lt;/span&gt; show (V.length rows)\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;                                             &lt;span class=\"fu\"&gt;++&lt;/span&gt; &lt;span class=\"st\"&gt;\" read.\"&lt;/span&gt;)\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;\n&lt;span class=\"fu\"&gt;&gt;&lt;/span&gt;         _ &lt;span class=\"ot\"&gt;-&gt;&lt;/span&gt; putStrLn &lt;span class=\"st\"&gt;\"usage: runhaskell Validate.lhs data-file.csv\"&lt;/span&gt;&lt;/code&gt;"},{"id":"2015-04-23-podcast-ben-wright-and-joesph-locke-on-creating-american-yawp","title":"Podcast: Ben Wright and Joesph Locke on Creating American Yawp","author":"laura-miller","date":"2015-04-23 06:53:38 -0400","categories":["Events","Podcasts"],"url":"podcast-ben-wright-and-joesph-locke-on-creating-american-yawp","layout":"post","content":"Democratizing the Digital Humanities: The American Yawp as Case Study After a year-long collaboration, over 350 historians have produced a beta edition of The American Yawp, a free and online, collaboratively built, open American history textbook designed for college-level history courses. This talk will explore the creation and dissemination of this project, the landscape of open educational projects in the humanities, the methods used to harness the energy of hundreds of academics, and the potential for large-scale collaboration and open resources to make practical the democratic promise of the digital humanities. Ben Wright is an assistant professor of history at Abraham Baldwin Agricultural College. His manuscript, Antislavery and American Salvation, is under advance contract with LSU Press. His digital projects include The American Yawp and abolitionseminar.org, a NEH-sponsored educational tool on the antislavery movement, designed for K-12 educators and their students. He also serves as managing editor of Teaching United States History, a critical forum discussing pedagogy in college-level American history courses. Joseph Locke is an assistant professor history at the University of Houston-Victoria, where he teaches courses in American history and researches the historical interplay between religion and the American South. His first book, Making the Bible Belt: Prohibition and the Politicization of Southern Religion, is forthcoming from Oxford University Press. This talk was recorded in Alderman Library on March 19, 2015.  Click below to stream the podcast, and follow along with the  presentation slides .  If you’d like to hear more from the Scholars’ Lab, subscribe to our podcast series on iTunesU . [podloveaudio src=”http://a1874.phobos.apple.com/us/r30/CobaltPublic7/v4/33/f1/f7/33f1f772-e042-97be-e987-b297c01feae8/304-8494564249797709976-wright_locke_final.mp3”]"},{"id":"2015-04-23-task-management-bullet-journal","title":"Task Management & Bullet Journal","author":"ronda-grizzle","date":"2015-04-23 09:15:06 -0400","categories":null,"url":"task-management-bullet-journal","layout":"post","content":"Confession: time management has always been a challenge for me. I’m easily distractable, and I’m exceptionally bad at attempting to do more than one thing at a time. When multiple “fires” get lit I lose most of my productivity on task switching and to overwhelm. I’ve tried several time management apps for computer and phone. Yet, I always end up spending too much time fiddling with the app to try to make it work well, or forget to look at my handy computerized to list after I’ve typed it. For a while, I thought EverNote was going to be a winner. But I end up, every time, back with good old pen and paper to-do lists, and they work better than any app I’ve tried. I also remember the items on my task list better when I write them out by hand than if I type them. The act of writing helps with the transfer of information to long term mental storage. I can’t remember where I first learned about Bullet Journal, but I suspect it was on Lifehacker or BoingBoing. I knew as soon as I watched the short video introduction by creator Ryder Carroll that this might be a system I could actually use. Bullet Journal works because it’s more than a task list. It provides a space to bring together tasks, meeting notes, reading notes, inspiration for future work and research, lists of various kinds, and questions that need answers. It’s also a hackable system, as the thousands of youtube videos about how people use and adapt it attest. Here’s the basic system, as outlined in the introductory video : Acquire a quad-ruled notebook, not too small and not too large. You want a notebook that’s compact enough to be carried with you, but not so small that you don’t have room to write. I use the 5” x 8” Moleskine, or a Moleskine knockoff when I can find one. Number the pages. Without numbered pages, you won’t be able to find anything later. On the first facing page, start an Index page. Being able to find all those notes you took and pages where you noodled on about a project idea you had is what makes the journal usable. On the left side of the first two-page spread create a simple monthly calendar by listing the days of the month down the page, filling in the days of the week beside them. On the facing page, create a list of the tasks you know need to be completed in that month. Once this is done, note the starting page number for that month on your index page. On the next two page spread, begin creating your daily calendar by writing the month and date, then listing under it the tasks and appointments for that day. The symbol set is what makes these daily calendar pages easy to scan and mentally parse: squares for tasks, circles for meetings/appointments, an exclamation point for inspiration/short notes, an eye for notes about topics you want to explore more deeply. These basic shapes can be annotated with stars or asterisks to indicate priority items. I added a symbol to mark personal items, so that I can combine my personal and work to do lists. With the symbol, I am able to easily find the items I need to based on where I am. Filling in half of the square on the diagonal marks partially done tasks. Any item that becomes irrelevant is struck through with a line. At the end of the month, per the instructions from bulletjournal.com, mark through any tasks that have become irrelevant, place an arrow in the boxes for tasks that are moving forward to the next month, and update your index page. Easy peasy. I’ve been using a bullet journal for about a year, and I really love it. I’m happy to report that Eric and Scott have adopted it, as well, and seem to like it. A few things  aren’t working for me, though, not because of bullet journal, but because of how I’m using it. The journal is great for keeping daily tasks on the radar, but I’m finding that I’m not tracking longer term tasks as well as I’d like. The long term tasks associated with larger goals just never seem get moved from that beginning of the month task list onto the daily pages. I’ve also noticed that tasks not completed on any given page sometimes get dropped when that page is no longer in view. The remedies for these issues are pretty simple. First, I need to apply agile methodology to breaking down my large projects (work and personal) into the smallest incremental tasks possible, just like I would if it were a large project to which multiple people were contributing. I’ve decided that I need a personal kanban board so that I have a clear picture of all the tasks associated with a project and the priority I’ve assigned to them, which task is in progress, which tasks are completed, and I’ve got the wall space in my cube picked out for it, since electronic, app-based kanban boards have proven ineffective for me, too. Second, instead of waiting to transfer tasks at the end of the month, I’ve started transferring them when I turn a page. Tasks that are on my current two page spread, no matter what day of the month it happens to be, include any tasks that were left undone on the previous two page spread. The first time a task gets shifted, it gets marked with a star to make it a priority item. Tasks that are shifted a second time are examined for why they’ve ended up in “the procrastination pile,” which is a whole other blog post! The point is, if a task is moved twice, there’s a reason why it’s not getting done. I have to get to the bottom of it and either complete the task or delegate it or consciously decide that it’s not something that needs to be done. The last benefit of bullet journaling is it forces me to pause at the end of every single day to set up the next day’s tasks and appointments. My mornings used to always start (OK, they sometimes still do!) in a rush of organizing my schedule for the day. It was stressful, ineffective, and created the feeling that I was always running to catch up with myself. By pausing at the end of each day to set the next day, I’m mentally prepared and ready to focus when I sit down at my desk, which is a much calmer and healthier way to approach my work. Do you use bullet journal? I’d love to know what you think and how you’ve hacked it."},{"id":"2015-05-04-ninjaflex-on-the-makerbot","title":"NinjaFlex on the Makerbot","author":"shane-lin","date":"2015-05-04 11:53:55 -0400","categories":["Experimental Humanities"],"url":"ninjaflex-on-the-makerbot","layout":"post","content":"Announcing version 0.1 public beta of the Scholars Lab Makerspace Ninjaflex profile for Makerbot Replicators ! We’ve had two spools of Ninjaflex flexible filament for about as long as we’ve had our Makerbot Replicator 2. We’ve tried to print with it from time to time, but seldom with very good result. With our Rep2 dialed in and printing PLA like a champ for the last few months, I decided that it was a pretty good time to finally figure out Ninjaflex. A few issues were immediately obvious when printing with the default Makerbot flexible filament profile. The extruded lines were too thin and there was significant “ooze”, causing a lot of thin, dangling strings when the nozzle shifted without printing. A quick search turned up a Ninjaflex profile for Makerbots on Thingiverse, with some helpful hints on parameters to change. Our profile is a slight modification of that, with a major bug fix and a handful of small adjustments. In my experience, the two really important variables are ”retractDistance” and “feedrate” (inconsistent camelcase as-is). RetractDistance controls the amount that the filament is retracted for moves; slightly increasing this to 1.3 (mm) dramatically reduces ooze. reedRate is the speed of extruder when extruding. Going slow is critical to success with Ninjaflex and so I’ve had good results with a consistent 20mm/s speed for all components. Additional issues unrelated to software has been feeding and clogging. When using our Replicator 1 Dual Extruder, I found that the friction of the Ninjaflex against the feed tube was too high for the Rep1 extruder’s motor to overcome, even when manually unspooled; I think that others have had some luck with printing a tubeless guide.  We just switched to the Replicator 2. But even there, with a full and heavy spool of Ninjaflex, I still relied on manual unspooling. Even so, our printer would still sometimes become clogged. Based on online commentary, this seems fairly common and, for us at least, is resolved by unloading, snipping off the nodule of material pooled at the end of the filament, and reloading. This seems to happen more frequently after the Ninjaflex has been heated and then left to cool without being extruded and when transitioning between Ninjaflex and PLA. Others have created custom-printed extruder parts to alleviate these issues, but we haven’t yet tried them out. Setting up a custom profile in the Makerbot software isn’t actually very intuitive. To create a profile, select Advanced Options” in the print settings and click “Create Profile”. Name the profile something descriptive and an appropriate base profile if you’re starting from scratch (any will be fine if you’re going to use our Ninjaflex Profile) After the profile is created, select it from the dropdown and click “Edit Profile” to open the JSON file which contains it. You can test out our profile by simply copy-pasting it in. Our profile is 0.2mm infill, 100% infill, no rafts or supports. Based on experimentation, rafts actually seem to work okay while supports tend to be harder to remove (although it is quite easy to cut them off, as well as perform general resurfacing and re-edging with scissors so long as you can get them in there). Infill actually works pretty well too, although they tend to be better with thicker walls (we use 3 instead of the usual 2). We’ve mostly printed well-supported models; I don’t know how well Ninjaflex would print latices, very thin structures, and extreme overhangs (I suspect “not well”). If you try out our profile, let us know how well it works for you!"},{"id":"2015-05-05-expanding-our-makerspace-community","title":"Expanding Our Makerspace Community","author":"laura-miller","date":"2015-05-05 10:46:07 -0400","categories":null,"url":"expanding-our-makerspace-community","layout":"post","content":"Are you a UVA graduate student or upper-level undergraduate in the humanities?  Interested working in our Makerspace? Our Makerspace is designed to foster experimentation with 3D modeling and printing, physical computing (e.g. Arduino, wearables) and more.  We are seeking part-time student consultants to help maintain the public space, field users’ basic maker and general computing questions, and connect researchers to Scholars’ Lab staff when necessary.  When not actively engaged with users, students will be asked to experiment with the equipment, to pursue their own research, and to publish their processes and observations on the Scholars’ Lab blog. They will also be expected to conduct informal workshops to train new users. Experience with 3D modeling and printing, electronics, sewing, and/or programming preferred, but can be learned on the job.  The successful candidate will be able to work up to 10 hours per week. An important aspect of the maker culture is apprenticeship and supporting makers in their pursuit of professional experience. We are looking for motivated individuals who are capable of working independently and value the opportunity to engage with and support a growing community.  Benefits of the job may include: access to expertise and mentoring in your field of interest, use of equipment and tools, and ability to shape Scholars’ Lab workshops and programming. Candidates should include a cover letter discussing their interest in working in the Scholars’ Lab, detailing any experience or interest in participating in a maker space, and outlining any previous experience with public service or assisting others in using technology. If you would like to apply, please fill out an application in CavLink ."},{"id":"2015-05-14-announcing-2014-2015-fellows","title":"Announcing 2014-2015 Fellows!","author":"purdom-lindblad","date":"2015-05-14 07:55:21 -0400","categories":["Announcements","Grad Student Research"],"url":"announcing-2014-2015-fellows","layout":"post","content":"We are thrilled to announce the 2015-2016 Scholar’s Lab fellows for both the Praxis Program and the Graduate Fellowship in the Digital Humanities . We are welcoming 8 fellows from 5 disciplines from the arts, humanities, and social sciences . Our graduate fellows are joining a robust and vibrant community of past fellows ! Graduate Fellows in the Digital Humanities We are looking forward to working with Brandon Walsh and Veronica Ikeshoji-Orlati, our 2014-2015 Graduate Fellows in the Digital Humanities. Veronica Ikeshoji-Orlati’s (Classical Art and Archaeology) dissertation is titled,  Music, Performance, and Identity in 4th century BCE South Italian Vase-Painting. Brandon Walsh’s (English) dissertation is titled,  AudioTextual: Modernism, Sound Recording, and Networks of Reception. Praxis Program We are delighted to welcome 6 diverse disciplinary team members to the 5th year of the Praxis Program: James Ascher (English) Bremen Donovan (Anthropology) Ethan Reed (English) Gillet Rosenblith (History) Rachel Trapp (Music, Composition) Lydia Warren (Music, Critical and Comparative Studies) Look forward to more details around the Praxis Program’s new project in the fall!"},{"id":"2015-05-19-can-you-get-the-data-out-of-this-file","title":"Can you get the data out of this file?","author":"ammon-shepherd","date":"2015-05-19 10:05:36 -0400","categories":["Geospatial and Temporal"],"url":"can-you-get-the-data-out-of-this-file","layout":"post","content":"That was the question I was asked by a student who came into the Scholars’ Lab this semester. My answer is always, “Yes. That can be done.” Wether I know how at the time is a different matter, but that’s the beauty of the Lab, the opportunity to learn new things. The Challenge A student came in with a database file and the need to view the data in the file. At first I thought it would be easy to pull the data into a database and export it out as a spread sheet. Once I got a copy of the SQL file, I realized it would be a bit more work than firing up phpMyAdmin to import the SQL file and export as a CSV or Excel file. First of all, the SQL file was a database dump from PostgreSQL. Second, it was a PostGIS data file. Solution To be able to view the data in the file I needed to set up a PostgreSQL database with PostGIS, and then use QGIS (a free, cross-platform alternative to ArcMaps) to actually visualize the geographic data stored in the SQL file. PostgreSQL Installing PostgreSQL is the first step. There are many ways to do it. For Windows, Linux, etc From PostgreSQL themselves: http://www.postgresql.org/download/ Handy tutorial for different OS’s: https://www.codefellows.org/blog/three-battle-tested-ways-to-install-postgresql For the Mac: Since I’m using a Mac and had some issues, I’ll detail that installation below. The easiest: http://postgresapp.com/ The next easiest: Use homebrew, brew install postgresql The hardest: Install from source, but that’s not really needed. The PostgreSQL site has a list of install options, including the two above: http://www.postgresql.org/download/macosx/ I went the homebrew way because of a required extension that doesn’t get installed with the PostgreSQL app or other means. Installing &lt;code class=\"hljs\"&gt;brew install postgres postgis\n&lt;/code&gt; Follow the instructions for starting and stoping the PostgreSQL server, with the addition of making life easier by installing and using lunchy . &lt;code class=\"hljs\"&gt;brew install lunchy\n&lt;/code&gt; Now you can start and stop PostgreSQL with lunchy start postgres and lunchy stop postgres We’ll also need to install pgrouting. This looks like it can be installed with homebrew, but it didn’t actually install for me. I ended up installing from source. Using this StackExchange answer as a guide, install pgrouting: http://gis.stackexchange.com/questions/26330/issues-with-installing-pgrouting-in-mac There is a bug in the main stable version of pgrouging, so you’ll need to grab the development branch from their github repo. https://github.com/pgRouting/pgrouting/archive/develop.zip Download and unzip that file. Enter the directory created by unzipping, and do the following steps: &lt;code class=\"hljs\"&gt;mkdir build\ncd build\ncmake -DPOSTGRESQL_INCLUDE_DIR='/usr/local/Cellar/postgresql/9.4.1/include/server' -DBoost_DIR='/usr/local/Cellar/boost/1.57.0' ..\nmake\nsudo make install\n&lt;/code&gt; Enabling the Extensions After installing and starting PostgreSQL, you’ll need to create/install/enable the extensions. Log into the PostgreSQL server: &lt;code class=\"hljs\"&gt;psql postgres\n&lt;/code&gt; View which extensions are already installed by typing \\dx on the PostgreSQL command prompt. Create an extension by typing create extension &lt;extension name&gt;, replacing &lt;extension name&gt; with the name of the extension, of course. You’ll need to have the following extensions installed: plpgsql hstore intarray pgcrypto postgis pgrouting Installing the extensions this way lets you see if there are any problems before trying to import the SQL file. Import the SQL file First set up a role/user and a database. &lt;code class=\"hljs\"&gt;createdb dbname\ncreateuser username\n&lt;/code&gt; I needed to change the name of the user in the SQL file to match the database/username in the PostgreSQL database, or I could have created a database/user with the same name as in the SQL file. But to change all of the names I did a one line perl call: &lt;code class=\"hljs\"&gt;perl -pi bak -e 's/alec/test1/g' 20130930_dbdump_Alec.sql\n&lt;/code&gt; This will change all instances of ‘alec’ to ‘test1’ in the file, and create a backup of the file. Now you can import the file by running: &lt;code class=\"hljs\"&gt;psql database &lt; filename\n&lt;/code&gt; QGIS QGIS is a free alternative to ArcMaps. Install from their website. Installation is pretty straight forward. http://qgis.org/en/site/forusers/download.html Connect to PostgreSQL database A tutorial: http://gis-techniques.blogspot.com/2012/04/how-to-connect-spatial-databasepostgis.html To connect to the PostgreSQL database, you’ll actually add the database as a layer. Go to the Layer menu -&gt; Add Layer -&gt; Add PostGIS Layers. Under the Connections section, click on the New button. &lt;code class=\"hljs\"&gt;Name = anything you want\nHost = localhost\nUser = the same as you used above in the createuser command\n&lt;/code&gt; Now select the tables you need and click the “Add” button. Enjoy multiple layers on your map!"},{"id":"2015-06-11-summer-in-the-makerspace-mucha-smart-dress-part-i","title":"Summer in the Makerspace: Mucha Smart Dress, Part I","author":"julia-schrank","date":"2015-06-11 10:59:18 -0400","categories":["Experimental Humanities","Grad Student Research"],"url":"summer-in-the-makerspace-mucha-smart-dress-part-i","layout":"post","content":"Bonjour à tous! I haven’t written in a long time because I wanted to wait for something very special to share. With Laura and Purdom’s encouragement, I am undertaking the creation of a wearable technology piece that I have dubbed (tentatively), the “Mucha Smart Dress,” which I hope to finish by mid-July…so I can wear it on my birthday! In this post, I will describe the project and detail the goals, inspirations, and supplies I have in mind. The Project [![Mucha Smart Dress](http://static.scholarslab.org/wp-content/uploads/2015/05/Mucha-Smart-Dress-1024x981.jpg)](http://static.scholarslab.org/wp-content/uploads/2015/05/Mucha-Smart-Dress.jpg) My sketch of the \"Mucha Smart Dress\" Although Alphonse Mucha  ultimately rejected the “Art Nouveau” label his critics ascribed to his œuvre, he is nonetheless considered one of the founding fathers of this movement. A wildly popular style in the decades of 1890 to 1910, Art Nouveau marked an effort towards gesamtkunstwerk, or “total art.” According to Mucha, his contributions to the style sought to surround society with beautiful works of art in order to inspire happiness and general well-being. He also maintained that objects’ design, in order to bring this tranquility, had to indicate and contribute to the object’s use. It is with this fusion of the inspirational and the functional that I wanted to make an Alphonse Mucha-styled wearable.  I have conceived of a “Smart Dress,” based on an amalgam of Mucha designs, that will not only pay homage to Mucha’s stunning legacy, but also be useful, wearable, and approachable. Despite the complexity of the design, I hope to make it a piece that I can wear on a normal day by toning it down with monochromatic colors and a more modern silhouette. The technology of the project will come in the form of a FLORA microcomputer housed in the dress’s center decoration (to be 3D printed with Ninjaflex) that will operate a series of LEDs sewn into the drapes of the fabric. With temperature changes, different color combinations of LEDs will light up, illuminating the silk overlay. This function is not entirely practical, yet it is firmly anchored in the physical world while also fusing the four panels of Mucha’s “The Seasons” quadriptych (see more details and a photo below) into one garment. While creating this project, I hope to also think critically about the utility (or lack thereof) of wearable technology in my research on Alphonse Mucha’s “French period” of decorative art and poster designs. For example, I’m already asking myself, “Are these clothes practical for everyday life?” in 2015 and in 1900, and “Is this practical (or impractical) nature a part of the designs’ appeal?” Goals Create a wearable technology piece to be displayed in the Makerspace. Overcome my Arduino anxiety (!) Make a floor-length dress for summer that actually fits and flatters me. Bring an Alphonse Mucha design to life. Inspirations This dress by Electromode showed me that Arduino-enhanced clothing could be chic. I truly admire all of the geekery-inspired pieces out there, but I couldn’t see myself incorporating them into my existing wardrobe. I got the idea for a temperature-sensitive focus from Alphonse Mucha’s 1896 lithographic quadriptych of  ”The Seasons.” The dress shape and much of its design I owe to Mucha’s 1898 print “The Arts: Painting.” I have, however, adapted the dress to cover the breasts, since in our society this sort of garment is not – strictly speaking – legal to wear in public. I was also inspired by “The Moon and the Stars,” with particular emphasis on the “Moon”   since her dress looks like it already has LEDs on it, non? *  The Mucha Foundation, upon my meeting with them, specified that they do not generally accept unauthorized reproductions of the artist’s work and “fair use” is not the same in the Czech Republic. Therefore I have posted screenshots from the Foundation’s website. Supplies (selected) Arduino supplies adafruit’s FLORA, optimized for wearables Conductive thread Neopixel Diffused 5mm Through-Hole LEDs, 50-count 3D Printing Ultimaker PLA Silver-Metallic Materials Thrifted dress, for the prototype (see below) Dark Silver Habotai Silk, for the final copy Repurposed leather (From my time à Paris) 2” Double-faced Silk Ribbon, Charcoal-purple and black [![Before Dress](http://static.scholarslab.org/wp-content/uploads/2015/05/Before-Dress-683x1024.jpg)](http://static.scholarslab.org/wp-content/uploads/2015/05/Before-Dress.jpg) I'll turn this thrifted find (in Makerspace Red!) into a hacked 19th-century design! Please let me know if you have any suggestions, alterations, or comments on this plan at any point during my work on this project!"},{"id":"2015-06-29-todo-introduce-code-concepts","title":"//TODO - Introduce Code Concepts","author":"wayne-graham","date":"2015-06-29 06:59:37 -0400","categories":["Research and Development"],"url":"todo-introduce-code-concepts","layout":"post","content":"One of the most fun (and challenging) things I get to do is to introduce people to programming concepts. I’ve done this in a lot of different environments ranging from intensive week-long courses with Humanities Intensive Learning and Training, to year-long apprenticeships our  Praxis Program, to day-long intros with events like Rails Girls and Rails Bridge . All of these have different trade offs in getting people introduced to writing for computers (software development). But one of the best ways I’ve found to work with someone who is interested in learning about how to write software is pair programming, and it’s something that is done in the Scholars’ Lab a lot (though I don’t get to do as much as I would like). If you are unfamiliar with pair programming, basically it takes two people sitting side-by-side (there are remote versions of this too) where one person types (the driver), and the other “tells” the driver what to type (the navigator). I have found this a hugely effective way to get people to start hearing how to approach a software problem, particularly how to break up a particular problem in to smaller problems. This is a skill developers learn over time, but is perhaps one of the most opaque to novice developers. Some of this has to do with the abstract way in which software languages work, the level of syntax knowledge a developer has, and the fact that experienced developers start skipping steps along the way based on their experience. Novice developer might look at a problem and begin to develop a solution, they often get locked in to their thinking that this is the only way. When this path doesn’t pan out as expected (which is usually the case), this can lead to a lot of frustration, and “Stack Overflow” code. As a seasoned developer, I’ll usually see several different paths through a problem, and when I hit that first roadblock, alter my approach. Last week I joked with one of our LAMI fellows on our IRC channel that she should write a plugin for one of our bots so we could generate memes with the memegen.link API . She responded back that she’d actually love to learn how, and in the spirit of the Ten Thousand, we met the next day to work this out. One of the bots in our IRC is HUBOT, which uses CoffeeScript  to interact with a node.js server. While I typically write in JavaScript for these purposes, I think CoffeeScript has some real advantages for a novice developer (particularly with closures and some of the funkier parts of JavaScript). The way I broke this down was to say we need a few things for this to do. First, we want to be able to see which meme templates can used, and a method to get a URL back with the meme. Using the conventions of HUBOT, I suggested an interface like this user&gt; hubot meme list\nhubot&gt; afraid - Afraid to Ask\nhubot&gt;... and user&gt; hubot meme me afraid \"top message\" \"bottom message\"\nhubot&gt; afraid - Afraid to Ask\nhubot&gt;... The meme list seemed to be easiest so we tackled that first. We looked at the list of templates and needed to simplify. Here I just wanted the keys and a label, so we built our own CoffeeScript object with this pattern: memes =\n  \"afraid\": \"'Afraid to Ask'\",\n  \"blb\": \"Bad Luck Brian\",\n  \"buzz\": \"X, X Everywhere\",\n  ... With the list of memes, we just needed a way to get this to the user. Using the template that all hubot-scripts use, we added: module.exports = (robot) -&gt;robot.respond /meme list/i, (msg) -&gt;\n    for code, meme of memes\n      msg.send \"#{code} - #{meme}\" When the bot hears “hubot meme list”, it iterates over each line of the memes and prints out the code (on the left) and the meme (on the right). After we got this working we shipped the feature. To actually get the input to generate a meme, I knew we were going to have to do something I try not to do too much with novice developers…regular expressions. There’s a joke that goes, “if you have a problem that you have to solve with regular expressions, you now have two problems.” And the one we need to parse the text input is particularly obscure. So much so that I had to pair with another developer (thanks Eric) to get it correct. I won’t go in to detail, but this is the line that we came up with to parse the pattern “hubot meme me afraid “top message” “bottom message”’ robot.respond /meme me (\\w+) (\\\"[^\"]+\\\") (\\\"[^\"]+\\\")/i, (msg) -&gt; As we were testing this out, former Scholars’ Lab fellow Alex Gil came in to ask some questions about Neatline and generated this https://twitter.com/elotroalex/status/614147822116581380 After we got everything working, I took the code @lilybeth and I wrote and abstracted it in a way that could be easily used by others, publishing it on npm, opening the source code on GitHub, and writing automation routines in Travis CI to automagically update the npm site when I push a new release to Github (this last part is insanely easy with Travis ). There are many things we can do to make this toy better (like better error handling text input), but for now this is an example of how a thing went from a “I’d like to know how to do this” to a published thing being used by more than just us (according to the download count). And, if you’d like to run your own hubot (or waynebot as we affectionately call it), I shared the code to run this on OpenShift with the RedisCloud cartridge addon which is based on code by Katie Miller ."},{"id":"2015-07-06-physical-computing-at-dhsi-2015","title":"Physical Computing at DHSI 2015","author":"ethan-reed","date":"2015-07-06 06:00:38 -0400","categories":["Digital Humanities","Experimental Humanities"],"url":"physical-computing-at-dhsi-2015","layout":"post","content":"In the beginning of June I had the pleasure of attending the Digital Humanities Summer Institute at the University of Victoria for the second year running. My experience this year was so good that I wanted to write a quick post sharing some of the highlights – so if you’re thinking of going to DHSI, are a regular interested in a class they’ve never taken, or have never heard of it and want to learn more, I hope this post sheds some light on the kinds of things people get up to at one of my favorite scholarly events of the year (and why I want to go again next summer)! I spent my week in Victoria in a class called “Physical Computing and Desktop Fabrication,” taught by the folks at UVic’s Maker Lab : Nina Belojevic, Devon Elliot, Shaun Macpherson, and Jentery Sayers. If you’ve read any of my other posts or talked to me about the kind of stuff I do in the Makerspace here at UVA,  I’ve taken a large number of cues from these folks in the past year as I’ve been learning more about critical making and physical computing. Meeting every day for five days in a row, the class covered a huge range of things that fall generally under the heading “physical computing,” which I think of broadly as just the interactions between computers and the physical world.  We worked with Arduinos, wearables, a 3D printer ( Replicator 2 ), a laser cutter ( Epilog ), SketchUp, photogrammetry, and more. If you are interested in exploring the full inventory of what’s available at the Maker Lab, you can find that inventory here as a post, or here as a spreadsheet . These technologies are capable of pretty amazing stuff - examples shared with us ranged from interactive paintings, wooden mirrors, and plants that tell you when they are thirsty, to technologies at CanAssist at UVic impacting the lives of those with disabilities.  If you want to check out the whole week’s schedule for a more detailed account of what went on (along with more of the examples shared by our instructors) check out the index at the class’s GitHub page . Along with trying out hardware, software, and discussing the larger implications of this kind of work, we also had a specific project for the week: to make a box. More specifically, our mission (quoted here from our class GitHub index ) was to “prototype your own box and explain how it operates both as a material and metaphor.” A deceptively simple task! But one that ended up producing awesome results - if you poke around on Twitter and #physcomp or some pictures Jentery posted from the class, you can see some of the amazing things others made over the course of the week. So what did I make? I called it… the gloom box. So what is the box and what does it do? Like our prompt for the week, the gloom box is simple on the outside: it is a small box with a sad face looking helplessly at a button and an LCD screen. The LCD screen displays the words “BATTERY LIFE” with a large number underneath. When you push the button, the number under “BATTERY LIFE” decreases, and when it hits zero the whole thing turns off - that’s it. The gloom box has no other functions. Before getting into why I made this thing (box as metaphor), a little more info on the thing itself (box as material). The box was cut using an Epilog laser cutter from a large sheet of baltic birch. I made a simple frame design for the six sides of the box using BoxMaker, then Nina and Shaun worked some magic to make it the correct dimensions for what I needed to put inside. The image on the top was something I made in a few seconds in Paint, then converted into a raster file that the laser cutter could read and engrave into the box. Like the outside of the box, the inside is also relatively simple: An Arduino Uno hooked up behind the scenes via a breadboard and a bunch of jumper cables to the button, battery, breadboard, and LCD screen seen on the outside. To get the button to work, I ended up writing about thirty lines of code after borrowing from some examples in the Arduino library and other people working out similar problems online (and plenty of help from Devon!).  Unfortunately, the number underneath “BATTERY LIFE” didn’t actually correspond to the 9-volt’s battery - the code for that turned out to be too complicated. So I had it “simulate” the loss instead, setting “BATTERY LIFE” to a very high number and decreasing it by a random value from 1-9 every time the button was pushed. In what was described as a “delicious” twist of irony, when walking the gloom box over to share our work with other classes on Friday, the battery actually died before anyone had a chance to play with it. The line between “simulating” a battery’s energy expenditures and actually expending said energy can be very thin, apparently. That covers the box as material, but what about the box as metaphor? Why make a box whose only purpose is to inform you of how your interaction with it has depleted its battery life? My original goal had been to make a box that was “aware” at some level of some of the material costs of its own production, operation, or maintenance, ideally one that would highlight these costs in a way unique to the kinds of things physical computing lets you do. Two recent pieces of writing had me thinking this way: the first is an amazing post of Bethany’s from about a year ago titled “Digital Humanities in the Anthropocene” ; the second was an article given as background reading for my DHSI class that I can’t find a link to online as it is still forthcoming, but can give the info for here: it’s called “Between Bits and Atoms: Physical Computing and Desktop Fabrication in the Humanities” and is written by Devon, Bethany, Jentery, and Kari Kraus and William J. Turkel in The New Blackwell Companion to Digital Humanities. These two articles, coupled with some other reading going on around the same time (including Timothy Morton’s Hyperobjects and N. Katherine Hayles’s How We Became Posthuman ), made the problem feel like an interesting one worth investigating. This small project turned out to be a great opportunity to explore certain problems I’ve been thinking about more generally. For example, the energy costs of “invisible” transactions that we make every day but don’t really see, like information moving from place to place, or from digital to analog - exchanges that feel like they aren’t physically embodied anywhere but actually are (a problem Hayles thinks through wonderfully in How We Became Posthuman ). Here I was thinking of the kinds of micro-expenditures that happen when we send a text, take a picture, or run some code. A box that made these “invisible” or “blackboxed” expenditures big, open, and obvious - even with something as basic as pushing a button - seemed like an interesting idea. Given the time frame I had to work on it, I ended up with something simple, a little silly, but also frustrating in potentially productive ways. A small gloomy energy puzzle, seemingly aware of its own helplessness in the face of its dilemma. It is reminiscent in simplicity and spirit of another example shared with us in class, the most useless machine . But perhaps more philosophically, it taps into questions of inevitability and the environment Roy Scranton asked in his 2013 NYT article, “Learning How to Die in the Anthropocene”, where he takes a question like “How do we make meaningful choices in the shadow of our inevitable end?” and asks it on the scale of civilization itself. So: is it a box-as-metaphor for planet Earth (no one person pushes the button enough for it to zero out, but enough people over enough time can make it happen – energy use understood collectively)? For the problem of energy in a more-or-less closed system (the only way to keep the box going is to introduce a new battery)?  For existential gloom in general, applied not just to humans and societies but objects as well? I only had so much time to think the thing through, but thankfully I have the schematics and code and can remake it in our Makerspace if I want to keep tinkering around with it and the ideas that it – or projects like it – might represent. I hope this gives a window into some of the kinds of things people get into at DHSI. And I want to thank again all the folks at DHSI for making this amazing week possible, as well as my instructors and the other wonderful people in our class who made it such a welcoming and enlightening experience. Thanks for making a great space to tinker around in, and hopefully I will see you next year!"},{"id":"2015-08-19-uva-library-fall-2015-gis-workshops","title":"UVa Library Fall 2015 GIS Workshops","author":"chris-gist","date":"2015-08-19 09:28:32 -0400","categories":["Announcements","Events","Geospatial and Temporal"],"url":"uva-library-fall-2015-gis-workshops","layout":"post","content":"All sessions are one hour and assume attendees have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials with expert assistance.  All sessions will be taught on Thursdays from 3PM to 4PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab).  Sessions are free to attend and are open to the UVa and larger Charlottesville community. September 3rd Making your First Map Getting started with new software can be intimidating.  This workshop introduces the skills you need to work with spatial goodness.  Along the way you’ll get a taste of Earth’s most popular geographic software and a gentle introduction to map making.  You’ll leave with your own cartographic masterpiece and tips for learning more in your pursuit of mappiness at UVa. September 10th Getting Your Data on a Map Do you have GPS points or a list of latitude and longitude you would like to show as points on a map?  This session will show you how to turn your data into map layers and how to connect them to make lines and polygons as well. September 17th Points on Your Map: Street Addresses and More Spatial Things Do you have a list of street addresses crying out to be mapped?  Have a list of zip codes or census tracts you wish to associate with other data?  We’ll start with addresses and other things spatial and end with points on a map, ready for visualization and analysis. September 24th Georeferencing – Putting Old maps and Aerial Photos on Your Map Have an old map or an aerial photograph that you would like to use as a spatial layer?  This session will teach you techniques to properly place your data and make it usable in GIS software.  We will also demo similar techniques for Google Earth. October 1st Taking Control of Your Spatial Data:  Editing in ArcGIS Until we perfect that magic “extract all those lines from this paper map” button we’re stuck using editor tools to get that job done.  If you’re lucky, someone else has done the work to create your points, lines, and polygons but maybe they need your magic touch to make them better.  This session shows you how to create and modify vector features in ArcMap, the world’s most popular geographic information systems software.  We’ll explore tools to create new points, lines, and polygons and to edit existing datasets.  At version 10, ArcMap’s editor was revamped introducing new templates, but we’ll keep calm and carry on. October 8th Collecting Your Own Spatial Data Research projects often rely on fieldwork to build new datasets.  In this workshop we’ll focus on tools for spatial data collection. First we’ll take a quick look behind the curtain to see how GPS really works and how to use that knowledge to our advantage.  Then we’ll evaluate free or low-cost options to gather locations and associated attributes using handheld GPS devices, smartphones, and apps.  This workshop will introduce you to a range of devices and methods for mobile spatial data collection."},{"id":"2015-09-08-augmented-reality-and-simulation","title":"Augmented Reality and Simulation","author":"wayne-graham","date":"2015-09-08 07:20:15 -0400","categories":["Research and Development"],"url":"augmented-reality-and-simulation","layout":"post","content":"A few weeks ago the Scholars’ Lab went on a field trip to the School of Architecture’s “ FabLab ” to check out a project Chris Gist and Melissa Goldman had been working on, a sand table that has a projector and a Kinect connected to a computer that projects a topology on to the sand. This is an amazing tool that was developed by a lot of different organizations, primarily to teach earth science concepts related to watershed science. However, it’s also an amazing tool for teaching landscape and topography skills to architects (not to mention seriously fun to play with – especially when you start to think about the possibilities of replacing the sand with kinetic sand ). While we were there, several people mentioned how cool it would be to capture and 3D print these landscapes created in the classroom. I’ve seen dozens of programs to build models from the structured light sensors in the Kinect ( Skanect is a good example), so how hard could this be? In an R&amp;D shop, these can be famous last words… I started brushing off my really rusty OpenGL (this system is written for Linux) and started poking around with the Kinect library that the project uses. Then I remembered how much “fun” OpenGL programming can be (coupled with rusty C++ skills) as I crashed the graphics drivers pretty spectacularly more than once (you can always tell because the screen goes blank and you can’t see anything anymore). After checking on the forums, the maintainer clued me in on a feature in the Kinect utilities that enables this feature, but needed a bit of a patch to work. From the KinectViewer utility you can record the output from the camera, and there is another utility that is not built by default that allows you to convert frames of this stream in to the Lightwave Object format (you may remember that from the Commodore Amiga days, if you’re that old). Right now (this will be folded in to the next release of the software), you need to clone my fork of the code ( https://github.com/waynegraham/Kinect ) and to run a special make task: ``` $ cd path/to/clone\n$ make PACKAGES=MYKINECT LWOWriter ``` The basic workflow was to use SARndbox to project the topology and then switch over to the KinectViewer utility to create the output. I was hoping this could be done simultaneously, but unless you have two Kinects, you can only use one at a time. When we record the stream, you get something that looks like this: Then, using the LWOWriter to pull a single frame, you get single Lightwave Object file (which can be read in most 3D packages). After a bit of clean up (I used Blender and Meshlab ), you get something that looks a bit more like the sand in the table. Sarndbox Test Output by waynegraham on Sketchfab I also wrote a short program to help process the movies in to Lightwave (there are more instructions on this gist as well, so check it out): [gist id=0e36c812ca64291797c1 file=converter.rb]"},{"id":"2015-09-10-virginia-woolf-natural-language-processing-and-the-quotation-mark","title":"Virginia Woolf, Natural Language Processing, and the Quotation Mark","author":"brandon-walsh","date":"2015-09-10 07:19:52 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"virginia-woolf-natural-language-processing-and-the-quotation-mark","layout":"post","content":"[Cross-posted on my personal blog ] For my fellowship in the Scholars’ Lab this year I’ll be working with Eric  to expand a project we began last year on Virginia Woolf and natural language processing. My dissertation focuses on sound recordings and modernism, and this year I will focus on how Woolf’s quotation marks offer evidence of her engagement with sound as a textual device. In my reading, the quotation mark is the most obvious point at which sound meets text, the most heavily used sound recording technology in use by writers. Patterns in quotation mark usage across large corpora can tell us a lot about the role that sound plays in literature, but, as you might expect, there are  lots of quotation marks - hundreds or thousands in any given text. Computational methods can help us make sense of the vast number and turn them into reasonable objects of study. You can find more information in  this post  about my thinking on quotation marks and some preliminary results from thinking about them in relation to Woolf. As I discuss there, finding quotation marks in a text is not especially challenging, but this year Eric and I will be focusing on a particular wrinkle in Woolf’s use of the marks, best conveyed in  The Hours, Michael Cunningham’s late-century riff on Virginia Woolf. In  The Hours, Cunningham offers a fictionalized version of Woolf meditating on her composition process: She passes a couple, a man and woman younger than herself, walking together, leisurely, bent towards each other in the soft lemon-colored glow of a streetlamp, talking (she hears the man, “told me something something something in this establishment, something something, harrumph, indeed”) (166). The repeated “ somethings “ of the passage suggest the character’s imperfect experience of the conversation as well as the limits of her senses. As the moment is conveyed through the character’s perspective, the conversation will always be incomplete. Recording technology was largely unreliable during the early days of the twentieth century, and, similarly, the sound record of this conversation as given by the text is already degraded before we hear it. Cunningham points to how the sounded voice is given character in the ears of the listener, and, in a print context, in the pen of the writer. A printed voice can speak in a variety of ways and in a variety of modes. Cunningham’s passage contains echoes of what will eventually be the famous first sentence of Woolf’s Mrs. Dalloway : “Mrs. Dalloway said she would buy the flowers herself.” The text implies that Mrs. Dalloway speaks, but it does not mark it as such: the same conversational tone in Cunningham remains here, but the narrator does not differentiate sound event from narrative by using quotation marks. We see moments of indirect speech like this all the time, when discourse becomes submerged in the texture of the narrative, but it doesn’t disappear entirely. Speech implies a lot: social relations, the thoughts of a speaking body, among others. Things get muddy when the line between narrative voice and speech becomes unclear. If quotation marks imply a different level of speech than unquoted speech, might they also imply changes in the social relations they represent? Mrs. Dalloway is filled with moments like these, and this year I’ll be working to find ways to float them to the surface of the text. Examining these moments can tell us how conversation changes during the period, what people are talking about and for, how we conceive of the limits of print and sound, and about changing priorities in literary aesthetics. The goal this year is to train the computer to identify moments like this, moments that a human reader would be able to parse as spoken but that are not marked as such. Our first pass will be to work with the quoted material, which we can easily identify to build a series of trigger words that Woolf uses to flag speech as sound (said, asked, called, etc.). With this lexicon, we can then look for instances in her corpus where they pop up without punctuation. Teaching the computer to classify these passages correctly will be a big task, and this process alone will offer me lots of new material to work with as I untangle the relationship between modernist print and sound. In upcoming posts I’ll talk more about the process of learning natural language processing and about some preliminary results and problems. Stay tuned! Works Cited: Cunningham, Michael. The Hours . New York: Picador USA : Distributed by Holtzbrinck Publishers, 2002. Print. Woolf, Virginia. Mrs. Dalloway . 1st Harvest/HBJ ed. San Diego: Harcourt Brace Jovanovich, 1990. Print."},{"id":"2015-09-21-music-genre-and-spotify-metadata","title":"Music Genre and Spotify Metadata","author":"brandon-walsh","date":"2015-09-21 06:48:25 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"music-genre-and-spotify-metadata","layout":"post","content":"Cross-posted on my personal site For the last couple weeks, I have been exploring APIs useful to sound studies for a sound recording and poetry project I am working on with former Scholars’ Lab fellow  Annie Swafford . I was especially drawn to playing around with Spotify, which has an API  that allows you to access metadata for the large catalog of music available through their service. The experiment described below focuses on genre: a notoriously messy category that we nonetheless rely on to tell us how to process the materials we read, view, or hear. Genre tells us what to expect from the art we take in, and our construction and reception of generic categories can tell us a lot about ourselves. In music, especially, genres and subgenres can activate fierce debates about authenticity and belonging. Does your favorite group qualify as “authentic” jazz? What composers do you have to know in order to think of yourself as a real classical music aficionado? Playing with an artist’s metadata can expose a lot of the assumptions that were made in its collection, and I was especially interested in the ways in which Spotify models relations among artists. I wanted to explore Spotify’s metadata in a way that would model the interpretive messiness of generic categories. To do so, I built a program that bounces through Spotify’s metadata to produce multiple readings of the idea of genre in relation to a particular artist. Spotify offers a fairly robust API, and there are a number of handy wrappers that make it easier to work with. I used a Python module called Spotipy for the material below, and you can find the code for my little genre experiment over on my GitHub page . If you do try to run this on your own machine, note that you will need to clone Spotipy’s repository and manually install it from the terminal with the following command from within the downloaded repository: $ python setup.py install Pip will install an older distribution of the code that will only run in Python 2, but Spotipy’s GitHub page has a more recent release that is compatible with Python 3. When run, the program outputs what I like to think of as the equivalent of music nerds arguing over musical genres. You provide an artist name and a number, and the terminal will work through Spotify’s API to produce the specified number of individual “mappings” of that artist’s genre as well as an aggregate list of all their associated genres. The program starts by pulling out all the genre categories associated with the given artist as well as those given to artists that Spotify flags as related. Once finished, the program picks one of those related artists at random and continues to do the same until the process returns no new genre categories, building up a list of associated genres over time. So, in short, you give the program an artist and it offers you a few attempts at describing that artist generically using Spotify’s catalog, the computational equivalent of instigating an argument about genre in your local record store. Here are the results for running the program three times for the band New Order: Individual genre maps Just one nerd's opinions on New Order: ['dance rock', 'new wave', 'permanent wave', 'new romantic', 'new wave pop', 'hi nrg', 'europop', 'power pop', 'album rock'] Just one nerd's opinions on New Order: ['dance rock', 'new wave', 'permanent wave', 'gothic metal', 'j-metal', 'visual kei', 'intelligent dance music', 'uk post-punk', 'metropopolis', 'ambient', 'big beat', 'electronic', 'illbient', 'piano rock', 'trance', 'progressive house', 'progressive trance', 'uplifting trance', 'quebecois', 'deep uplifting trance', 'garage rock', 'neo-psychedelic', 'space rock', 'japanese psychedelic'] Just one nerd's opinions on New Order: ['dance rock', 'new wave', 'permanent wave', 'uk post-punk', 'gothic rock', 'discofox', 'madchester', 'britpop', 'latin', 'latin pop', 'teen pop', 'classic colombian pop', 'rai', 'pop rap', 'southern hip hop', 'trap music', 'deep rai'] Aggregate genre map for New Order: ['dance rock', 'new wave', 'permanent wave', 'new romantic', 'new wave pop', 'hi nrg', 'europop', 'power pop', 'album rock', 'gothic metal', 'j-metal', 'visual kei', 'intelligent dance music', 'uk post-punk', 'metropopolis', 'ambient', 'big beat', 'electronic', 'illbient', 'piano rock', 'trance', 'progressive house', 'progressive trance', 'uplifting trance', 'quebecois', 'deep uplifting trance', 'garage rock', 'neo-psychedelic', 'space rock', 'japanese psychedelic', 'gothic rock', 'discofox', 'madchester', 'britpop', 'latin', 'latin pop', 'teen pop', 'classic colombian pop', 'rai', 'pop rap', 'southern hip hop', 'trap music', 'deep rai'] In each case, the genre maps all begin the same, with the categories directly assigned to the source artist. Because the process is slightly random, the program eventually maps the same artist’s genre differently each time. For each iteration, the program runs until twenty randomly selected related artists return no new genre categories, which I take to be a kind of threshold of completion for one understanding of an artist’s genre. The results suggest an amalgam of generic influence, shared characteristics, common lineages, and overlapping angles of approach. The decisions I made in how the program interacts with Spotify’s metadata suggest a definition of genre like the one offered by Alastair Fowler: “Representatives of a genre may then be regarded as making up a family whose septs and individual members are related in various ways, without necessarily having any single feature shared in common by all” (41). Genre is fluid and a matter of interpretive opinion - it is not necessarily based on objective links. The program reflects this in its results: sometimes a particular generic mapping feels very coherent, while at other times the script finds its way to very bizarre tangents. The connections do exist in the metadata if you drill down deeply enough, and it is possible to reproduce the links that brought about such output. But the more leaps the program takes from the original artist the more tenuous the connections appear to be. As I wrote this sentence, the program suggested a connection between garage rock revivalists The Strokes and big band jazz music: such output looks less like a conversation among music nerds and more like the material for a Ph.D. dissertation. As the program illustrates, generic description is the beginning of interpretation - not the ending. Of course, the program does not actually search all music ever: it only has access to the metadata for artists listed in Spotify, and some artists like Prince or the Beatles are notoriously missing from the catalog. Major figures like these have artist pages that serve as stubs for content drawn largely from compilation CDs, and the program can successfully crawl through these results. But this wrinkle points to a larger fact: the results the program produces are as skewed as the collection of musicians in the service’s catalog. Many of the errors I had to troubleshoot were related to the uneven nature of the catalog: early versions of the script were thrown into disarray when Spotify listed no related artists for a musician. On occasion, the API suggested a related artist who did not actually have an artist page in the system (often the case with new or less-established musicians). I massaged these gaps to make this particular exercise work (you’ll now get a tongue in cheek “Musical dead end” or “Artist deleted from Spotify” output for them), but the silences in the archive offer significant reminders of the commercial politics that go into generic and archival formation, particularly when an archive is proprietary. I can imagine tweaking things slightly to create a script that produces only those archival gaps, but that is work for another day. In the meantime, I’ll be trying to figure out how Kanye West might be considered Christmas music . Works Cited: Fowler, Alastair David Shaw. Kinds of Literature: An Introduction to the Theory of Genres and Modes . Repr. Oxford: Clarendon Press, 1997. Print."},{"id":"2015-09-23-introducing-the-2015-2015-scholars-lab-fellows","title":"Introducing the 2015-2016 Scholars' Lab Fellows","author":"purdom-lindblad","date":"2015-09-23 10:05:45 -0400","categories":["Announcements","Grad Student Research"],"url":"introducing-the-2015-2015-scholars-lab-fellows","layout":"post","content":"The Lab has been a busy place lately with 8 new fellows from across the arts, humanities, and social sciences!  Our  Graduate Fellows in the Digital Humanities  and  Praxis Program Fellows  join a distinguished community of  past fellows . Graduate Fellowship in the Digital Humanities This year, we are delighted to work closely with Veronica Ikeshoji-Orlati and Brandon Walsh. [gallery columns=”2” ids=”12229,10140”] Veronica is in her 6th year of the Classical Art &amp; Archaeology program in the McIntire Department of Art. Her dissertation project, entitled “Music, Performance, and Identity in 4th century BCE South Italian Vase-Painting,” explores connections between the iconography of musical performance and constructions of identity in Apulia. During her fellowship year, she will be working with Wayne Graham to optimize her dissertation database and develop visualizations of her research, as well as experiment with integrating digital tools into the practice of iconographical analysis. Brandon’s research focuses on modern and contemporary literature and digital humanities. His dissertation entitled “AudioTextual: Modernism, Sound Recording, and Networks of Reception” examines how authors, amateur readers, and sound artists have used sound recording to invent and reinvent modernist literature throughout the last century. During his fellowship year, he will be working with Eric Rochester to apply machine learning and natural language processing techniques to speech in Virginia Woolf’s novels. Praxis Fellows [gallery ids=”12233,12236,10883,12234,12231,12238”] We are entering our 5th (!) year of the Praxis Program and are welcoming six talented graduate fellows: James Ascher (English) Bremen Donovan (Anthropology) Ethan Reed (English) Gillet Rosenblith (History) Rachel Trapp (Music, Composition) Lydia Warren (Music, Critical and Comparative Studies) This year’s cohort will be exploring various ways to represent time, both physically and digitally. We have guiding questions, but will be scoping the project as we go. Keep track of our experiments on the Time page of the Praxis Program and our on blog . Past Praxis teams developed Prism, a web application for crowd-based interpretations of texts, and re-imagined  Ivanhoe, a platform for playfully (and collaboratively) interpreting texts and artifacts."},{"id":"2015-09-23-time-and-praxis-2015-2016","title":"Time and Praxis: 2015-2016","author":"ethan-reed","date":"2015-09-23 10:46:14 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"time-and-praxis-2015-2016","layout":"post","content":"Time is a massive concept. If you were asked to think about it – how it works, feels, changes, what it looks like, how people go about talking about it, or representing it – where would you start? As a person interested and invested in critical theory, my initial reflex would be to go to philosophers, phenomenologists – writers like Martin Heidegger, Maurice Merleau-Ponty, and so on. Or check in with narrative theorists, think again about what Gérard Genette says about time and narrative in Narrative Discourse . My second reflex would be to look at art and cultural objects. I’ve been trying to watch and/or reconsider as many time-travel movies as I had time for, everything from Primer or Donnie Darko to something more straightforward like The Terminator . Sometimes you find something unexpected – for example, trying to untangle Primer after watching it last week, Gillet (a fellow Praxis member) commented that manipulating time in films can have a certain scare-factor, almost like it’s a specific branch of horror movies. I’d never thought about it that way before. Time, and the manipulation of time, has the power to frighten us. Also, games: like Braid, where manipulating time is the only way to solve puzzles and progress through the narrative. Or (at Jeremy’s suggestion) checking out Ian Bogost’s A Slow Year . And of course, literature. I thought first to a classics like H.G. Wells’ 1895 The Time Machine or Ray Bradbury’s 1952 short story “A Sound of Thunder,” where tourist big-game hunters go back in time to kill dinosaurs, but (spoilers) someone accidentally steps on a butterfly and changes the whole course of history. Then to Kurt Vonnegut’s 1969 [ Slaughterhouse Five _](https://en.wikipedia.org/wiki/Slaughterhouse-Five)(probably because it’s the first book I ever wrote a book report on), where a character becomes “unstuck in time” as he dips in and out of his life experiences. Or even to something like Ruth Ozeki’s 2013 _ A Tale for the Time Being (which I’m reading for my oral exams), where relationships with past and present “time beings” might, in some ways, influence one another, even when those cross-temporal relationships are mediated textually. There are so many movies, games, novels, short stories, poems, and plays I could go to here it’s nuts – not even looking at works with entangled narrative structures like the film  Memento or Faulkner’s  Absalom, Absalom! (for example). These above are just the first that come to mind, each of which interacts with time and experiences time a little differently. This is my first time writing here as a Praxis Fellow for the 2015-2016 year. Along with  producing a charter, this year we’re all trying to think about time in as many different ways as possible, staying wide open with it and willing to explore any new possibilities, each coming at it from our own unique angle. Throughout the year, in our work and in posts like this, I’ll try to keep track of and share what I feel like I am able to bring to our work from my own unique perspective (apparently even if sometimes that means just talking about a bunch of time-travel stories I’m excited about). We’re only a few weeks in, and I can already say how excited I am to be working with the Praxis team and all the folks at the Scholars’ Lab this coming year. I can’t wait to see what we come up with!"},{"id":"2015-09-30-preserving-reconstructing-teaching-in-3d","title":"Preserving, Reconstructing, & Teaching in 3D","author":"jennifer-grayburn","date":"2015-09-30 11:52:49 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"preserving-reconstructing-teaching-in-3d","layout":"post","content":"Cross-posted on my personal blog. The destruction of historic monuments has been a frequent topic in the news lately due to the Syrian and ISIS conflicts. The destruction of  historic mosques  and, most recently, the  Temple of Baal in Palmyra  have sent shockwaves through the international community. The public outcry for cultural casualties has been so overwhelming that it has  prompted backlash and criticism  questioning the value of such monuments when confronted with overwhelming human suffering and death. This is not the place to debate the value of historical monuments and I cannot speak for anyone other than myself, but I anticipate that the focus of horror expressed by many institutions and academics is not the rejection of human suffering, but rather an acknowledgement of our own limitations to make a difference. Art and architecture play a key role in political legitimization and cultural identity, especially during wartime when these factors can bolster or inhibit support. The ability to preserve the history and cultural heritage of this region through our scholarship expresses a hope for a time when such artifacts (and the history they represent) can be studied and valued openly again. The variety of digital tools and methodologies embraced by the digital humanities has offered a small, but powerful way to preserve what we still can.  Project Mosul, for example, is using crowd-sourced photographs to reconstruct monuments digitally. Harvard and Oxford have also  created their own “monuments men”  to scan some of the most threatened monuments. Artist  Morehshin Allahyari  is also considering the relationship between preservation, destruction, and technology in her art as she reconstructs and 3D prints the artifacts lost to ISIS destruction. This summer, my own research took me as far as possible from the war-torn Middle East to the quiet fields of Iceland. The Monasticism in Iceland archaeology project provided me with an opportunity to work directly with artifacts and grapple with the complicated considerations of heritage preservation for the first time. Iconoclasm is not an issue in Iceland as it is in the Middle East; rather, site isolation and exposure to Iceland’s volatile climate create difficulties for sharing and preserving Iceland’s material past. A beautifully carved twelfth-century stone from Hítardalur represents these concerns in a particularly striking way. Likely a remnant of a failed medieval monastery, this unique mustachioed face lies in the field where it was discovered, open to the elements. When first told about this unique carving, I assumed it would be safely locked in a museum, climate controlled and secured. I did not expect to find this rare stone in a field adjacent to a private farmhouse, exposed to weather, theft, and accidents. Over the years, this open exposure has weathered the details of the face and two additional sculptures of similar composition were lost. I have never worked with artifacts outside of a museum setting and it was difficult for me to grasp that there are scores of objects that museums cannot accommodate, that the removal of artifacts—even for their protection and accessibility—can be interpreted as illegitimate or even criminal overreach. In fact, it raises multiple questions about artifact and heritage ownership that I cannot even begin to answer. As the cases of the Parthenon/Elgin Marbles and Kennewick Man demonstrate, accessibility, preservation, and ownership do not always coincide. [![Hítardalur steinn - Copy](http://static.scholarslab.org/wp-content/uploads/2015/09/Hítardalur-steinn-Copy.jpg)](http://static.scholarslab.org/wp-content/uploads/2015/09/Hítardalur-steinn-Copy.jpg) Hítardalur sculpture, photograph taken in the mid-twentieth century. [![2015-05-13 09.52.51](http://static.scholarslab.org/wp-content/uploads/2015/09/2015-05-13-09.52.51.jpg)](http://static.scholarslab.org/wp-content/uploads/2015/09/2015-05-13-09.52.51.jpg) Hítardalur sculpture, photograph taken in June 2015. Note the deterioration of facial details. I admit that I am still grappling with these issues, as my priorities of accessibility and preservation are clearly based on my own academic training and affiliation. This concern, however, prompted me to consider ways that I can participate in this dialogue in my limited capacity as a foreign scholar with limited resources. With the Middle Eastern examples and mentorship of my colleagues in the Scholars’ Lab (including the work and expertise of  Edward Triplett ), I jumped into the digital modeling and photogrammetry methods that have been so successfully implemented by larger art and archaeology projects to see what I could do personally. The resulting model and 3D print preserves the current state of the medieval Icelandic sculpture, but highlights both the potentials and limitations of these technologies for preservation and pedagogy. Armed only with my camera, I started by taking a number of photographs of the Hítardalur sculpture at varying heights and distances. My goal was to capture the sculptural relief and texture of the stone in as much detail as possible. After looking into different software, I invested in  Agrisoft PhotoScan  to compile a point cloud and build the mesh into a digital model. The software makes this easier than I anticipated and I was pleased with my early results. Because the sculpture was too heavy to lift myself, I was not able to photograph the base and, as a result, the digital model was open on the bottom. This is not a problem in itself, but the shell of this model would have been too fragile and had too many overhangs to 3D print properly. I exported the model to netfabb  and  meshm ixer —both available for free—to make the model watertight (closed off on all sides) and reorient it to sit flat on a printer platform. [![Screen Shot 2015-05-16 at 1.33.20 PM](http://static.scholarslab.org/wp-content/uploads/2015/09/Screen-Shot-2015-05-16-at-1.33.20-PM.png)](http://static.scholarslab.org/wp-content/uploads/2015/09/Screen-Shot-2015-05-16-at-1.33.20-PM.png) Position of the camera for the photographs used to make the Hítardalur model. [![Screen Shot 2015-05-16 at 1.32.36 PM](http://static.scholarslab.org/wp-content/uploads/2015/09/Screen-Shot-2015-05-16-at-1.32.36-PM.png)](http://static.scholarslab.org/wp-content/uploads/2015/09/Screen-Shot-2015-05-16-at-1.32.36-PM.png) Finished Hítardalur model. [![Screen Shot 2015-05-16 at 1.37.58 PM](http://static.scholarslab.org/wp-content/uploads/2015/09/Screen-Shot-2015-05-16-at-1.37.58-PM.png)](http://static.scholarslab.org/wp-content/uploads/2015/09/Screen-Shot-2015-05-16-at-1.37.58-PM.png) Finished Hítardalur model with texture added. Printing the model had its own difficulties stemming from technical issues with the printers. After two failed prints on MakerBot ’s Replicater 2 caused by a ‘glitch’ in the SD card, I reformatted the card and switched to the Ultimaker 2. Using the Ultimaker software, Cura, I shrank the model and set the slicer settings to a lower quality to print a quick, rough prototype. With this successful print, I increased the size and print quality to produce an approximately four-inch model. [![2015-09-13 11.57.55](http://static.scholarslab.org/wp-content/uploads/2015/09/2015-09-13-11.57.55.jpg)](http://static.scholarslab.org/wp-content/uploads/2015/09/2015-09-13-11.57.55.jpg) First failed print of the Hítardalur sculpture using PLA and the Replicator 2. [![2015-09-13 11.58.57](http://static.scholarslab.org/wp-content/uploads/2015/09/2015-09-13-11.58.57.jpg)](http://static.scholarslab.org/wp-content/uploads/2015/09/2015-09-13-11.58.57.jpg) Larger of the two succesfully printed Hítardalur prototypes. Printed using PLA and the Ultimaker 2. With a successful model and print, I am now left with the burning question: So what? It is true that I have preserved the sculpture in its current form in case it ever disappears or further weathers away. The digitization also offers a better way to share and teach the sculpture in a multi-dimensional way across vast distances and languages. But the model’s value and efficacy are ultimately limited by its online accessibility. Museums and institutions are increasingly compiling vast open-access databases of digital images and models of their own collections, but an isolated model like this is easy to miss. This sculpture, for example, only appears in Iceland’s main archival database as an unnamed feature in a photograph of the farm. A model like this would likely need to be contextualized in larger project database, perhaps one dedicated to medieval, monastic, or sculptural Icelandic works, to increase accessibility and public interest. The limitations of the 3D print are perhaps more obvious than the limitations of the 3D model. While the model has texture and can be shared online, the print varies in material, texture, color, weight, detail, and size from the original. Yet, the print is not necessarily meant to duplicate or replace the original sculpture. Its value lies in its ability to capture physical characteristics lost in digital form. Right now, the main way to teach artifacts is by digital photographs (or models when available). In some cases, such privileged photographs and models provide a chance to get a larger and more detailed view of an object than you can in real life (consider the zoom features in the Google Cultural Institute and Artstor ). Still, nothing replaces the opportunity to experience an object or monument in context and in person. While the 3D print (especially the small prototypes) cannot reconstruct this experience, there is potential for 3D printing (especially when an artifact is printed in its actual dimensions) to mimic some of the physical features of the original that are lost in digital form. Students, for example, can interact with it as an object (rather than an isolated image) and analyze its forms in new and critical ways. The opportunity to teach historical content while simultaneously training students to look, analyze, and think critically about the physical world around them through 3D prints is enticing. The next step in this project is to slice the digital model I have and print it as close to life size as possible. Due to the smaller size of the printers, this will require some experimentation with slicing the model, printing enlarged sections separately, and reassembling the parts. I have also requested an order for  sandstone filament —which will better mimic the texture, if not the color of the natural sculpture’s stone. If this medium does in fact enhance pedagogical opportunities for material-based studies, there are almost unlimited opportunities for multiple disciplines to replicate real-world conditions and design more authentic teaching opportunities. ![_DSC0022](http://static.scholarslab.org/wp-content/uploads/2015/09/DSC0022.jpg) All attempted prototype prints of the Hítardalur model."},{"id":"2015-10-19-inktober-105-three-sketches","title":"Inktober 10/5: Three Sketches","author":"ethan-reed","date":"2015-10-19 07:03:49 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"inktober-105-three-sketches","layout":"post","content":"[Cross-posted from my post on our Praxis page .] Wanted to put up a few time sketches for our own version of Inktober . I’m aiming for every other day or so, so here’s three. All three have to do with time and consumption. I love going to the movies, and usually opt for the biggest popcorn possible (bucket) but wonder how differently and quickly the ‘time of eating’ passes depending on how big your eating vessel is (I’m sure plenty of others have written about this - just Googling around quickly I found an entire website dedicated to using smaller plates ). The second one is another thing some people do to pass time together - having a drink at a bar or restaurant, which can go slow or fast. Here I tried to draw the rings that beer leaves behind as a marker of how much time passed where for the beer in this particular glass. Beer time! Reminds me of Slughorn’s Hourglass in Harry Potter, which goes faster or slower depending on the quality of conversation. Last is a cigarette - read a poem for orals by Sherman Alexie with James Dean in it (called “Tourists”), got me thinking of the images of Dean with a cigarette in his red jacket (no color in that photo, but pretty sure that’s the jacket) and got thinking of how so many people frame “breaks” from their day, little slices of time, around something like cigarette “breaks.” As though they were carrying a pack of minutes in their pocket."},{"id":"2015-10-21-inktober-1013-time-pieces-and-graphs","title":"Inktober 10/13: Time Pieces and Graphs","author":"ethan-reed","date":"2015-10-21 10:04:30 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"inktober-1013-time-pieces-and-graphs","layout":"post","content":"[Cross-posted from my post on our Praxis page. ] Hello all - three more images for Inktober. The first is a continuation from my previous Inktober post, very simply, two tubes of toothpaste, both almost empty. Was still thinking of the consumption of objects, for two reasons: first, because of their importance to a historical thinker deeply interested in time (and on my orals list), one Karl Marx, who writes on everything from the length of the working day to labour-time itself here in the first chapter of Capital . And second, because a lot of people use toothpaste in their everyday life. It is something they consume very slowly, and often even forget that they have or that it’s running out; they measure out its consumption bit by bit over the course of many weeks, but its presence over time is ‘at hand,’ or taken for granted in a certain way. To start with Marx though, in the first chapter of Capital  (titled: Commodities) he defines the value of a commodity as “the labour time socially necessary for its production.” He then goes on to quote himself from the Economic &amp; Philosophic Manuscripts of 1844 saying that “As exchange values, all commodities are merely definite quantities of congealed labour-time .” What a phrase! Congealed labour-time . As though the object in hand were physically made up of hours from the lives of others. The idea that an hour of life itself can be estranged from an individual and sold for monetary compensation, and that these hours of others then congeal into an object which sits on my bathroom sink for a set number of weeks is fascinating, and which Marx thinks through seriously, though too much to talk about here. My second point though, that people use toothpaste everyday, has to do with the thinking of another philosopher I’ve been reading while studying for my oral exams - Martin Heidegger . Though he’s interested in his big work  with ontological questions (like, what kind of being does the number 1 have vs. the color brown, a chair, the solar system, a historical event, my cat, etc) he also believes that things ‘are’ for humans (or Dasein in his terminology, the being for whom being is an issue or question) in a way that is more natural. In his thinking, when I see a desk, I don’t go for some abstract, philosophical, Aristotelian route and think of it as a substance with attributes (such as squareness or brownness), I think more of whether or not it is the right size, or close enough to a reading light, or if it is an heirloom that needs to be treated carefully. That these considerations actually make up, in many ways, the ‘being’ of the table itself for Dasein (humans) was fascinating to me when I first started reading. Hence: toothpaste! I brush my teeth twice a day and use toothpaste from a tube that looks like one of these. It is something that fits in my hand and I can tell how much time I have left with this particular tube just by picking it up. Using a brand new tube is very different from the ones I have drawn here, crumpling up tubes to try to make them last, to extend their lives . The “time” of a tube of toothpaste, its lifespan, is relatively slow (or long) compared to, say, a quart of milk or loaf of bread. The same tube persists through many weeks of use, and there is even a certain feeling of ‘letting go’ and beginning again when I have to stop crushing up an old familiar tube and go buy new. And trying a new brand, flavor, or type of toothpaste is a strangely serious time commitment, a decision that persists in your everyday experience (and teeth) for weeks or months. This connects with another picture, those of time pieces. I wanted to draw the things I look at during the day to tell what time it is. When this Dasein thinks of time (me), he probably thinks first of where he looks to ‘tell the time,’ the things I use to make sure that I am not late, or that I am getting enough sleep, or that I remember to eat lunch. For me these are my computer and my phone. My window into Greenwich Mean Time is almost always small numbers on a screen that sometimes fail to update to Daylight Savings. A lot of people have watches though, so I wanted to draw one of them too. I wonder how these objects of mediators of this standard time influence how time actually feels for the humans who use them. Last is more abstract. I was thinking of Johanna Drucker’s work in Graphesis: Visual Forms of Knowledge Production, and particularly an article she wrote for DHQ called “Humanities Approaches to Graphical Display”  in which she gives some examples of how humanists can use graphical representations. So I decided to draw some graphs about how we might experience time in a 24-hour period. I was envisioning that a given person would travel along the path of the line, x-axis being each hour passing, and y-axis being… something else. Experience? Not sure - point being, although the same number of ‘hours’ passed for each line, the length, trajectory, and experience of each line (or sets of lines) was very unique. Would love to draw some more of these, or at least think through them a little more seriously before the month is out."},{"id":"2015-10-28-inktober-1021-when-things-break","title":"Inktober 10/21: When Things Break","author":"ethan-reed","date":"2015-10-28 08:40:22 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"inktober-1021-when-things-break","layout":"post","content":"[Cross-posted from my post on our Praxis page .] The third of my four installments (here’s one and two  of Inktober). And oh boy, get ready for some strange-looking sketches in this one. I tried to use a new kind of pen that has two tips and can make way thicker marks, which has resulted in many many failed drawings and a few semi-successful ones. I present to you the semi-successful ones. (For those interested in drawing implements, I’ve been using a 0.35mm Pigma Micron  but this time tried out a Tombow Dual Brush Pen, N15 black ). The problems I had with my tools this week, believe it or not, fits thematically with what I’ve been trying to draw: when things break. Or when things (tools) don’t work the way you want them to. What does this have to do with time? Let’s look at the first picture I’m sharing, a drawing of one of my shoes. You probably can’t tell from what I drew here, but this shoe is thrashed on the inside. The back heel has completely torn through the cloth and this bluish plastic is exposed. There’s also a split starting on the outer-toe area through the leather but I couldn’t really draw it that well. On a side note, looking so closely at my shoe for so long reminded me of this scene from Don DeLillo’s Underworld - from “Sometimes I think…” to the bottom of p. 543 - in which a boy is asked to name as many parts of a shoe as possible. I, in particular, had problems with the vamp . Point being, the shoe is beat, from toe cap to insole. It’s spent. Both of my shoes are. I’ve worn these sambas practically every day since I bought them and often play soccer in them. I will have to buy a new pair very soon. What’s interesting is the way in which I learn that they need to be replaced: I need to buy a new pair because the shoe, as a tool, has “stopped working.” It’s a trigger of some kind that produces an almost automatic response: thing you need breaks? Time to get a new one! Normally, I don’t think about my shoes at all. They’re comfortable and easy to walk in, so they’re just kind of there - they’re “present at hand” (or “at foot”! Ha!). I know they exist, but I’m no more aware of them than I am of my feet, legs, or hips while walking around. That is, until something goes wrong - the felt wears out and chafes my heel; I start to get blisters; the bottom cushioning wears through; the shoe splits at a side. For Heidegger, when a piece of equipment breaks or malfunctions it acquires a new kind of visibility, as though it were semi-transparent before while being used. (If you’re wondering what Heidegger is doing here or want more context, check out my first and second posts). Most importantly, this new visibility changes how and what the thing is - the implication being that situational contexts actually change what things are  for us (Dasein) in the world. So when my shoe ‘breaks,’ it changes how it is  in the everyday-ness of my world. It stops being a shoe shoe and starts being a trash shoe . Its time is up. In his 1935 essay “The Origin of the Work of Art,” Heidegger himself actually looks at a famous pair of shoes as an example of how an artistic representation of them can pull together and have the power to disclose  a kind of life-world despite simply being ‘equipment.’ And it seems to me that a totally trashed pair of shoes does this very differently than a pair fresh out of the box. So I wonder: when a thing’s lifespan as a thing is over (a lifespan determined in large part by its use context: i.e. a pair of boots that sits in a closet all year vs. pair used by a carpenter or roofer on worksites almost every day), what kind of lifespan does it acquire as something broken? How does the life of a trash  shoe different from a shoe  shoe, assuming that it’s possible these two shoes might be  different things for me? An illustrative example would be something like comparing how we think of the ‘lifespan’ of a car battery with the ‘lifespan’ of a dead car battery : the first has to do with human use, the possibility of travel, cold winter mornings, brand reliability, maintenance, usually a matter of years; the latter with molecular half-lives, recycling practices, landfills, all on a much longer scale. Another sketch with the new pen, this of an abandoned vehicle, engine- and battery-less. With some color! (If you want to see the original I was trying to draw, check it out here ). It’s a kind of mythical image for me. In his song “Bad As Me” Tom Waits calls it simply “the car in the weeds.” So two main questions: on the one hand, how do we conceive of time passing for things that are ‘invisible’ and ‘present-at-hand’ - things that are there in the background almost all the time, but that we take for granted until lost or broken? And on the other, how do we conceive of time passing for things that are out of sight and  mind - things that have been cast aside, like this car is now, or my shoe soon will be? One more: a pencil with a broken tip, and a pencil that has been sharpened way past its point of practical usefulness. Sometimes when things break, when their ‘time is up,’ they acquire a certain kind of urgency. They come to the fore. It’s almost like they come back into existence from some kind of constantly humming background noise. All of the sudden you have to decide what to do with this thing - do you fix it? Replace it with something else? Replace it with a new version of itself? Leave it in the weeds ( or the center of the Pacific )? These questions may feel simple, or perhaps too straightforward, but they are the kinds of questions that matter for Daisen (us, humans, beings for whom being is a question) when it comes to things in our world. And because they matter to us in this way, these questions matter for how these objects are _or _are not  in our worlds - and by world I mean the highly contingent world with which a given person is familiar and in which they live their day-to-day lives, one that is changing all the time. Reminds me of another scene from another 20th century novel, the “Time Passes” section of Virginia Woolf’s To the Lighthouse  (much of which you can read here on Google Books ). Describing a home left more-or-less uninhabited for years, she writes: “So with the house empty and the doors locked and the mattresses rolled round, those stray airs, advance guards of great armies, blustered in, brushed bare boards, nibbled and fanned, met nothing in bedroom or drawing-room that wholly resisted them but only hangings that flapped, wood that creaked, the bare legs of tables, saucepans and china already furred, tarnished, cracked. What people had shed and left - a pair of shoes, a shooting cap, some faded skirts and coats in wardrobes - those alone kept the human shape and in the emptiness indicated how once they were filled and animated; how once hands were busy with hooks and buttons; how once the looking-glass had held a face; had held a world hollowed out in which a figure turned, a hand flashed, the door opened, in came children rushing and tumbling; and went out again. … Loveliness and stillness clasped hands in the bedroom, and among the shrouded jugs and sheeted chairs even the prying of the wind, and the soft nose of the clammy sea airs, rubbing, snuffing, iterating, and reiterating their questions - “Will you fade? Will you perish?” - scarcely disturbed the peace, the indifference, the air of pure integrity, as if the question they asked scarcely needed that they should answer: we remain.” I’d like to think more on just how my busted shoe, the car in the weeds, and the too-short pencil go about ‘remaining’ in the world, and how this might differ from their still-in-use counterparts."},{"id":"2015-11-06-podcast-james-neal","title":"Podcast: James Neal","author":"ronda-grizzle","date":"2015-11-06 05:40:39 -0500","categories":["Podcasts"],"url":"podcast-james-neal","layout":"post","content":"Practicing Digital Humanities Speaker Series: James Neal Public Libraries and Academic Libraries: Digital Partners? James Neal\nDigital Services Librarian\nPrince George’s County Memorial Library System Summary:\nThe growth and development of technology, computers, software, and the Internet have changed the ways in which libraries function, operate, and are being used by the communities they serve. Public libraries have been playing a bit of catch-up in many ways related to the growth of digital services due to the fact that public libraries are also still serving users whose information needs include more traditional resources. Is there a common ground in mission and scope that public libraries and academic libraries serve together? In what ways are these services complementary and what are the ways in which each of these institutions can learn from and share with one another? Speaker Bio:\nJames is a graduate of the MLS program at the University of Maryland College of Information Science, Maryland’s iSchool in the Information and Diverse Populations concentration. His experience at the University of Maryland acquiring the Master of Library Science degree was highlighted and dominated by his participation in the Information and Diverse Populations program. This allowed him to address the issue of underrepresented groups in librarianship and successful ways of working with colleagues and patrons from diverse backgrounds. “The Information and Diverse Populations (IDP) specialization of the College of Information Studies at the University of Maryland focuses on instruction about and research into the design, development, provision and integration of information services, resources, technologies, and outreach that serve diverse populations” (from UMD website). James maintains a strong interest in the future of public libraries, and began working at the Prince George’s County Memorial Library System in August 2014, moving to his current position of Digital Services Librarian in August 2015. James is very active on social media. You can follow him on Twitter, Instagram, Tumblr, Google+, and Pinterest . The Practicing Digital Humanities Speaker Series is sponsored by UVa Library’s Intellectual Crossroads Team, Scholars’ Lab, and the Academic Engagement group. As always, you can listen to (or subscribe to ) our podcasts on the Scholars’ Lab blog, or on iTunesU . [podloveaudio src=”https://itunesu.assets.itunes.com/apple-assets-us-std-000001/CobaltPublic69/v4/39/06/ed/3906ede0-f193-b362-e669-0d1ff87a1a18/313-2999196782334166045-neal.mp3”]"},{"id":"2015-11-12-when-old-technology-meets-the-new-accessing-windows-95-cd-roms-through-wine","title":"When Old Technology Meets The New: Accessing Windows 95 CD-ROMs through Wine","author":"nora-benedict","date":"2015-11-12 08:46:10 -0500","categories":["Grad Student Research"],"url":"when-old-technology-meets-the-new-accessing-windows-95-cd-roms-through-wine","layout":"post","content":"Up to this point in my academic career I have worked primarily with physical books and I certainly feel most comfortable with this medium. Anything remotely technological frightens me and I’m particularly inept when it comes to simple computer issues (think: getting my computer to talk to my printer, resolving internet connectivity issues, etc.). All of this changed drastically a few weeks ago when I found myself desperately in need of an Argentine literary supplement from the 1930s for my dissertation, “The Fashioning of Jorge Luis Borges: Newspapers, Magazines, and Print Culture in Argentina (1930-1951),” which deals with the physical features of Jorge Luis Borges’s works, read through the lens of analytical bibliography and genetic criticism. I would normally InterLibrary Loan the materials, but runs of this particular periodical are very scarce (and, as a result, not circulating). That being said, while researching in Argentina this summer, I happened to purchase a book about the literary supplement in question, complete with a digital edition on CD-ROM. In my naïve state, I thought I could simply pop this disc into any PC and be able to access the files. Wrong. This was a CD-ROM for Windows 95/98 and not adequately suited for the current Windows 10! I soon realized that I would not be able to solve this problem on my own. After firing off a round of frantic emails to recommended experts in old machines at UVa, I wandered into the Scholars’ Lab with my CD-ROM in the hopes that these experts could work their magic and help me gain access to the digitized images contained on it. We first tried simply putting the CD-ROM into a computer with the hopes that the files could easily be extracted or copied onto my machine. No such luck. There was some sort of encoding that resulted in an error message (“access denied!”) every time we clicked on the strange “.jpe” files. Word quickly spread in the Scholars’ Lab that some crazy student was trying to use a CD-ROM for Windows 95 and I met with several experts that were all excited to see the CD-ROM and give a crack at accessing its materials. One of these kind souls, Wayne Graham, was particularly interested in doing everything possible to help me. Soon after handing over the CD-ROM to Wayne, he was able to pull up the program on a PC in the Scholars’ Lab, and I proceed to jumped for joy upon seeing the following screens: Thinking we (and by “we” I mean Wayne) had found a way into the files, we click into the index (“INDICE”) and found a listing of all of the titles published in this literary journal from 1933-1934: YES. THIS MUST BE IT! Nope. Another dead end. When Wayne clicked on any of the above listed files, the same error message (“access denied!”) popped up. Back to the drawing board I went. Trying not to become terribly discouraged, I shared my technological problem with a dear friend, James Ascher (Praxis Fellow 2015-2016), who immediately wanted to take a look at the CD-ROM. We talked about what Wayne had tried and he thought it might be fun to try an emulator. When he asked me to open terminal and I looked blankly at him as if he were speaking Greek, he saw this as the perfect teaching moment to walk me through all of the (complex!) steps Wayne had taken to try and get to those pesky files. James, thus, explained the basics of the command processor, Bash; I must confess that I have never felt more out of my element. I learned about simple codes like “pwd,” “cd Documents,” “ls,” and “cd ~,” among others. James had me play around with these commands for a bit until I felt comfortable enough to move in and out of directories with ease: He also explained how “mkdir” and “rmdir” can be useful tools, but only when used cautiously since one incorrect key stroke could remove an entire directory. While I was getting familiar with giving my computer commands, James started to run the CD-ROM on an emulator (a program that allows one computer system to behave like another) on his computer. No luck. That same error message kept popping up again! After working through several other ideas and reading through the meta-data on the files, James came to the conclusion that we should copy all of the files from the CD-ROM to the UVa BOX for storage (and then I wouldn’t have to keep bringing the disc to grounds everyday). In the process James tried the emulator a second time with the copied files on his computer and we discovered that the error message had something to do with the physical disc itself. SUCCESS! Instead of receiving an error message when he clicked on a random file, we were both presented with the following: The next challenge would be installing and teaching me how to emulate the program files on my own computer (i.e. the REAL challenge). My first task was to “install Homebrew” by copying the following command into Bash: Easy enough! I was on my own for the next set of instructions (the real test!). After brew was installed, I typed gave my computer the following command to install Wine, the emulator I would use to run the program: All seemed to be going according to plan and, after letting the program install completely (about an hour or so), I restarted terminal and was very excited to start my very own simulation. Unfortunately, when I typed the command to run the program files through wine, I got a NEW error message: NO SUCH FILE OR DIRECTORY?? I tried not to panic and, instead, went about searching various directories for the installed wine. With some guidance from James, I eventually found it in the cellar (quite an apt place for wine, right??). Having located the program, I now needed to download the text editor, Atom, to tell my computer where to find wine (and thus run the emulator): After completing this intermediary step (there were a few other minor hiccups along the way), I was ready to roll! With a few simple keystrokes I now had access to the entire periodical run (1933-1934) from the convenience of my very own computer. Even though the images were digitized in 1999, I was extremely impressed by their quality and the ability to zoom in and out on each and every page. Eager to share my success with Wayne (and, of course, thank him immensely for his help and patience), I dropped by the Scholars’ Lab and showed how I could now access all of the material from my very own computer. While I was typing my commands into Bash, Wayne decided to help me write myself a shortcut to make the process even quicker: Now all I have to do is enter my terminal and type “rms” and the entire program launches. Mission accomplished!: I don’t think I can thank the Scholars’ Lab enough for all of their continued help and encouragement along the way. If it weren’t for them, I wouldn’t have the ability to access these files from the convenience of my own computer and describe the fascinating changes that Borges’s early fictions take from their first publications in this periodical (1933-1934) to later inclusion in book form (1935)."},{"id":"2015-11-17-3d-printing-historical-objects-enhancing-the-qualities-inherent-to-the-past","title":"3D Printing Historical Artifacts: Enhancing the Qualities Inherent to the Past","author":"margaret-furr","date":"2015-11-17 10:28:50 -0500","categories":["Digital Humanities","Makerspace"],"url":"3d-printing-historical-objects-enhancing-the-qualities-inherent-to-the-past","layout":"post","content":"  Earlier this fall semester, I ventured to test out the Makerspace’s 3D printer by reproducing a 3D version of Kepler’s platonic solid model . This model was a historical object that I desired to examine in physical form while taking a class on the Scientific Revolution. I desired to study the artifact in such a way so that I would be able to hold the object in my hands and examine it from any perspective, rather than observe only a part of the object in an image, from one perspective. While I had this desire, I didn’t think that I would ever gain the opportunity to study the model in physical form. Everything changed when I became acquainted with both the online 3D printing community and the Makerspace’s 3D printer. The online community enabled me to download the model’s 3D version that a maker had already digitized, and the printer liberated me to output a 3D physical artifact from the 3D digital model. As I downloaded and printed Kepler’s platonic solid model, I began to question whether the replicated versions of objects preserved any of the qualities inherent to the original model. My main question, as I downloaded and printed Kepler’s platonic solid model, was concerning the extent to which the printed model was genuine, and authentic, in comparison to the original image. I questioned whether the 3D printed versions preserved the historical work’s inherent quality or rather carried different qualities. I asked this question as (1) I contemplated how Kepler, in the time period he lived in, would never have been able to use digital technologies to design such a model in order to understand the universe’s nature, and (2) I worked to use the right types of supports and overhangs to print the model effectively. Encountering other applications of 3D printing technologies since asking this question, however, I have come to realize that rather than minimizing the extent to which a historical work’s quality is preserved, 3D printing expands the qualities inherent to the past, opening up the past to more people in new ways. The application of 3D printing technology that I recently encountered was an example in which 3D technologies were used to help blind people see, or experience art through tactile means. After feeling inspired by this application of 3D printing technology, I challenged myself to extend my thoughts beyond how much a 3D print preserves a historical piece of work, like Kepler’s platonic solid model, which he drew out as an image, and rather consider the experience that people can gain from exploring something as tactile as a 3D print. By doing so, I returned to the initial reason for my desiring to print the model – my hoping to touch the object and examine how it fit into the 16th-17th century from a physical perspective – and decided that I think that 3D printing enhances the qualities inherent to historical works. I’m curious though, what has your experience been with 3D printing historical artifacts?"},{"id":"2015-11-18-a-former-fellows-new-adventures-in-data-science","title":"A Former Fellow's New Adventures in Data Science","author":"claire-maiers","date":"2015-11-18 06:51:23 -0500","categories":null,"url":"a-former-fellows-new-adventures-in-data-science","layout":"post","content":"Hello All.  I’m happy to report that after several years away, I’ll be blogging at the Slab again this year.  Thanks in large part to my experience with the Praxis Fellowship, I was selected to participate in the UVa Presidential Fellowship in Data Science .  Over the course of the next year, I will be collaborating with a PhD student in systems engineering on a data-driven project.  From time to time, I’ll post updates about struggles, questions, and successes with the project.  Today, I’m going to start with a basic description of what we think we might be up to: In a nutshell, Nick and I will be trying to model the patterns by which ideas move through social systems.  There already exists a robust literature within sociology that focuses on identifying how and when ideas will be adopted and spread.  But there are some shortcomings to this body of work.  Existing research tends to focus on the adoption and diffusion ideas.  There are not many studies that consider the persistence or the decline of an idea.  This means that we don’t have holistic models that capture the entire lifespan of ideas. Nick and I will be taking a stab and remedying this situation by looking at just one social system; we’ll be tracking concepts and methods over time within academia.  Our first goal is to generate a typology of the common patterns by which ideas are adopted, persist, and decline.  If we manage that, our next task will be to develop a predictive model for when a concept or method will jump to a new discipline. We will be using the text of articles contained in the JSTOR database, and we’ll be experimenting with a couple of methods.  One option is to use a supervised method.  We’ll generate a list of terms consisting of significant concepts and methods and then track those overtime through the database.  The second option is to use topic modelling to generate a list of topics and then track those over time.  If you’re thinking that this project has some similarities to Goldstone and Underwood’s PMLA project, you’re right. It does.  If you’re not familiar, check out their excellent post to learn more about topic modeling. We’ve already had a number of challenges in accessing and thinking about how to structure our data….look for more on that in future posts."},{"id":"2015-12-03-reflections-on-a-year-of-dh-mentoring","title":"Reflections on a Year of DH Mentoring","author":"brandon-walsh","date":"2015-12-03 09:43:19 -0500","categories":["Digital Humanities","Grad Student Research","Research and Development"],"url":"reflections-on-a-year-of-dh-mentoring","layout":"post","content":"[ Cross-posted on the Digital Humanities at Washington and Lee University blog ] This year I am working with Eric Rochester  on a fellowship project that has me learning natural language processing (NLP), the application of computational methods to human languages. We’re adapting these techniques to study quotation marks in the novels of Virginia Woolf (read more about the project here ). We actually started several months before this academic year began, and, as we close out another semester, I have been spending time thinking about just what has made it such an effective learning experience for me. I already had a technical background from my time in the Scholars’ Lab at the beginning of the process, but I had no experience with Python or NLP. Now I feel most comfortable with the former of any other programming language and familiar enough with the latter to experiment with it in my own work. The general mode of proceeding has been this: depending on schedules and deadlines, we meet once or twice every two weeks. Between our meetings I would work as far and as much as I could, and the sessions would offer a space for Eric and me to talk about what I had done. The following are a handful of things we have done that, I think, have helped to create such an effective environment for learning new technical skills. Though they are particular to this study, I think they can be usefully extrapolated to apply to many other project-based courses of study in digital humanities. They are primarily written from the perspective of a student but with an eye to how and why the methods Eric used proved so effective for me. Let the Wheel Be Reinvented Before Sharing Shortcuts I came to Eric with a very small program adapted from Matt Jockers’s book on_ Text Analysis with R for Students of Literature_ that did little beyond count quotation marks and give some basic statistics. I was learning as I built the thing, so I was unaware that I was reinventing the wheel in many cases, rebuilding many protocols for dealing with commonly recognized problems that come from working with natural language. After working on my program and my approach to a degree of satisfaction, Eric pulled back the curtain to reveal that a commonly used python module, the Natural Language ToolKit ( NLTK ), could address many of my issues and more. NLTK came as something of a revelation, and working inductively in this way gave me a great sense of the underlying problems the tools could address. By inventing my own way to read in a text, clean it to make its text uniformly readable by the computer, and breaking the whole piece into a series of words that could be analyzed, I understood the magic behind a couple lines of NLTK code that could do all that for me. The experience also helped me to recognize ways in which we would have to adapt NLTK for our own purposes as I worked through the book. Have a Plan, but Be Flexible After discussing NLTK and how it offered an easier way of doing the things that I wanted, Eric had me systematically work through the NLTK book for a few months. Our meetings took on the character of an independent study: the book set the syllabus, and I went through the first seven chapters at my own pace. Working from a book gave our meetings structure, but we were careful not to hew too closely to the material. Not all chapters were relevant to the project, and we cut sections of the book accordingly. We shaped the course of study to the intellectual questions rather than the other way around. Move from Theory to Practice / Textbook to Project As I worked through the book, I was able to recognize certain sections that felt most relevant to the Woolf work. Once I felt as though I had reached a critical mass, we switched from the book to the project itself and started working. I tend to learn from doing best, so the shift from theory to execution was a natural one. The quick and satisfying transition helped the work to feel productive right away: I was applying my new skills as I was still learning to feel comfortable with them. Where the initial months had more the feel of a traditional student-teacher interaction, the project-based approach we took up at this point felt more like a real and true collaboration. Eric and I would develop to-do items together, we would work alongside each other, and we would talk over the project together. Document Everything Between our meetings I would work as far and as much as I could, carefully noting places at which I encountered problems. In some cases, these were conceptual problems that needed clarifying, and these larger questions frequently found their way into separate notes. But my questions were frequently about what a particular line of code, a particular command or function, might be doing. In that case, I made comments directly in the code describing my confusion. I quickly found that these notes were as much for me as for Eric–I needed to get back in the frame of mind that led to the confusion in the first place, and copious notes helped remind me what the problem was. These notes offered a point of departure for our meetings: we always had a place to start, and we did so based on the work that I had done. Communicate in as Many Ways as Possible We met in person as much as possible, but we also used a variety of other platforms to keep things moving. Eric and I had all of our code on GitHub so that we could share everything that we had each been working on and discuss things from a distance if necessary. Email, obviously, can do a lot, but I found the chat capabilities of the Scholars’ Lab’s IRC channel to be far better for this sort of work. If I hit a particular snag that would only require a couple minutes for Eric to answer, we could quickly work things out through a web chat. With Skype and Google Hangouts we could even share the code on the other person’s computer even from hundreds of miles away. All of these things meant that we could keep working around whatever life events happened to call us away. Recognize Spinning Wheels These multiple avenues of communication are especially important when teaching technical skills. Not all questions or problems are the same: students can work through some on their own, but others can take them days to troubleshoot. Some amount of frustration is a necessary part of learning, and I do think it’s necessary that students learn to confront technical problems on their own. But not all frustration is pedagogically productive. There comes a point when you have tried a dozen potential solutions and you feel as though you have hit a wall. An extra set of eyes can (and should) help. Eric and I talked constantly about how to recognize when it was time for me to ask for help, and low-impact channels of communication like IRC could allow him to give me quick fixes to what, to me at least, seemed like impossible problems. Software development is a collaborative process, and asking for help is an important skill for humanists to develop. In-person Meetings Can Take Many Forms When we met, Eric and I did a lot of different things. First, we would talk through my questions from the previous week. If I felt a particular section of code was clunky or poorly done, he would talk and walk me through rewriting the same piece in a more elegant form. We would often pair program, where Eric would write code while I watched, carefully stopping him each time I had a question about something he was doing. And we often took time to reflect on where the collaboration was going - what my end goal was as well as what my tasks before the next meeting would be. Any project has many pieces that could be dealt with at any time, and Eric was careful to give me solo tasks that he felt I could handle on my own, reserving more difficult tasks for times in which we would be able to work together. All of this is to say that any single hour we spent together was very different from the last. We constantly reinvented what the meetings looked like, which kept them fresh and pedagogically effective. This is my best attempt to recreate my experience of working in such a close mentoring relationship with Eric. Obviously, the collaboration relies on an extremely low student-to-teacher ratio: I can imagine this same approach working very well for a handful of students, but this work required a lot of individual attention that would be hard to sustain for larger classes. One idea for scaling the process up might be to divide a course into groups, being training one, and then have students later in the process begin to mentor those who are just beginning. Doing so would preserve what I see as the main advantage of this approach: it helps to collapse the hierarchy between student and teacher and engage both in a common project. Learning takes place, but it does so in the context of common effort. I’d have to think more about how this mentorship model could be adapted to fit different scenarios. The work with Eric is ongoing, but it’s already been one of the most valuable learning experiences I have had."},{"id":"2015-12-03-the-ghost-in-the-graph-a-recap-on-time-things-and-entanglement","title":"The Ghost in the Graph: A Recap on Time, Things, and Entanglement","author":"ethan-reed","date":"2015-12-03 09:05:13 -0500","categories":["Digital Humanities","Experimental Humanities","Grad Student Research","Visualization and Data Mining"],"url":"the-ghost-in-the-graph-a-recap-on-time-things-and-entanglement","layout":"post","content":"[This post is the protein-rich version of a series of related posts from our Praxis site, with fresh reflections on the process and product now that I’m done. If you want to see originals, check out the project idea, the data itself as I recorded it, a first attempt at a visualization, and a second attempt at a series of visualizatio ns .] Time through things. This was the motivating idea behind a week-long project I started at the beginning of November for Praxis. Everyone on the team decided to track, monitor, or experiment with lived time in one way or another for a full week. James looked at clouds ; Lydia at music ; Gillet at time indoors and outdoors ; Bremen at policing and affect; Rachel at language use . I chose to think about time through things. So I wrote down in a notebook everything I used in a seven day period, from when I woke up to when I went to bed. Obviously what counts as “use” and what counts as a “thing” gets conceptually gritty very quickly. To stay sane, I took them in their most intuitive, ordinary senses, which means Yes, my methodology was in some ways arbitrary, but also Yes, I managed not to go nuts while seeing it through. So the data is not perfect, but it’s there! Why did I want to do this? If you have a chance to look at my other posts this year related to Praxis’s current explorations of time and ways of representing it ( When Things Break, Time Pieces, and Three Sketches in particular) it becomes clear I’m interested in nonhuman and what some might call posthuman ways of thinking about time. For me, this means I’m thinking about the way time works for stuff, things, and entities that aren’t people. Which can be difficult because as humans we tend to anthropomorphize everything - humans think through human lenses. Jacques Derrida famously argued that Western philosophy itself is anthropomorphic (and ethnocentric) - others have argued related things in different venues . And these ideas make a lot of sense. If the human species were physically different (blind but great sense of smell; two brains per body; a strain of bacteria; four-dimensional ) our understandings of lots of things – basically everything – would be affected. But even if it’s a difficult project, thinking of nonhuman and posthuman time also feels like an important project. For example, this kind of thinking might help us wrap our heads around our role in systems or entities bafflingly larger than us, like the geologic time of our planet and our power as a species to shape its geologic future. For a few of my favorite examples of this kind of thinking, see  Timothy Morton, Dipesh Chakrabarty,  Bethany Nowviskie  and  China Miéville . A less “planetary” example would be re-thinking how we as decision-making entities are influenced or “made to do” things by the non-human entities that surround us. As Bruno Latour wonders : are you smoking the cigarette or does the cigarette smoke you? Well, as he says: both. So how am I trying to think about posthuman or nonhuman time? To answer this, let’s look at the data and what I did with it. My data, as presented on the site  in big blocks of words, almost looks like a kind of poetry (maybe  uncreative poetry in the vein of Kenneth Goldsmith). I took this strange data and did my best to represent it visually, both for practice with d3 tools and also to eke more meaning out of what I’d done. For this visualization and the ones below, I’m posting images rather than the graphs themselves - if you want to play around with the sometimes sluggish originals, check out the links at the top of this post. For my first attempted visualization, I borrowed from Jason Davies’ “Parallel Sets” visualization (with significant help from Wayne ). You can find here the github page I took Davies’ code from originally, and his  license here . Alas, I began with his beautiful graph and turned it quickly into an incomprehensible scribble. Behold: As a visualization of data, what I made is pretty close to nonsensical. And also unwieldy - the original doesn’t load right and can slow the page down significantly. This monstrosity didn’t come as a surprise to me, as I didn’t clean my data or prep it for what the code expected. I more or less just tried to crowbar my data into Davies’ code/setup until something came out the other end that looked anything remotely like a graph. So while I don’t know exactly what’s happening here, things are happening. Some sort of nightmare causality is at work, even if only my laptop really knows what’s going on (or not going on) as it tries to make sense of what I’m feeding it. But when I take a step back, what it comes up with also feels kind of poetic, almost like the data itself. When you mouse over one of the catastrophic clusters and happen upon a single thread, the graph tries to produce a new narrative of objects for you. The pseudo-stories are wandering and garbled, but also charming and original. For example: “spoons -&gt; mouse pad -&gt; backpack -&gt; chair -&gt; yogurt -&gt; metro card.” Or my personal favorite: “pillow -&gt; sheets -&gt; sheets -&gt; sheets -&gt; sheets -&gt; sheets.” More useful, I think, are my  second round of visualizations  . These are force-directed graphs, also from d3js.org, one of Mike Bostock’s many visualizations . As with Davies’ parallel sets graph, I used what Bostock had up on d3, replaced his data set with mine, and with a lot of help from Eric figured out how to get it to read my .csv file. With this method, I produced one force-directed graph for each day and a sprawling, magnificent mess at the end combining all seven days. In the graphs, each node (or point) represents a “thing” that I used. If you mouse over the node you can see which thing it represents. Every edge (or connecting line) represents a connection between those things – in this case, a connection between A and B means that A was used right after or right before B in my linear data. This means that the linearity, as well as the order of use, is collapsed in these representations. What we’re left with, however, is something new and potentially worth looking at on its own. Different days have different shapes. Thursday’s things live in big billowing petals that loop out on long, solitary walks of minimal connection: Sunday’s things live in much tighter, centralized clusters – most activity is shared activity, a miniature city of things: But what do these static, force-direct graphs of relationships between things have to do with time? According to what I gather on the subject, modern physics has some counter-intuitive insights to offer regarding our ordinary bodily understanding of time, such that our intuited experience of it doesn’t necessarily correspond to how events happen in the nether realms of relativity. For example, according to the relativity of simultaneity, one observer might see A -&gt; B -&gt; C (with accurate measurements), while another with equal accuracy observes B -&gt; C -&gt; A. But while orders of events may be muddled, the fact of causality remains. So in one way of thinking, time for a person is less something that passes by moment to moment in a linear progression than it is a static line segment of every casually linked event, all existing simultaneously. In which case, static, simultaneous representations of multiple events might actually have an unusual kind of purchase when it comes to representing causal/temporal relationships. So what happens when we try to think about this with regards to stuff ? I’m thinking about causal relations between things that might not involve humans. For example, if humans were to disappear tomorrow (as in Weisman’s_ The World Without Us ), _things would very well continue to interact with one another. To expand on examples from my data: a pillow getting heavier, mustier, and moldier as moisture creeps into a dilapidating room, staining the sheets on which it rests, while both weigh down on a mattress whose metal springs start to rust and give (pillow -&gt; sheets -&gt; mattress); freeze-thaw cycles crack and crumble sections of road until a telephone pole tips and snaps onto the hood of a parked convertible, sending glass onto the street (road -&gt; pole -&gt; car); and so on. Other writers have thought about this before: in a previous post I linked to Virginia Woolf’s depiction in To the Lighthouse of a home left uninhabited for years. In Ray Bradbury’s “There Will Come Soft Rains” from The Martian Chronicles (not available as text online, but awesomely available as a recording read by Leonard Nimoy ), we watch from a disembodied point of view as a futuristic home in California, long abandoned, slowly breaks down, from its automated kitchenware and mouse-Roombas to the automated poetry-reading voice in the bedroom. Point being, things can very well interact with one another without humans. There may be no humans to perceive them and thus classify these encounters as between discrete, meaningful things, but from our current vantage we can at least imagine them. In this sense, these thing-graphs are speculative. That said, there is a ghost in the graph – a body, my body, invisible save for traces left in having connected thing-nodes through use-edges. But in these graphs, the time that body spent making those connections evaporates, like steam from a cup of coffee. And what’s left is the cup of coffee itself - and all its associates. Though these associates were used in a specific linear order, I wanted instead to think of them as bound together causally, all entangled simultaneously within a given frame of reference (24 hours). Is this a chronology? Is it a timeline? A time network? Regardless of observers arguing over linear orders-of-events, these things on this day have been strung together. A glass of water and a faucet, a toothbrush and toothpaste: somehow, someway, they were (are!) all tangled up with one another. Can I ever really think of “thing time” without using my very human body to think it through and write it all down? Of course not. The ghost is in the graph – the ghost (with a lot of help) put the graph online, is talking to you about the graph right now. But I can certainly try to imagine how we might think of these entanglements in less anthropomorphic ways, to de-center (in  the vein of Latour ) the role of the human as sole prime agent, mover of all things. Rather, I have tried to show how these objects have lives of their own - and how, as active shaping forces, these lives are causally, temporally, entangled with ours."},{"id":"2015-12-08-classical-archaeology-and-the-makerspace","title":"Classical Archaeology and the Makerspace","author":"jennifer-grayburn","date":"2015-12-08 08:06:41 -0500","categories":["Digital Humanities","Grad Student Research","Makerspace"],"url":"classical-archaeology-and-the-makerspace","layout":"post","content":"Cross-posted on my personal blog. A few weeks ago, R. Benjamin Gorham, a Ph.D. candidate in Classical Art &amp; Archaeology at the University of Virginia, visited the Makerspace for a consultation on photogrammetry and 3D printing. Ben has been using GIS, drones, and photogrammetry during his summer excavations in Morgantina, Sicily and wanted to experiment with 3D printing his models. The physical reconstruction of archaeological sites offers exciting opportunities for both teaching and research, and I asked Ben to share a bit about his digital project: The American Excavations at Morgantina: Contrada Agnese Project is an ongoing archaeological investigation at the ancient Graeco-Roman city of Morgantina, Sicily. As the Supervisor of Geospatial Studies at The Contrada Agnese Project, my goal is to translate the data from the field into useful, visual forms which can be studied, measured, and employed in publications and conferences. Part of this duty involves the creation, curation, and testing of a GIS database which serves as a nexus for all geographic data acquired in the field, including findspots, architectural features, and aerial imagery. Using a quadcopter drone we capture hundreds of images from the air every day, between 5 and 50 meters above our ongoing excavations. Agisoft Photoscan allows us to then combine these images with photos taken on the ground to create 3D models of our trenches and extant architecture, which are then hosted on our website and embedded in our GIS document. The Scholars’ Lab at UVa has allowed us to take this one step further, through the production of 3D-printed models of our trenches. We are using the Makerspace to generate hand-held versions of the buildings and trenches which are part of our ongoing excavations. This enables us to preserve every season’s results in a physical form. Since archaeology is an inherently destructive science, we are constantly removing, changing, and re-burying the stratigraphic records in the soil which we study in order to reveal more about the past, and these 3D-printed models serve as instructive units which can be examined, shared, and explored long after our project has either backfilled or dug past interesting features and periods of ancient history. This creates a permanent physical record of our project which would otherwise be partially lost every time our season concludes for the summer. The Contrada Agnese Project is currently taking applications for Summer 2016 student volunteers. Please contact Ben, rbg8jn@virginia.edu, or consult the application flier for more details. [![Untitled](http://static.scholarslab.org/wp-content/uploads/2015/12/Untitled.jpg)](http://static.scholarslab.org/wp-content/uploads/2015/12/Untitled5.jpg) Photogrammetry can only model what is visible in photographs, and Ben’s initial model only included the surface layer of the ground and trenches. The topographical irregularities, however, would not be possible to print without substantial supports. [![Untitled1](http://static.scholarslab.org/wp-content/uploads/2015/12/Untitled1.jpg)](http://static.scholarslab.org/wp-content/uploads/2015/12/Untitled1.jpg) To close the model and make it suitable for 3D printing, we needed to extend the y-axis so it lay flat on the printing platform. We imported the model into Meshmixer and used the Extrusion editing function to extend the base to the depth of the trenches. [![Untitled3](http://static.scholarslab.org/wp-content/uploads/2015/12/Untitled3.jpg)](http://static.scholarslab.org/wp-content/uploads/2015/12/Untitled4.jpg) We uploaded our model into Cure to slice it and generate the gcode for the Ultimaker 2. [![Untitled4](http://static.scholarslab.org/wp-content/uploads/2015/12/Untitled4.jpg)](http://static.scholarslab.org/wp-content/uploads/2015/12/Untitled4.jpg) Unfortunately, this was the beginning of the end of our Ultimaker’s initial golden age of printing. We began experiencing extrusion problems, which is evident from our first attempt to print. [![Untitled5](http://static.scholarslab.org/wp-content/uploads/2015/12/Untitled5.jpg)](http://static.scholarslab.org/wp-content/uploads/2015/12/Untitled5.jpg) We switched to the Makerbot software and our Makerbot Dual printer to complete the print. [![Untitled6](http://static.scholarslab.org/wp-content/uploads/2015/12/Untitled6.jpg)](http://static.scholarslab.org/wp-content/uploads/2015/12/Untitled6.jpg) While the detail quality is not as clear as we would like, we were able to generate a successful print showing all of the trenches and topographical features. As we fix the Ultimaker 2, we will continue to experiment with printing size and quality to meet the project's needs."},{"id":"2015-12-17-learning-to-use-3d-printers-for-the-digital-humanities","title":"Learning to Use 3D Printers for the Digital Humanities","author":"margaret-furr","date":"2015-12-17 11:03:21 -0500","categories":["Digital Humanities","Experimental Humanities"],"url":"learning-to-use-3d-printers-for-the-digital-humanities","layout":"post","content":"As someone who was primarily educated as a humanist and has also worked on projects involving data, I have experience in courageously facing the steep curve of learning new technologies. Curious about both the arts and the sciences and seeing where the two can enhance each other to assist in better, more critical thinking, I am passionate about learning technologies for the humanities. I must admit that when I first started learning how to use the 3D printers, which I was fascinated by from mechanical and philosophical viewpoints, I worried about breaking them or failing at a print. The day that someone, who was using one of the printers made me aware that the nozzle was clogged with filament was the day that I realized that when learning to fix printer and printing problems, I had to be fierce, have a sense of humor, and be willing to learn while not knowing 100%. I have carried this attitude into printing as I have overcome challenges related to cleaning out the nozzle, not being afraid to pull components apart, exploring alternative ways to develop a model, such as a model for visualizing data, figuring out how to print something in a way that removes stringiness, and addressing the problem of a too hot printer plate. Carrying this attitude pays off because as digital data and information are increasingly available for the humanities and also in other fields, new ways to understand that data and information through artistic prints can be of value. As humanists, including myself, continue to be printing warriors, facing initial fears such as “a 3D printer, what is this?” or “data? how does that relate to art and humanities scholarship?”, I think that necessary bridges between artists and humanists and technologists and engineers can be strengthened. I also see how appreciation for different disciplines and how disciplines work together can be increased. One way that I myself did this semester was by printing data on DC population sizes across districts. As scholars refine data available on attributes of locations at different historical periods, I envision classroom engagement increasing by understanding historical trends across locations with 3D maps. Check out the map with stringiness on the left and the cleaner map on the right below!"},{"id":"2016-01-14-working-with-d3-part-one","title":"Working with D3, Part One.","author":"ammon-shepherd","date":"2016-01-14 08:15:41 -0500","categories":["Visualization and Data Mining"],"url":"working-with-d3-part-one","layout":"post","content":"Track-n-Treat Halloween is great. Free candy. And I have six kids to go out and get it for me. :) I cull some of the finest chocolates from their bags after trick-or-treating and enjoy them throughout the next week. We usually eat everything within a week… This year I decided to track how much candy I ate, and which ones, in the week after Halloween. It was only because we needed to do something like this for Praxis . Otherwise I eat candy without a second thought. Getting started with d3 D3 is a Decent tool for visualizing Data, hence the name Data-Driven Documents. It is basically a JavaScript framework for making charts, graphs, maps, or anything you can images, based on data from a file, database, etc. The best way to learn d3 is to practice it over and over. I suggest looking at one or both of these d3 tutorials first. https://www.dashingd3js.com/table-of-contents http://alignedleft.com/tutorials/d3/about After you’ve been through one, or both of them, try looking at my examples here: https://github.com/mossiso/track-n-treat Making the chart This post is an expanded version of the actual code files which contain lots of comments. A note about data How you structure the data is actually pretty important. It can make it super easy or super hard to get at the information you want. The most simple method usually is the best way to start. In my case, every time I ate a piece of candy, I took a note of what day it was, what time, and which candy. Something like this: day: 2015-11-01, time: 11:15, candy: Snickers As it so happens, this is a great format. Each day on it’s own line. The data gets a little redundant, the day and time information are repeated many times, but it makes it super easy to read and we can easily manipulate it later with d3. For d3 to use this, we have several options. CSV and JSON are the easiest to work with, so I’ll pick one of them. A CSV option would look like this: &lt;code&gt;2015-11-01, 13:15, Snickers\n2015-11-01, 15:24, KitKat\n2015-11-02, 19:33, Twix\n2015-11-02, 17:42, KitKat\n2015-11-03, 21:51, Snickers\n&lt;/code&gt; A JSON option is a bit more intense. Here I’m creating an array of objects. [ \n   {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;: &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;2015-11-01&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;: &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;17:25&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;: &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;Heath Bar&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;},\n   {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;: &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;2015-11-01&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;: &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;17:25&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;: &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;Kitkat Bar&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;},\n   {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;: &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;2015-11-01&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;: &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;20:25&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;: &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;Kitkat Bar&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;},\n   {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;: &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;2015-11-02&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;: &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;17:38&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;: &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;Whopper&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;}, \n   &lt;span class=\"pl-ii\"&gt;...&lt;/span&gt; \n ] This is an array (designated with square brackets [] ) of objects (designated with curly braces {} ). The object consists of names (day, time, candy) and their associated values. If we assign the above to the variable ‘array’, we can access the first element of the array with array[0] . That would return the first line above. A good tutorial on JavaScript arrays is at Lynda.com (free subscription through the library ). Search for ‘Introducing the JavaScript Language with Joe Chellman’. To access any element within the object, we can use dot notation. array[0].day would return 2015-11-01 . array[0].time would return 17:25 . array[3].candy would return Whopper . What would array[2].time return? How would you get the value “Kitkat Bar” (there are two ways)? Setting up the document First off, we’ll create a generic HTML page with the standard HTML &lt;!DOCTYPE html&gt;\n&lt;&lt;span class=\"pl-ent\"&gt;html&lt;/span&gt; &lt;span class=\"pl-e\"&gt;lang&lt;/span&gt;=&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;en&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&gt;\n    &lt;&lt;span class=\"pl-ent\"&gt;head&lt;/span&gt;&gt;\n        &lt;&lt;span class=\"pl-ent\"&gt;meta&lt;/span&gt; &lt;span class=\"pl-e\"&gt;charset&lt;/span&gt;=&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;utf-8&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&gt;\n        &lt;&lt;span class=\"pl-ent\"&gt;title&lt;/span&gt;&gt;D3 Test&lt;/&lt;span class=\"pl-ent\"&gt;title&lt;/span&gt;&gt;\n        &lt;&lt;span class=\"pl-ent\"&gt;link&lt;/span&gt; &lt;span class=\"pl-e\"&gt;href&lt;/span&gt;=&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;styles.css&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt; &lt;span class=\"pl-e\"&gt;rel&lt;/span&gt;=&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;stylesheet&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt; &lt;span class=\"pl-e\"&gt;type&lt;/span&gt;=&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;text/css&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt; /&gt;\n&lt;span class=\"pl-s1\"&gt;        &lt;&lt;span class=\"pl-ent\"&gt;script&lt;/span&gt; &lt;span class=\"pl-e\"&gt;type&lt;/span&gt;=&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;text/javascript&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt; &lt;span class=\"pl-e\"&gt;src&lt;/span&gt;=&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;d3/d3.v3.js&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&gt;&lt;/&lt;span class=\"pl-ent\"&gt;script&lt;/span&gt;&gt;&lt;/span&gt;\n    &lt;/&lt;span class=\"pl-ent\"&gt;head&lt;/span&gt;&gt;\n    &lt;&lt;span class=\"pl-ent\"&gt;body&lt;/span&gt;&gt;\n    &lt;/&lt;span class=\"pl-ent\"&gt;body&lt;/span&gt;&gt;\n&lt;/&lt;span class=\"pl-ent\"&gt;html&lt;/span&gt;&gt; Everything is pretty standard. The &lt;script&gt; tag is what pulls in the d3 library from the folder named ‘d3’. &lt;script type=\"text/javascript\" src=\"d3/d3.v3.js\"&gt;&lt;/script&gt; Next we create a couple of divs, one for placing some text and links, and another that we’ll use to attach our graph to. This code goes in between the &lt;body&gt; and &lt;/body&gt; tags in the code above. &lt;&lt;span class=\"pl-ent\"&gt;div&lt;/span&gt; &lt;span class=\"pl-e\"&gt;id&lt;/span&gt;=&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;wrap&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&gt;\n        &lt;&lt;span class=\"pl-ent\"&gt;h1&lt;/span&gt;&gt;Track-n-Treat&lt;/&lt;span class=\"pl-ent\"&gt;h1&lt;/span&gt;&gt;\n        &lt;&lt;span class=\"pl-ent\"&gt;p&lt;/span&gt;&gt;How many candies did I eat each day the week after Halloween?&lt;/&lt;span class=\"pl-ent\"&gt;p&lt;/span&gt;&gt;\n        &lt;&lt;span class=\"pl-ent\"&gt;div&lt;/span&gt; &lt;span class=\"pl-e\"&gt;id&lt;/span&gt;=&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;graph&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&gt;&lt;/&lt;span class=\"pl-ent\"&gt;div&lt;/span&gt;&gt;\n      &lt;/&lt;span class=\"pl-ent\"&gt;div&lt;/span&gt;&gt; Diving into D3 The rest of the code is written in JavaScript. The JavaScript can live anywhere in the code: in between the &lt;header&gt; and &lt;/header&gt; tags, or anywhere in between the &lt;body&gt; and &lt;/body&gt; tags. I just put it after the above code after the &lt;body&gt; tag. To have JavaScript in the body of the HTML document, we’ll surround it with &lt;script&gt; tags like so: &lt;span class=\"pl-s1\"&gt;&lt;&lt;span class=\"pl-ent\"&gt;script&lt;/span&gt; &lt;span class=\"pl-e\"&gt;type&lt;/span&gt;=&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;text/javascript&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&gt;&lt;/span&gt;\n&lt;span class=\"pl-s1\"&gt; &lt;span class=\"pl-c\"&gt;/* Add some JavaScript here.&lt;/span&gt;&lt;/span&gt;\n&lt;span class=\"pl-s1\"&gt;&lt;span class=\"pl-c\"&gt;    '/*' starts a multi-line comment, and the next line ends it.&lt;/span&gt;&lt;/span&gt;\n&lt;span class=\"pl-s1\"&gt;&lt;span class=\"pl-c\"&gt;*/&lt;/span&gt;&lt;/span&gt;\n\n&lt;span class=\"pl-s1\"&gt;&lt;span class=\"pl-c\"&gt;// This is a single line comment&lt;/span&gt;&lt;/span&gt;\n&lt;span class=\"pl-s1\"&gt;&lt;/&lt;span class=\"pl-ent\"&gt;script&lt;/span&gt;&gt;&lt;/span&gt; Our first line of JavaScript, and d3, is one to pull in a file that has the data in it. It is the first line within our &lt;script&gt; tags above. &lt;code&gt;d3.json(\"data/track-n-treat.json\", function(data) {\n  // More d3 code will go in here\n\n}\n&lt;/code&gt; This function surrounds all of the d3 code that makes the graph. Supply the path (relative to this html file) and a variable name for the data (within the function’s parenthesis) Here we pull in the data from a separate file. d3 calls the main d3 method, .json calls d3’s json method that takes care of loading all of the data from the json file. The json function needs a file path (data/track-n-treat.json) that is relative to where this file is, and a function that creates an internal variable/object to hold the data. So now all of the data from the json file is available as a variable, in our case it is named ‘data’. (We could change that to anything we want.) It’s as if we had this in the code: data &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; [ \n   {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;2015-11-01&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;17:25&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;Heath Bar&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;},\n   {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;2015-11-01&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;17:25&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;Kitkat Bar&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;},\n   {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;2015-11-01&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;20:25&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;Kitkat Bar&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;},\n   {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;2015-11-02&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;17:38&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;Whopper&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;}, \n   ... \n ] Some Variables Next, we’ll set some variables to use later. The ‘margin’ variable is really an object, so we can call the elements within using dot notation like so: margin.top returns ‘40’, or margin.right will return ‘40’. &lt;span class=\"pl-k\"&gt;var&lt;/span&gt; margin &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; {top&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;40&lt;/span&gt;, right&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;40&lt;/span&gt;, bottom&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;40&lt;/span&gt;, left&lt;span class=\"pl-k\"&gt;:&lt;/span&gt;&lt;span class=\"pl-c1\"&gt;40&lt;/span&gt;},\n    width &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;700&lt;/span&gt;,\n    height &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;300&lt;/span&gt;,\n    workingHeight &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; height &lt;span class=\"pl-k\"&gt;-&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;margin&lt;/span&gt;.&lt;span class=\"pl-c1\"&gt;top&lt;/span&gt; &lt;span class=\"pl-k\"&gt;-&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;margin&lt;/span&gt;.&lt;span class=\"pl-c1\"&gt;bottom&lt;/span&gt;; Axes D3 can take care of a lot of the functionality of creating and placing the x and y axes. D3 puts the axis in the right spot, puts the tick marks on, spaces them appropriately, and labels the tick marks. There are two parts to creating an axis. First, create a scale. Second, apply the scale to the axis. Let’s start by making the x axis. The X-Axis (scale) We want the x axis to show the days. We’ll use a time scale, since we’re plotting days. We’ll first set ‘x’ to be a function that converts dates that we plug into it, into pixels on the screen. Think of this as a range or scale converter. The domain represents the minimum and maximum values that exist in the data. The range is the minimum and maximum values as represented on the web page. This will basically be a date to pixel converter. For a great write up on how d3 scales work, look here: http://www.jeromecukier.net/blog/2011/08/11/d3-scales-and-color/ Our next line of code creates the x function and assigns the domain and range for the scale. This code goes right after the variables we created above. &lt;span class=\"pl-k\"&gt;var&lt;/span&gt; x &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;d3&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;time&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;scale&lt;/span&gt;()\n          .&lt;span class=\"pl-en\"&gt;domain&lt;/span&gt;([ &lt;span class=\"pl-k\"&gt;new&lt;/span&gt; &lt;span class=\"pl-en\"&gt;Date&lt;/span&gt;(data[&lt;span class=\"pl-c1\"&gt;0&lt;/span&gt;].&lt;span class=\"pl-smi\"&gt;day&lt;/span&gt;), &lt;span class=\"pl-smi\"&gt;d3&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;time&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;day&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;offset&lt;/span&gt;(&lt;span class=\"pl-k\"&gt;new&lt;/span&gt; &lt;span class=\"pl-en\"&gt;Date&lt;/span&gt;(data[&lt;span class=\"pl-smi\"&gt;data&lt;/span&gt;.&lt;span class=\"pl-c1\"&gt;length&lt;/span&gt; &lt;span class=\"pl-k\"&gt;-&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;1&lt;/span&gt;].&lt;span class=\"pl-smi\"&gt;day&lt;/span&gt;), &lt;span class=\"pl-c1\"&gt;1&lt;/span&gt;)])\n          .&lt;span class=\"pl-en\"&gt;rangeRound&lt;/span&gt;([&lt;span class=\"pl-c1\"&gt;0&lt;/span&gt;, width &lt;span class=\"pl-k\"&gt;-&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;margin&lt;/span&gt;.&lt;span class=\"pl-c1\"&gt;left&lt;/span&gt; &lt;span class=\"pl-k\"&gt;-&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;margin&lt;/span&gt;.&lt;span class=\"pl-c1\"&gt;right&lt;/span&gt;]); Let’s walk through this in a bit more detail. D3’s time.scale function takes a ‘domain’ and a ‘range’, which both take a minimum and maximum value. We plug the minimum and maximum dates into the domain section and we set the pixel limits in the range section. The minimum date is the first day I ate candy, the maximum date is the last day I ate candy. To calculate the first day, we can get the date from the data array: ‘data[0].day’ corresponds with the first element in the ‘data’ array (which is an object), and the value of that objects ‘day’ key. Since the json file is assigned to the ‘data’ variable, we can get the first ‘day’ by accessing data[0].day . We put that in the default JavaScript Date function to return the date as a String. Getting the last day is similar, we can also get the last day from the data in the json file. We just need to get the last object element in the array, and get the value of the day element. But how do we specify which is the last element in the array if we don’t know how many elements there are? We could count, but what if we change the data? We can do a little math to calculate the last element in the array. The JavaScript builtin .length method gives us how many elements are in an array. Since array elements begin counting at 0, we just get the length, number of elements, minus 1 to give us the index of the last element. We can then we put that into d3’s time.day.offset function which adds or subracts a given amount of days from the day that you input. In our case we’ll offset by one day, so that the axis goes from the first day until the day after the last day that I ate candy. The range is basically the width we specify above, but subtract some of the padding. So the range would be from 0 to 620 (700 - 40 - 40). A visualization of what the domain to range looks like 2015-11-1      2015-11-4         2015-11-8\n   &lt;span class=\"pl-k\"&gt;|&lt;/span&gt;---------------&lt;span class=\"pl-k\"&gt;|&lt;/span&gt;-----------------&lt;span class=\"pl-k\"&gt;|&lt;/span&gt;\n  /                /                  \\\n /                /                    \\\n&lt;span class=\"pl-k\"&gt;|&lt;/span&gt;----------------&lt;span class=\"pl-k\"&gt;|&lt;/span&gt;----------------------&lt;span class=\"pl-k\"&gt;|&lt;/span&gt;\n0                  88.5                620 The X-Axis (axis) We create an x axis by calling the d3.svg.axis method, and assigning it to a variable. Let’s call it ‘xAxis’. &lt;span class=\"pl-k\"&gt;var&lt;/span&gt; xAxis &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;d3&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;svg&lt;/span&gt;.&lt;span class=\"pl-c1\"&gt;axis&lt;/span&gt;()\n              .&lt;span class=\"pl-en\"&gt;scale&lt;/span&gt;(x)\n              .&lt;span class=\"pl-en\"&gt;orient&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;bottom&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;)\n              .&lt;span class=\"pl-en\"&gt;ticks&lt;/span&gt;(&lt;span class=\"pl-smi\"&gt;d3&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;time&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;days&lt;/span&gt;, &lt;span class=\"pl-c1\"&gt;1&lt;/span&gt;)\n              .&lt;span class=\"pl-en\"&gt;tickFormat&lt;/span&gt;(&lt;span class=\"pl-smi\"&gt;d3&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;time&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;format&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;%a %d&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)); Again, this can all be written out on one line, but we separate each chained method onto its own line to make the code more legible. We plug in the scale created above using the .scale method, and we assign an orientation using the .orient method. .ticks sets the ticks or marks on the x axis. d3.time.days sets a range of days within the dates used in the scale (above). ‘1’ means show each day in that range (a ‘2’ would show every other day in the range). .tickFormat sets the format for the tick to be a date using the d3.time.format function in the form ‘DDD ##’. Y-Axis (scale) The y-axis represents how many candies eaten each day. The height of this axis is determined by the maximum number of candies eaten in a single day. A few times I ate multiple candies at the same time. We’ll need to get the number of candies for each time period in a day and add them all up. This will determine the max height of the y axis. We’ll use the d3 nest function which manipulates the data array. The key function pulls out all of the separate days as a key, the value is all of the times that are associated with that day. We can then use the rollup function to turn the values into something else. In this case it returns the length of the values array, which is the number of candies eaten that day. &lt;span class=\"pl-k\"&gt;var&lt;/span&gt; timesPerDay &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;d3&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;nest&lt;/span&gt;()\n                    .&lt;span class=\"pl-en\"&gt;key&lt;/span&gt;(&lt;span class=\"pl-k\"&gt;function&lt;/span&gt;(&lt;span class=\"pl-smi\"&gt;d&lt;/span&gt;) { &lt;span class=\"pl-k\"&gt;return&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;d&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;day&lt;/span&gt;; })\n                    .&lt;span class=\"pl-en\"&gt;rollup&lt;/span&gt;(&lt;span class=\"pl-k\"&gt;function&lt;/span&gt;(&lt;span class=\"pl-smi\"&gt;t&lt;/span&gt;) { &lt;span class=\"pl-k\"&gt;return&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;t&lt;/span&gt;.&lt;span class=\"pl-c1\"&gt;length&lt;/span&gt;; })\n                    .&lt;span class=\"pl-en\"&gt;entries&lt;/span&gt;(data); The key part of the nest function turns the data from this: [ \n{&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;2015-11-01&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;17:25&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;Heath Bar&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;},\n{&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;2015-11-01&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;17:25&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;Kitkat Bar&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;},\n{&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;2015-11-01&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;20:25&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;Kitkat Bar&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;},\n{&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;2015-11-02&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;17:38&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;Whopper&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;}, \n{&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;2015-11-02&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;18:38&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;Whopper&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;}, \n... \n] Into this: [\n  {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;key&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;2015-11-01&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;,\n    &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;values&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; [\n    {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;2015-11-01&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;17:25&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;Heath Bar&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;},\n    {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;2015-11-01&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;17:25&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;Kitkat Bar&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;},\n    {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;2015-11-01&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;20:25&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;Kitkat Bar&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;},\n    ]\n  },\n  {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;key&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;2015-11-02&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;,\n    &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;values&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; [\n    {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;2015-11-02&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;17:38&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;Whopper&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;}, \n    {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;2015-11-02&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;18:38&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;Whopper&lt;span class=\"pl-pds\"&gt;\"&lt;/span&gt;&lt;/span&gt;}, \n    ...\n    ]\n  },\n  ...\n] And the rollup part of the nest function further converts the data into this: [\n  {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;key&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;2015-11-01&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;values&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;3&lt;/span&gt; },\n  {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;key&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;2015-11-02&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;values&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;2&lt;/span&gt; },\n  ...\n] .entries is where we plug in which data is to be ‘key’ed and ‘rollup’ed. We assign the result to the variable ‘timesPerDay’. Next we need to pull out the highest number from this array. &lt;span class=\"pl-k\"&gt;var&lt;/span&gt; maxEats &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;d3&lt;/span&gt;.&lt;span class=\"pl-c1\"&gt;max&lt;/span&gt;(timesPerDay, &lt;span class=\"pl-k\"&gt;function&lt;/span&gt;(&lt;span class=\"pl-smi\"&gt;v&lt;/span&gt;) { &lt;span class=\"pl-k\"&gt;return&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;v&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;values&lt;/span&gt;; }) The d3.max function looks at an array and returns the highest value. It takes an array and a function. The array is the ‘timesPerDay’ array we created above. The function allows us to specify which part of the array to count. Here the v stands for the array of day objects, and v.values is the ‘values’ element within each day object, which holds the number of times a candy was eaten for that day. So d3.max is now just looking at the different ‘values’ fields and returns the highest number. Since we’re just using numbers (not dates), we use a regular linear scale for the y axis. We use the maximum number of candy in a day as the maximum for the domain. &lt;span class=\"pl-k\"&gt;var&lt;/span&gt; y &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;d3&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;scale&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;linear&lt;/span&gt;()\n          .&lt;span class=\"pl-en\"&gt;domain&lt;/span&gt;([&lt;span class=\"pl-c1\"&gt;0&lt;/span&gt;, maxEats ])\n          .&lt;span class=\"pl-en\"&gt;range&lt;/span&gt;([workingHeight, &lt;span class=\"pl-c1\"&gt;0&lt;/span&gt;]); The range is the working height (the height minus the padding) to 0. What happens if the range is 0 to workingHeight The Y-Axis (axis) No we can create the y axis using the scale above. &lt;span class=\"pl-k\"&gt;var&lt;/span&gt; yAxis &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;d3&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;svg&lt;/span&gt;.&lt;span class=\"pl-c1\"&gt;axis&lt;/span&gt;()\n              .&lt;span class=\"pl-en\"&gt;scale&lt;/span&gt;(y)\n              .&lt;span class=\"pl-en\"&gt;orient&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;left&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n              .&lt;span class=\"pl-en\"&gt;tickPadding&lt;/span&gt;(&lt;span class=\"pl-c1\"&gt;8&lt;/span&gt;); Most of this is self-explanatory. The .scale calls the y scale we made above. The .orient sets the axis on the left hand side. .tickPadding determines the space between the tick marks. We set all of this to the variable name ‘yAxis’. We’ll use the variables ‘xAxis’ and ‘yAxis’ later in the code. Put it all together The main SVG element This is where the magic happens. First, we create a variable/object for the svg elements to live under, because we want to add to it later. d3 calls the main d3 method. select specifies which part of the DOM we want to target. We’re going to target an HTML div tag with the id ‘graph’. &lt;span class=\"pl-k\"&gt;var&lt;/span&gt; svg &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;d3&lt;/span&gt;.&lt;span class=\"pl-c1\"&gt;select&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;#graph&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n            .&lt;span class=\"pl-en\"&gt;append&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;svg&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n              .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;class&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;graph&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n              .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;width&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, width)\n              .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;height&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, height)\n            .&lt;span class=\"pl-en\"&gt;append&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;g&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n              .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;transform&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;translate(&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class=\"pl-k\"&gt;+&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;margin&lt;/span&gt;.&lt;span class=\"pl-c1\"&gt;left&lt;/span&gt; &lt;span class=\"pl-k\"&gt;+&lt;/span&gt;&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;, &lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class=\"pl-k\"&gt;+&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;margin&lt;/span&gt;.&lt;span class=\"pl-c1\"&gt;right&lt;/span&gt; &lt;span class=\"pl-k\"&gt;+&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;)&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;); This targets, or selects, the HTML tag with the id of ‘graph’. We then append an svg tag to the div tag, give it a class of ‘graph’, and set the width and height. Then we append an svg group element g where the chart will reside. We’re going to offset this group to create some padding where the axes will go. To better understand the transform and translate functions, take a minute to read through the section on ‘SVG Transform as a Coordinate Space Transformation’. https://www.dashingd3js.com/svg-group-element-and-d3js The graph section We attach the graph using the selectAll method, and attaching to our previously created svg variable/object. &lt;span class=\"pl-smi\"&gt;svg&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;selectAll&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;.graph&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n    .&lt;span class=\"pl-c1\"&gt;data&lt;/span&gt;(timesPerDay)\n    .&lt;span class=\"pl-en\"&gt;enter&lt;/span&gt;().&lt;span class=\"pl-en\"&gt;append&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;rect&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n      .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;class&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;bar&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n      .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;x&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-k\"&gt;function&lt;/span&gt;(&lt;span class=\"pl-smi\"&gt;d&lt;/span&gt;) {&lt;span class=\"pl-k\"&gt;return&lt;/span&gt; &lt;span class=\"pl-en\"&gt;x&lt;/span&gt;(&lt;span class=\"pl-k\"&gt;new&lt;/span&gt; &lt;span class=\"pl-en\"&gt;Date&lt;/span&gt;(&lt;span class=\"pl-smi\"&gt;d&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;key&lt;/span&gt;)) &lt;span class=\"pl-k\"&gt;+&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;14&lt;/span&gt;; })\n      .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;y&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-k\"&gt;function&lt;/span&gt;(&lt;span class=\"pl-smi\"&gt;d&lt;/span&gt;) { &lt;span class=\"pl-k\"&gt;return&lt;/span&gt; workingHeight &lt;span class=\"pl-k\"&gt;-&lt;/span&gt; (workingHeight &lt;span class=\"pl-k\"&gt;-&lt;/span&gt; &lt;span class=\"pl-en\"&gt;y&lt;/span&gt;(&lt;span class=\"pl-smi\"&gt;d&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;values&lt;/span&gt;)); })\n      .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;width&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-c1\"&gt;24&lt;/span&gt;)\n      .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;height&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-k\"&gt;function&lt;/span&gt;(&lt;span class=\"pl-smi\"&gt;d&lt;/span&gt;) { &lt;span class=\"pl-k\"&gt;return&lt;/span&gt; workingHeight &lt;span class=\"pl-k\"&gt;-&lt;/span&gt; &lt;span class=\"pl-en\"&gt;y&lt;/span&gt;(&lt;span class=\"pl-smi\"&gt;d&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;values&lt;/span&gt;) }); The data we use in the data method is the timesPerDay array created above. The .enter method represents the elements that will be appended to the svg group. The elements that we will append are rectangles, rect . Basically, this allows us to loop through the array and places the bar on the graph depending on the x and y positions. The x position for the bar uses the x function created above, the ‘date’ and ‘14’ for some padding. We give it the date, because the x scaler that we created above will turn that into a pixel within the range that we can use. The y axis starts from the top of the screen, so we set the y location of the bar to start at the highest value of the height (so it starts at the bottom of the screen), then subract from that value to move the bar up the screen. We move it up the height of the axis minus the number of candies that day. The height is the workingHeight minus the number of candies. Attach the Axes This just attaches the x axis to a svg group element &lt;span class=\"pl-smi\"&gt;svg&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;append&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;g&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n    .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;class&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;x axis&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n    .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;transform&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;translate(0, &lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class=\"pl-k\"&gt;+&lt;/span&gt; workingHeight &lt;span class=\"pl-k\"&gt;+&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;)&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n    .&lt;span class=\"pl-c1\"&gt;call&lt;/span&gt;(xAxis); Attach the y axis &lt;span class=\"pl-smi\"&gt;svg&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;append&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;g&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n    .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;class&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;y axis&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n    .&lt;span class=\"pl-c1\"&gt;call&lt;/span&gt;(yAxis); Finish the JavaScript code and HTML page We need to close the function we started above, and close the HTML script tag });\n&lt;/&lt;span class=\"pl-ent\"&gt;script&lt;/span&gt;&gt;\n\n&lt;/&lt;span class=\"pl-ent\"&gt;body&lt;/span&gt;&gt;\n&lt;/&lt;span class=\"pl-ent\"&gt;html&lt;/span&gt;&gt; And there you have it. A bar graph showing how many candies I ate each day for the week after Halloween. See it here. Check here for all of the code together without comments:  https://github.com/mossiso/track-n-treat/blob/master/eats-no-comments.html Next up, a graph to show how many times a day and which candies I ate for each day."},{"id":"2016-01-19-working-with-d3-part-2","title":"Working with D3, Part 2","author":"ammon-shepherd","date":"2016-01-19 07:03:46 -0500","categories":["Visualization and Data Mining"],"url":"working-with-d3-part-2","layout":"post","content":"When did I eat all those candies? This second visualization will answer the above question, and also which candies I ate. This visualization will show each day, and within each day it will show the time period that I had candy, and an image of the candy will designate what kind of candy, and how many at that time period. D3 code This uses the exact same data file as the previous chart, data/track-n-treat.json, but we build it in an entirely different way. We’ll also spice it up with some images. Begin the HTML document We start this file much like the previous. &lt;!DOCTYPE html&gt;\n&lt;&lt;span class=\"pl-ent\"&gt;html&lt;/span&gt; &lt;span class=\"pl-e\"&gt;lang&lt;/span&gt;=&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;en&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&gt;\n    &lt;&lt;span class=\"pl-ent\"&gt;head&lt;/span&gt;&gt;\n        &lt;&lt;span class=\"pl-ent\"&gt;meta&lt;/span&gt; &lt;span class=\"pl-e\"&gt;charset&lt;/span&gt;=&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;utf-8&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&gt;\n        &lt;&lt;span class=\"pl-ent\"&gt;title&lt;/span&gt;&gt;Track-n-Treat&lt;/&lt;span class=\"pl-ent\"&gt;title&lt;/span&gt;&gt;\n        &lt;&lt;span class=\"pl-ent\"&gt;link&lt;/span&gt; &lt;span class=\"pl-e\"&gt;href&lt;/span&gt;=&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;styles.css&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class=\"pl-e\"&gt;rel&lt;/span&gt;=&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;stylesheet&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class=\"pl-e\"&gt;type&lt;/span&gt;=&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;text/css&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt; /&gt;\n        &lt;&lt;span class=\"pl-ent\"&gt;link&lt;/span&gt; &lt;span class=\"pl-e\"&gt;href&lt;/span&gt;=&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;https://fonts.googleapis.com/css?family=Inknut+Antiqua&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class=\"pl-e\"&gt;rel&lt;/span&gt;=&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;stylesheet&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class=\"pl-e\"&gt;type&lt;/span&gt;=&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;text/css&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&gt;\n&lt;span class=\"pl-s1\"&gt;        &lt;&lt;span class=\"pl-ent\"&gt;script&lt;/span&gt; &lt;span class=\"pl-e\"&gt;type&lt;/span&gt;=&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;text/javascript&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class=\"pl-e\"&gt;src&lt;/span&gt;=&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;d3/d3.v3.js&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&gt;&lt;/&lt;span class=\"pl-ent\"&gt;script&lt;/span&gt;&gt;&lt;/span&gt;\n    &lt;/&lt;span class=\"pl-ent\"&gt;head&lt;/span&gt;&gt;\n    &lt;&lt;span class=\"pl-ent\"&gt;body&lt;/span&gt;&gt;\n      &lt;&lt;span class=\"pl-ent\"&gt;h1&lt;/span&gt;&gt;Track-n-Treat 2015&lt;/&lt;span class=\"pl-ent\"&gt;h1&lt;/span&gt;&gt;\n      &lt;&lt;span class=\"pl-ent\"&gt;p&lt;/span&gt;&gt;Tracking how many candies I ate in the week after Halloween.&lt;/&lt;span class=\"pl-ent\"&gt;p&lt;/span&gt;&gt;\n\n      &lt;&lt;span class=\"pl-ent\"&gt;div&lt;/span&gt; &lt;span class=\"pl-e\"&gt;id&lt;/span&gt;=&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;wrapper&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&gt;\n        &lt;&lt;span class=\"pl-ent\"&gt;div&lt;/span&gt; &lt;span class=\"pl-e\"&gt;id&lt;/span&gt;=&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;chart&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&gt;&lt;/&lt;span class=\"pl-ent\"&gt;div&lt;/span&gt;&gt;\n      &lt;/&lt;span class=\"pl-ent\"&gt;div&lt;/span&gt;&gt; A new line here that wasn’t in the previous document is the ‘link’ tag pulling in a Google font. We’ll add that as the default font, to give the text a more Halloweeny look. The two HTML div tags are what the svg will be attached to. Having them allows the svg to scroll because the graph will be longer horizontally than the screen. Begin the JavaScript and D3 We start the JavaScript and D3 code the same way as before. &lt;span class=\"pl-s1\"&gt;&lt;&lt;span class=\"pl-ent\"&gt;script&lt;/span&gt; &lt;span class=\"pl-e\"&gt;type&lt;/span&gt;=&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;text/javascript&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&gt;&lt;/span&gt;\n&lt;span class=\"pl-s1\"&gt;  &lt;span class=\"pl-smi\"&gt;d3&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;json&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;data/track-n-treat.json&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-k\"&gt;function&lt;/span&gt;(&lt;span class=\"pl-smi\"&gt;data&lt;/span&gt;) {&lt;/span&gt;\n&lt;span class=\"pl-s1\"&gt;    &lt;span class=\"pl-k\"&gt;var&lt;/span&gt; w &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;2500&lt;/span&gt;;&lt;/span&gt;\n&lt;span class=\"pl-s1\"&gt;    &lt;span class=\"pl-k\"&gt;var&lt;/span&gt; h &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;300&lt;/span&gt;;&lt;/span&gt;\n\n&lt;span class=\"pl-s1\"&gt;    &lt;span class=\"pl-k\"&gt;var&lt;/span&gt; barW &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;60&lt;/span&gt;;&lt;/span&gt;\n&lt;span class=\"pl-s1\"&gt;    &lt;span class=\"pl-k\"&gt;var&lt;/span&gt; barH &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;55&lt;/span&gt;;&lt;/span&gt; We start with the d3 method to pull in the json file. This includes a function that surrounds the rest of the d3 code. We also set an arbitrary width and height. We’ll use these variables later for the dimensions of the main svg element. We also set the width and height that will be used as a representation of a candy bar. Massaging the data We need to nest the data, and group the candy by day and time. We’ll use the d3 ‘nest’ method and set the new array to the variable ‘nested’ For more help on understanding nesting see: http://bl.ocks.org/shancarter/raw/4748131/ Nesting our data will reformat the array from looking like this: [ \n   {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;2015-11-01&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;17:25&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;Heath Bar&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;},\n   {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;2015-11-01&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;17:25&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;Kitkat Bar&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;},\n   {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;2015-11-01&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;20:25&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;Kitkat Bar&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;},\n   {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;2015-11-02&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;17:38&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;Whopper&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;}, \n   &lt;span class=\"pl-k\"&gt;...&lt;/span&gt; \n ] To look like this: [\n   { &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;key&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;2015-11-01&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;,\n     &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;values&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; [\n                 {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;key&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;17:25&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;,\n                  &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;values&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; [\n                              {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;Heath Bar&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;2015-11-01&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;17:25&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;},\n                              {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;Kitkat Bar&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;2015-11-01&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;17:25&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;}\n                            ]\n                 },\n                 {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;key&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;20:25&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;,\n                  &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;values&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; [\n                              {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;Kitkat Bar&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;2015-11-01&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;20:25&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;}\n                            ]\n                 }\n               ]\n   },\n   {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;key&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;2015-11-02&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;,\n    &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;values&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; [\n                {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;key&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;17:38&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;,\n                 &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;values&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; [\n                            {&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;candy&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;Whopper&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;2015-11-02&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;:&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;17:38&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;}\n                           ]\n                }\n              ]\n   }\n   &lt;span class=\"pl-k\"&gt;...&lt;/span&gt;\n ] Here is the code to do that: &lt;span class=\"pl-k\"&gt;var&lt;/span&gt; nested &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;d3&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;nest&lt;/span&gt;()\n    .&lt;span class=\"pl-en\"&gt;key&lt;/span&gt;(&lt;span class=\"pl-k\"&gt;function&lt;/span&gt;(&lt;span class=\"pl-smi\"&gt;d&lt;/span&gt;) { &lt;span class=\"pl-k\"&gt;return&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;d&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;day&lt;/span&gt;; })\n    .&lt;span class=\"pl-en\"&gt;key&lt;/span&gt;(&lt;span class=\"pl-k\"&gt;function&lt;/span&gt;(&lt;span class=\"pl-smi\"&gt;t&lt;/span&gt;) { &lt;span class=\"pl-k\"&gt;return&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;t&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;time&lt;/span&gt;; })\n    .&lt;span class=\"pl-en\"&gt;entries&lt;/span&gt;(data); Basically, this makes a group of all the lines in the original array that have the same day. Then within that group, it makes a group for each time. It’s like an onion, with different layers of data. Making the graph &lt;span class=\"pl-k\"&gt;var&lt;/span&gt; svg &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;d3&lt;/span&gt;.&lt;span class=\"pl-c1\"&gt;select&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;#chart&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;).&lt;span class=\"pl-en\"&gt;append&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;svg&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n    .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;class&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;graph&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n    .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;width&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, w)\n    .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;height&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, h)\n  .&lt;span class=\"pl-en\"&gt;append&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;g&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n    .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;transform&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;translate(20,20)&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;); This creates the actual SVG canvas/element where the graph will be displayed. Select the div element with an id of ‘graph’, append an svg tag/element, give it a CSS class of ‘graph’, and set the width to the ‘w’ variable and the height to the ‘h’ variable as we assigned above. Finally, append a group tag/element. This group is offset 20 pixels down and 20 pixels over from the original origin point. To better understand transform and translate read the section on SVG Transform as a Coordinate Space Transformation at https://www.dashingd3js.com/svg-group-element-and-d3js The 20 pixels down and over are for padding, especially useful if there are axis lines and markers. We don’t have axes in this visualization, but the padding is still useful. Doing it old school This is where all of the magic really happens. The data is used to place the points on the graph, create the bars, etc. Instead of using built in d3 functions for looping through the data, we’ll use default JavaScript control structures, the forEach loop. First we’ll assign a default x position. This will determine the first position on the x-axis, which should start at 0. &lt;span class=\"pl-k\"&gt;var&lt;/span&gt; xPos &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;0&lt;/span&gt;; Let’s also declare some variables that we’ll use later. &lt;span class=\"pl-k\"&gt;var&lt;/span&gt; prevDayTime, thisDayTime, thisDay, prevDay; Loop the loop Next we’ll do a series of loops through the data, each loop associated with the groups created in the nested array that was created above. The first level is the day object. The JavaScript ‘forEach’ is a relatively new built in function specifically for iterating over arrays. The first time through ‘forEach’ returns an object where the ‘key’ is the date and the ‘values’ is an array of objects. In the second ‘forEach’, we go through the ‘values’ of the first level. This returns an object, where the ‘key’ is the time and the ‘values’ is an array of objects that contain the date, time and candy. Next, the third ‘forEach’ of the levelTwo ‘values’ array gives us each instance when the candy was eaten. Here is the basic structure of the ‘forEach’ loops. We’ll add more code in the next steps. &lt;span class=\"pl-smi\"&gt;nested&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;forEach&lt;/span&gt;(&lt;span class=\"pl-k\"&gt;function&lt;/span&gt;(&lt;span class=\"pl-smi\"&gt;dayObject&lt;/span&gt;) {\n    &lt;span class=\"pl-smi\"&gt;dayObject&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;values&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;forEach&lt;/span&gt;(&lt;span class=\"pl-k\"&gt;function&lt;/span&gt;(&lt;span class=\"pl-smi\"&gt;timeObject&lt;/span&gt;, &lt;span class=\"pl-smi\"&gt;dayIndex&lt;/span&gt;) {\n        &lt;span class=\"pl-smi\"&gt;timeObject&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;values&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;forEach&lt;/span&gt;(&lt;span class=\"pl-k\"&gt;function&lt;/span&gt;(&lt;span class=\"pl-smi\"&gt;candyObject&lt;/span&gt;, &lt;span class=\"pl-smi\"&gt;timeIndex&lt;/span&gt;) {\n        });\n    });\n}); In the function section of the last two ‘forEach’ methods, we create an object and an index. The ‘index’ for each of these is the index number of the array element. For the last ‘forEach’, the ‘candyObject’ contains the day, time and candy. Making the graph The first thing we’ll need to do is create an svg group element to contain each day. &lt;span class=\"pl-smi\"&gt;nested&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;forEach&lt;/span&gt;(&lt;span class=\"pl-k\"&gt;function&lt;/span&gt;(&lt;span class=\"pl-smi\"&gt;dayObject&lt;/span&gt;) {\n    dayGroup &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;svg&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;append&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;g&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;);\n\n  &lt;span class=\"pl-smi\"&gt;dayObject&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;values&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;forEach&lt;/span&gt;(&lt;span class=\"pl-k\"&gt;function&lt;/span&gt;(&lt;span class=\"pl-smi\"&gt;timeObject&lt;/span&gt;, &lt;span class=\"pl-smi\"&gt;dayIndex&lt;/span&gt;) {\n    &lt;span class=\"pl-smi\"&gt;timeObject&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;values&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;forEach&lt;/span&gt;(&lt;span class=\"pl-k\"&gt;function&lt;/span&gt;(&lt;span class=\"pl-smi\"&gt;candyObject&lt;/span&gt;, &lt;span class=\"pl-smi\"&gt;timeIndex&lt;/span&gt;) { Then we need to pull some information from that specific instance of eating a candy. We need the day and the day and time together. We don’t need to declare these variables with var because we did that already above. thisDayTime &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;candyObject&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;day&lt;/span&gt; &lt;span class=\"pl-k\"&gt;+&lt;/span&gt;&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt; &lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;&lt;span class=\"pl-k\"&gt;+&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;candyObject&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;time&lt;/span&gt;;\n      thisDay &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;candyObject&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;day&lt;/span&gt; As we loop through each of the instances, we’ll need to check several things. Is this the last instance at this time? Is this the last instance for this day? Is this the same time and day, but a separate instance? (Sometimes I had two or three candy bars within the same minute, ziggy-piggy .) If the current day and time is the same as the previous day and time, then the x position should be the same. Later in the code we add 25 to the current x position, so that the next time through the loop, the x position of the box is further away from previous box. If it’s the same time and day, then we subtract that 25 we add on later to put it back to the same position as the previous time. &lt;span class=\"pl-k\"&gt;if&lt;/span&gt; (thisDayTime &lt;span class=\"pl-k\"&gt;===&lt;/span&gt; prevDayTime) {\n        xPos &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; xPos &lt;span class=\"pl-k\"&gt;-&lt;/span&gt; barW &lt;span class=\"pl-k\"&gt;-&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;5&lt;/span&gt;;\n      } The candy man cometh Next we start creating the candy elements of the graph. We could use boring bars and boxes, or we could use images of actual fun size candy bars and those yummy fun size packets of M&amp;M’s and Reese’s Pieces. We can do this by appending an image to the dayGroup svg group element we created before. &lt;span class=\"pl-smi\"&gt;dayGroup&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;append&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;image&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n    .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;xlink:href&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;images/&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class=\"pl-k\"&gt;+&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;candyObject&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;candy&lt;/span&gt; &lt;span class=\"pl-k\"&gt;+&lt;/span&gt; &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;.jpg&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n    .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;class&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;bar&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n    .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;x&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, xPos)\n    .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;y&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, h &lt;span class=\"pl-k\"&gt;-&lt;/span&gt; (barH &lt;span class=\"pl-k\"&gt;*&lt;/span&gt; timeIndex &lt;span class=\"pl-k\"&gt;+&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;190&lt;/span&gt;))\n    .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;width&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, barW)\n    .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;height&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, barH); The first .attr tells d3 where to find the image to use; the xlink:href states there is a path to follow. The next part is the path to the image. We use the name of the candy supplied from the candyObject object. All of the image files have the same name as those used in the data (the json file), so each different candy can use the specific image for that candy. So candyObject.candy returns ‘Kitkat’, or ‘Snickers’ or ‘Twix’, etc. The + plus signs in the code combine the string parts (those within the single quotes) with the variable. All of the jpg files are in the ‘images’ directory, which is in the same directory where this file is. Next, assign the svg element a class of ‘bar’ so we can style it if we need to. The x position is dealt with elsewhere. The first time through the loop, for the very first instance, it is zero. Subsequent instances have an increased x position, or the same x position if the time is the same as the previous time. The next bit of trickery is where to place the image vertically. Remember the y axis starts from the top of the screen, so we set the y location of the candy bar box to start at the highest value of the height (so it starts at the bottom of the screen), then subtract from that value to move the box up the screen. Some time periods have multiple candies (I ate two or three within one minute), so their x position is the same, but the y position needs to be higher. To do this we’ll use the array’s index number to provide a variable in a made up formula for increasing the height. We want the stacked candy boxes to start at a specific point, lets say 190 pixels above the bottom of the svg ‘canvas’ that we create. The first candy for a time will then start at ‘400 - (35 * 1 + 190)’. That means starting at the bottom of the screen ‘400’, we’ll subtract 190, so half way up the canvas, plus move it up 35 for some padding. Halfway up the canvas gives us room for the times and date bars. Subsequent candy boxes will be bumped up ‘35 * 2’ or ‘35 * 3’ providing a stacked look. Play with the numbers to see how the boxes move up or down. Time for some labels We need to add the time that each candy was eaten. We do this in a similar manner as adding the candy images, but appending text rather than images. &lt;span class=\"pl-smi\"&gt;dayGroup&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;append&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;text&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n   .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;class&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;time&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n   .&lt;span class=\"pl-c1\"&gt;text&lt;/span&gt;(&lt;span class=\"pl-smi\"&gt;candyObject&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;time&lt;/span&gt;)\n   .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;x&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, xPos)\n   .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;y&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, h &lt;span class=\"pl-k\"&gt;-&lt;/span&gt; (barW &lt;span class=\"pl-k\"&gt;*&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;2&lt;/span&gt;))\n   .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;width&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, barW)\n   .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;height&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, barH); This is very similar to the above. We append a text element to the dayGroup element we created before. The .text line is where we add the text. The text is just the time, which we can get from the candyObject . candyObject = {'day': '2015-11-xx', 'time': 'xx:xx', 'candy': 'Xxxx'} Each candyObject object has this info associated with it. The x ’s in the example above are replaced each time it goes through the loop. The x position, xPos, is set and the same as the candy wrapper image. The y position will be the same for each time period. We’ll set it initially at the height of the ‘canvas’, so it starts at the very bottom. Then we’ll subtract some amounts to get it to a good spot. We can use the width variable for a set number, and then let’s multiply that by 2 for fun. We could have just used a static number, like ‘100’, but this way the height adjusts to any changes in the other aspects of the graph. If we change how big the candy bar wrappers are, then the time labels automatically adjust. Play around with those numbers, replace the ‘h’ and ‘barW’ with actual numbers. A box for each day We’re placing something on the screen for the candy wrapper image and the time each time we loop through. For the instances when there are multiple candies at the same time, there are actually multiple times overlayed. They are in the exact same place, so you only see the one. This won’t work for the boxes that represent the days. We only want one box. One way to solve this is to only draw the day box the very first time an instance of that day comes up in our loop. This is where our thisDay and prevDay variables come into play. We can check the current date with the previous date. If the current date is the same as the prevDate variable, then it is the same day, so don’t draw a box. The only time thisDay will not equal prevDay is when thisDay ’s instance is the next day. Hmm, that’s really hard to explain, and this probably confuses more than clarifies. &lt;span class=\"pl-c\"&gt;// add the day boxes&lt;/span&gt;\n&lt;span class=\"pl-k\"&gt;if&lt;/span&gt; (thisDay &lt;span class=\"pl-k\"&gt;!==&lt;/span&gt; prevDay) {\n  &lt;span class=\"pl-smi\"&gt;dayGroup&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;append&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;rect&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n     .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;class&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;day&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n     .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;x&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, xPos)\n     .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;y&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, h &lt;span class=\"pl-k\"&gt;-&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;110&lt;/span&gt;)\n     .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;width&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, (&lt;span class=\"pl-smi\"&gt;dayObject&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;values&lt;/span&gt;.&lt;span class=\"pl-c1\"&gt;length&lt;/span&gt; &lt;span class=\"pl-k\"&gt;*&lt;/span&gt; barW &lt;span class=\"pl-k\"&gt;+&lt;/span&gt; barW&lt;span class=\"pl-k\"&gt;/&lt;/span&gt;&lt;span class=\"pl-c1\"&gt;2&lt;/span&gt;))\n     .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;height&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, barH); Here we append a rectangle to the dayGroup object, give it a CSS class, set the x position to xPos, and the y position to a set number. This time we took the lazy way and just subtracted a static number from the height. The width is the tricky part here. We want the width of the bar to be the length of all of the time elements for that day. We can get the number of times for a day from the dayObject object. The key is the date, and the value is an array of times. So we can get the number of times in a day with dot notation and the array length method, dayObject.values.length . That gives us a single digit, so we need to multiply that by the width of the candy bar image, dayObject.values.length * barW . Then add some more for padding. We can dynamically get a number by using the candy bar image width divided by 2, barW/2 . That gives us a number that approximates the width taken up by all of the time elements for a day. Making the box is only half the job. We should add some text so we know which day it is that we are adding. Let’s use the month and day in the format ‘MMM DD’, so the month name using three letters, and the two digit day of the month. dateForm &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; &lt;span class=\"pl-smi\"&gt;d3&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;time&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;format&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;%b %d&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n  &lt;span class=\"pl-smi\"&gt;dayGroup&lt;/span&gt;.&lt;span class=\"pl-en\"&gt;append&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;text&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n     .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;class&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;date&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;)\n     .&lt;span class=\"pl-c1\"&gt;text&lt;/span&gt;(&lt;span class=\"pl-en\"&gt;dateForm&lt;/span&gt;(&lt;span class=\"pl-k\"&gt;new&lt;/span&gt; &lt;span class=\"pl-en\"&gt;Date&lt;/span&gt;(thisDayTime)) )\n     .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;x&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, xPos &lt;span class=\"pl-k\"&gt;+&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;5&lt;/span&gt;)\n     .&lt;span class=\"pl-en\"&gt;attr&lt;/span&gt;(&lt;span class=\"pl-s\"&gt;&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;y&lt;span class=\"pl-pds\"&gt;'&lt;/span&gt;&lt;/span&gt;, h &lt;span class=\"pl-k\"&gt;-&lt;/span&gt; barW &lt;span class=\"pl-k\"&gt;-&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;10&lt;/span&gt;);\n} To get the date we’ll use d3’s default time formating method, d3.time.format . dateForm = d3.time.format('%b %d') creates a function that will take a date object and return it in the format ‘Month ##’ .text( dateForm(new Date(thisDayTime)) ) From the inside out: thisDayTime is the year-month-day and time variable we create above. We use Javascript’s new Date() function to return a string from the time. Then we use the dateForm function we created to format the date the way we want. Make a little buffer If it’s the last time of the day, and the last candy for that time, add in a space bumper. Without it the days are squished together. This adds a visually appealing space between the days. &lt;span class=\"pl-k\"&gt;if&lt;/span&gt; ( ( dayIndex &lt;span class=\"pl-k\"&gt;==&lt;/span&gt; (&lt;span class=\"pl-smi\"&gt;dayObject&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;values&lt;/span&gt;.&lt;span class=\"pl-c1\"&gt;length&lt;/span&gt; &lt;span class=\"pl-k\"&gt;-&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;1&lt;/span&gt;) ) &lt;span class=\"pl-k\"&gt;&amp;&amp;&lt;/span&gt; ( timeIndex &lt;span class=\"pl-k\"&gt;==&lt;/span&gt; (&lt;span class=\"pl-smi\"&gt;timeObject&lt;/span&gt;.&lt;span class=\"pl-smi\"&gt;values&lt;/span&gt;.&lt;span class=\"pl-c1\"&gt;length&lt;/span&gt; &lt;span class=\"pl-k\"&gt;-&lt;/span&gt;&lt;span class=\"pl-c1\"&gt;1&lt;/span&gt;) ))\n{\n  xPos &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; xPos &lt;span class=\"pl-k\"&gt;+&lt;/span&gt; barW;\n} We use an if statement to check if it’s the last day and the last time for that day. If so we need to increase the x position by the width of the candy bar image. Otherwise it will be at the same position as the last time. We don’t need to add an actual svg element, just increase the x position for the next time through the loop. Increment the incrementables Finally, we need to increment the x position and set the prevDayTime variable to thisDayTime and the prevDay variable to thisDay . xPos &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; xPos &lt;span class=\"pl-k\"&gt;+&lt;/span&gt; barW &lt;span class=\"pl-k\"&gt;+&lt;/span&gt; &lt;span class=\"pl-c1\"&gt;5&lt;/span&gt;;\nprevDayTime &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; thisDayTime;\nprevDay &lt;span class=\"pl-k\"&gt;=&lt;/span&gt; thisDay; Finish it Close off the forEach loops });\n    });\n  }); Close off the encompassing d3 function, and close off the HTML tags to finish of the file. });\n      &lt;/&lt;span class=\"pl-ent\"&gt;script&lt;/span&gt;&gt;\n  &lt;/&lt;span class=\"pl-ent\"&gt;body&lt;/span&gt;&gt;\n&lt;/&lt;span class=\"pl-ent\"&gt;html&lt;/span&gt;&gt; You can see the visualization here:  http://mossiso.com/d3-instructions/day.html And get the code without comments here: https://github.com/mossiso/track-n-treat/blob/master/day-no-comments.html"},{"id":"2016-02-02-apply-for-2016-2017-graduate-fellowship-in-digital-humanities","title":"Apply for 2016-2017 Graduate Fellowship in Digital Humanities","author":"purdom-lindblad","date":"2016-02-02 04:48:47 -0500","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"apply-for-2016-2017-graduate-fellowship-in-digital-humanities","layout":"post","content":"The Scholars’ Lab is proud to announce that applications for our prestigious Graduate Fellowship in the Digital Humanities are being accepted for the 2016-2017 academic year. Applications are due February 26, 2016 . The fellowship supports ABD graduate students doing innovative work in the digital humanities at the University of Virginia. The Scholars’ Lab offers Grad Fellows advice and assistance with the creation and analysis of digital content, as well as consultation on intellectual property issues and best practices in digital scholarship and DH software development. Fellows join our vibrant community, have a voice in intellectual programming for the Scholars’ Lab, make use of our dedicated grad office, and participate in one formal colloquium at the Library per semester. Supported by the Jeffrey C. Walker Library Fund for Technology in the Humanities, the Matthew &amp; Nancy Walker Library Fund, and a challenge grant from the National Endowment for the Humanities, the Graduate Fellowship in Digital Humanities is designed to advance the humanities and provide emerging digital scholars with an opportunity for growth. Eligibility Applicants must be ABD, having completed all course requirements and been admitted to candidacy for the doctorate in the humanities, social sciences or the arts at the University of Virginia. Applicants must be enrolled full time in the year for which they are applying. A faculty advisor must review and approve the scholarly content of the proposal. Applicants are strongly encouraged to have Praxis Program or equivalent experience . Experience can include work on a collaborative digital project, comfort with programing and code management, public scholarship, and critical engagement with digital tools. How to Apply Email a complete application package will include the following materials to Purdom Lindblad : a cover letter, addressed to the selection committee; a Graduate Fellowship Application Form ; a dissertation abstract; a summary of the applicant’s plan for use of digital technologies in his or her dissertation research; a summary of the applicant’s experience with digital projects; a description of UVa library digital resources (content or expertise) that are relevant to the proposed project; and 2-3 letters of nomination and support, at least one being from the applicant’s dissertation director. Deadline: February 26, 2016\nNotifications: March 31, 2016 Questions about Grad Fellowships and the application process should be directed to Purdom Lindblad, Head of Graduate Programs at the Scholars’ Lab."},{"id":"2016-02-02-bigger-nozzles-faster-printing","title":"Bigger nozzles, faster printing","author":"shane-lin","date":"2016-02-02 18:39:56 -0500","categories":["Makerspace"],"url":"bigger-nozzles-faster-printing","layout":"post","content":"This week at the SLab Makerspace, we’ve been experimenting with faster 3D printing at lowering resolutions with larger extruder nozzles. The diameter of the standard Ultimaker 2 nozzle/block assembly is 0.4mm. When we recently installed Anders Olsson’s upgraded heater block (after the stock thermocouple end came off inside of our OEM heater block), we gained the ability to swap in E3D compatible nozzles. The Olsson block itself came with an assortment of sizes, from 0.25mm to 0.8mm. The flow rate of Cura’s normal quickprint settings with the 0.4mm nozzle is 2mm^3/s .  This figure is calculated by multiplying together the nozzle diameter, the layer height, and the print speed (0.4mm x 0.1mm x 50mm/s), basically the volume of a line of filament deposited onto a model or build platform. With the stock nozzle, Cura will produce a warning when trying to extrude more than 8mm^3/s. With stock configuration, our Ultimaker 2 worked most reliably printing PLA at around 5-6mm^3/s or less at 210C temperature. After installing the 0.8mm filament, we’ve been able to push this up to 20mm^3/s and potentially higher by printing at 0.5mm layer height at 50mm/s. On a practical level, this means that we can now print a low resolution version of this cute octopus model in 27 minutes instead of the 3 hours estimated under our previous settings. Printing with substantially higher layer height required that we recalibrate the bed to a somewhat farther distance, but was otherwise very straightforward. The filament used here is Colorfabb Leaf Green PLA/PHA, which we’ve had very good luck with. The lower z-resolution is obvious in these prints, but I think we can use the larger nozzle with shorter layer heights to achieve less pronounced quality differences, still with a modest gain in speed, because the reduction in X-Y axis resolution does not seem to be as dramatic. The larger nozzle diameter will offer faster print speeds and allow more efficient toolpaths. The standard 0.8mm walls will now only require a single pass to lay down. I think this is a case where the larger 2.85mm filament offers some much-needed leeway. The feeder would have to insert the thinner 1.75mm filament at nearly 10mm/s to keep up with this kind of flow rate; I’m not at all confident that our Replicators would be up to that. In the coming weeks, we’ll try to push the flow even higher and experiment with improving quality with thin-layer wide-nozzle settings. We’ll also be testing prints with extremely fine detailing using the 0.25mm nozzle. In any case, the ability to easily change out nozzles on our UM2 opens up the possibility of tailoring our printer to the specific needs of the print. Pushing speed limits will allow us to produce truly rapid prototypes and to demonstrate the basic operation of 3D printers to many more students over the course of the day."},{"id":"2016-02-02-ready-for-praxis-apply-by-february-26-for-the-2016-2017-cohort","title":"Ready for Praxis? Apply by February 26 for the 2016-2017 cohort","author":"purdom-lindblad","date":"2016-02-02 04:37:47 -0500","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"ready-for-praxis-apply-by-february-26-for-the-2016-2017-cohort","layout":"post","content":"UVa grad students! Apply by February 26 for a unique, funded opportunity to work with an interdisciplinary group of your peers, learning digital humanities skills from the experts, and collaboratively designing and executing an innovative digital project. The 2015-2016 Praxis cohort is in full swing, thanks to a generous support by UVa Library and GSAS . Each year, the Scholars’ Lab Praxis Program provides six UVa graduate students with hands-on experience in digital humanities collaboration and project creation. Team members work to take a shared project from conception to design and development, and finally to publication, community engagement, and analysis. Praxis is a unique and well-known training program in the international digital humanities community. Our fellows blog about their experiences and develop increased facility with project management, collaboration, and the public humanities, even as they tackle (most for the first time, and with the mentorship of our  faculty and staff) new programming languages, tools, and digital methods. Praxis aims to prepare fellows with digital methodologies to apply both the the fellowship project and their future research. In 2012-2013, the Scholars’ Lab joined with like-minded institutions to create the Praxis Network, made up of allied but differently-inflected humanities education initiatives from around the world, mainly focused on graduate training, and all engaged in rethinking pedagogy and campus partnerships in relation to the digital humanities. The Praxis Network Student Directory showcases Praxis Program alumni have traveled diverse career paths, including tenure-track teaching positions and digital humanities positions within academic libraries and research centers. We will welcome six new, competitively-selected Praxis students in late August 2016. The Praxis fellowship replaces teaching responsibilities for the academic year. Fellows are expected to devote approximately 10 hours per week in the fall and spring semesters, to learning together and building a collaborative digital humanities project in the Scholars’ Lab. Fellows join our vibrant community, have a voice in intellectual programming for the Scholars’ Lab, and can make use of a dedicated grad lounge. All University of Virginia graduate students working within or committed to humanities disciplines are eligible to apply to join the 2016-17 cohort. We particularly encourage applications from women, LGBT students, and people of color, and will be working to put together an interdisciplinary and intellectually diverse team. The application process for Praxis is simple! You apply individually, and we assemble the team, through a process that includes group interviews ( which will be scheduled for 3/7/2016, 3/9/2016, and 3/11/2016 ) as well as input from peers. To start, we only ask for a letter of intent, sent to Purdom Lindblad . The letter should include: the applicant’s research interests; summary of the applicant’s plan for use of digital technologies in your research; summary of what skills, interests, methods the applicant will bring to the Praxis Program summary of what the applicant hope to gain as a Praxis Fellow Questions about Praxis Fellowships and the application process should be directed to: Purdom Lindblad"},{"id":"2016-02-11-welcome-alison-booth","title":"Welcome, Alison Booth!","author":"laura-miller","date":"2016-02-11 04:03:10 -0500","categories":["Announcements","Digital Humanities"],"url":"welcome-alison-booth","layout":"post","content":"We have some big news! The Scholars’ Lab is thrilled to welcome Alison Booth as our new academic director! Professor Alison Booth, a Department of English faculty member and a preeminent scholar of digital humanities, has been appointed academic director of the University of Virginia’s Scholars’ Lab, Arts &amp; Sciences Dean Ian Baucom and Dean of Libraries Martha Sites announced Monday. ![ABooth](http://static.scholarslab.org/wp-content/uploads/2016/02/ABooth-300x206.jpg) Professor Alison Booth, the new academic director of the Scholars’ Lab Located in Alderman Library, the Scholars’ Lab provides students and researchers from across disciplines opportunities to partner with expert staff as they explore the digital humanities, geospatial information, and other scholarly discoveries at the intersection of digital, textual and physical worlds. “We are resetting the director of the Scholars’ Lab to align more closely with faculty research interests” Sites said. “Professor Booth will join the College and the Library in building a digital humanities environment at UVA that will spur innovative scholarship and exciting collaborations.” Booth has been a faculty member at the University of Virginia since 1986. In addition to digital humanities, her specialties include transatlantic Victorian studies, biography, and women’s history. Booth also directs Collective Biographies of Women (CBW), a digital project based on her book, How to Make It as a Woman: Collective Biographical History from Victoria to the Present (University of Chicago Press). With a National Endowment for the Humanities startup grant in the Institute for Advanced Technology in the Humanities (IATH), the CBW project is developing tools and visualizations for analyzing the narratives and the networks of documents that represent historical women. Booth’s books include Greatness Engendered: George Eliot and Virginia Woolf and “Homes and Haunts: Touring Writers’ Shrines and Countries” (forthcoming, Oxford University Press, 2016). She is the editor of Famous Last Words: Changes in Gender and Narrative Closure, and the Longman Cultural Edition of Wuthering Heights, as well as co-editor of the Norton Introduction to Literature (10th edition). “We are fortunate to have Alison at the helm of the Scholars’ Lab,” said Baucom. “Her leadership will forge connections and inspire the type of creative research that advances knowledge within and beyond the liberal arts.” Booth will begin her term as director of the Scholars’ Lab immediately and will balance this appointment with her existing responsibilities in the College. “This is a wonderful chance to refresh UVA’s pioneering role in a fast-paced field, said Booth. “The Scholars’ Lab in the Library is the right place to draw in students, fellows, and faculty for interdisciplinary collaborations across Grounds. I look forward to working with the Institute for Advanced Technology in the Humanities (IATH), Sciences, Humanities &amp; Arts Network of Technological Initiatives (SHANTI), Advanced Research Computing Service (ARCS), and all the units and leaders who have a stake in advanced computing for research and learning.”"},{"id":"2016-02-22-3d-printing-in-the-classroom-course-assignments-and-the-makerspace","title":"3D Printing in the Classroom: Course Assignments and the Makerspace","author":"jennifer-grayburn","date":"2016-02-22 05:08:01 -0500","categories":["Digital Humanities","Makerspace"],"url":"3d-printing-in-the-classroom-course-assignments-and-the-makerspace","layout":"post","content":"Cross-posted on my personal blog.  During the first week of the spring semester, the Makerspace was a flurry of activity, our Ultimaker 2 printing feverishly throughout the day. Groups of students came in and out, selecting and slicing models, checking on their 3D prints, and assembling different components. This week marked the beginning of a new semester-long assignment created by Slavic Librarian Kathleen Thompson, Slavic Lecturer Jill Martiniuk, and myself to better integrate 3D fabrication and physical computing resources in the classroom. After months of discussing course objectives and possible digital components, including website creation, Arduino programming, and 3D printing, Kathleen and Jill determined that 3D printing would allow students to engage physically with ‘icons’ of Russian identity for Yuri Urbanovich’s ‘Understanding Russia: Symbols, Myths, and Archetypes of Identity’ course and to think critically about how such icons are made, circulated, and contextualized/decontextualized. Logistically, this assignment only required that we block off enough time for the students to stop by and print their selected models (mostly selected from Thingiverse ). Most the students had never been to the Makerspace before and this offered a great opportunity to introduce the equipment and resources to a new audience. It was clear that the process and time required to make these objects encouraged the students to think critically about their objects, with some questioning whether or not an object was “too negative” or even if certain models they selected would print properly. The students were able to control various aspects of their prints, including filament color, print scale, and assembly options, and many students took this opportunity to think symbolically about what and how their object communicated. As students have the opportunity to change or modify their prints later in the semester, they will again be confronted with how these features contribute to its message. I asked Kathleen to share more about her assignment, the implementation of this first phase, and how the students applied their interpretations in the classroom: “Bringing Symbols to Life” is intended to provide students with new ways of examining and expressing the symbolic world of Russian self-identification. Symbols take on new meaning when we can actually manipulate and interact with them, rather than simply seeing them on a page. Interacting with an object allows us to differentiate between ‘symbol’ as an abstract and ‘object’ as a thing; if an object is decontextualized and recontextualized, is it still a symbol of Russianness? What assumptions do we make about symbols of Russianness, and how can we challenge those assumptions? ![2016-01-26 13.21.57](http://static.scholarslab.org/wp-content/uploads/2016/02/2016-01-26-13.21.57-225x300.jpg) The 'scales of justice' as it prints. The students also printed two cubes--one white, one red--to represent conflicting forces on each side of the scale. To answer these questions, groups of 6-7 students collaborated to choose and print a 3D object that, to them, represents Russia. We chose to have students print 3D objects, rather than simply view photographs or slides, because we suspected that having a tangible physical object that they would have to create and then handle would give them a new perspective on how Russians think of themselves. They presented their objects during the second week of class, along with a brief justification of the object they chose and its usefulness as a symbol of Russian identity. The six groups chose the following objects: a bust of Joseph Stalin, a Rubik’scube, the onion dome of an Orthodox cathedral, a Soyuz rocket, St. Basil’s cathedral, and a set of unbalanced scales containing a white box and a red box on each side. Having the items present in class, and able to be touched and passed around, fostered what we think was a more robust discussion of identity than a set of images on a page – for example, being able to see up close the many intricate parts of St. Basil’s cathedral that make what one group called it a symbol of “chaotic Russia”, where cultures and traditions coexist yet also clash, seemed more meaningful than simply looking at it on a screen. The Rubik’s cube and the scales spurred the most discussion, as students noted not only the scale’s metaphorical representation of a power imbalance between Russia and the West, but also its literal imbalance and fragility as a model (it had to be handled very carefully due to its delicate construction). The Rubik’s cube, which was meant to represent what its group called the “polyperipheral” environment, history, and geography, inspired students to discuss scientific and mathematical components of Russian identity and the ever-present Russian fear of feeling “left behind” in technological advancements. ![2016-01-25 17.14.42](http://static.scholarslab.org/wp-content/uploads/2016/02/2016-01-25-17.14.42-225x300.jpg) St. Basil's Cathedral, iconic landmark in Moscow. ![2016-01-25 17.15.57](http://static.scholarslab.org/wp-content/uploads/2016/02/2016-01-25-17.15.57-225x300.jpg) The onion dome of St. Basil's Cathedral before assembly. Students are currently designing and curating two physical exhibits of their objects: one in the Makerspace in Alderman Library, and another in the Slavic Department’s hallway on the second floor of New Cabell Hall. Since creating two exhibits means that objects have to be printed twice, students may alter (or switch entirely) their objects if they choose, provided they give justification. The Rubik’s cube group, for example, expressed interest in re-printing their cube to make the parts movable, which they said would better reflect the changes that “polyperipheral” Russia has experienced especially in the last twenty years. Throughout the semester, students will also revisit their objects to examine how they might function as expressions of Russian identity in various contexts, and at the end of the semester, they will write and present longer reflections on their objects as useful symbols. Starting February 25th, all printed icons of the “Bringing Symbols to Life” exhibition will be on display in the Slavic Department display case, located outside the Slavic Department offices in 258 New Cabell Hall."},{"id":"2016-03-02-teaching-archaeology-of-the-middle-east-in-the-time-of-daesh-the-merits-of-incorporating-allahyaris-material-speculation-with-3d-printing","title":"Teaching Archaeology of the Middle East in the Time of Daesh: the Merits of  Incorporating Allahyari’s “Material Speculation” with 3D Printing","author":"jennifer-grayburn","date":"2016-03-02 08:58:35 -0500","categories":["Digital Humanities","Grad Student Research","Makerspace"],"url":"teaching-archaeology-of-the-middle-east-in-the-time-of-daesh-the-merits-of-incorporating-allahyaris-material-speculation-with-3d-printing","layout":"post","content":"Cross-posted on my personal blog. Anthropology Ph.D. Candidate Sue Ann McCarty frequently visits the Makerspace to print archaeological artifacts. Over multiple conversations, we’ve discovered that we share a similar passion for 3D modeling and printing in the classroom. Sue Ann recently applied her research to a course she taught at James Madison University, and I asked her to share more about the benefits and challenges of integrating digitally-oriented assignments and methods in the classroom: The adjoining regions of southeastern Turkey, northern Syria and northern Iraq—in which my dissertation research is focused—share a rich musical tradition of lamentations within a genre known in Turkish as the uzun hava : long, winding melodic meditations on suffering and loss that express pain as a heartfelt wail of mourning that nevertheless remains beautiful. This musical tradition has never been more apt than in 2015 when the Islamic State (better known under the acronyms ISIS, ISIL or, as used here, Daesh) systematically targeted the region’s cultural heritage sites for destruction and broadcast their actions globally as a form of promotional propaganda. Archaeological sites that were first excavated in the 1870s such as the Assyrian cities of Nimrud and Nineveh, the Mosul Museum and more recent religious shrines from both the Muslim and Syriac Christian traditions have all been looted, severely damaged or destroyed. Given these recent events, I found myself faced with a challenge during the Fall 2015 semester: how do you engage students in an emotionally wrenching topic…particularly when the subjects of study stand a decent chance of being destroyed before your eyes during the semester? How do you teach a class focused on the archaeology of a region where—just before the beginning of the semester—the elderly archaeologist of Palmyra Khaled al-Asaad was publicly beheaded by Daesh; how can you best honor his memory and the memory of all the other lives lost in the current regional conflict? Should the on-going destruction even be discussed in a class focused on the ancient world, and, if so, how? These were the questions I faced while re-writing the syllabus for my Fall 2015 Archaeology of the Middle East class. I discovered that destruction of memory is one of the key themes underlying these acts—and the one that ultimately provided a lifeline for my class. While planning my syllabus I encountered the “Material Speculation” project of artist Morehshin Allahyari. Ms. Allahyari’s creative and insightful approach to the topic uses photogrammetry—reconstructed 3D scans of photos and objects—as well as 3D printing to reproduce artifacts destroyed by Daesh in the Mosul Museum and at archaeological sites like Palmyra, Hatra, Nineveh and Nimrud. Each 3D printed reproduction is embedded with a memory card that contains information about the original context of the artifact. In an ironic and clever twist, Ms. Allahyari notes that the plastic from which each new reproduction is printed is a petroleum product and that control of petroleum resources is one of the main sources of conflict in the region. (It’s worth noting that the PLA—Polylactic Acid—plastic that is used in many small-scale 3D printers is eco-friendly and made of cornstarch.) Ms. Allahyari plans to make public the materials necessary for anyone who can gain access to a 3D printer to create, re-create and remember the lost cultural heritage of Syria and Iraq; she posted her first public plan in February 2016. With “Material Speculation” the artifacts destroyed by Daesh are detached from their original material boundaries; they become infinitely reproducible, capable of being distributed globally and accessed across cultural, linguistic and economic strata. In an age of refugees, the destroyed art itself has become detached from its homeland. Printing is an empowering but also peaceful response to violent acts of intolerance. ![151119_007](http://static.scholarslab.org/wp-content/uploads/2016/03/151119_007-300x225.jpg) Peyton Fitzgerald and Emilie Gregory learn how to use the printing software. Ms. Allahyari’s work inspired me to incorporate a 3D printing project into my class. At the end of the semester—by which time, I grimly assumed, more sites would probably have been destroyed by Daesh—student “excavation teams” would each be responsible for printing objects from a specific damaged site and would present an “excavation report” in the style of a conference paper describing its history and damage. How were we going to do the 3D printing, though? Although I am a doctoral candidate in the University of Virignia’s Department of Anthropology—working with Pati Wattenmaker at the UVA excavations at Kazane Höyük in Şanlıiurfa, Turkey, 30 miles from the Syrian border post at Akçakale/Sabi Abyad—I was teaching my Archaeology of the Middle East class as an adjunct instructor at James Madison University in Harrisonburg. This presented a unique opportunity to observe the pedagogical benefits and problems associated with two very different ways of incorporating 3D printing into student life as well as two different 3D printing labs: UVA’s Makerspace in Alderman Library’s Scholar’s Lab and James Madison University’s 3SPACE Lab, each of which generously contributed to my efforts. I realized throughout the semester that each school’s 3D printing approach has its benefits and challenges. At both schools 3D printers are located in a number of different nodes around their respective campuses—most notably, in their engineering schools, their health sciences departments and in a facility accessible to undergraduate College of Arts and Sciences (UVA) or General Education (JMU) students. There is no single resource at either school for finding out where 3D printers are located, to whom they are accessible or how to access them. ![151119_012](http://static.scholarslab.org/wp-content/uploads/2016/03/151119_012-300x225.jpg) Front row: Dominic Traver, Alexandra Bowen, Brianna McDonald, Samantha Hill and Megan Walmsley plan their printing projects; back left, standing Center for Instructional Technology Instructional Designer Jamie Calcagno-Roach trains Emilie Gregory and Peyton Fitzgerald how to use the printing software. Also pictured, Hannah Sullivan, Chris Molloy, Carlisa Childress and Colton Wells talking about their printing plans. The accessibility and security of the 3D printers are approached very differently at UVA and JMU. At UVA, the most easily-accessible printers are located in the Alderman Library Scholar’s Lab Makerspace under the wider umbrella of the Digital Humanities. Because it is part of a publically-accessible research library, the Makerspace ethos encourages casual experimentation and creativity. It is accessible for drop-in users or by appointment as part of the regular services provided by the library. At the moment, it contains one Ultimaker and 2 Makerbot printers (with more varieties on the way) and individuals using them can book printers for multiple days to print if needed. (This is useful because complex 3D printing projects can take a day or more to print fully.) All 3D printers at the current stage in their technological development are glitchy; being able to monitor whether they’re working and fix printing errors or problems encourages a successful print. One of the wildly knowledgeable and friendly Makerspace staff members is always present to answer questions and fix the machines if necessary. The open access to the Makerspace means that students can drop by at their convenience between 1:00 and 5:00 (or by appointment at other times). By contrast, JMU’s printing lab—located in Burruss Hall in the center of campus—is locked when not in use and is only accessible during scheduled classes, by special appointment when an attendant is present or during 3D printing club weekend hours. This is partly a product of security concerns due to the number of computers and printers present in the room. It is also a product of safety concerns, because printer users are required to be formally trained by someone who can inform them of the hazards associated with the machines (like the very hot printing tips). Unlike the Makerspace, JMU’s printing lab is designed for entire classes of students to use the printers simultaneously in a classroom setting; it includes nine Apple computers attached to Affinia printers and another printer for more detailed varieties of printing. (My class really had the maximum number of people possible present in the lab with 3-4 students at each printer for our training session.) All users must receive at least one class session of formal training in machine function and safety before using the printers. This classroom space can accommodate a larger number of students at the same time than the Makerspace is able to handle; however, printing projects can only be created during the hours when the lab is open. This limits users to projects that can be printed in an hour or two unless someone is available to unlock the lab at a later time. The amount of PLA printing filament used in the 3SPACE lab is also more tightly controlled than the Makerspace; students are required to weigh their projects and report the weight of the plastic used after each session so that the cost of supplies can be accounted for due, in part, to the higher volume of students using the machines. Large or detailed individual projects that aren’t for specific classroom purposes might involve a fee; semester-long classes in the lab include a lab materials fee for spools of PLA filament. ![151119_008](http://static.scholarslab.org/wp-content/uploads/2016/03/151119_008-300x225.jpg) Front to back on right: David Szady, Ximena Calvo and Catherine Grimes wait for their first prints to start in the JMU 3SPACE Lab. From a pedagogical perspective, the differences between these print labs presented both opportunities and challenges—and I made a number of very significant mistakes of my own along the way. Chief among the latter was the assumption when I designed the course syllabus that students would have easy access outside of classroom hours to JMU’s 3SPACE printers in order to print their projects, particularly if the individual object was complicated and would take a long time to print. I incorrectly assumed that they had the same relatively open-access policy that the UVA Makerspace applied to its printers. This meant that I had to schedule extra hours—as it turned out, on a Sunday—for the students to print their group projects. I also had to scale down the requirements for the final group printing project as a result. The size of my class and its need for extended formal training were also issues that I should have anticipated through better coordination with the friendly and helpful people at JMU’s very interesting and supportive Center for Instructional Technology  and the  Institute for Visual Studies and Math, which run the 3SPACE Lab collaboratively. I am grateful for the patience, enthusiasm and dedication of the staff members in both the Center for Instructional Technology and in the Scholar’s Lab, especially Jamie Calcagno-Roach, Jennifer Grayburn and Shane Lin. From the students’ perspectives, how did the 3D printing project turn out? From the first class, a number of skeptical students stayed enrolled specifically in order to learn 3D printing. One of the great joys of the semester was listening to their excitement during our first 3SPACE training session as they talked with one another about plans for their own future projects, dreaming up new ideas for fun things they could print, both personal and academic. Students were actively dreaming of new ways that 3D printing might be beneficial for archaeological use such as creating new comparative collections of animal bones for zooarchaeologists, making artifacts from museum collections more widely available for indigenous peoples who approve their distribution, teaching stratigraphy by printing each layer of an excavation separately, reconstructing features or just simply allowing the public to inspect a reproduction of an artifact up close in a way that might damage the original. ![151119_013](http://static.scholarslab.org/wp-content/uploads/2016/03/151119_013-300x225.jpg) Carlisa Childress shows off her first 3D printed object, a JMU logo. The final student projects focused on the badly damaged ancient cities of Dura Europos, Nineveh, Nimrud, and Palmyra. I had hoped originally for students to print objects derived from these specific sites. However, Ms. Allahyari’s “Material Speculation” plans were still unavailable and there are relatively few other 3D printing plans for relevant objects or buildings published. In the last weeks we had to generalize and print whatever was available (for example, the Temple of Baalshamin and the Temple of Baal from Palmyra courtesy  of Thingiverse, as well as a winged human-headed lion lamassu sculpture from the British Museum’s 3D printing collection). The students were frustrated that they didn’t get more time in the lab. (At the end of the semester we only managed to have two printing sessions, one of which had to be scheduled for a Sunday morning so not everyone could come.) We were also unable to print individual objects for each student by the end of the semester. (Due to our final time and space limitations, I required one print per “excavation team”.) Perhaps the most important question is—was there an actual educational benefit to incorporating 3D printing into the class (other than that it was fun )? Did the students understand the intellectual connection between 3D printing, Daesh’s cultural heritage destruction and the reproduction of memory? As the museum exhibition artist Gary Staab recently said in a Smithsonian Magazine article that described an exhibition model in which he combined his own sculpting with 3D printing to recreate the Neolithic mummy known as Ötzi, “I also find the physical act of making stuff is such a great memory aid. If you want to learn something, you draw it. If you want to know it, you sculpt it. If you have to physically make it in three-dimensions, that burns it into your memory and those facts stay hard and fast,” (Wei-Haas 2016). Three-D printable objects are mnemonic devices, acting as tactile, visceral, ontological connections to their progenitors while also incorporating something new: the labor of the student who reproduces, remembers, touches and observes these objects, physical phantoms of their former selves. The student physically contributes to their memory and reproduction, and it is this, I think, that makes them powerful tools not only in the classroom but as a global tool for fighting the kind of destruction promulgated by Daesh. With 3D printing, memory of sites and objects isn’t just widely distributed, observed, studied and remembered; it has to be actively physically reproduced with time, materials and labor. Each act of reproduction individually chips away just a tiny bit at the destructive force that obliterated Version 1.0, empowering the maker, the memory and the other globally-distributed reproducers. Ömür Harmanşah (2015) points out that Daesh’s destruction isn’t just iconoclasm; it is more like reality TV, an ever-escalating attempt to grab the world’s attention by enacting The Unthinkable. This also humiliates populations whose pride is intertwined with protecting and remembering local monuments in front of a global audience, like schoolyard bullies who tease those they perceive as weak in front of their peers. Like schoolyard bullies, the best way to shut down aggressors is for everyone, united, to join together in opposition, to respect and remember the individuals, sites and objects they attempt to desecrate, both ancient and modern. Obviously our priorities today must fall firmly on the living people who are suffering the deprivations of warfare and occupation; but it’s important, too, to remember that the value of these sites and objects doesn’t really lie in their physical remains. It lies in the fact that this is all we have to remember the dead, to remember the acts of past peoples—workers and kings, everyday families, mothers and children, farmers, travelers and priests—real human beings whose lives are commemorated only by the tactile remains they crafted. Daesh attempts to destroy the honor of the living and the memory of the dead through these acts but the resilience, persistence and memory of the living people of Syria and Iraq is stronger than their oppressors. ![IMG_2579_ClassPhoto](http://static.scholarslab.org/wp-content/uploads/2016/03/IMG_2579_ClassPhoto-1024x768.jpg) James Madison University Anth 395 Archaeology of the Middle East Class Photo: the students are holding their 3D printed objects related to material destroyed at Palmyra, Nimrud, Nineveh and Dura Europos after their final presentations. Pictured, front row l-r: Erin Woods, Rani Bertram, Carlisa Childress, Peyton Fitzgerald, Catherine Grimes, Alexandra Bowen, Brianna McDonald, Jaime Lantzy, Courtney Bryce, Haile Bennett, Ximena Calvo; Middle Row l-r: Patrick Jones, Lauryn Poe, Timmis Maddox, Samantha Hill, David Szady; Back row l-r: Dylan Hickey, Dominic Traver, Haile Bennett, Colton Wells. Not pictured: Connor Amano, Emily Gregory, Chris Molloy, Hannah Sullivan References Cited Allahyari, Morehshin. “Material Speculation: ISIS (Work in Progress).” 2015. Accessed 02/22/2016. http://www.morehshin.com/2015/05/25/material-speculation-isis/ . Harmanşah, Ömür. “ISIS, Heritage, and the Spectacles of Destruction in the Global Media.” Near Eastern Archaeology, Vol. 78, No. 3, Special Issue: The Cultural Heritage Crisis in the Middle East (September 2015): 170-177. Wei-Haas, Maya. “An Artist Creates a Detailed Replica of Ötzi, the 5,300-Year-Old ‘Iceman’.”_ Smithsonian Magazine_, February 17, 2016. Accessed 02/22/2016. http://po.st/gy5vhe ."},{"id":"2016-03-22-eggs-and-baskets-lessons-on-data-foraging","title":"Eggs and Baskets: Lessons on Data Foraging","author":"claire-maiers","date":"2016-03-22 05:48:00 -0400","categories":["Grad Student Research"],"url":"eggs-and-baskets-lessons-on-data-foraging","layout":"post","content":"It’s been a (long) while since my inaugural post on my Data Science Fellowship project.  This post takes the form of a piece of advice for other soon-to-be data gathers, and it comes down to this: don’t put all your eggs in one basket. It sounds cliché, and—in retrospect at least—extremely obvious.  But it is an important piece of advice nevertheless.  What I’m talking about is the way in which we secure data for research projects. Nick and I built our proposal around a single database.  Before submitting an official request for data, we talked with the folks that ran the database, we solicited advice from others who have worked with text as data, and we thought carefully about what we were requesting.  We knew the approximate timeline for receiving a data set, and we worked out a deadline that would allow us enough time to complete our analysis.  I felt that we were being thorough and doing the best we could to ensure quick delivery of our data.  Originally, the developers at the database told us to expect the data as much as a month in advance of that deadline.  That deadline has long come and gone.  Recently, we learned that due to some complexities of our request, they may not be able to deliver the data set before our fellowship concludes. (If I am able to speak with the developers, I’ll update this post at a later with more details about exactly what made extracting a data set so difficult.  Right now, what I’ve come to understand is that the size of our request may have been unprecedented and revealed some problems with running large inquiries.) My advice is to pursue as many sources as possible until you have a workable data set in your hands.  This is especially important if you are working with a strict deadline, as so often the case for those of us writing dissertations or completing fellowships.  I recognize that there may not always be that luxury, but if there is more than one potential data source for your work, try to work with more than one. I want to emphasize that this post is in no way meant as a criticism of the developers who tried to deliver our data.  Sometimes, despite the best of intentions, things don’t work out.  Nick and I had great interactions with our contacts there, and we do feel that they did everything in their power to meet our deadline. I would not hesitate to request data from them again in the future. The silver lining is that we do expect to get the data eventually and hope to execute our proposal at that point (though this will be after the our institutional support comes to an end).  While Nick and I had discussed and prepared for a number of obstacles to our research, failure to gather data was not one of them.  You can be sure that the next time I go data gathering, I’ll take more than one basket."},{"id":"2016-04-11-saving-arduino-sensor-data","title":"Saving Arduino Sensor Data","author":"ammon-shepherd","date":"2016-04-11 07:16:18 -0400","categories":["Makerspace"],"url":"saving-arduino-sensor-data","layout":"post","content":"We had a need to take the temperature of an environment over a period of time, and record those temperatures for later analysis. There are a number of options for recording sensor data. If connected to a computer, the data can be saved by reading the serial output and storing that in a file. If there is an SD card connected to the Arduino, the data can be saved directly to the SD card. In this case, the Arduino needed to function by itself without being connected to a computer, so the sensor data needed to be saved to an SD card. We also needed the temperature sensor to be quite a distance from the Arduino and batteries, to minimize radiant heat affecting the temperature, so I soldered the sensor on to lengths of wire. A number of extension shields offer SD card readers. We had a WiFi shield with an SD card reader, so that is the one I used. There are some limitations with this set up. The biggest issue is that this set up does not include a time stamp with the temperatures. In order to get a timestamp, it is best to record the data with the Arduino connected to a computer, or a Raspberry Pi. See here for code to capture the sensor data on a computer (like a Raspberry Pi) using Python: https://github.com/mossiso/arduino-tuts/tree/master/raspberry-pi Another way is to use a dedicated SD card shield with an RTC (Real Time Clock) built in, such as this one from adafruit: https://www.adafruit.com/products/1141 Hardware Arduino Uno Wifi Shield or other shield with an SD card reader. This sketch uses a Wifi shield with SD card. This one uses the Adafruit CC3000 shield. Instructions for installation here: https://learn.adafruit.com/adafruit-cc3000-wifi/cc3000-shield Tips for installing male headers or stackable headers: https://learn.sparkfun.com/tutorials/arduino-shields#installing-headers-preparation SD card Temperature sensor https://www.sparkfun.com/products/10988 Battery pack This set up works well with an external battery pack. https://www.adafruit.com/products/248?&amp;main_page=product_info&amp;products_id=248 Wiring Diagram Code ```\n// SPI and SD libraries. SPI for connecting SD card to SPI bus.\n#include #include const int sdPin = 4; // Temperature pin set to analog 0\nconst int temPin = 0; // Delay time. How often to take a temperature reading, in miliseconds\n// 20 minutes = 1200000 milliseconds\nconst int delayTime = 1200000; // File variable\nFile tempsFile; void setup() {\n  // Serial output for when connected to computer\n  Serial.begin(9600);\n  while (!Serial) {\n    ; // wait for serial port to connect. Needed for native USB port only\n  } Serial.print(“Initializing SD card…”);\n  if(!SD.begin(sdPin)) {\n    Serial.println(“initialization failed!”);\n    return;\n  }\n  Serial.println(“Initialization done.”); tempsFile = SD.open(“temps.txt”, FILE_WRITE); if (tempsFile) {\n    Serial.println(“Printing temperatures”);\n    tempsFile.println(“Printing temperatures:”);\n    tempsFile.close();\n    Serial.println(“Done.”);\n  } else {\n    Serial.println(“Error opening file in setup.”);\n  } } void loop() {\n  / ** ** ** **/\n  // Open SD card for writing\n  tempsFile = SD.open(“temps.txt”, FILE_WRITE); if (tempsFile) {\n    // Temperature readings\n    float voltage, degreesC, degreesF;\n    voltage = getVoltage(temPin);\n    degreesC = (voltage - 0.5) * 100.0;\n    degreesF = degreesC * (9.0/5.0) + 32.0; // write temps to Serial\nSerial.print(\"Celsius: \");\nSerial.print(degreesC);\nSerial.print(\" Fahrenheit: \");\nSerial.println(degreesF);\n\n// write temps to SD card\ntempsFile.print(\"Celsius: \");\ntempsFile.print(degreesC);\ntempsFile.print(\" Fahrenheit: \");\ntempsFile.println(degreesF);\n\n// close the file\ntempsFile.close();   } else {\nSerial.println(\"Error opening file in loop.\");   } delay(delayTime); } float getVoltage(int pin)\n{\n  return (analogRead(pin) * 0.004882814);\n}\n```"},{"id":"2016-05-17-reading-speech-virginia-woolf-machine-learning-and-the-quotation-mark","title":"Reading Speech: Virginia Woolf, Machine Learning, and the Quotation Mark","author":"brandon-walsh","date":"2016-05-17 07:03:28 -0400","categories":["Digital Humanities","Experimental Humanities","Grad Student Research"],"url":"reading-speech-virginia-woolf-machine-learning-and-the-quotation-mark","layout":"post","content":"[Cross-posted on the my personal blog  as well as the WLUDH blog . What follows is a slightly more fleshed out version of what I presented this past week at HASTAC 2016  (complete with my memory-inflected transcript of the Q&amp;A). I gave a bit more context for the project at the event than I do here, so it might be helpful to read my past two posts on the project here  and here  before going forward. This talk continues that conversation.] This year in the Scholar’s Lab  I have been working with Eric on a machine learning project that studies speech in Virginia Woolf’s fiction. I have written elsewhere about the background for the project  and initial thoughts towards its implications . For the purposes of this blog post, I will just present a single example to provide context. Consider the famous first line of Mrs. Dalloway : Mrs Dalloway said, “I will buy the flowers myself.” Nothing to remark on here, except for the fact that this is not how the sentence actually comes down to us. I have modified it from the original: Mrs Dalloway said she would buy the flowers herself. My project concerns moments like these, where Woolf implies the presence of speech without marking it as such with punctuation. I have been working with Eric to lift such moments to the surface using computational methods so that I can study them more closely. I came to the project by first tagging such moments myself as I read through the text, but I quickly found myself approaching upwards of a hundred instances in a single novel-far too many for me to keep track of in any systematic way. What’s more, the practice made me aware of just how subjective my interpretation could be. Some moments, like this one, parse fairly well as speech. Others complicate distinctions between speech, narrative, and thought and are more difficult to identify. I became interested in the features of such moments. What is it about speech in a text that helps us to recognize it as such, if not for the quotation marks themselves? What could we learn about sound in a text from the ways in which it structures such sound moments? These interests led me towards a particular kind of machine learning, supervised classification, as an alternate means of discovering similar moments. For those unfamiliar with the concept, an analogy might be helpful. As I am writing this post on a flight to HASTAC and just finished watching a romantic comedy, these are the tools that I will work with. Think about the genre of the romantic comedy. I only know what this genre is by virtue of having seen my fair share of them over the course of my life. Over time I picked up a sense of the features associated with these films: a serendipitous meeting leads to infatuation, things often seem resolved before they really are, and the films often focus on romantic entanglements more than any other details. You might have other features in mind, and not all romantic comedies will conform to this list. That’s fine: no one’s assumptions about genre hold all of the time. But we can reasonably say that, the more romantic comedies I watch, the better my sense of what a romantic comedy is. My chances of being able to watch a movie and successfully identify it as conforming to this genre will improve with further viewing. Over time, I might also be able to develop a sense of how little or how much a film departs from these conventions. Supervised classification works on a similar principle. By using the proper tools, we can feed a computer program examples of something in order to have it later identify similar objects. For this project, this process means training the computer to recognize and read for speech by giving it examples to work from. By providing examples of speech occurring within quotation marks, we can teach the program when quotation marks are likely to occur. By giving it examples of what I am calling ‘implied speech,’ it can learn how to identify those as well. For this machine learning project, I analyzed Woolf texts downloaded from Project Gutenberg . Eric and I put together scripts in Python 3 that used a package known as the Natural Language Toolkit ] for classifying. All of this work can be found at the project’s GitHub repository . The project is still ongoing, and we are still working out some difficulties in our Python scripts. But I find the complications of the process to be compelling in their own right. For one, when working in this way we have to tell the computer what features we want it to pay attention to: a computer does not intuitively know how to make sense of the examples that we want to train it on. In the example of romantic comedies, I might say something along the lines of “while watching these films, watch out for the scenes and dialogue that use the word ‘love.’” We break down the larger genre into concrete features that can be pulled out so that the program knows what to watch out for. To return to Woolf, punctuation marks are an obvious feature of interest: the author suggests that we have shifted into the realm of speech by inserting these grammatical markings. Find a quotation mark-you are likely to be looking at speech. But I am interested in just those moments where we lose those marks, so it helps to develop a sense of how they might work. We can then begin to extrapolate those same features to places where the punctuation marks might be missing. We have developed two models for understanding speech in this way: an external and an internal model. To illustrate, I have taken a single sentence and bolded what the model takes to be meaningful features according to each model. Each represents a different way of thinking about how we recognize something as speech. External Model for Speech: “I love walking in London,” said Mrs. Dalloway .  ”Really it’s better than walking in the country.” The external model was our initial attempt to model speech. In it, we take an interest in the narrative context around quotation marks. In any text, we can say that there exist a certain range of keywords that signal a shift into speech: said, recalled, exclaimed, shouted, whispered, etc. Words like these help the narrative attribute speech to a character and are good indicators that speech is taking place. Given a list of words like this, we could reasonably build a sense of the locations around which speech is likely to be happening. So when training the program on this model, we had the classifier first identify locations of quotation marks. Around each quotation mark, the program took note of the diction and parts of speech that occurred within a given distance from the marking. We build up a sense of the context around speech. Internal Model for Speech: “ I love walking in London,” said Mrs. Dalloway. “ Really it’s better than walking in the country .” The second model we have been working with works in an inverse direction: instead of taking an interest in the surrounding context of speech, an internal model assumes that there are meaningful characteristics within the quotation itself. In this example, we might notice that the shift to the first-person ‘I’ is a notable feature in a text that is otherwise largely written in the third person. This word suggests a shift in register. Each time this model encounters a quotation mark it continues until it finds a second quotation mark. The model then records the diction and parts of speech inside the pair of markings. Each model suggests a distinct but related understanding for how sound works in the text. When I set out on this project, I had aimed to use the scripts to give me quantifiable evidence for moments of implied speech in Woolf’s work. The final step in this process, after all, is to actually use these models to identify speech: looking at texts they haven’t seen before, the scripts insert a caret marker every time they believe that a quotation mark should occur. But it quickly became apparent that the construction of the algorithms to describe such moments would be at least as interesting as any results that the project could produce. In the course of constructing them, I have had to think about the relationships among sound, text, and narrative in new ways. The algorithms are each interpretative in the sense that they reflect my own assumptions about my object of study. The models also reflect assumptions about the process of reading, how it takes place, and about how a reader converts graphic markers into representations of sound. In this sense, the process of preparing for and executing text analysis reflects a certain phenomenology of reading as much as it does a methodology of digital study. The scripting itself is an object of inquiry in its own right and reflects my own interpretation of what speech can be. These assumptions are worked and reworked as I craft algorithms and python scripts, all of which are as shot through with humanistic inquiry and interpretive assumptions as any close readings. For me, such revelations are the real reasons for pursuing digital study: attempting to describe complex humanities concepts computationally helps me to rethink basic assumptions about them that I had taken for granted. In the end, the pursuit of an algorithm to describe textual speech is nothing more or less than the pursuit of deeper and enriched theories of text and speech themselves. Postscript I managed to take note of the questions I got when I presented this work at HASTAC, so what follows are paraphrases of my memory of them as well as some brief remarks that roughly reflect what I said in the moment. There may have been one other that I cannot quite recall, but alas such is the fallibility of the human condition. Q: You distinguish between speech and implied speech, but do you account at all for the other types of speech in Woolf’s novels? What about speech that is remembered speech that happened in earlier timelines not reflected in the present tense of the narrative’s events? A: I definitely encountered this during my first pass at tagging speech and implied speech in the text by hand. Instead of binaries like quoted speech/implied speech, I found myself wanting to mark for a range of speech types: present, actual; remembered, might not have happened; remembered incorrectly; remembered, implied; etc. I decided that a binary was more feasible for the machine learning problems that I was interested in, but the whole process just reinforced how subjective any reading process is: another reader might mark things differently. If these processes shape the construction of the theories that inform the project, then they necessarily also affect the algorithms themselves as well as the results they can produce. And it quickly becomes apparent that these decisions reflect a kind of phenomenology of reading as much as anything: they illlustrate my understanding of how a complicated set of markers and linguistic phenomenon contribute to our understanding that a passage is speech or not. Q: Did you encounter any variations in the particular markings that Woolf was using to punctuate speech? Single quotes, etc., and how did you account for them? A: Yes - the version of Orlando that I am working with used single quotes to notate speech. So I was forced to account for such edge cases. But the question points at two larger issues: one authorial and one bibliographical. As I worked on Woolf I was drawn to the idea of being able to run such a script against a wider corpus. Since the project seemed to impinging on how we also understand psychologized speech, it would be fascinating to be able to search for implied speech in other authors. But, if you are familiar with, say, Joyce, you might remember that he hated quotation marks and used dashes to denote speech. The question is how much can you account for such edge cases, and, if not, the study becomes only one of a single author’s idiosyncrasies (which still has value). But from there the question spirals outwards. At least one of my models (the internal one) relies on quotation marks themselves as boundary markers. The model assumes that quotation marks will come in pairs, and this is not always the case. Sometimes authors, intentionally or accidentally, omit a closing quotation mark. I had to massage the data in at least half a dozen places where there was no quotation mark in the text and where its lack was causing my program to fail entirely. As textual criticism has taught us, punctuation marks are the single most likely things to be modified over time during the process of textual transmission by scribes, typesetters, editors, and authors. So in that sense, I am not doing a study of Woolf’s punctuation so much as a study of Woolf’s punctuation in these particular versions of the texts. One can imagine an exhaustive study that works on all versions of all Woolf’s texts as a study that might approach some semblance of a correct and thorough reading. For this project, however, I elected to take the lesser of two evils that would still allow me to work through the material. I worked with the texts that I had. I take all of this as proof that you have to know your corpus and your own shortcomings in order to responsibly work on the materials - such knowledge helps you to validate your responses, question your results, and reframe your approaches. Q: You talked a lot about text approaching sound, but what about the other way around - how do things like implied speech get reflected in audiobooks, for example? Is there anything in recordings of Woolf that imply a kind of punctuation that you can hear? A: I wrote about this extensively in my dissertation, but for here I will just say that I think the textual phenomenon the questioner is referencing occurs on a continuum. Some graphic markings, like pictures, shapes, punctuation marks, do not clearly translate to sound. And the reverse is true: the sounded quality of a recording can only ever be remediated by a print text. There are no perfect analogues between different media forms. Audiobook performers might attempt to convey things like punctuation or implied speech (in the audiobook of Ulysses, for example, Jim Norton throws his voice and lowers his volume to suggest free indirect discourse). In the end, I think such moments are playing with an idea of what my dissertation calls audiotextuality, the idea that all texts recordings of texts, to varying degrees, contain both sound and print elements. The two spheres may work in harmony or against each other as a kind of productive friction. The idea is a slippery one, but I think it speaks to moments like the implied punctuation mark that come through in a particularly powerful audiobook recording."},{"id":"2016-05-23-3d-printing-in-the-classroom-outcomes-and-reflections-on-a-slavic-course-experiment-12","title":"3D Printing in the Classroom: Outcomes and Reflections on a Slavic Course Experiment (1/2)","author":"jennifer-grayburn","date":"2016-05-23 11:54:56 -0400","categories":["Makerspace"],"url":"3d-printing-in-the-classroom-outcomes-and-reflections-on-a-slavic-course-experiment-12","layout":"post","content":"Cross-posted on my personal blog. In a  previous post, UVA’s Slavic Librarian, Kathleen Thompson, and Slavic Lecturer, Jill Martiniuk, outlined the early stages of a 3D printing assignment for Yuri Urbanovich’s ‘Understanding Russia: Symbols, Myths, and Archetypes of Identity’ course. Kathleen and Jill now describe the unexpected obstacles and opportunities of this assignment in a two part blog post: If we had to describe this project in two words or fewer, we would definitely do so using the phrase “learning process”. Two days after the original blog post about the project and the accompanying exhibit went live, we had students curate the exhibit of their objects in the hallway of the Slavic Department, on the second floor of Cabell Hall. We had arranged for use of half of a display case that had been occupied by faculty books and items of Slavic ephemera, which we had permission to remove and re-arrange as needed. ![3D-exhibit-curating](http://static.scholarslab.org/wp-content/uploads/2016/05/3D-exhibit-curating-225x300.jpg) Students putting the final touches on their exhibit. The exhibit curation coincided with the Slavic Department’s biweekly Russian Tea &amp; Conversation event, in which students, faculty, staff, and community members interested in speaking Russian or learning about Russian culture gather to eat, drink tea, converse, and occasionally listen to speakers on special topics. The 2470 students brought their objects to this event, and after a brief introduction to the project from us, they presented their objects to the group and explained why they chose the objects they did. The students then took their objects to the display case and set up the exhibit (for which we had printed a brief blurb and some attributive text). General response to the project, and the objects, at this event seemed positive, which is why we were quite surprised the next day to hear that one faculty member had a very strong negative reaction to one of the objects in the display case. After some deliberation, we decided to remove the object from the exhibit, and a few weeks later (due to class not meeting because of Spring Break) we discussed this reaction with the students in class. The object in question was a  bust of Joseph Stalin, which the group printed in red. The faculty member who raised the objection is well-known for his work on Stalin’s system of forced labor camps, known as the Gulag. His most recent book on the topic features an image of Stalin on the front cover, which was displayed in the same case in which the students’ exhibition was curated. For an American student whose only experience of Stalin comes from textbooks, using this bust to represent Russia was a positive choice, because Stalin is seen in the U.S. as a strong leader who helped defeat Hitler during World War II. For the objecting faculty member, whose family has personal experience with the effects of Stalin’s policies, the bust carries entirely different connotations: genocide, violence, and turbulence. When we shared this with the entire class, the group that printed the Stalin bust maintained that they stood by their choice, and some of their classmates expressed the common sentiment that Germany is not usually characterized only by Hitler’s negative aspects, so why should Russia be characterized only by Stalin’s? We wanted to emphasize that the students were not in trouble, nor were their project grades affected, and we led them through a discussion of the ways in which their objects can be interpreted as symbols by different groups with various degrees of distance from those symbols. This discussion raised some salient points about the effect of a two-dimensional representation versus a three-dimensional representation, and (for us, at least) really drove home the point that 3D objects absolutely can bring symbols to life in new and unexpected ways. ![3D-exhibit-final-product](http://static.scholarslab.org/wp-content/uploads/2016/05/3D-exhibit-final-product.jpg) The finished product. Note the placement of the Stalin bust next to the book at the left. About a month after that discussion, towards the end of the semester, we gave students the option to re-print their objects if they wished, since at least one group had expressed interest in doing so – the group that printed the Rubik’s cube, for example, had thought about re-making their object to have movable parts. None of the groups chose this option, and headed into the work of their final projects with their original objects intact. The final project asked groups to complete two assignments. First was a 1,000-word essay addressing the ways in which the group’s view of their object changed, highlighting the following questions: In what ways does your object serve (or no longer serve) as a symbol of Russia? What might better symbolize Russia, or how could your symbol be improved? Did you have any misconceptions in your initial plan to work with this object? What were they? Where did they come from? Is this object ‘loaded’ with any preconceived notions from a Western perspective? From a Russian perspective? The second assignment was to create and give a 3-minute presentation addressing the following questions: What do the objects chosen by the entire class, and the ways the class arranged those objects, say about how we view Russia? What do these objects have in common? How do you think Russians might symbolically represent their views of the United States? Generally, student groups were satisfied with their objects as symbols of Russia, though each group recognized shortcomings in their initial conceptions of their objects and was able to articulate ways to make their object a stronger symbol. The Stalin group, for example, would have printed a figure of Stalin to represent him as a human being, rather than as a figurehead or an icon on a pedestal, based on the visceral reaction to the bust that could be seen as “glorifying” a figure whose history is murky at best. The group that printed St. Basil’s cathedral felt that their object lacked political symbolism, which they would have changed by adding a 3D model of the Kremlin to surround the church. The onion dome group offered two ideas for improvement: first, making their onion dome a dynamic object with multi-colored layers that represent what they call “the multiple forces at work in Russian society”, or second, printing a model of a nesting doll ( matryoshka ) to represent the seemingly outwardly-united Russia that depends on one leader at its core. The group with the Soyuz rocket would have changed the color of their object; rather than the white that represents Russia’s multicultural identity, they would have printed a red rocket to emphasize the importance of Communism and revolutions in Russia’s history. They might also have added an astronaut to the rocket, or adorned it with embellishments, to portray Russian opulence created by human hands. The Rubik’s cube group had initially expressed a desire to enhance their object with moving parts; this desire still held true at the end of the semester, though an acceptable compromise would have been to scatter the colors throughout the cube so that it did not have a uniform appearance and could thus express their claim that the puzzle that is Russia has not yet been “solved” in their lifetimes. Finally, the group that printed the scales stated that their object could have been more meaningful if the scales had been movable, rather than fixed, and if each side had had multiple boxes that could be moved to portray the fluctuating power balance between the U.S. and Russia. As a final note, a few days after classes ended for the semester, I (Kathleen) came across a link on  my Twitter feed  to a story about upcoming Victory Day celebrations in the Siberian city of Novosibirsk. (Russians celebrate Victory Day every May 9 to commemorate the end of World War II, which they refer to as “The Great Patriotic War”). The story focused on a set of billboards being erected in the city for the celebrations,  all of which prominently featured images of Joseph Stalin . This story adds another twist to our discussion of the use of Stalin as a symbol of Russia; for many Russians, he is still seen as a positive figure in the country, despite his treatment of Soviet citizens during his rule. Like the Russian  matryoshka doll, the use of Stalin as a representation of Russia is complex and multi-layered, and can be unpacked and re-arranged in a myriad of ways. In our next post, we will address which parts of this project worked and which did not, students’ and the instructor’s final thoughts on the project, and ideas for improving the implementation of such a project in the future."},{"id":"2016-06-08-3d-printing-in-the-classroom-outcomes-and-reflections-on-a-slavic-course-experiment-22","title":"3D Printing in the Classroom: Outcomes and Reflections on a Slavic Course Experiment (2/2)","author":"jennifer-grayburn","date":"2016-06-08 07:00:54 -0400","categories":["Makerspace"],"url":"3d-printing-in-the-classroom-outcomes-and-reflections-on-a-slavic-course-experiment-22","layout":"post","content":"Cross-Posted on my personal blog. UVA’s Slavic Librarian, Kathleen Thompson, and Slavic Lecturer, Jill Martiniuk, conclude their two-part evaluation of the 3D printing assignment for Yuri Urbanovich’s ‘Understanding Russia’ course . Considering both student and professor feedback, Kathleen and Jill offer suggestions to continue and improve this interactive assignment for future courses: Since this project was an experiment, some parts of it were bound to work better than others, and while we can say that this has generally been a positive experience for both students and instructors, both we and Prof. Urbanovich have several ideas on what could be improved for future iterations of a similar assignment. Overall, we think that this experiment was a success. It achieved its initial goals of getting students to think differently about symbols and their power and use, and the reaction to the one particular object almost certainly would not have been so strong if the depiction of that particular concept had not been in 3D. We base this conclusion on both instructor and student evaluations of both the course and the project, and the ways in which the project was integrated into the course. Students were given two evaluation forms to fill out at the end of the semester: one was the standard online evaluation through UVaCollab, and the other was a written evaluation handed out during the final exam. Not surprisingly, participation in the written evaluation was stronger than the online evaluation; only 12 of the 35 students filled out the online evaluation, and only one of those mentioned the 3D project. That comment suggested more class time to discuss the project in the week leading to the final presentation. Overall, however, students rated the course as worthwhile (81% said that they “strongly agreed” with that sentiment, and 9% “agreed”), and felt that they had learned a great deal in the course (66% said that they “strongly agreed” with that sentiment, and 33% “agreed”). We were more interested to see the written course evaluations because Prof. Urbanovich emphasized that students should mention the 3D projects as part of their evaluation. 31 of the 35 students submitted those written evaluations during the final exam! Of those, 17 mentioned the 3D project. Of the 17 that mentioned the 3D project, the average score for the course was 4.941 (out of 5); the overall average for the course was 4.548, so the evaluations that mentioned the project gave the course a higher rating than evaluations that did not. Course ratings remained high even among students who did not find the project very meaningful. Common negative reactions touched upon one of our biggest concerns, which was the integration of the project into the course: “a bit irrelevant”; “seemed unnecessary”; and “it was interesting but maybe didn’t fit well with this course” were a few notes of feedback. Most responses were quite positive, with students citing the project’s ability to spur creative thinking about Russian identity as a plus: “It was good to see how my perspective of Russia changed over the course of [the project]”; “[It] provided a more open ended approach to my exploration and learning about Russia”; and “I liked it in terms of analyzing preconceived notions and then applying model to material learned throughout the semester” were a few notes of feedback in this case. A couple of students made suggestions for improving the project in the future, such as making the groups smaller and providing clearer expectations of the final presentation. Finally, one student offered this advice: “Keep the 3D project, we all love it!” With this last note come two caveats: one, we realize that one student’s positive feedback about the project does not a collective opinion make. Two, these evaluations were written in the context of a final examination for which the professor was present, which, although anonymous, may not lend itself to the most honest criticism. Online evaluations are completely anonymous and not compulsory, so their efficacy as true measures of a group’s feeling about a course remains questionable. What worked: Discussions generated by the objects were thought-provoking, and making the students’ work public compelled them to do a good job of selecting and justifying their objects. Having the students curate their own exhibit gave them ownership of the project. Not forcing them to discuss their objects every class session probably helped them not feel burned out by the project, too. Even the backlash from that one object was a valuable learning experience, as it gave students some insight on how symbols are perceived by certain groups of people, and how powerful even the smallest or most innocently-intended representations can be. What didn’t work: The 3-minute presentation was supposed to be a soundbite, but the extra work created by having students record and re-record their presentations proved too cumbersome. General student reaction to the idea of a soundbite was negative, whereas reaction to a brief presentation was more positive (and, we think, more inclusive – it allows students to listen to one another in real time, rather than after the fact). We had originally thought about having students curate an online exhibit of their objects to explore how (if at all) digital exhibits reflect meaning in ways different from physical exhibits, using the Tumblr platform. Student reaction to this idea was lukewarm at best; after setting up the Tumblr account for the class, we abandoned the idea partially due to this reaction, and partially because we could not assign any tangible pedagogical value to the blog. We’re still mulling over whether or not Tumblr is a viable pedagogical tool; it may be, but perhaps not for a project like this, where the 3D component of the exhibit is vital to understanding how perceptions of objects as symbols change according to the medium. Curating a second exhibit in Alderman Library was neither useful nor interesting to the students or to us, since they – and we – did not see any value in putting up another display in an area with light foot traffic in which attention is not directed at the space in which the display would have been. Since this wasn’t our own class, it felt a little odd popping in to Prof. Urbanovich’s class once a week, or every couple of weeks, to talk to students about project components and issues that arose along the way. We wonder if students took us, and the project, as seriously as they might have if Prof. Urbanovich had been the sole conductor of the experiment, because some students may have perceived that they were test subjects (for lack of a better term), even though we were careful to explain early on that this was an experimental project. A few days after Part I of this blogpost went live, I (Kat) spoke to Prof. Urbanovich about his thoughts on the scholarly value of the project, and whether or not he would continue to include the project in future iterations of this course. He said that he would absolutely keep the project as part of the course, and suggested the following changes to it: - A more detailed explanation of the goals and aims of the project, and more discussion of the groups’ presentations and final papers. ** - In general, more in-class discussion of the symbols that students chose; Prof. Urbanovich noted that the most useful class meetings were the ones driven by student discussion, rather than instructor lecturing. ** - Pursuant to that, a teaching assistant or “project leader” who would not only introduce the project and work with students to print the models, and guide them through the exhibit-building and presentations, but would also facilitate bi-weekly in-class discussions relating symbols to that week’s topics. ** - Possibly leave room for comparison of U.S. and Russian symbols expressing a particular idea, and greater room for discussion of how Russians symbolize the U.S. ** - More discussion of the various meanings conveyed by 2D symbols, as opposed to 3D symbols. ** One major change that Prof. Urbanovich suggested would be to delay the initial printing of 3D objects in favor of showing symbols that already exist in Russia, and asking students to discuss their perceptions of those symbols. He gave the example of the Allies Monument that stands in Moscow’s Victory Park . He finds that students are usually shocked by the idea of a monument portraying soldiers from France, the U.S., the U.K., and the U.S.S.R. as allies, because they do not immediately make the connection between them as the group united against Hitler in World War II. Even more surprising to students is the fact that this monument was erected in 2005, some sixty years after the war ended! Giving students an idea of what symbols already exist, and then asking them to devise their own, might help give students new theoretical approaches to inform their choices, and lend increased significance to their cultural interactions. Finally, concerning the scholarly value of such a project, Prof. Urbanovich said that this project was very timely given the current tenuous state of U.S.-Russian relations, which he does not necessarily foresee stabilizing in the near future. For that reason alone, this project is worthwhile, because he has repeatedly and enthusiastically supported the facilitation of open discussion about the ways in which cultures perceive one another as a way of coming to a mutual understanding. Those who might question the need for a 3D project when a 2D project might suffice need only examine what happened when the bust of Stalin was printed – in a sense, “came to life” – to understand that such a project has fairly deep implications indeed.   For now, we continue to process our thoughts on this project, and will work to improve its structure and implementation for a more robust classroom experience. We will be participating in a roundtable, “Digital Humanities In and Out of the Classroom” at the Association for Slavic, East European, and Eurasian Studies (ASEEES) Annual Convention in Washington, D.C., in November 2016 to discuss this project with Slavic scholars working in the digital realm. We are also developing a 2-week undergraduate course called “Making a Digital Museum”, using online museum collections as a basis for creation of physical and virtual Russian-culture exhibit. We hope to tie this course to the return of the Fabergé eggs to the Virginia Museum of Fine Arts in Richmond, Va., in late October, for which the museum is preparing a renovated and expanded exhibit space. "},{"id":"2016-06-08-using-dh-to-explore-movement-and-meaning","title":"Using DH to Explore Movement and Meaning","author":"kelli-shermeyer","date":"2016-06-08 07:52:32 -0400","categories":["Grad Student Research"],"url":"using-dh-to-explore-movement-and-meaning","layout":"post","content":"Enjoy this guest post by Kelli Shermeyer, Doctoral candidate in the UVA English department, in which she describes her work with Professor Holly Pickett’s English 380 course at Washington &amp; Lee. This work is supported by ASC grant expanding collaboration between Washington &amp; Lee and the Scholars’ Lab . Cross-posted on the Washington &amp; Lee’s DH blog . “Playwrights write plays for the stage, not the study,” or so Roland Broude reminds us. Yet in my field of English literature, it’s quite common to study a play primarily as a textual object rather than a performance whose final form, tone, and affect all rely on extra-textual features. We don’t typically account for changes in play’s text during its first rehearsals (often these changes are implemented after the play text has been sent to print!), refinements in timing and intonation that occur during a show’s run, or even accidental line drops, forgotten words, or ad libs contrived by actors in reaction to something that happened during a particular performance. The reality of theater is that plays are constantly rewritten in a multitude of ways and we don’t have a lot of good ways to talk about that beyond acknowledgement. In our world of the single-author study and the copyright, one of the consequences of seeing dramatic texts primarily as “literature” is the following assumption that the play is entirely the property of its author, who, as Broude argues, “exercises over it a droît moral: his is the sole right to establish the text, and, once it has been established, to alter it.” Teaching from this paradigm limits engagement with the performer or designer’s role in creating the play’s affect or meaning. My work as a teacher, researcher, and theater director is to employ the digital humanities to help create ways that empower students to see a play as a complex nexus of decisions rather than a static textual object (for even the text itself is not stable). The problem that quickly surfaces is that performance (in many of its forms) is actually rather tricky to write about, because while we may have access to many versions and editions of the textual object (script), each enactment of that script is essentially ephemeral—a portion of it remains unrecoupable. Peggy Phelan has claimed that the ontology of performance is essentially its irreproducibility and she acknowledges the difficulty this presents in analyzing performance art. We can try to fix parts of performance in a variety of non-performative forms such as narrative, photograph, or video recording, but those other media can only offer ekphrasis, not full reproduction. The ephemerality of performance gives it much of its affective weight and political potential. While we may not be able to entirely recapture performance outside of ekphrasis, my hope is that we can develop tools and methods for examining dramatic texts and performances that can help us to translate some of the harder-to-capture elements of performance into forms on which we can engage in various kinds of analysis or reflection. One of the questions I am currently thinking through is how can we “read” movement? There’s some interesting work from the dance world that begins to think through these issues. Choreographer William Forsythe’s work with the Ohio State University (called Synchronous Objects ) is particularly fascinating. Earlier work by choreographers such Rudolf Laban developed notational systems for dancers based on certain ideas about the body in space (Labanotation, for example). But I’ve been struggling to try to find a way that connects movement and text (like a script) in a meaningful way. How do certain textual features invite us to think about certain movements? What in the text tells us to move to a particular place or in a particular way? Asking students these questions is also a way of approaching the critical practices like the close reading and formal analysis which still remains important to much (but not all!) of our work in literary studies. As a way to experiment with the relationship between movement and language, I worked with Holly Pickett’s English 380 class on two activities to help us discuss the relationship between the text and blocking of a scene. (Blocking is both a noun and a verb: it describes both the pattern of movement in a given scene and the act of directing/designing those movement patterns in rehearsal). First, I gave them the “to be or not to be” monologue from Hamlet Act 3, scene 1. I selected this text because I thought it was one they may be marginally familiar with and one that doesn’t contain many stage directions within the language (for example, when a character says “come here,” cuing another actor’s movement). I instructed them to draw Hamlet’s path throughout the monologue—where does he start, end, and where does he move throughout the speech? I did not give them any instructions on how to notate pauses, changing positions or how long Hamlet took to walk somewhere as I was interested in seeing how they would choose to notate this. I also asked them to use Prism to mark up the monologue, indicating where Hamlet started moving, stopped moving, or changed position in their blocking. I did not tell them in which order to do these tasks just that they had to do both of them. At the end of the allotted 20 minutes, I taped all of the drawings on the board and pressed the visualization button on Prism to see what we found. The Prism results revealed that there was a great variety in blocking styles, yet there were definable loci of energy around certain parts of the text. (Here’s the full visualization ) In this first image, you can see that a lot of students notated something around “end them? To die; to sleep,” but there disagreement as to what Hamlet was doing at those moments. We zeroed in on the word “end” as Prism showed there was some debate as to what movement happened on that word. The Prism showed that students either had Hamlet change position without changing location (indicated by the blue) or stop moving all together (indicated by the red). No one had Hamlet begin moving on this word (which would have been indicated by green). Throughout the whole monologue, “die” and “death” continued to appear as words where students thought some kind of movement or position change should occur, but we couldn’t agree as to what that movement should be: I have no definitive way to explain this: only a director’s hunch that there’s some sort of affective energy around the word and concept of death that we associate with anxiety that incites us to movement—we (or at least most of us) do not want to be still when facing death. Part of my future work is figuring out how to interpret these results. The other part of the activity—the drawings of Hamlet’s path—are much harder to read. Most drawings started Hamlet out on the center of the stage, not accounting for the first bit of text printed in the monologue directing that “Hamlet enters.” To me, this suggests some kind of connection between what we know about Hamlet, the role of this speech in the play, and center stage—we know Hamlet is the central figure and associate this important monologue with the center stage. But aside from that, the patterns varied. Most were well-balanced with Hamlet spending time on both sides of the stage (my mentors would be proud that both sides of the audience would get an equally good view of the actor). Some incorporated gestures or moments of intentional stillness. Many contained loops. Professor Pickett explained that she chose to do this in her drawing because of how she views the speech as rhetorically winding and wanted to create a movement pattern that reflected her reading. Several of the students actually marked words on their drawings as well connecting the text directly to their movement patterns: Some used no text at all and focused on the shape of Hamlet’s movements: And here’s one that was purposely playing with Hamlet’s winding rhetoric: So how do we make meaning out of all of this data? I’m in the process (the slow, painful process) of developing a tool (or likely, a set of tools) to help students visualize the connection between play text and movement patterns. By considering the way language suggests movement will, I hope, allow for a richer consideration of the formal stylistics of particular plays, but also in the long run create corpus of data on the way people see theatre texts. I’m interested in what new areas of inquiry open up if I can use a digital tool to process many blocking patterns of the same scene (i.e. perform a kind of distant reading on the movement patterns that I had the students create). At the least, we can get students to think more deeply about the way that the dramatic text is a living document brought to life, challenged, and enriched by a consideration of the ways its interacts with the body, and embodiment. This is important work for me both as a literary scholar and a theater director because of the reciprocal relationship between movement and interpretation. The director interprets the text to find places to block movement, and then the audience uses those movements to interpret certain moments on stage. Thinking about the relationship between movement and interpretation helps us to counter the belief that the playwright alone fixes the meaning of his or her “original text” and to recognize the larger networks of people, practices, traditions, and texts that make theater mean something. Broude, Roland. Performance and the Socialized Text. Textual Cultures: Texts, Contexts, Interpretation, Volume 6, Number 2, 2011, 24. Ibid. 25. Phelan, Peggy. Unmarked : The Politics of Performance. London ; Routledge, 1993. It’s entirely worth noting that there should be a lively debate about if there are elements of performance that should not be recorded or analyzed as well. Is the kind of work we’re doing creating a richer context for talking about performances, or are we violently decontextualizing aspects of performance that can’t be understood without the full (but sadly unrecoverable) picture? (Many thanks to Purdom Lindblad for first asking me a version of this question!) I was made aware of this interesting work in dance through sitting in on “The Art of Dance” taught by Kim Brooks Mata in the summer of 2015."},{"id":"2016-06-11-managing-director","title":"Managing Director","author":"alison-booth","date":"2016-06-11 05:57:51 -0400","categories":["Announcements","Digital Humanities"],"url":"managing-director","layout":"post","content":"We are so excited to be inviting applications for the position of Managing Director of Scholars’ Lab, in the University of Virginia Library. See posting 0618965 on the UVA HR site jobs.virginia.edu for more details (if need be, go to “Search Postings” link on upper left, enter the posting number). We did slightly reword the posting from June 9, which said “Ph.D. required.”  See the 6-10-16 edition: “Graduate study (Ph.D. preferred) in the humanities, library/information sciences, social sciences or related discipline.” We were always open to the experienced, advanced applicant, Ph.D. requirement having been in HR wording but not in spirit. Now letter and spirit match. We do hope for familiarity with graduate education and doctoral scholarship, for the good reason that Scholars’ Lab trains Ph.D. students in advanced research in digital humanities through the Praxis program and several DH dissertation fellowships each year. So graduate study is essential in the background of the new managing director. Scholars’ Lab engages in R&amp;D, notably Neatline and the Makerspace, various projects by staff such as Take Back the Archive and Collective Biographies of Women, and a fascinating array of research and pedagogy in many disciplines, from GIS to textual analysis, including collaboration with the Carter G. Woodson Institute for African-American and African Studies. UVA Library is a hub of diverse and innovative digital work, with teams working with many methods and platforms in addition to Scholars’ Lab. It’s going to be dazzling in the coming years, with our new University Librarian, John Unsworth. The Scholars’ Lab is fostering a community of practice by hosting the dh.virginia.edu site (in beta). Just this week in the Library, the Rare Book School course on digitization, run by Bethany Nowviskie and my colleague Andy Stauffer, hosted an engaging presentation by the Scholars’ Lab’s Jeremy Boggs and Purdom Lindblad on feminist interface and rich-prospect browsing. In other words, there is an active spirit of collaboration and a deep bench of expertise. As Academic Director, I see an auspicious alignment of stars in the current administration of Arts and Sciences, ITS, and other schools. It will be a great moment to unite energies and participate in this next phase of open-access humanities research, more library-centered than ever.   . Laura Miller and I will be happy to answer any questions about the position."},{"id":"2016-06-12-are-you-our-new-managing-director","title":"Are you our new Managing Director?","author":"laura-miller","date":"2016-06-12 05:10:28 -0400","categories":["Announcements","Digital Humanities"],"url":"are-you-our-new-managing-director","layout":"post","content":"We are thrilled to announce an exciting job opportunity on the leadership team here in the Scholars’ Lab. Read on for more details! Are you an innovative leader with a deep research portfolio and strong background in the management of digital humanities projects and practitioners? The University of Virginia Library seeks a Managing Director to help shape and steward our internationally respected Scholars’ Lab. This individual will work closely with the Academic Director to develop and support the deep resources for digital scholarship within the vibrant intellectual community at UVA. The Managing Director will inform the vision of the Scholars’ Lab and ensure its smooth and effective operation as a pan-university center within the Library, overseeing Scholars’ Lab staff, resources, and budget. He or she will also help to integrate Scholars’ Lab goals and activities into the service profile of the University Library. **Primary Responsibilities: **\nIn collaboration with the Academic Director, the Managing Director will: shape digital humanities services for the UVA Library; provide oversight of day-to-day operations of the Scholars’ Lab; develop, oversee, and retain talented staff; coordinate the use of all Lab resources (funding, space, personnel); develop the Lab’s budget, in collaboration with the Academic Director; help to prepare and submit grants; help to plan and organize intellectual programming, fellowships, pedagogical initiatives, and outreach; represent the Scholars’ Lab, serving as a liaison with University staff, faculty, and affiliated centers, as well as national and international peers; engage in collaborative planning with colleagues across the Library to promote and support digital scholarship; assist in designing and shaping projects; and help to establish high-level goals, intake processes, workplans, and MOUs for digital project collaborations. The Managing Director will keep abreast of new methodologies and practices relevant to digital humanities and will engage in professional development and their own (often collaborative) research projects related to the mission of the Lab, culminating in publication of results and/or presentation at appropriate venues. **Specialized Knowledge and Skills: ** Graduate study (Ph.D. preferred) in the humanities, library sciences, social sciences, or a related discipline; experience in an administrative position that includes supervision of personnel, experience with a technical area of digital humanities and the ability to advise on hardware and software purchasing and implementation; excellent oral and written communication skills; excellent organizational skills and ability to manage multiple priorities; demonstrated leadership, problem-solving, and decision-making skills; familiarity with recent scholarship and emergent best practices in the digital humanities; ability to teach in workshop or classroom setting and experience with effective digital pedagogy. Salary and Benefits: Salary is commensurate with experience, and expected to range between approximately $95 and $105k per annum. Excellent benefits, TIAA/CREF and other retirement plans along with generous funding for travel and professional development. To Apply:\nReview of applications to begin immediately and continue until position is filled. Apply through the posting on the University of Virginia’s online employment website. (If you need to search the Jobs@UVa portal, the posting number is 0618965 .)  Complete application, and attach cover letter and CV, with contact information for three current, professional references."},{"id":"2016-07-01-whats-the-scholars-lab","title":"What's the Scholars' Lab?","author":"alison-booth","date":"2016-07-01 09:59:41 -0400","categories":["Announcements"],"url":"whats-the-scholars-lab","layout":"post","content":"Obviously, there are lots of ways to find out what this center is and does, including an exploration of this website. I encourage anyone to see our Charter (not far from here, http://scholarslab.org/about/charter/). But if you want to know how we have been recently identifying ourselves, here’s a go: Within the University of Virginia Library, the Scholars’ Lab since 2006 has been a center for advanced digital scholarship in humanities, information and library science, social sciences, and related fields, emphasizing interpretative and theoretical as well as technological innovation and inquiry. The Scholars’ Lab consists of a collaborative staff of professionals serving a flexible community of graduate fellows, faculty, and staff whose activities intersect here. Who? Ten (now nine) people, and soon to be eleven or twelve; by some counts, two more. Academic Director, Head of Research and Development, Head of Public Programs, Information Architect, GIS Specialist, Project Manager, two Developers. Searching for Managing Director. Posting for second GIS Specialist in process. Our Head of Graduate Programs is irreplaceable, but we hope for a renewal of this position. Two additional specialists (you should see what they do with drones, Occulus Rift, photogrammetry!) have affiliated with the Digital Media Lab, Content Stewardship, and Scholars’ Lab. Members of the Scholars’ Lab reach out across the Library and schools of the University, frequently teaching workshops or courses here and elsewhere and presenting their research in conferences or publications. SLab, open to anyone, exists as much in communications and events as in a place located on the fourth floor of Alderman library, with its offices and graduate lounge, classrooms (shared by the Library), workstations in the public space, presentation facility, and Makerspace.  This is not a drop-off service center, but rather an incubator for consultation and potential sustained collaboration, helping to identify or provide the resources, expertise, and tools that a project needs to become a reality.  In supporting faculty research, we work in alliance especially with the Institute for Advanced Technology in the Humanities (IATH), a center for faculty fellowships and advanced digital research, and with Sciences, Humanities, and Arts Network of Technological Initiatives (SHANTI), a center that develops platforms, tools, and networks for teaching and research.  The Scholars’ Lab hosts and convenes an advisory committee for a website, dh.virginia.edu (in development), an online crossroads leading any user, within the university and outside it, into the network of diverse activities in digital humanities at UVA and elsewhere (with lists of people and projects and a calendar).  We continue to offer a series of lectures, visiting workshops, and short or long courses open to all.  In our service to faculty and students at the University, we collaborate with the subject liaisons of the Library and with several centers based in the Library and schools, including, for example, Research Data Services and the Carter G. Woodson Institute for African-American and African Studies.\nCurrent projects include:\n·      a summer program, Leadership Alliance Mellon Initiative, introducing minority undergraduates to research that may lead to graduate study;\n·      annual cohorts of six Praxis Fellows who learn while creating a group project (graduate students in various fields)\n·      several DH dissertation fellowships annually\n·      Consulting and collaboration with a range of faculty- and student-initiated projects from many fields and schools\n·      Neatline, an Omeka plug-in that has built on years of Scholars’ Lab innovation in visualizing geo-temporal data in humanities research;\n·      A Makerspace for 3D printing and innovative design\n·      Research on “adovocacy by design,” feminist interface, and rich-prospect browsing, in collaboration with Take Back the Archive (on the history of sexual assault at the University of Virginia) and Collective Biographies of Women (a study of the history of representations of women in printed collections of biographies)\n·      GIS consultation and instruction\n·      Affiliated staff working on photogrammetry, augmented reality, and various digital multimedia tools\n·      Continuing innovations in textual editing and literary history as well as textual analysis such as topic modeling\n·      collaboration with IATH and SHANTI as well as VP for IT Ron Hutchins and the Deans of the Library and Arts and Sciences on a conference on DH at UVA, Fall 2016;\n·      With the Woodson Institute and others, a series of lectures in 2016-2017 on digital diversity, accessibility, race, and gender."},{"id":"2016-07-03-what-has-scholars-lab-been-up-to-or-a-community-of-practice-communicating","title":"What has Scholars' Lab been up to? Or, a community of practice, communicating.","author":"alison-booth","date":"2016-07-03 12:57:44 -0400","categories":["Announcements"],"url":"what-has-scholars-lab-been-up-to-or-a-community-of-practice-communicating","layout":"post","content":"Like any active research/development/teaching/service team, the Scholars’ Lab faces a challenge keeping up with each others’ diverse collaborations, and communicating what we do.  Since January, I’ve learned about the Library’s systems of recording interactions with students and faculty, and I’ve encountered various applications that will somehow track our data, but I think we need more than CRM (customer relationship management).  We’ve grown our own self-recognition system, as simple as: monthly activity statements, in a template of categories, shared on Box.  (And Slack is working for us.)  Happily, it turns out that staff enjoy writing these very short notes ( gee, I actually have done more than I remembered! ) and reading each other’s ( oh, I’m glad to know she’s doing that ; or, oh, I talked to that faculty member separately, hmmm ).  We have biweekly scrums, but there’s much more detail in the shared self-reflective accounts. ![](http://giving.virginia.edu/campaign/wp-content/uploads/sites/2/2013/10/librarymain.jpg) Alderman Library (not in July) I post some excerpts of my “seasonal” compilation of activities in April and May, 2016, from the staff’s monthlies.  I don’t think this somehow scores points compared to other flourishing parts of this library, or other flourishing centers of digital humanities, but I think it profiles a community of practice centered in our practice of communicating. I was genuinely surprised and excited by the depth and breadth of the “monthlies” by Ammon, Chris, Eric, Jeremy, Laura, Purdom, Ronda, and Scott.  Now, if I could only find the monthly time to add my own chronicles of collaboration! Thanks to Ron Hutchins, Worthy Martin, Judy Thomas, Martha Sites, David Germano, and Rennie Mapp for the regular meetings and exciting planning, this spring. April and May, 2016: Consultations/Collaborations, Faculty and Students, Library Colleagues—indicating people/projects, not hours or frequency of repeat sessions Nineteen consultations and collaborations with faculty from 12 departments/programs : Women and Gender Studies, Carter G. Woodson Center for African-American and African Studies, History, English, Curry Education School, German, Kinesiology, Religious Studies, Biology, Sociology, Archeology/Art History, Slavic At least twenty students from doctoral to undergraduate (in addition to Praxis), in such fields as Music and Anthropology: individual consultations, not including hands-on aid in Makerspace; from learning Python to project design to GIS **Scholars’ Lab as Starting Point, Incubator, Training Center, and Curator of Projects in DH ** With a wealth of centers and nodes of digital expertise and activity at UVA, Scholars’ Lab has taken initiative to improve collaboration and communication, launching http://dh.virginia.edu, a website, in May (in development).  Many faculty and student projects may be born and in due course put to rest/sustainable pasture through Scholars’ Lab, and we guide people to find the collaborators and resources they need in between.  We are a consultation hub and work closely with IATH, SHANTI, Research Data Services, Arts and Sciences and Provost’s Office administration, Library leadership, and ITS, among others. **Some of the Continuing Projects: **Neatline; Geoblacklight; Makerspace; Take Back the Archive (American Studies); Collective Biographies of Women (with IATH); For Better for Verse; Participatory Media; Salem Witch Trials; text analysis (18th-c. English); projects in NLP, GIS, 3D printing, augmented reality, data mining from social media, sonification Speakers and Events Series.   In Spring, 2016, we held eight events open to the public and the Library, three of which focused on Praxis fellows (current or alum) or the Visiting Fellow presenting on their work.  In spring, 2016, we began planning a DH@UVA fall conference and a lecture series for 2016-2017 on diversity and access issues in DH. Teaching Courses and Support of Course Projects Members of the staff collaborate with the Teaching and Learning group in the Library and the subject liaisons, particularly Chris Ruotolo, to offer workshops on Neatline and GIS, or to offer short and for-credit courses.  One staff member taught an undergraduate course in American Studies.  SLab staff taught workshops for undergraduate and graduate courses in English, Architecture, German, and other fields.  Staff have collaborated on a course-based project on the archeology of Flowerdew Hundred. Research, Presentations, Publications Members of Scholars’ Lab are active in professional development; for instance, taking a course in Design Thinking; learning new methods of topic modeling; keeping up with the latest in bots and 3-D printers as well as drones and VR; presenting research that is informed by narrative theory, critical race studies, queer theory, or cultural geography; adapting research on user experience and information design.  Our work will be presented at DH2016 in Krakow, by six members of the staff, at a pre-conference workshop on biographical data and several other sessions.  SLab members presented at the British Library and at Yale University this spring.  A co-authored essay was proposed, accepted, and is now in process for publication in DH+Lib."},{"id":"2016-07-11-lower-the-stakes","title":"Lower the Stakes","author":"ronda-grizzle","date":"2016-07-11 07:02:53 -0400","categories":["Technical Training"],"url":"lower-the-stakes","layout":"post","content":"I recently received the greatest compliment on my technical training ability that I’ve ever gotten. When the Seminar on the Acquisition of Latin American Library Materials ( SALALM ) conference was held here at UVA this past May, I led a Neatline workshop for 45 attendees of the conference. For 90 minutes, we worked through the Lab’s standard introduction to the Neatline interface together, solving network issues and technical glitches as we went (something that I could not have accomplished without the able assistance of my Scholars’ Lab colleagues, who all volunteered to serve as “back of the room” tech support!), and at the end of the session my 45 students all had a small working Neatline exhibit and ideas about how they might apply that digital tool in their own research and at their own institutions. The compliment? One of my students told me that she’d had so much fun, that she wasn’t aware of the time passing. So how does that happen? A novice level student having so much fun that she experiences a flow state while learning a complex interface, in a very crowded, noisy classroom where people are trying to follow the instruction, or get their wireless passwords to let them connect to the network, or to figure out why their browser did that weird thing they weren’t expecting? We lower the stakes. We take the training very seriously, but we make it as much fun for the students as possible. Our Neatline workshop is designed to reduce risk for the students. We always teach in our sandbox installation of Neatline, using a pre-populated set of Omeka records and a carefully scripted workflow that introduces students to every part of the interface with just enough repetition that concepts are reinforced without inducing boredom. Students are not working on a site where their work will later be graded, or will be in public view. They’re not working with data that matters to them professionally or academically. We deliberately lower the stakes so that they don’t feel as much pressure as they otherwise might. Playfulness is a good path to learning. Making the workshop as playful as I can helps people to let go of their tension around the vulnerability that learning requires (adult learners can be especially stressed about this!), and a less stressed student is a student who is more able to stay engaged, and who later retains more of what they’ve learned. Are you a trainer or instructor? How do you lower the stakes for your students?"},{"id":"2016-07-22-robocamp-2016","title":"Robocamp 2016","author":"ammon-shepherd","date":"2016-07-22 12:02:04 -0400","categories":["Research and Development"],"url":"robocamp-2016","layout":"post","content":"I had the pleasure of spending a week with the folks in the Architecture School learning and playing with their Kuka robot (named Karl, http://www.robotsinarchitecture.org/ ). This was the first run of a hopefully recurring camp to introduce faculty and staff to the robot arm in the Fab-lab in the A-school. Most of the participants were A-school faculty, but there was also me from the Library, someone from the Music school’s Maker space, and someone from the Bio-engineering school (they just purchased a Kuka KL 3000, a much bigger bot). Over four days we learned how to make the robot do marvelous and wonderful tricks. Day 1: Push some clay around. The object of this day was to learn the basics of the software that runs the Kuka robot. We picked a tool that we would have the robot push into the clay at a certain number of intervals. The idea being to pour plaster onto the clay to create a permanent design: [gallery link=”file” size=”medium” columns=”2” ids=”12823,12824”] This was the most basic tool, just an extension of the arm itself, with different shapes on the end. Day 2: Cut a foam block This day we used the robot to cut a foam block with a hot wire. We drew a line in the software that would correspond to a long vertical cut in the block. What we didn’t know at the beginning, was that after the first cut the block is rotated 90 degrees, and the cut is made again. This produced four long pieces that could be rotated to form a column with a unique profile on each side. I wrote my name with the line, which produced this: The focus of this day was to help us think spatially. We could add twists to the curves which led to columns that were hard to mentally visualize how they would turn out. Here is a sample of the columns our group generated: [gallery columns=”2” size=”medium” ids=”12835,12836,12834,12833”] Day 3: Light writing We used a little bit more sophisticated tool end this day. It was an LED light that could switch colors. We designed a shape and assigned a color to each line or curve or segment. I didn’t get very creative on this one, but others had great ideas: [gallery size=”medium” ids=”12839,12837,12838,12840,12841,12842,12843”] Day 4: 3D extruding This day was a more demonstrative than hands-on. We went through the basics of how to get the robot to use a 3D pen to print filament in 3D space. Rather than limiting printing to a series of layers on top of each other, the printing can happen in true 3D space. Finally, we sat together and discussed how robotics can and should impact our fields, and how the robo-camp went in general. The discussion was light, and basically just touched on the following topics, or simply asked these questions. No deep discussion or trying to answer, just a quick question- and thought-dump session. What can robots do that humans can’t, besides going faster and more accurate? What can robots do for humanities research? How can they improve and help data visualization? What are the components necessary for the robot to function? The robot has motion, but it needs sensory input, data, in order to act on it. One of the former students presented an example from their year of working with the robot and noted a frustration that perhaps Libraries can and should be able to help with. This group wanted to use the robot to print cement in true 3D (not like traditional 3D printers that basically print 2D layers, and then print layer upon layer). In order to accomplish this, they had to design and build their own tool. The idea was to have a nozzle that changed shape while extruding the cement, thereby adding another layer of control and design to the printed object. The group was frustrated in the many hours spent in figuring out how to get motors to operate, when there are people out there (and probably students on campus) who know how to do this in their sleep. They spent time developing the tool instead of refining the product. Sometimes creating the tool is a beneficial and important step in creating a product. Other times it detracts from the end goal. Libraries could/should be a great resource for connecting two groups that can benefit from each others collaboration. The Scholars’ Lab can/should be such a hub of networking and connection. All in all it was a great experience. Mind opening and enlightening."},{"id":"2016-08-12-neatline-implementation-grant","title":"Neatline Implementation Grant","author":"eric-rochester","date":"2016-08-12 05:29:52 -0400","categories":["Announcements"],"url":"neatline-implementation-grant","layout":"post","content":"You may have noticed on Twitter or elsewhere that the NEH announced funding for almost 300 humanities projects . Congratulations to all! One of the projects awarded was our Neatline Omeka plugin! We’re really excited by the possibilities that this will open up for this project and the ways that we’re planning on improving it. So what do we hope to accomplish? What should you be looking for from the future of Neatline? Primarily, we’re going to focus on graphesis and sustainability . First, Neatline has always been an experiment in trying to embody the principles of graphesis . It’s motivated by the belief that interacting with your project and your data in a visual, hands-on, and messy way informs and changes the way that you think about your project. Of course, modern technology is limited in how well it can enact these principles. As mobile platforms and tablets have matured, however, they offer a more hands-on, material experience. We’d like to incorporate these new technologies and leverage them to improve the experience of creating and exploring Neatline exhibits. Related to this, we want to improve the Neatline Text add-on. This allows Neatline exhibits to incorporate and interact with long-form text. Unfortunately, the editing interface is still very rough. Improving this would go a long way to making Neatline a more compelling platform for creating geotemporal exhibits around a text. And in general, we’ll revisit the editing interface for Neatline to streamline it for common tasks, while making less used features still easily accessible. Second, Neatline is itself also maturing, and like any software project, it’s aging gracefully in some ways, and—ahem—less so in others. We’d like to take this opportunity to make Neatline a more sustainable project. This is a broad goal, so let’s tease it apart. What do I mean by this? The Omeka team at RRCHNM are working on a new version of the platform, called Omeka S, that will make it significantly easier to host and managing multiple Omeka instances. This will involve significant changes to the plugin architecture, however, so we’ll update Neatline to work with this new version of Omeka. Currently, Neatline uses OpenLayers for its mapping component. After we included it in Neatline, however, OpenLayers released version 3, which is a major rewrite of this component for modern browsers. Because of licensing issues, it also no longer supports Google maps. We’ll take this opportunity to evaluate upgrading to OpenLayers 3 or going with a different mapping component altogether, such as LeafletJS . For the timeline component, Neatline uses the SIMILE Timeline . This software is also showing its age, so we’ll look at using a different timeline component, possibly creating one ourselves. This will also give us the chance to evaluate the data models that we use to represent time and change them to accommodate ambiguity and fuzzy dates better. This will also make this part of Neatline better suited to messy humanities data. But the most important sustainability feature that we’d like to work on is building a more sustainable and active community. This includes better documentation, better tutorials, and better support for getting new developers set up to work on Neatline. We’d also like to make it easier for users to contribute in many ways to the project, whether through code, documentation, design, or other ways. We’re really excited about the opportunities that this grant opens up for us, and we’re so grateful to the NEH ODH for providing the resources for this."},{"id":"2016-08-23-programmatically-building-high-level-charts-with-bokeh","title":"Programmatically Building High-level Charts with Bokeh","author":"scott-bailey","date":"2016-08-23 05:43:39 -0400","categories":["Technical Training","Visualization and Data Mining"],"url":"programmatically-building-high-level-charts-with-bokeh","layout":"post","content":"A couple of months ago, while preparing for the Digital Humanities 2016 conference, I was trying to build a series of charts to visualize data results from some topic modeling I had done. Specifically, I had a data file in which each row was a document and the columns were topic proportions. Reading across any row, you could understand the document’s composition according to the twenty topics of the topic model. Reading down a column, you could understand how a topic was more or less present across the text corpus, in this case a series of logically and chronologically ordered documents. For the charts, I wanted to create a separate chart for each column of the data table so that I could see the distribution of the topic across documents and understand how it was more or less present in different sections of the corpus. I figured bar graphs would be a straightforward way to do this, with each bar representing the topic proportion for a single document. I initially tried to do this with D3, a Javascript library frequently and regularly used to create interactive visualizations. I’ve used D3 before, and while I do find it somewhat tedious, it’s usually not too difficult to craft the more straightforward types of graphs, like bar graphs. I ran into an issue, though, which is that I wanted to take my single data file and produce twenty different bar graphs, and I wanted to do it programmatically rather than having to write code for each graph. This is certainly possible in D3, but while I made progress toward it, my Python loving self grew quickly frustrated with how hard it seemed to be to iterate through the columns of the data table, something I knew I could do quickly and easily with Python and pandas, perhaps the most regularly used data analysis library in Python. I decided, then, that it was a great opportunity to learn to do a bit of data visualization in Python. There are a number of different libraries available to you: matplotlib, Seaborn, Bokeh, and more. I chose to use Bokeh due to its easy integration with pandas and its multi-level API. With Bokeh you can quickly and easily create high-level charts, such as bar graphs, box plots, or histograms, with a minimal amount of code and configuration. Or you can work with Bokeh’s mid-level interface, creating figure elements such as circles and adding them to figures you create. Finally, if you want complete control over every bit of the chart, you can work directly with Bokeh’s models . Since I just wanted to create a series of bar charts, I worked directly with Bokeh’s high-level chart API. Here’s the script I wrote with extensive comments. ```\n# import pandas to handle data\nimport pandas as pd\n# from bokeh’s high-level charts api, import Bar to create the bar graph,\n# output_file to save the generate html files, and show to immediately show\n# the generated html files\nfrom bokeh.charts import Bar, output_file, show def break_names(df):\n    “\"”Parse filenames into paragraph numbers and titles, adding those\n    into the dataframe”””\n    # grab the column with filenames and convert it to a list\n    filenames = df[‘file’].tolist()\n    # create empty lists to hold paragraph numbers and titles\n    # every document in the corpus has a unique paragraph number and title\n    para_nums = []\n    titles = []\n    # iterate over the filenames, split them, and push each component into the\n    # correct list\n    for filename in filenames:\n        num_and_name = filename.split(‘/’).pop()\n        chunks = num_and_name.split(‘-‘)\n        para_nums.append(chunks[2])\n        titles.append(chunks[3])\n    # create new dataframe columns from the lists of paragraph numbers\n    # and titles\n    df[‘para_nums’] = pd.Series(para_nums)\n    df[‘titles’] = pd.Series(titles)\n    # return the dataframe with the added columns\n    return df def main():\n    # read in the csv with the data\n    data = pd.read_csv(‘barth_composition.csv’)\n    # create the dataframe with paragraph number and title columns\n    cleaned_data = break_names(data)\n    # get the column names from the dataframe\n    columns = cleaned_data.columns.values\n    # use a list comprehension to create a list of all the column names that\n    # contain the word ‘topic’, e.g. ‘topic-01’\n    topics = [topic for topic in columns if ‘topic’ in topic]\n    # for each topic/column create a bar graph\n    # see documentation here:\n    # http://bokeh.pydata.org/en/latest/docs/user_guide/charts.html#bar-charts\n    for topic in topics:\n        plot = Bar(\n            cleaned_data,\n            ‘para_nums’,\n            values=topic,\n            ylabel=’Proportion’,\n            xlabel=’Paragraph Number’,\n            title=”Proportion per pararaph of “ + topic\n        )\n        # save the html output file\n        output_file(‘charts/’ + topic + ‘.html’)\n        # open the html output file in the browser\n        show(plot) if name == ‘ main ’:\n    main()\n``` I really like Bokeh, and creating a series of bar charts was incredibly straightforward. There is a definite limitation with Bokeh’s output options though. At the time I wrote this script, the primary output type is an HTML file, which is entirely reasonable given that Bokeh describes itself as “a Python interactive visualization library that targets modern web browsers for presentation.” It is deliberately positioned as a Python alternative to writing D3 yourself. There have been requests for the Bokeh team to add the ability to directly save output as either SVG or PNG, but, given this Github issue thread, it seems like there are significant technical blocks to implementing this in the near future. That said, the interactive graph window does allow you to click an icon to save a static version of the graph as it currently appears in the browser as a PNG. While this isn’t particularly efficient, it works just fine when you need to generate a few static figures for papers or presentations. Since I’m thinking of creating a website that includes these visualizations, I’m not bothered by the HTML output, though in the future, when I’m just creating figures for a presentation, I might use Seaborn, built on matplotlib, which does allow you to save directly to PNG and other formats. On the whole, especially if you’re already a Python person and find writing D3 a bit tedious, I strongly recommend taking a look at Bokeh."},{"id":"2016-08-26-fall-2016-uva-library-gis-workshops-series","title":"Fall 2016 UVa Library GIS Workshops Series","author":"chris-gist","date":"2016-08-26 06:16:27 -0400","categories":null,"url":"fall-2016-uva-library-gis-workshops-series","layout":"post","content":"Fall 2016 UVa Library GIS Workshop Series All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be taught on Wednesdays from 3PM to 4PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community. September 21st\n**Making Your First Map with ArcGIS\n**Here’s your chance to get started with geographic information systems software in a friendly, jargon-free environment.  This workshop introduces the skills you need to make your own maps.  Along the way you’ll get a taste of Earth’s most popular GIS software (ArcGIS) and a gentle introduction to cartography. You’ll leave with your own cartographic masterpieces and tips for learning more in your pursuit of mappiness at UVa. September 28th\n**Georeferencing a Map - Putting Old maps and Aerial Photos on Your Map\n**Would you like to see historic map overlaid on modern aerial photography?  Do you need to extract features of a map for use in GIS?  Georeferencing is the first step.  We will show you how to take a scan of a paper map and align in it in ArcGIS. October 5th\n**Getting Your Data on a Map\n**Do you have a list of Lat/Lon coordinates or addresses you would like to see on a map?  We will show you how to do just that.  Through ArcGIS’s Add XY data tool and Geocoding (address matching), it is easy to take your tabular lists and generate points on a map. October 12th\n**Points on Your Map: Street Addresses and More Spatial Things\n**Do you have a list of street addresses crying out to be mapped?  Have a list of zip codes or census tracts you wish to associate with other data?  We’ll start with addresses and other things spatial and end with points on a map, ready for visualization and analysis."},{"id":"2016-09-09-coming-soon-dhuva-conference-oct-1415-2016","title":"Coming soon: DH@UVA Conference Oct. 14&15, 2016","author":"alison-booth","date":"2016-09-09 10:53:39 -0400","categories":null,"url":"coming-soon-dhuva-conference-oct-1415-2016","layout":"post","content":"After you enjoy NEH@50 celebrations, and in between this fall’s workshops and speakers, come to an intimate conference on digital humanities, right here on Grounds, Friday and Saturday October 14 and 15, 2016.  We’ve been developing an interesting conference program aimed at building the DH community at UVA.  At this event we’ll also be further introducing the design and purposes of the website (in development): http://dh.virginia.edu/ .  This event’s reflection on the present and future is a joint effort of ITS, the Library, Arts &amp; Sciences, and the Provost’s Office; Scholars’ Lab, IATH, and SHANT I. Events are free and open to the public. We’ll be looking for faculty and graduate students to participate in the program, so please reply to Rennie (see below) or to me, booth@virginia.edu if interested in a lightning talk, participation in a roundtable, or otherwise. For more preliminary detail, here is the Save the Date message prepared by the Project Manager of the conference, Rennie Mapp:\nDear members of the UVa DH community: Please plan to attend DH@UVa 2016 on Friday, October 14 and Saturday, October 15 in the Harrison/Small Auditorium of the Special Collections Library. We’ll be considering the present and future of the Digital Humanities at the University of Virginia. Conference sessions will include panel discussions with outside speakers, lightning talks, break-out conversations, pop-up brainstorming, and opportunities to talk informally with others at UVa who are interested in the Digital Humanities (broadly speaking). The public, faculty, staff of the libraries and other organizations, and students at the University are most welcome! Soon I’ll be notifying you about open registration for these events on our website. The conference is FREE but we appreciate notice that you’ll attend so that we can plan food, refreshments, and space. In the meantime, please mark your calendar and consider giving a lightning talk or formulating a good topic for a break-out conversation. If you have any questions or thoughts about the conference, please contact me, Rennie Mapp (conference manager), at rcmapp@gmail.com This conference is sponsored by Ron Hutchins, VP for Information Technology. It is organized by Alison Booth, Director of Scholars’ Lab; David Germano, Director of SHANTI; and Worthy Martin, Director of IATH–in consultation and with the active support of Archie Holmes, Associate Provost for Academic Affairs; John Unsworth, Dean of Libraries and University Librarian; and Francesca Fiorani, Associate Dean for Arts and Humanities."},{"id":"2016-09-21-are-you-our-next-head-of-graduate-programs","title":"Are you our next Head of Graduate Programs?","author":"alison-booth","date":"2016-09-21 11:24:07 -0400","categories":null,"url":"are-you-our-next-head-of-graduate-programs","layout":"post","content":"“We build up people and practices more than products.” That’s part of our charter, and the Head of Graduate Programs is essential to this mission.  We in the Scholars’ Lab and University of Virginia Library are thrilled to alert you to the new job posting https://jobs.virginia.edu/applicants/Central?quickFind=80148 As the description on the University of Virginia job site emphasizes, the primary responsibilities focus on the Praxis Program and other graduate DH Fellowships.  “Mentoring, managing day-to-day operations, and coordinating staff support for both team-based and individual graduate fellowship programs at U.Va. Library. Developing intellectual programming in the digital humanities for the Scholars’ Lab and building community among emerging scholars at U.Va. Fostering collaboration on humanities training and research support with internal and external partners.”  The Head of Graduate Programs works closely with graduate programs around the university to arrange fellowships and collaborates with librarians in enhancing the digital curriculum.  Feel free to imagine how you might weave your own interests and research into these responsibilities (we make time for staff members to pursue research projects).  We look for talent in communication and community building, experience in the context of digital research and instruction, and familiarity with graduate studies in your discipline(s): “Master’s degree or equivalent in the humanities or humanistic aspects of information science.” This is a great time to join the broad and deep teams of digital scholarship at the University of Virginia, with our new Dean of Libraries John Unsworth and the University’s reconfirmed commitment to innovative humanities research in the biggest tent of DH.  Scholars’ Lab collaborates with the Carter G. Woodson Institute for African-American and African Studies and the Institute of the Humanities and Global Cultures .  The Head of Graduate Programs can help shape both the research agenda and the series of workshops and public programs that serve the entire University community, along with the Scholars’ Lab’s directors, the Head of Public Programs, and others in this collaborative center within the Library. Salary and Benefits: Salary is commensurate with experience, and expected in the $65,000 - $75,000 range per annum. Excellent benefits, TIAA/CREF and other retirement plans along with generous funding for travel and professional development. To Apply:\nReview of applications to begin immediately and continue until position is filled. (If you need to search the Jobs@UVa portal, the posting number is 0619604).  It is easy to apply: complete the application, and attach cover letter and CV, with contact information for three current, professional references."},{"id":"2016-11-09-3d-printing-for-fun-and-presentation","title":"3D printing for fun and presentation","author":"ammon-shepherd","date":"2016-11-09 04:30:56 -0500","categories":["Makerspace"],"url":"3d-printing-for-fun-and-presentation","layout":"post","content":"This post is a quick update on a couple of projects by students who frequent the makerspace. All of the students who use the space are doing amazing things, and we hope to highlight some of those projects like this more often. The project write ups are written by the students themselves. So without further introduction, here are Brandon Phan and his Pokédex phone case and Arian Azizi and civil war bullets for a class presentation. [Header image source: https://commons.wikimedia.org/wiki/File:International_Pok%C3%A9mon_logo.svg] Brandon Phan - Pokédex 3D printing a pokedex case has been the highlight of my 3rd week as a first year. I learned so much about the process and the materials used in 3D printing. It’s really mesmerizing to watch the nozzle create something you saw on the computer screen. Everyone who worked there appreciated my eagerness and were did their best to help me finish my case and learn more about the machine. We messed up a few times but we were always able to fix it without having to redo the whole case. I learned the difference between PLA and ABS. I learned how different machines work. I learned about nozzles and layer sizes. I learned about how to print. Along the way of my educational adventure, I got a pokedex case for my phone that helps me catch pokemon on Pokemon Go. Whenever, I walk with Pokemon Go open, some people ask where I got it. I point them in the direction of the MakerSpace lab in Alderman West Wing. Many people don’t know about it, and while it has been my secret for a little bit, I hope more people come as long as they don’t take all my printing timeslots! Also it saves me a lot of money as a first year who doesn’t have a car. Instead of buying guitar picks, I can just make my own! ![pokedex](http://static.scholarslab.org/wp-content/uploads/2016/11/pokedex-1024x768.jpg) Image source: Brandon Phan ## Arian Azizi - Civil War Bullets The most important part of any presentation is of course…the presentation. No matter how comprehensive and long the content, if the actual delivery of the material itself fails to deliver, the entire project itself falters. In order to create a robust presentation then, the perception of the information must engage the audience. And what better way to get the attention of viewers than providing them with something more than a mere PowerPoint slide?  Having a hands-on experience always leaves an impression on a group that simple text will never be able to match. Thus when I was pondering over how best to capture the attention of my audience, the notion of tangible setpieces sprung to mind. In my presentation, which had the aim of conveying all of the emotion, strategy, and information of the Civil War in a mere 10 minutes, I pulled back to one very basic question: how was the war fought? The simple answer of “guns and bullets” may have checked the box off information-wise, but would it really make the audience invested in the trials and tribulations of the soldiers? It was then that I considered the idea of having the audience be able to actually look at these very tools up close. It is one thing to see pictures of bullets in photographs centuries old, but it is another thing entirely to hold one such model personally. To turn a rifle bullets over in your hands and imagine the destruction that such a small object could inflict. The chosen bullet in question, the Minié ball, was such an improvement over the older musket balls that instead of merely lodging inside of the target, it would tear muscles and cause bones to shatter. These very bullets, popularized through the Civil War itself, marked a transition in the art of warfare itself. The 3D printing of these bullets was an interesting component of the project. Created out of plastic filament, these scale models were built from the ground up, crafting an entire replica of a metal bullet in under an hour.  The presentation itself went exceptionally well. Beginning with passing about the bullets and discussing their influence on the conflict, I went through the content and conveyed the importance of the battles. With bullets in hand, my audience were able to actually imagine the very bullet models in their hands being shot out of rifles into enemy soldiers.  In fact the presentation went so well that my final grade was a 100 on this assignment, with my instructor citing the bullets as “clinching the brilliance” of the entire presentation. Not only did I illustrate the material in a graphic way, but I allowed for the audience to become engaged themselves in this narrative I unfolded around the Civil War.  Perhaps it was no surprise then that immediately afterwards, the class gathered around me and asked how I had created these bullets, and what else they would be able to 3D print. And as I subsequently told them, only their imaginations were their limit. ![civil war bullets](http://static.scholarslab.org/wp-content/uploads/2016/11/IMG_20161109_112829970-1024x576.jpg) Image source: Ammon Shepherd"},{"id":"2016-12-03-hybrid-literature-ruth-ozekis-a-tale-for-the-time-being","title":"Hybrid Literature: Ruth Ozeki’s A Tale for the Time Being","author":"christian-howard","date":"2016-12-03 09:10:04 -0500","categories":["Digital Humanities","Experimental Humanities","Grad Student Research"],"url":"hybrid-literature-ruth-ozekis-a-tale-for-the-time-being","layout":"post","content":"As a scholar of contemporary literature, I have naturally been drawn to the incredible literary innovation that has exploded in the wake of digital developments. I’m certainly not alone in my interest, and critics such as Katherine Hayles, Marie-Laure Ryan, Wolfgang Hallet, and Jan-Noël Thon have discussed the role of new media in literary studies, including video games, hypertext fiction, and comic books. Yet here I want to focus on contemporary writers who have begun exploiting new technologies in even more subtle ways, using technology as a means of supplementing more traditional printed books. More specifically, these texts employ a “both-and” approach, relying upon traditional publishing platforms (no matter how international the dissemination) while including new media elements that extend beyond print to reach the burgeoning generation of digital readers. Such works range from Indra Sinha’s online stories and sketches that extend the fictional world of his Animal’s People to Ali Smith’s incorporation of images and “Google poems” in Artful and David Mitchell’s creation of a live Twitter account for one of his characters in Slade House . Even as these hybrid texts experiment with new technologies and print platforms, so do they use new technology for the purposes of publishing and branding, in order to reach a different audience, and as a means of developing a new, innovative aesthetic. So let’s look more closely at one of these hybrid texts in particular, namely, Ruth Ozeki’s 2013 novel, A Tale for the Time Being . So here’s the back-story: Ozeki’s A Tale for the Time Being pivots between the fictional worlds of Naoko Yasutani (“Nao,” pronounced “Now”) and a fictional persona known as Ruth Ozeki, who is modeled upon Ozeki herself. The story takes place on a small Canadian island, where Ruth discovers the diary of a sixteen year-old Japanese girl preserved within a Hello Kitty lunchbox. Ruth, who is herself part Japanese and able to translate the diarist’s – Nao’s – scrawl, subsequently begins reading Nao’s diary, and A Tale for the Time Being alternates between recording Ruth’s English translation (complete with footnotes) of Nao’s diary and Ruth’s own life struggles as she engages with Nao’s text. Published simultaneously not only in hardback and paperback editions, but also as an e-book and an audio download, Ozeki’s novel stands as a pioneering work in terms of experimental publication. That is, Canongate (the publisher of the first published edition of A Tale in the UK) was particularly attentive to creating a brand for Ozeki’s novel that reached across print and digital audiences. As such, the covers of all these editions feature the face of a girl superimposed on a landscape, which is partially concealed behind a peeled-back red circle (reminiscent of the Japanese flag) – see Figure 1 below. ![Figure 1. Screen grab from the Canongate website](http://static.scholarslab.org/wp-content/uploads/2016/12/bundle_visual-300x185.jpg) Figure 1. Screen grab from the Canongate website Cate Cannon, the head of marketing at Canongate, has remarked on this focus during the publication planning of Ozeki’s novel. She says: “The animation and design translates across our digital outdoor advertising, our website and all our editions, creating a brand identity for this novel that is enriching, engaging and progressive” (Montgomery). Nonetheless, the real innovative aspect of Canongate’s publishing lies not in the similarities between each edition, but rather in the subtle differences, for each of the covers of these versions differed slightly based on medium. The hardback version, for instance, was published with the complete image of the girl’s face printed on the cover; this image was wholly concealed by a red sticker that the reader could physically peel back. This format was unique to the hardback version, and the paperback edition simply printed the image as pictured above, with the red circle partially concealing the girl’s face. Yet the paperback also employed technology in a unique way to create a kind of hybrid cover: this has resulted in the first “interactive book cover.” Book designer Zoë Sadokierski explains: “Published by Canongate, art director Rafi Romaya collaborated with creative agency Big Active. ….The ‘interactive’ aspect of the physical book cover involves Blippar technology (augmented reality) – using the camera on a smart phone/tablet, you can link to audio/visual material, via the Blippar app” (Sadokierski). In other words, if you have a paperback copy and have downloaded the Blippar app, you can point your smartphone or tablet at the cover: a virtual sticker will appear on your device, which you can “peel back” to reveal the girl’s face; this face has even been animated to move for 15 seconds (you can view the Blippar “interactive cover” here: https://www.youtube.com/watch?v=N7vpb7UPK6E ). This attention to technology and branding is further manifested in Ozeki’s creation of a supplementary “book trailer,” in which Ruth (played by Ozeki) discovers Nao’s diary (see Ozeki’s website, OzekiLand : http://www.ruthozeki.com/ ). Even as it blends media by applying a form previously reserved for television shows and movies to a novel, this book trailer appeals to Ozeki’s tech-savvy audience of e-readers, and indeed, the e-book links to YouTube videos, including interviews with Ozeki and video reviews of the novel. While these various covers and animations may sound gimmicky, we can nonetheless see how media and technology have become increasingly important both for authorial branding and in order to reach a larger, more tech-savvy audience. As I’ve examined Ozeki’s supplementary use of technology in her otherwise fairly traditional print novel, I’ve been struck by several questions: How does the use of augmented reality – both through applications such as Blippar and videos of fictional works in which the author and main character appear as the same individual – alter our understanding of the reality-fictional divide? Do digital spaces provide a kind of “hyper reality,” and if so, how might we interpret this hyper-reality? In what other ways are “hybrid” literary works experimenting with technology, and what implications does such experimentation have upon the development of literature? In short, I’m excited to see how contemporary writers continue to engage with digital developments! Works Cited Montgomery, Angus. “An Interactive Book Cover From Canongate and Big Active.” Design Week . N.p., 22 Feb. 2013. Web. 30 Aug. 2016. Ozeki, Ruth. A Tale for the Time Being . New York: Penguin Books, 2013. Print. Sadokierski, Zoe. “On Publication Design: Interactive Book Covers.” zoesadokierski.blogspot.com . N.p., 23 Feb. 2013. Web. 30 Aug. 2016."},{"id":"2016-12-13-discussions-in-the-digital-humanities-and-learning-new-technologies-in-the-scholars-lab","title":"Discussions in the Digital Humanities and Learning New Technologies in the Scholars’ Lab","author":"sarah-mceleney","date":"2016-12-13 06:25:55 -0500","categories":null,"url":"discussions-in-the-digital-humanities-and-learning-new-technologies-in-the-scholars-lab","layout":"post","content":"Over the course of the Fall 2016 semester, Praxis fellows participated in weekly meetings to discuss key topics relevant to the field of the digital humanities.  At the beginning of the academic year, we considered the numerous debates about the definition of the digital humanities, relying on the collection of articles, Debates in the Digital Humanities (2016).  Digital humanities, as an academic discipline, has been a notoriously tricky field to define, and conflicting definitions abound in the literature, leading to a vast range of ways that different people understand the discipline. After our discussions and study of the digital humanities as a discipline, we began to learn about important technologies for digital humanists. Over the course of the fall semester, we practiced using HTML and CSS, as well as some Javascript.  Praxis fellows were introduced to the Javascript library, JQuery, as well as the CSS frameworks, Bootstrap and Materialize. We also worked extensively with the version control system Git as a method for working collaboratively on projects.  None of the fellows had used Git before, and after the tutorials and practice sessions led by Scholar’s Lab staff, we attained the ability to confidently use Git for our projects. This semester’s introduction to the digital humanities has been has helped us realize how many ways that the discipline of digital humanities is defined, with some scholars focusing on social activism, some on technological aspects, and others on the qualitative analysis of digital media.  Of course, the different angles of digital humanities scholarship can and do overlap and intersect, which make the digital humanities, as I see it, such a dynamic and engaging interdisciplinary field."},{"id":"2016-12-13-praxis-on-choosing-a-subject-of-study-or-how-did-we-come-to-the-kardashians","title":"Praxis on Choosing a Subject of Study, or, How did we come to the Kardashians?","author":"alicia-caticha","date":"2016-12-13 07:28:04 -0500","categories":null,"url":"praxis-on-choosing-a-subject-of-study-or-how-did-we-come-to-the-kardashians","layout":"post","content":"![img_2038_1024](http://static.scholarslab.org/wp-content/uploads/2016/12/img_2038_1024-300x225.jpg) Keeping up with the Praxis 2016-2017 cohort. Unlike previous cohorts of Praxis, we did not have a topic or project assigned to us when we came into the program. Rather, our fearless leaders at the Scholars Lab took a chance. Our assignment: “time, temporality, go!” The first impulse was to directly challenge the traditional conceptions of time, linear time. Our cohort, made up of scholars of English literature, Russian literature, US history, and European art history, came to the topic of cyclical time from multiple different viewpoints. Joseph, whose academic work focuses on music and collective memory, immediately grasped the nature of cyclical time. Similarly, Alyssa and Jordan understood cyclical time’s ramifications in memory, story telling, and formal literary construction. I, on the other hand, an art historian who works with ephemeral objects, objects that are seen by humans and then over time melt, break, and disintegrate… I had trouble wrapping my head around the concept. Once an object is gone, how can it be viewed and perceived by the viewer and human subject? After meetings in which we discussed Gilles Deleuze, George Kubler, Albert Einstein, among others, I finally found myself opening up to the fact that time, as we humans perceived, is a construction—one that I was having trouble removing myself from because of the conditioning I had received as a member of a western-centric, euro-centric society.  As an art historian, my views had been directly—if unknowingly—shaped by the history of my field. Emmanuel Kant, after all, established the notion that time is something we experience.  A key step to this breakthrough was our discussion of the density of time and brainstorming its many variations:  literary time  cinematic time visual time collective memory nostalgia As a group, we kept coming back to the topic of collective memory and nostalgia. Where was the line between the two? Was nostalgia a conservative form of collective memory? The concept of “radical nostalgia” challenged this notion. Radical, or insurgent, nostalgia, reframes nostalgia as an act not inherently conservative, especially when harnessing radical or progressive events and movements of the past (see Peter Glazer’s Radical Nostalgia: Spanish Civil War Commemoration in Americ__a, 2005). In our discussion, it became radically apparent that our own experience with the development of contemporary collective memory, how it was shaped in our daily lives, was the product of one dominant force: social media . We were in the trenches of the 2016 election cycle. Donald Trump’s tweets were in the news everyday. Conservatives were accusing Facebook’s content editors of having a liberal bias. How was social media shaping how we perceived the 24-hour news cycle, and thereby our collective understanding of culture and politics? How could we visualize the social media landscape? How could we use social media as a data set? Was there a cohesive data set across all media that we could mine for a productive analysis of social media usage and its cultural ramifications? Enter the Kardashian family empire. Reality TV, tabloid coverage, Twitter, Instagram, Snapchat, iPhone and android apps, online blogs… in many ways it emerged as a story of archive creation, expansive real time narrative construction, and yes, even a study of collective memory.  Who are the Kardashians? The Kardashians are an Armenian-American family living in Los Angeles headed by the matriarch Kris Jenner, who married Robert Kardashian in 1978 (divorced in 1991) and later the Olympian Bruce Jenner, now Caitlyn Jenner, in 1991 (divorced in 2015). The family first came to public attention in 1995, during the high profile murder trial of O.J. Simpson during which Robert Kardashian, was part of the defense’s legal team. They found themselves in the tabloids again in 2007 when a sex tape was leaked of daughter Kim Kardashian and her then-boyfriend Ray J. Taking advantage of such notoriety the family debuted their reality television series Keeping Up with the Kardashians later that year. By embracing nearly ever media platform at their disposal, the Kardashians have promoted their brand and developed their family narrative in real time, garnering interest in how they will ‘cover’ certain life events in their TV show airing months later. Commenting on news events, promoting awareness of the Armenian genocide, while producing extravagant television specials, the Kardashian family has harnessed the 24/7 media landscape in new and unprecedented ways; their rise to fame paralleling the rise of social media and the internet as we know it. Stay tuned as we, the Praxis 2016-2017 cohort, dissect and analyze the Kardashian media ecology…."},{"id":"2016-12-13-the-state-of-dh-in-slavic-studies-by-kathleen-thompson","title":"The state of DH in Slavic Studies, by Kathleen Thompson","author":"ammon-shepherd","date":"2016-12-13 05:42:33 -0500","categories":["Digital Humanities","Makerspace"],"url":"the-state-of-dh-in-slavic-studies-by-kathleen-thompson","layout":"post","content":"In a final wrap up, of what has become a four-part series of blog posts on using 3D-printing in a humanities course, Kathleen Thompson reports back on the ASEEES conference and the state of DH in Slavic studies. The previous posts can be read here: Part 1: Printing in the Classroom: Course Assignments and the Makerspace Part 2: 3D Printing in the Classroom: Outcomes and Reflections on a Slavic Course Experiment (1/2)   Part 3:  3D Printing in the Classroom: Outcomes and Reflections on a Slavic Course Experiment (1/2) Kathleen Thompson also provided three videos of printing the onion dome pictured above: [embed]https://youtu.be/AThw04OFa6o[/embed] Onion Dome movie 1 [embed]https://youtu.be/WpUtw0_IB40[/embed] Onion Dome movie 2 https://youtu.be/NOVC8kWo33Y Onion Dome movie 3 After receiving a PhD from the University of Virginia’s Slavic department in August 2015, Kathleen Thompson worked as the interim Slavic librarian for the 2015-2016 academic year. Since the conclusion of that position, she has been working as a research assistant in the Slavic department. Her current research interests involve digital pedagogy, 21st-century Russian women writers, culture studies, and digital networks. In late November, Jill Martiniuk and I attended the 48th annual Association for Slavic, East European, and Eurasian Studies (ASEEES) Convention (link: http://aseees.org/convention) to present our project on 3D printing in the classroom. This convention is one of two major conferences in the field of Slavic studies, and is regarded as the largest and most well-attended because it includes scholars and students from a wide range of specializations, at all levels of their careers, discussing issues germane to locales ranging from the Caucasus to the Baltics, from Crimea to Siberia, and everywhere in between. This year’s convention played host to four debuts: one, the inaugural meeting of the newly-formed Slavic Digital Humanities affiliate group (link: http://slavic-dh.org/ ); two, the debut of the ASEEES Commons repository ( https://aseees.hcommons.org/ ); three, a THATCamp meeting (link: http://aseees2016.thatcamp.org/ ) preceding the convention; and four, a panel stream of seven sessions (link: http://www.slavic-dh.org/panels2016/ ) devoted to digital humanities in our field. Jill and I both attended THATCamp, and participated in the sixth DH session, “Digital Humanities In and Out of the Classroom”, which was a roundtable highlighting projects linking DH and pedagogy. At both THATCamp and the roundtable, we were eager to share our findings from our spring-semester project, get feedback from other instructors or scholars working with similar technology, and learn about projects in different media and pedagogical contexts. Since we were unsure how familiar our audience would be with 3D printing, not to mention DH in general, Jill and I prepared a slideshow (link: https://dl.dropboxusercontent.com/u/64498698/3D-printing-in-Slavic-classroom-presentation.pptx ) to highlight the creation, implementation, and outcomes of the project. We also included photos of the students’ work, as well as of the Makerspace itself and a couple of videos I shot of an onion dome being printed. I brought the onion dome with me to the roundtable and passed it around to the 25+ audience members and panelists, most of whom seemed to enjoy handling it. Unfortunately, after we briefly discussed our project and made some comments about issues we had to consider when creating and implementing the project – the standard “why are we doing this and what are our learning goals?” questions – the discussion moved towards other projects and their creators’ responses to similar issues. As this was a roundtable on general approaches to pedagogical issues with DH projects, Jill and I were not too bothered by this shift, though we were slightly disappointed not to receive more feedback.  Even so, throughout the weekend, we heard from many convention-goers and THATCamp participants that ours was the first 3D printing project they had heard of in the field, and interest in our project on a one-to-one basis was surprisingly high. We came away from the conference encouraged and excited about the state of digital humanities in the Slavic field, and the direction in which it seems to be trending. Slavic scholars are engaging in thoughtful, critical work in several areas, if the titles of the THATCamp breakout sessions and DH panels are any indication. The THATCamp sessions were: Introduction to DH Digital Public Scholarship Digital Pedagogy Digital Archives Course Development Programming for Humanists GIS Databases &amp; Visualizations Topic Modeling Network Analysis Textual Analysis &amp; TEI Working with Eastern European Languages – Character Recognition, Encoding, and other Issues. The DH panel and roundtable titles were: Platforms for Digital Scholarship The Researcher-Librarian Interface in Digital East European Studies Seeing Through Data: How Does DH Change How We View Culture? Computational Poetics: Digital Approaches to the Analysis of Rhyme, Meter, and Text Length Locating Text and Image in the Digital Humanities DH In and Out of the Classroom Mapping and GIS in the Slavic and Eurasian Humanities Approximately 80 convention attendees signed up for THATCamp, with probably 50-60 actual attendees at the meeting. About 30 convention attendees were present at the Slavic DH affiliate group business meeting, with several expressing interest in future topic streams and initiatives at next year’s convention as well as serving on the group’s board of elected officers. The panels and roundtables that Jill and I attended garnered audiences ranging in size from 10-40 people, with the most interest focused on digital archives, content management systems and blogs, network analysis, and topic modeling. Interestingly, the launch of the ASEEES Commons site seemed to rouse a little less enthusiasm (at present, it has 14 members and 5 public groups), though that could be due to a general lack of knowledge about the “whys” and “whats” of the site – or exhaustion from an already fulfilling and enriching weekend of scholarship. Some of my main takeaway points from the convention follow: Right now, it seems that the majority of Slavic scholars practicing DH are historians, though there is also a sizable number of librarians, linguists, sociologists, cultural scholars, and literature scholars – many of whom are collaborating with one another across disciplines. Even so, few of us humanists have coding skills beyond the basics, so some of the most valued relationships are between us and the computer-savvy scholars (often graduate or undergraduate students) who make our project dreams a reality. WordPress and Omeka are the two most-used platforms for DH projects across most disciplines, mostly because of low buy-in and learning curves, variety of presentation options, and sustainability over the long term. Questions that were repeatedly raised about practicing DH in the Slavic field were “why?”, “how?”, “who else is doing this?”, and (the ever-present) “how can we get our work to be taken seriously?” In light of that last question, I noted a serious push to move the conversation from “legitimizing the field” to “legitimizing the outcome”, suggesting that we as a field may have finally validated DH as a worthwhile pursuit, and are focusing more on its sustainable, valuable practice. The excitement level about DH work in our field seems high enough to support a wide range of initiatives, collaborations, and scholarship, though the phrases “critical mass” and “bursting bubble” came up more than a few times throughout the convention. Some future issues to consider include: how do we make sure that graduate and undergraduate students receive proper credit, and training, for their past and future work on DH projects? How can we use DH as a platform for public scholarship to reach our wider communities? How can we start local, and expand to the global? How does DH help build and change relationships between faculty, researchers, students, and communities? If you’re interested in seeing some of the projects we learned about, check out this list: Digital Public Scholarship https://1917resources.aseees.hcommons.org/ - A resource on all things 1917 for the upcoming 100th anniversary of the Russian Revolution; will eventually be open-source https://notevenpast.org/ - A resource on local history research at the University of Texas-Austin, which includes the popular podcast 15-Minute History ( https://15minutehistory.org ) and projects that have emerged from Not Even Past ( https://thinkinginpublic.org/ ) http://soviethistory.msu.edu/ - 17 Moments in Soviet History, a digital archive of primary sources across Soviet history http://blackseanetworks.org/ - A collaborative initiative on the Black Sea as a network Digital Pedagogy http://dhrees.yale.edu/ - Digital Humanities and Russian and East European Studies at Yale http://campuspress.yale.edu/emigres/ - Avant-Gardes and Émigrés: Slavic Studies and Digital Humanities https://utopiaafterutopia.com/ - a contemporary culture research initiative on politics and aesthetics in the post-Soviet world http://blackandbluedanube.wordpress.com/ - The Black and Blue Danube  symposium and “Legacies of the Second World” working group, out of Colgate University Digital Archives http://www.laits.utexas.edu/txczechproject/home - the Texas Czech Legacy Project http://chnmdev.gmu.edu/rpi/ - Russian Perspectives on Islam http://cultural-opposition.eu/ - Courage: Connecting Collections https://perspectives.ushmm.org/ - Jewish Perspectives on the Holocaust http://luchsveta.org/ - Luch Sveta, an online repository of video clips for learning Russian language at all levels Data and Visualizations http://104.236.44.207/notes/ - sandbox of Andrew Janco (Digital Scholarship Librarian, Haverford College) Topic Modeling http://agoldst.github.io/dfr-browser/demo/#/model/list - PMLA topic modeling, designed for browser http://sprout025.sprout.yale.edu/topics/sr/200/#/model - Slavic Review topic modeling, designed for browser Blogging http://russianhistoryblog.org/ - An experiment in digital Russian history https://joanneuberger.wordpress.com/ - Joan Neuberger, co-author of Not Even Past http://amynelson.net/ - Amy Nelson, co-author of 17 Moments in Soviet History; see more DH-specific posts at https://siriusreflections.org/ http://www.soundingthespacerace.com - Sounding The Space Race, coming December 2016"},{"id":"2016-12-13-time-twitter-and-keeping-up-with-the-kardashians","title":"Time, Twitter, and Keeping Up with the Kardashians","author":"alyssa-collins","date":"2016-12-13 07:54:42 -0500","categories":["Grad Student Research"],"url":"time-twitter-and-keeping-up-with-the-kardashians","layout":"post","content":"We started this semester thinking about time and the ways time is structured, pathologized, and altered. And when it came to finding an access point for these questions, a project, if you will, we found it in an unlikely source: ![A cover of Cosmopolitan in which the Kardashians are pictured. It includes the title ](http://www.awesomelyluvvie.com/wp-content/uploads/2015/10/Cosmopolitan-Kardashians.jpg) Cosmo names the Kardashians as America's \"First Family\" Initially, as serious grad students, we were a bit resistant/hesitant  to stake our entire project on such an unlikely primary source. However, after several elaborate, energized, and playful discussions, we came to the conclusion that  Kim Kardashian and her family (a.k.a “the Kardashians”) are interesting set of figures who actively make and re-make their own history through a network of media platform (both new and traditional/”mainstream”). Because they loom large on the landscape of American contemporary culture and have connected themselves to so many people, the Kardashians are situated within a nexus of not only conversations about time and information dissemination, but also conversations about race, gender, and sexuality.  For our cohort, the Kardashians also are  an exciting place of research where we can pair intellectual play and rigor . So, eventually we all got on board.  ![Brainstorming Session](http://static.scholarslab.org/wp-content/uploads/2016/12/IMG_7612-copy-300x235.jpg) Brainstorming Session We decided that by looking at the ways their reality TV show, Keeping Up With the Kardashian (KUTWK), we can not only ask what it means to “keep up with” these people (what apps you must buy, what headlines you must read, what games you must play, and what places you must go to be a good fan/follower), but also (and maybe more interestingly) how “history” or cultural moments are visited and revisited through the conjuction of social media and television show.  Moments are first visited in tweets that are happening IRL (in real life) and IRT (in real time) and then revisited after a period of three months or time it takes for production to complete an episode of KUWTK .  So this semester (and next) we will be asking:   Why do people like the Kardashians? What do they offer? How does Keeping Up With the Kardashians move in and out of time?  This includes looking at “historical Kardashian moments” that seem to take up a great deal of historical narrative space. How are these stories told and retold? We think that if we pair two of these media timelines: Twitter and television (KUWTK) we can begin to uncover the way these narratives work together (or even against each other).  Here are some possible moments we might look to:  * The OJ Simpson trial (1994)\n\n \t\n* Kim’s sex tape (leaked in 2007) \n\n \t\n* The Kardashian’s trip to Armenia (2015)\n\n \t\n* Khloe’s marriage to Lamar Odom and his very public illness (2015)\n\n \t\n* Caitlyn Jenner’s coming out (2015)\n\n \t\n* Kanye West’s ongoing beef with Taylor Swift (2009 - 2016)\n\n \t\n* The robbery in Paris and the possible retirement of Kim Kardashian from public life. (2016) In the ten years it has been on television, how has _KUWTK_ move in and out of different genres? How do the Kardashians publicly enter and exit discourses: race, gender, sexuality, and class. On the other hand, how are the Kardashians situated in these discourses by fans, scholars, and journalists?  It seems that nothing they do is uncontroversial and considering the public response to events in the Kardashians’ lives is also an important to understanding how they fit into the American historical landscape.  Lastly, in thinking about the Kardashians we are interested to think about how their use and presentation of history funneled through different media platforms operates as a model for other kinds and forms of celebrity (both insidious and benign). How might we see their use of both mediums as a test case for the ways in which news cycles and history has possibly sped up in the 21st century?  We are excited to see what we come up with and we hope you come along for the ride! "},{"id":"2016-12-13-why-not-build-another-digital-humanities-tool","title":"Why not build another digital humanities tool?","author":"joseph-thompson","date":"2016-12-13 07:50:24 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"why-not-build-another-digital-humanities-tool","layout":"post","content":"One of the recurrent issues I noticed when our Praxis cohort began discussing the meaning of the digital humanities was the field’s need to justify its existence. At the beginning of the semester, we read articles about digital humanities as a “tactical term” and the kind of institutional, financial affiliations necessary to sustain DH labs and staff. All of this background on the history of the field proved useful in understanding where DH stands within academia and why it has received so much recent criticism as an instrument in the further neoliberalization (insert your personal definition here) of the university. Scholars within and outside the field have noted, with some justification, how DH might further the financialization and quantification of, well, everything and how that might lead us further away from the inquiries that should ostensibly drive our intellectual pursuits in the humanities.   So, if there is this “dark side” of DH, then what does it look like and what can we do to push against it?   An emphasis on empirical, quantitative projects and the “mining” of data and texts has characterized many (most?) DH projects, creating easy fodder for critics concerned with the ever-growing reach of neoliberalism. These types of projects have served as the very foundation for many DH literary studies and have yield generative results into topics like genre devices, authorial style, and the history of the novel. But beyond yielding new insights into literary texts, these types of projects offered a way to justify the field of DH. If humanities scholars could use computational analysis to produce projects illustrated with graphs, charts, and numerical data, then perhaps the humanities could maintain its relevance in a political/academic climate that seemingly values STEM over the arts.    The other way that DH seems to claim its relevancy is by emphasizing the production of tools. Our own Scholars’ Lab staff has created new tools like Neatline and previous Praxis cohorts have developed Prism and Ivanhoe, making useful, open-sourced tools and bringing attention to the work being done here at UVA. Of course, there’s no problem with producing such tools and many will say that’s exactly what DH should do. The problem will arise if every DH lab in the world starts producing new tools for every new project. At that point, the future of DH might look a lot like the iTunes app store, more a marketplace of things rather than ideas.   So, will this year’s Praxis cohort create a slick new tool for the DH marketplace? You guessed it, no.   For one reason, our cohort simply does not have the time or expertise to build the coolest new tool on the web. Several of us entered this program with little to no knowledge of coding, web design, or software development. My goal is to gain an introduction to if not a handle on those skills while I’m here.   The other reason is political.   During a DH presentation at the 2016 American Studies Association conference, I heard a panelist describe the Internet as the “island of misfit toys.” I took this to mean that in a push for innovation and the need to justify the field’s existence a proliferation of DH projects have led to a mass of broken, unmaintained, and otherwise unusable tools. This stress on tool creation has fed into the critiques of DH as a neoliberal undertaking bent on the production of quantifiable and marketable results rather than the humanistic analysis that might yield more ideas than products.   Our cohort won’t be producing a fancy new tool, and that’s okay. But we will use existing digital tools in innovative ways and maybe offer inspiration for other scholars with similar amounts of time and DH experience to pursue projects of their own."},{"id":"2016-12-16-working-with-an-archive-of-the-now","title":"Working with an Archive of the ‘Now’","author":"jordan-buysse","date":"2016-12-16 05:28:16 -0500","categories":["Experimental Humanities","Grad Student Research","Visualization and Data Mining"],"url":"working-with-an-archive-of-the-now","layout":"post","content":"Given our subject matter for the 2016-17 Praxis cohort, we recognized early on that we would be grappling with a very different sort of archive than we’ve grown accustomed to as humanists. Instead of the stacks, journal databases, manuscripts, and historical objects, we’d have to take a serious look at Facebook, Instagram, Vine, Snapchat, and Twitter. The following is a brief summary of some of our efforts with Twitter, a platform which we realized early on would be key in meaningfully keeping up with the Kardashians. We’ve centered our thinking around Twitter because at first glance, given its relatively open API, the possibilities for analysis seemed endless. What we’ve found is that this openness is a bit deceptive. While it’s easy enough to click around and ask questions of our archive through regular browsing, it’s another matter entirely to start answering those questions quantitatively. It’s hard to turn data that you can see into data you can use. It’s one thing to know this intellectually, but quite another to experience the many roadblocks of a proper social media data set. The first thing we found in attempting to wrangle data from the platform is that Twitter’s API prioritizes an archive of the ‘now’. One of the ways to sample twitter data is with the R package rtweet, which allows the user to search tweets based on keywords or user data such as followers, user ‘likes’, and retweets. Because rtweet and other tools like it use Twitter’s “streaming API,” which searches for tweets as they happen, the experience is a bit like trying to sip from a firehose. A query of the name ‘Kardashian’ on a given Wednesday morning yields output like this: The CSV format allows us to view the data in a slightly more manageable format as follows: To be sure, there’s more we could do to clean up this data, but to what end? Using social media networks to answer our questions about celebrity culture means making a choice. It’s either the comprehensive and immense archive of ‘now’, or some more carefully and narrowly selected data set. We’re moving towards the latter. Indeed, our training as humanists has primed us to theorize and uphold the significance of the small data set closely read and applied. So what does that archive look like? It seems like a simple enough change; instead of watching the deployment of the Kardashian name across twitter in real time, why not focus in on key users themselves? Even if we were to make a generous coterie of, say, thirty Kardashian and Kardashian-adjacent twitter accounts, fresh problems arise. ‘Now’-ness is baked into the tools available to us. Our efforts to delve into deeper histories of this celebrity hit a snag about a year and a half back from the present moment; you can only reach back 3,200 tweets into a given account before twitter’s API cuts you off. The workarounds are either inelegant or expensive. Twitter is a uniquely profitable and profit-driven archive, and the analytics tools to reach back further than those 3,200 tweets are out of our price range. But for our purposes, there’s a workaround somewhere between the streaming API’s technical affordances and the browser’s ease of use. It is possible to reach back into twitter’s deep archives simply using the site’s search feature, where users can search for tweets by user across certain date ranges, all of the way back to a user’s first tweets. These, in turn, can be scraped from the web and archived using a browser. The solution to our data-gathering questions, it seems, lies in moving between the user-centric practice of ‘browsing’ and the computational problem of data wrangling. Our efforts will involve translating the archive between these modes. To do so, we’ll have to rely on our best instincts developed from traditional humanist archives in tandem with the technical affordances and scalability of the archive of the ‘now’."},{"id":"2017-01-12-blippar-and-augmented-reality-literature","title":"Blippar and Augmented Reality Literature","author":"christian-howard","date":"2017-01-12 11:39:05 -0500","categories":null,"url":"blippar-and-augmented-reality-literature","layout":"post","content":"As I was writing about Ruth Ozeki’s A Tale for the Time Being and the various technological innovations that Ozeki’s novel employs, I became increasingly interested in Blippar, an augmented reality application. Blippar is by no means restricted to literary projects, but as this is my primary interest, I naturally focused my search on ways in which literary works are incorporating augmented reality into their design. I found the results as fascinating as the implications are endless. Literary works are currently employing Blippar in two primary ways: As a means of advertising and as a way of garnering user/reader participation and interactivity. The two overlap, of course, but the degrees vary according to purpose. Ravinder Singh, for instance, has made the cover of his novel, This Love that Feels Right… blippable. According to Blippar’s website: “By blipping the cover of the book you can see a miniature Ravinder Singh come to life in 3D. He introduces his book and prompts the reader to share a review or take a selfie to win the chance to meet him and discuss love, life and books over a steaming hot cuppa.” Figure 1. Screen-grab from Blippar’s website illustrating Ravinder Singh’s This Love that Feels Right… Yet extending beyond advertising, graphic artists and novelists are employing Blippar much more intensively, in some cases even supplementing almost every panel of their comics with augmented-reality blipps. Ram Devineni, Lina Srivastava, and Dan Goldman have created a storytelling project titled Priya’s Shakti that aims to both educate men and women about sexual violence even as it fights against rape in India. Figure 2. Screen-grab from Blippar’s website illustrating the comic book, Priya’s Shakti Priya’s Shakti is available for free download on Amazon, so, curious, I downloaded the comic and read it using Blippar. Not only has Priya’s Shakti given India its first female “super-hero”, but the comic itself is a work of art that comes to life with Blippar’s augmented reality, providing further information, links to videos, and even sound effects. This is, I believe, just the beginning of augmented reality’s dance with literature. Figure 3. Screen-shot of Blippar’s enhanced graphics of a page of Priya’s Shakti Figure 4. Screen-shot from a video available through Blippar’s augmentation of Priya’s Shakti In addition to enhancing the literary scene, Blippar has likewise expanded to education, sports, retail, healthcare, and business, among others. Perhaps most interestingly, Blippar recently launched the first “augmented reality retail destination” in Covent Garden (read about it on Blippar’s blog: https://blippar.com/en/resources/blog/2016/11/22/covent-garden-becomes-worlds-first-augmented-reality-retail-destination-powered-blippar/ ). Blippar also encourages its users to become “blippbuilders” by creating their own interactive augmented reality experiences. Naturally, I had to play around with blippbuilding myself, so I created a blipp for the UVA Makerspace, with images of our 3D printers and Makerspace technology, and links to both a video about Makerspaces and the UVA Scholars’ Lab website. Here are a few screenshots of my blipp: Figure 5. Blippbuilding in progress Figure 6. Uploading the blipp Figure 7. The completed blipp of the UVA Scholars’ Lab Makerspace"},{"id":"2017-01-20-reading-the-kardashians","title":"Reading the Kardashians","author":"joseph-thompson","date":"2017-01-20 05:11:57 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"reading-the-kardashians","layout":"post","content":"Have you ever wondered what it would be like to read the dialogue that passes between members of the Kardashian family on Keeping Up with the Kardashians ? To have those seemingly intimate conversations and confessions in the form of a literary production that is open to analysis, interpretation, and text mining? Me neither. Not until recently, that is. Our Praxis cohort has asked this very question to find out what the Kardashian conversations and their fourth wall monologues might reveal about that the ways they control and create the narrative around their lives. Of course, answering this question requires access to the transcripts of the shows. While I half assumed some KUWTK fan might have transcribed the shows already, my searches for this material came up empty but feel free to point us in that direction if you know where they are. Without such a trove, a few of us began searching for ways to borrow the closed captioning service from the DVDs. There are programs that do that, but one problem that immediately arose is that our library only holds the first season of KUWTK on DVD. True, we could ask the library to purchase the remaining 11 seasons, but justifying that purchase and then waiting for their arrival might seriously slow us down. Another potential problem comes from the sheer time it would take to download physical copies of the show. Again, time is a serious concern here since our tenure as Praxis fellows expires at the end of this semester, and we still have to complete some sort of analysis of this material. Following a brief search of other options, Alyssa Collins and I decided to try CCExtractor, a program that allows the collection of closed captioning transcripts from streaming services and hosts it’s code in GitHub . Not only would this save us the time and money required to gather transcripts from DVDs, but it also offered us a way to practice the technical skills we’ve been developing in Praxis. Alyssa and I jumped into CCExtractor, simply following the tutorial available on their website and using the documentation provided in their GitHub repository. We used my computer since I had installed Homebrew already. This allowed us to substitute “brew” for “apt-get” in the command line and run the CCExtractor scripts. After a few hiccups, which we overcame with the assistance of Eric Rochester and Jeremy Boggs (thanks!), we downloaded the closed captioning transcripts for KUWTK ’s first episode, titled “I’m Watching You” and originally aired on October 14, 2007, into srt and vtt files. We then opened the vtt file with Atom and had the entire transcript in a readable form, complete with time stamps for each person’s line. What will we do with this information? I don’t have an answer for that yet, but just looking at the data opened up a lot of questions. Who speaks the most for the Kardashians? Does this speaking time correlate to on-screen time? In other words, are there family members who appear with regularity but don’t speak? How can we use this text material to consider the literary devices at work in an episode of reality television? There are a lot of unknowns, but I’m excited at the prospect of having this data to analyze."},{"id":"2017-01-24-spring-2017-uva-library-gis-workshop-series","title":"Spring 2017 UVa Library GIS Workshop Series","author":"chris-gist","date":"2017-01-24 05:30:42 -0500","categories":["Announcements","Events","Geospatial and Temporal"],"url":"spring-2017-uva-library-gis-workshop-series","layout":"post","content":"All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be taught on Wednesdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up! February 1st\n**Making Your First Map with ArcGIS\n**Here’s your chance to get started with geographic information systems software in a friendly, jargon-free environment.  This workshop introduces the skills you need to make your own maps.  Along the way you’ll get a taste of Earth’s most popular GIS software (ArcGIS) and a gentle introduction to cartography. You’ll leave with your own cartographic masterpieces and tips for learning more in your pursuit of mappiness at UVa. February 8th Georeferencing a Map - Putting Old maps and Aerial Photos on Your Map Would you like to see historic map overlaid on modern aerial photography?  Do you need to extract features of a map for use in GIS?  Georeferencing is the first step.  We will show you how to take a scan of a paper map and align in it in ArcGIS. February 15th Getting Your Data on a Map Do you have a list of Lat/Lon coordinates or addresses you would like to see on a map?  We will show you how to do just that.  Through ArcGIS’s Add XY data tool and Geocoding (address matching), it is easy to take your tabular lists and generate points on a map. February 22nd Points on Your Map: Street Addresses and More Spatial Things Do you have a list of street addresses crying out to be mapped?  Have a list of zip codes or census tracts you wish to associate with other data?  We’ll start with addresses and other things spatial and end with points on a map, ready for visualization and analysis. March 1st Taking Control of Your Spatial Data: Editing in ArcGIS Until we perfect that magic “extract all those lines from this paper map” button we’re stuck using editor tools to get that job done.  If you’re lucky, someone else has done the work to create your points, lines, and polygons but maybe they need your magic touch to make them better.  This session shows you how to create and modify vector features in ArcMap, the world’s most popular geographic information systems software.  We’ll explore tools to create new points, lines, and polygons and to edit existing datasets.  At version 10, ArcMap’s editor was revamped introducing new templates, but we’ll keep calm and carry on. March 15th Easy Demographics Need to make a quick demographic map or religious adherence?  This workshop will show you how easily navigate Social Explorer.  This powerful online application makes it easy to create maps with contemporary and historic census data and religious information. March 22nd Historic Census Data Would you like to map the poverty in Philadelphia around the turn of the 20th Century?  How about a racial breakdown by state in the 1860s?  This workshop will focus on how to download historic census boundary and tabular data to make historic demographic maps. March 29th Introduction to ArcGIS Online With ArcGIS Online, you can use and create maps and scenes, access ready-to-use maps, layers and analytics, publish data as web layers, collaborate and share, access maps from any device, make maps with your Microsoft Excel data, customize the ArcGIS Online website, and view status reports. You can also use ArcGIS Online as a platform to build custom location-based apps."},{"id":"2017-01-31-call-for-applicants-for-tuition-fellowships-for-the-digital-humanities-summer-institute","title":"Call for Applicants for Tuition Fellowships for the Digital Humanities Summer Institute","author":"laura-miller","date":"2017-01-31 07:19:03 -0500","categories":["Technical Training"],"url":"call-for-applicants-for-tuition-fellowships-for-the-digital-humanities-summer-institute","layout":"post","content":"Want to learn more about the skills, methods, and inquiry entailed in digital humanities?  The Digital Humanities Summer Institute (DHSI) at the University of Victoria has a tradition of transformative training. The University of Virginia, as a sponsoring institution, provides five tuition-free fellowships for the summer of 2017. This deal ends April 1. Apply ASAP.  The Committee will begin selection on February 6. Please look at DHSI site for the available courses .  Spaces in the workshops are filling up.  The selection committee, organized by the Scholars’ Lab, will receive applications. To apply, please email scholarslab@virginia.edu with responses to the following items.  We may be in touch for further information. your name a one page resume or CV with contact information in addition to email your University of Virginia affiliation and status (e.g. 3rd year PhD in art history; job title in library or department); availability for June 5-9 or 12-16, 2017 (or both) your preferred courses a short, specific statement (300-500 words) about your experience, qualifications, research interests, and why you would like to attend DHSI other sources of funding you could apply for that would cover travel and housing"},{"id":"2017-02-03-graduate-applications-for-the-praxis-program-2017-2018","title":"Graduate Applications for the Praxis Program, 2017-2018","author":"alison-booth","date":"2017-02-03 12:04:36 -0500","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"graduate-applications-for-the-praxis-program-2017-2018","layout":"post","content":"UVa graduate students! Apply by February 28 for a unique, funded opportunity to work with an interdisciplinary group of your peers, learning digital humanities skills from the experts, and collaborating on the design and execution of an innovative digital project. The 2016-2017 Praxis cohort is in full swing, thanks to generous support by UVa Library and GSAS. Each year, the Scholars’ Lab, UVa Library’s center for advanced digital research in the humanities, runs a Praxis Program . This program provides six UVa graduate students with hands-on experience in digital humanities collaboration and project creation. Team members work to take a shared project from conception to design and development, and finally to publication, community engagement, and analysis. Praxis is a unique and well-known training program in the international digital humanities community. Our fellows blog about their experiences and develop increased facility with project management, collaboration, and the public humanities, even as they tackle (most for the first time, and with the mentorship of our faculty and staff) new programming languages, tools, and digital methods. Praxis prepares fellows to apply their own digital skills and methods to the fellowship project and to research and careers in the future. In 2012-2013, the Scholars’ Lab joined with like-minded institutions to create the Praxis Network, made up of allied humanities education initiatives from around the world, mainly focused on graduate training, and all engaged in rethinking pedagogy and campus partnerships in relation to the digital humanities. The Praxis Network Student Directory showcases Praxis Program alumni who have traveled diverse career paths, including tenure-track teaching positions and digital humanities positions within academic libraries and research centers. We will welcome six new, competitively-selected Praxis students in late August 2017. The Praxis fellowship replaces teaching responsibilities for the academic year. Fellows are expected to devote approximately 10 hours per week in the fall and spring semesters, to learning together and building a collaborative digital project in the Scholars’ Lab. Fellows join our vibrant community, have a voice in intellectual programming for the Scholars’ Lab, and can make use of a dedicated grad lounge. Eligibility : All University of Virginia doctoral students studying in any humanities and arts fields (architecture, media studies, interpretative social sciences, education, or other fields will also be considered). In general, studio or performing arts or creative writing are less appropriate applicant fields. Feel free to inquire about your disciplinary area and its fit with the Praxis program. We particularly encourage applications from women, LGBT students, and people of color, and will be working to put together an interdisciplinary and intellectually diverse team.\nThe application process for Praxis is simple! You apply individually, and we assemble the team, through a process that includes group interviews scheduled in early March, as well as input from peers. First step to apply : submit a letter of intent to scholarslab@virginia.edu. Letter of Intent includes:\n– the applicant’s research interests, with a title and a sentence about an example of recent work such as a thesis or essay;\n– summary of the applicant’s plan for use of digital technologies in your research;\n– summary of what skills, interests, methods, or experience the applicant will bring to the Praxis Program;\n– summary of what the applicant hopes to gain as a Praxis Fellow. Questions about Praxis Fellowships and the application process should be directed to scholarslab@virginia.edu"},{"id":"2017-02-03-graduate-fellowships","title":"Digital Humanities Fellows","author":"alison-booth","date":"2017-02-03 12:15:32 -0500","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"graduate-fellowships","layout":"post","content":"The Scholars’ Lab is proud to announce that applications for our prestigious Graduate Fellowship in the Digital Humanities are being accepted for the 2017-2018 academic year. Applications are due February 28, 2017 . The fellowship supports ABD graduate students doing innovative work in the digital humanities at the University of Virginia. The Scholars’ Lab offers Grad Fellows advice and assistance with the creation and analysis of digital content, as well as consultation on intellectual property issues and best practices in digital scholarship and DH software development. Fellows join our vibrant community, have a voice in intellectual programming for the Scholars’ Lab, make use of our dedicated grad office, and participate in one formal colloquium at the Library per semester. Supported by the Jeffrey C. Walker Library Fund for Technology in the Humanities, the Matthew &amp; Nancy Walker Library Fund, and a challenge grant from the National Endowment for the Humanities, the Graduate Fellowship in Digital Humanities is designed to advance the humanities and provide emerging digital scholars with an opportunity for growth. Eligibility Applicants must be ABD, having completed all course requirements and been admitted to candidacy for the doctorate in the humanities, social sciences or the arts at the University of Virginia. Applicants must be enrolled full time in the year for which they are applying. A faculty advisor must review and approve the scholarly content of the proposal. Applicants are strongly encouraged to have Praxis Program or equivalent experience . Experience can include work on a collaborative digital project, comfort with programing and code management, public scholarship, and critical engagement with digital tools. How to Apply Email a complete application package including the following materials to scholarslab@virginia.edu : a cover letter, addressed to the selection committee, containing: a summary of the applicant’s plan for use of digital technologies in his or her dissertation research; a summary of the applicant’s experience with digital projects; a description of UVa digital resources (content or expertise) that are relevant to the proposed project; a Graduate Fellowship Application Form ; a dissertation abstract; and 2-3 letters of nomination and support, at least one being from the applicant’s dissertation director. Deadline: February 28, 2017\nNotifications: March 31, 2017 Questions about Grad Fellowships and the application process should be directed to scholarslab@virginia.edu ."},{"id":"2017-02-09-why-study-popular-culture-why-study-the-kardashians","title":"Why Study Popular Culture? Why Study the Kardashians?","author":"alicia-caticha","date":"2017-02-09 10:27:57 -0500","categories":["Grad Student Research"],"url":"why-study-popular-culture-why-study-the-kardashians","layout":"post","content":"“When I got my first television set, I stopped caring so much about having close relationships.” — Andy Warhol “In the future, everybody will be world famous for fifteen minutes,” is without a doubt Andy Warhol’s most famous quote. Eerily predictive of the twenty-first century’s stars who are famous for “being famous,” this quote encapsulates the seemingly democratized nature of celebrity created by reality television, the internet, and social media. Despite his embrace of pop culture, Warhol’s oeuvre fits seamlessly into the Academy’s notions of “high” culture and specifically, “high” art. Even as Warhol’s work subverted the modernist cannon and the teleological evolution towards abstraction, this very critique cemented its value among cultural critics and academics, as well as the market. Only recently, has the study of popular culture become an important touchstone for Academic inquiry. Scholars of media studies, material culture, and visual culture (a field in direct opposition to art history in concept, if not in practice) have certainly led the charge on this front.  However, others in the Academy still question the relevance of pop culture and address studies of it with skepticism, if not disdain. And so the question still remains: what is the value of studying popular culture? Why should Praxis spend a year thinking about and studying the Kardashian family and their media empire? Part of our goal for this project is to explore this question in its own right, but in the meantime, here are four reasons why it is critical that we study the Kardashians now :  In 2015 the season premiere of the Kardashians was the most viewed Sunday cable program—notably ahead of the finale of AMC’s Mad Men—averaging about 4.24 million viewers, ranking it the number one Sunday night program for adults 18-34, and women 18-49. By ignoring and degrading such a popular program, we are not just looking down upon the tv program in question, but disregarding its audience. Television, a medium based on repetition, reinforces dominant social meanings and prevailing ideologies. Reality TV creates the illusion of a false intimacy between spectator and subject through the repetitive depiction of everyday tasks and conversations. If we understand television as a medium that cements dominant social ideologies through repetitive viewing, Reality TV and its pretense of the ‘real’ reinforces these ideologies two-fold. But upon closer look at the themes prevailing throughout Keeping Up with the Kardashians complicate what we might perceive to be dominant social norms. The Kardashians have subverted the traditional family sitcom in favor a matriarchal family structure, transforming the traditional private sphere of the home into their center of business, while maintaining heteronormative assumptions about the objectification of women. Considering the popularity of the Kardashians, what can this tell us about dominant American social ideologies?  Studying the popular is political. Audiences often make and remake their identities, in terms of gender, sexuality, race, ethnicity, class, and any other mode of intersectionality, in response to the popular culture they consume. Those identities, in turn, inform their personal political choices. If we want to understand the political culture of any historical moment, we should understand its popular culture, too. And, love them or hate them, the Kardashians are popular. They constitute a large portion of the ubiquitous feed of mediated information that the populace consumes through social media, tabloid journalism, music, and reality television. While the definition of popular culture as the cultural activities of “the people” has been scrutinized by scholars such as Stuart Hall for its essentialist view of the binary between “the people” and “the elite,” the  old Arnoldian meanings of the terms “culture” and “art” developed in response to early industrialization, mutually reinforcing the aesthetic, intellectual, and social values of an anti-bourgeois elite class. By studying popular culture, by giving it space alongside more traditional subjects of study, we push towards eradicating this long established distinction between high and low. As noted in the previous bullet, Stuart Hall rejects this essentialist definition of popular culture in favor of a definition which stresses a constant tension between high and low (and between the hegemonic and counterhegemonic). Popular culture is dynamic, constantly shifting from marginalized to widespread acceptance, at times assimilating into “high culture” or vice versa. This understanding of a dynamic relationship of give and take between high and low culture is especially important in the 21st century, as the internet and mass media have converged these themes, see for example, Lady Gaga’s Collaboration with Jeff Koons . In fact, the Kardashians are no stranger to the contemporary art world. Kim Kardashian’s book of Selfies was widely praised as a pop art exploration of selfhood and identity construction."},{"id":"2017-02-15-over-the-moon-and-down-to-earth-scholars-lab-versions-of-space","title":"Over the Moon and Down to Earth: Scholars' Lab versions of space","author":"alison-booth","date":"2017-02-15 18:53:48 -0500","categories":["Announcements","Digital Humanities","Geospatial and Temporal","Makerspace"],"url":"over-the-moon-and-down-to-earth-scholars-lab-versions-of-space","layout":"post","content":"Something about space has gone to our heads here in the Scholars’ Lab.  Any day, I expect to come out of a trance to find I’ve been wearing a motion capture suit while I geo-referenced an historic map, programmed an arduino, flew a drone, and printed a replica of an artifact from an ancient burial site. Of course, I only imagine doing all this myself. But the interdisciplinary collaborations of Scholars’ Lab staff perform this sort of spatial magic in and around the Library every day. The rich array of spatial research has been a forte of the Scholars’ Lab since before I joined the group (my own project had its early days here).  GIS specialists and the Neatline project (now ably led by Eric Rochester and Ronda Grizzle ) have been located in the Scholars’ Lab for years.  But we have some more recent additions that make for an exciting mix of geospatial perspectives. We recently welcomed Geospatial Consultant Drew MacQueen, who has been serving in Facilities Management at the University of Virginia. Drew will work closely with Chris Gist on the many GIS services needed in research and teaching at the University; see the Library’s guide .  For example, a longstanding project on the Salem witch trials led by Religious Studies professor Ben Ray recently identified the site of execution, based on Chris’s topographical analysis of the viewshed and historic maps . Will Rourk and Arin Bennet joined the Scholars’ Lab team in 2016, bringing to bear their expertise in spatial research and the high-end equipment of the former DML.  They collaborate with faculty and staff in IATH, SHANTI, the Scholars’ Lab, and elsewhere on cultural heritage, much of it on or near Grounds, but some as remote (or, now, accessible) as the Tibetan capital, Lhasa.  It turns out, the extended team of Scholars’ Lab people participate in interrelated projects, from mapping to 3-D printing. The Makerspace was thriving when I arrived a year ago, organized by Jeremy Boggs and Laura Miller, and increasingly guided by Ammon Shepherd, with a team that includes DH dissertation fellow Shane Lin and graduate and undergraduate staff and drop-in experimenters.  (See the Makerspace technologists.) The tentacles of this experimentation reach far. And there were cookies.  **On February 15, 2017 we held the first of a new series of **brown-bag lunches, this one to discuss Daniel Punday’s “Space Across Narrative Media: Towards an Archeology of Narratology, ” Narrative 25 (Jan 2017): 92-112.  Some argued that architecture is a better metaphor than archeology for Punday’s narratological approach, in the context of building and inhabiting things human beings make, such as the storyworlds of games or novels. It was delightful to discuss this thought-provoking piece with 19 people, graduate students and faculty from Spanish, German, history, Slavic, computer science, and English, and library staff in archeology and Scholars’ Lab as well as IATH.  One of Punday’s examples, Jason Nelson’s I made this. You play this.  We are enemies . playfully makes a point about the different spaces of the game’s action and the orienting background (captured from commercial sites).  We can say, we made this. We play this. We are energetic participants.   We like to playfully make things that have points–the coordinates of spatial data."},{"id":"2017-02-23-amanda-visconti-is-our-new-managing-director","title":"Amanda Visconti Is Our New Managing Director","author":"alison-booth","date":"2017-02-23 08:40:13 -0500","categories":null,"url":"amanda-visconti-is-our-new-managing-director","layout":"post","content":"I am thrilled to announce yet more delightful news: Amanda Visconti will join the Library as Managing Director of the Scholars’ Lab.  We had terrific contenders for this position, and Amanda won us over!  She brings a high level of qualities seldom found in one person: intellectual commitments and depth, knowledge, and training in humanities; versatile and robust technical skills and experience collaborating on advanced digital research; immersion in the library world of information, teaching, service, and research; gifts in creative community building, especially through social media; imagination, integrity, and effective management.  She already practices the ethics and aims of our Charter, and will be a delightful co-director as DH@ UVA is developed into more coordinated collaboration across schools and entities, and into a curriculum with a certificate. Amanda wrote the first entirely-digital dissertation in literary humanities: Infinite Ulysses, which could be described as crowd-sourced annotation-as-usability-testing.  The process of building it, and its ongoing participatory development, have made it a model both in Modernist and Joyce studies and in DH: award-winning, widely reviewed, with tens of thousands of users.  She has been assistant professor (tenure track) and digital humanities specialist librarian in the Libraries and Information Science Department, Purdue University.  She is much in demand as a speaker and contributor to DH publications.  Recently, she launched the influential Digital Humanities Slack ( TinyUrl.com/DHSlack ).  Having earned the M.S. In Information, DH, and HCI at the University of Michigan, she earned her doctorate working with John Unsworth’s former advisee, Professor Matt Kirschenbaum, and others at the University of Maryland.  Just as our collaboration with Washington and Lee is ongoing, we look forward to formalizing regular interactions with MITH and other regional groups such as Roy Rosenzweig CHNM at George Mason, University of Richmond, and elsewhere, with Amanda’s leadership. Happily, Amanda is prepared to guide the Scholars’ Lab’s agenda well beyond literary DH (see my recent post on the SLAB blog about how spatial we are).  She is looking forward, as well, to advancing our programs and projects related to social justice, diversity, and accessibility. Please help us welcome Amanda to the Scholars’ Lab and Library!  She will certainly be on Grounds by May 1 (we’re hoping to see her sooner).  Communication is one of her fortes, as we’re already in good contact on Slack and other ways."},{"id":"2017-02-23-brandon-walsh-is-our-new-head-of-graduate-programs-starting-april-24-2017","title":"Brandon Walsh is our New Head of Graduate Programs, starting April 24, 2017","author":"alison-booth","date":"2017-02-23 04:10:46 -0500","categories":null,"url":"brandon-walsh-is-our-new-head-of-graduate-programs-starting-april-24-2017","layout":"post","content":"I’m thrilled to announce that Brandon Walsh will join the Library as Head of Graduate Programs in the Scholars’ Lab. This competitive national search revealed the talent that is growing out there: people dedicated to libraries and the pedagogy/service/advanced research model, with the combination of versatile and active technological skills and deep training in humanities or other disciplines. Brandon rose to the top. Currently the Mellon Digital Humanities Fellow, Visiting Assistant Professor of English at Washington and Lee University, he has recently collaborated with the Praxis Program from Lexington, and helped to build a DH curriculum and center in W&amp;L’s library. Brandon holds the PhD in English, UVA; his dissertation, “AudioTextual: Modernism, Sound Recordings, and Networks of Reception,” provides a model of interdisciplinary work for our current graduate students, uniting sound studies, Modernist literary studies, and DH. We’re excited by the work he has continued to publish or present on open educational resources, DH pedagogy, and sound-language studies in audio data sets. Brandon brings further skills needed in the Head of Graduate Programs position: gifts as a teacher (he won a teaching award here); ability to teach digital methods and tools, as in his participation HILT; proven ability to manage grant funds and host public events; integrity and geniality in all kinds of personal interaction. Knowledge of an R1 public university, and this particular one, will help him quickly take a co-leadership role in the Lab. The cohorts of DH and Praxis fellows, and an increasing group of student interns (undergraduate and graduate) will benefit from his inspirational presence. After his teaching duties at Washington and Lee are completed in April, he will be settling in on April 24 to Purdom Lindblad’s former desk in the Scholars’ Lab. Please help us welcome him. Hint: you may see him around C’ville, or he will be virtually present in helping to select next years’ fellows."},{"id":"2017-02-28-disrupt-the-humanities-managing-director-job-talk","title":"Disrupt the Humanities? (Managing Director job talk)","author":"amanda-visconti","date":"2017-02-28 01:00:12 -0500","categories":["Digital Humanities"],"url":"disrupt-the-humanities-managing-director-job-talk","layout":"post","content":"When scholars share their job talks after being hired, DH and libraries interviewing processes become a little less mysterious. Academia has many genres of presentation. It can be difficult to draft your first job talk without seeing how others in your field translate a job ad’s requirements into a job talk’s expression of their unique expertise.  Lee Skallerup Bessette,  Celeste Tuong Vy Sharpe,  Chris Bourg, and Brandon Walsh  all share successful job talks on their websites (I shared the talk for my previous role, too). I’ve long been a fan of the digital humanities at the University of Virginia, and I’m excited to join the Scholars’ Lab and University of Virginia Libraries teams as Managing Director of the Scholars’ Lab . In this post, I’ll share the talk I presented as part of my campus interview for this role. The presentation prompt asked for a discussion of my past projects: The digital has the potential to be a disruptive force in humanities scholarship, giving scholars the means to critique, reimagine, and transform ideas, theories, material artifacts, and even interpersonal relationships. Using examples from your own collaborations with faculty, graduate students, and staff, please discuss how this disruption can be successfully embodied in scholarly inquiry, as well as in the cultivation of people and organizations. The presentation My digital humanities is not disruptive. Disruption can be a force for good, especially against systemic problems. But to my ear, disruptive is not what I want DH to be. Disruption is inherently non-introspective; you disrupt others, rather than looking hard at yourself. Disruption can ignore a history of similar work, and the humanities can’t afford that mistake. For example, a movement to make the humanities more public that ignored the fan fiction communities’ existing public humanities—people using the web to improve one another’s close readings and counterfactual interventions—would be deeply flawed. DH has diverse roots in pre-digital humanities scholarship, including highly recognizable non-digital, algorithmic criticism of the kinds Stephen Ramsay explores in his book Reading Machines . We have much to gain by connecting our efforts today to what has worked in the humanities of the past, and to the humanities that also thrives outside academia. So when I share some of my recent collaborative DH work with you today, I’d like us to consider this work not in terms of how it disrupts the humanities, but through a variety of more positive and generative lenses. DH as… An applied humanities Experimental Public and participatory A radically interdisciplinary humanities, embracing social science, science, and art Open to any challenges to which we can bring our skills of humanities thinking I’m hopeful for the humanities Bethany Nowviskie has imagined in her work “ on capacity and care ”. Nowviskie cautions: “Please do not mistake [capacity and care work] for something idealistic and motherly and sweet. I offer care as a hard-nosed survival strategy… to increase the reach and grasp (which is at the root of the word “capacity”—the “capture”) of the humanities.” Seeing both the positive and negative sides of disruption, and turning to other concepts with different values, aren’t only reactions fueled by thinking these are good ways for a person to be in the world. There are significant gains for pushing our scholarship to be more intellectually generous, more locally interconnected, and better at crediting key emotional labor such as team-building and mentorship. So in the next twenty minutes today, I want to tell you a bit about two of my recent DH projects, one where the bulk of the work is my own, and one where I’m only nominally the creator. This work articulates some of my hopes for what the digital humanities could be, a DH deeply part of the humanities rather than opposed to it, a DH that’s more interested in the right column up on the screen [ in the image above this paragraph, for blog readers ]—in seeing innovation in maintainance, the quality rather than the quantity of impact, on “people over projects” (as the Scholars’ Lab puts it). The first project I’ll discuss, Infinite Ulysses, shows humanities scholarly inquiry that’s been reimagined and opened beyond academia, rather than disrupted. I’m a textual scholar. I decided that instead of focusing on the scholarly editing aspect of digital editions, my skills and interest were more in line with work like that of Alan Galey, whose Visualizing Variation project created code that lets editors of digital editions intervene in their texts in unique ways. Rather than creating a scholarly edition, I focused more on interface design aimed at opening a literary edition to a public audience. I asked: What if we build a digital edition and invite everyone? What if millions of scholars, first-time readers, book clubs, teachers and their students, show up and annotate a text with their “infinite” annotations (that is, digital marginalia of interpretations, questions, contextualizations, and other comments)? How would we do that, and what would it do to the text and our understandings of it? I worked through this speculative experiment, with its admittedly unlikely hypothetical of a massive public desire to read Ulysses online. But I also built a prototype that was immediately useful in exploring these questions, through the generous annotations and feedback of an actual, modestly sized community of readers. Infinite Ulysses (at InfiniteUlysses.com ), is a participatory digital edition of James Joyce’s challenging novel Ulysses . Its goal was to support a public conversation about the novel, bringing readers of different backgrounds into participating in the same discussion space. The design, code, usertesting, and thinking that went into this project doubled as my literature dissertation. This is what reading a page on Infinite Ulysses looks like. There are three main activities you can pursue: While reading the novel, you can highlight parts of the text and annotate these with your questions, interpretations, reactions, translations, and other comments. You can also read the annotations created by other readers. And I tried, at a basic level, to personalize your reading experience by filtering the displayed annotations so that you could just be shown the ones that fit your needs, interests, and background. For example, a reader can choose to see the top-rated annotations first, or the newest annotations; or to not see any spoilers; or to only see annotations tagged as clues to the book’s mysteries, references to Irish politics, or jokes you might miss. There is some solid interest in this approach. My work was cited in The New York Times this past summer, drew around 13,000 unique visitors in its first month of beta testing, and drew over 25,000 unique visitors during its beta year (April 2015-2016), despite little intentional publicity after the first month. I share these numbers to show that there’s interest in this scholarship, but in doing so I feel uncomfortably close to getting caught up in the allure of large-scale impact. Much more important to me, and to the humanities, are the far smaller number of people who actually repeatedly visited and annotated Ulysses on the site. What matters is the students who hear about my dissertation and are encouraged to also pursue the most appropriate methods for their research questions. I care about my impact at the scale of individual readers, like the South Korean reader who found my edition more accessible for her translation and understanding. Or the 90-year-old retired man who wrote that he’d always wanted to read Ulysses, and now he felt like he could take his laptop to a bar, pull up a stool, and maybe actually read the thing this time using my site. There are other facets to the scholarship of Infinite Ulysses, clear areas of research inquiry: My 1st research question was an interdisciplinary DH one: How can we design digital editions that are not just theoretically accessible to the public, but invite and assist public participation? And are there ways to design for meaningful public participation in literature, that don’t require the public to learn scholarly rhetoric? My 2nd question was from the field of information science: How can we design public DH websites to handle a huge influx of readers and annotations, so that individual user experience doesn’t suffer and diverse user needs are met? How would each user find the “signal” of the best annotations for them to read, from the noise of “infinite” public annotations? My 3rd question was from the field of literature: Textual scholars often create editions as their scholarship. By separating out historical textual scholarship values from how the editions realizing those values looked, can we imagine other forms still holding true to the values of our field? What can we learn by accepting scholarly editions as just one of many ways of embodying textual scholarship values? Now, I’m here to speak about collaborative DH today, and you might be wondering how a literature dissertation can be a collaborative rather than a largely solo activity. I did perform the full scholarly labor you’d expect from a literature dissertation—most people would agree successfully defending the dissertation and becoming a tenure-track assistant professor shows that. But beyond my dissertation-length work, the larger project around my dissertation relied strongly on collaboration and the intellectual generosity of others. A huge number of people made Infinite Ulysses possible, so much so that I can’t name everyone right now, but I’ve put as many as possible up on the screen [ image above previous paragraph, and this credits webpage  goes into more detail ]. My advisor Matt Kirschenbaum and committee members Neil Fraistat, Kari Kraus, Melanie Kill, and Brian Richardson are amazingly intellectually generous scholars. They met with me as a team regularly throughout the entire dissertation, which was hugely helpful in keeping everyone informed about and happy with the developing look of the dissertation. I also benefitted from the feedback of colleagues at the MITH DH center where I worked 2009-2015, such as Stephanie Sapienza and Ed Summers . Their questions, guidance, and encouragement were critical in realizing my project. Through social media, DHer Michael Widner at Stanford and I discovered we were working on similar tech (the Drupal annotator module). Despite being on opposite coasts, we Skyped to discuss our work, and he ended up generously allowing me to build on top of some of his Lacuna Stories project code that hadn’t yet been publicly released, allowing me to spend more time on a different piece of coding more closely tied to my research questions. The people who visited Infinite Ulysses and shared annotations and/or feedback on the site are collaborators, too . The majority of the site’s annotations were authored by people other than me. Ten types of user testing, data gathering, and participatory design, with a variety of people, over the course of the site’s development, directly shaped the growth of the site. Infinite Ulysses was a collaborative project with a large portion created by one person. Now, I want to tell you about a project that’s far more of a distributed and balanced collaborative, the Digital Humanities Slack . Slack is a social media platform that functions like a set of interconnected, themed chat rooms. Using the Slack platform, I created and manage the scholarly forum of the Digital Humanities Slack (or DH Slack). Started a little over a year ago, it now supports over 1,200 digital humanists through around 70 “channels” (thematic chat rooms on topics like libraries and the digital humanities, open access, data sharing, and various DH methods and theories). The DH Slack is open to anyone with a curiosity about DH and/or related interests (e.g. digital libraries, museums, and archives)—those interested just visit tinyurl.com/DHslack to join. Absolutely no DH expertise is required, and we have several specific channels devoted to DH beginners, students, job seekers, and asking all kinds of DHy questions. The DH Slack is ““mine”” [ all the air quotes ] in an extremely limited sense, if any. I set up the Slack, publicize its existence, moderate it, periodically started discussions (more in its early days than now), answer questions about using the Slack, and lead policy-making such as adapting a DH-specific code of conduct. That code of conduct covers things like not publicly posting screenshots of Slack conversations without first getting the consent of everyone shown in the shot, which is why I’m largely showing tweets rather than screengrabs. Five other DHers generously collaborate with me as additional Slack moderators: Alan G. Pike, Sam Abrams, Alex Gil, Ed Summers, and Paige C. Morgan . We take our code of conduct seriously, and have taken firm action when it’s been called for, to protect our community members. The DH Slack exists because of its members, who build a humanities community through their conversations and sharing. Even the most collaborative and democratic of teams needs one person to really be in charge of project management, and I feel like my current role with the DH Slack is similar—helping things along, being available and responsible should problems arise, and making the final call after group discussion. We’re still figuring out how Slack can be useful to our scholarship: Can it allow different kinds of conversations than Twitter? On the screen, you can see examples of DHers moving between the Slack and Twitter to have different kinds of conversations—in that middle tweet, a technical discussion that was limited by Twitter’s format switched to the Slack to continue more freely. Some of our members work at non-profits or in K-12 teaching, or aren’t professionally involved in academics; some are undergrads or grad students. It’s clear that the Slack, like many other platforms, can support geographically dispersed collaborators. But can we use the Slack to teach and support people interested in DH who don’t know of any potential collaborators, don’t have mentors geographically near them, or who aren’t inside academia? For example, I’m hoping to do a Slack hangout this spring where I’ll be available on the Slack to help anyone who wants to work through my Programming Historian lesson on creating their own first research website . If DH mentors can periodically be virtually available to support questions about specific tech tutorials, maybe we can ease new DHers experimentation with digital methods. Interesting uses of the DH Slack I’ve seen so far include allowing remote participants at MITH’s recent “ Night Against Hate ” scholar-activist hackathon, mentorship for new digital humanists, a place to connect regional DH networks such as in Tennessee, Baltimore, and Australia and languages such as Spanish, and as a space to expand Twitter conversations. We’ve been used as a model by other scholars for their own Slack instances, and I authored an invited guest post about the DH Slack for the London School of Economics blog this past summer. If you’d like a place to discuss and learn about the digital humanities, or a friendly place to ask questions, please join us via TinyUrl.com/DHSlack . Both the DH Slack and Infinite Ulysses model a digital humanities that’s not disruptive, but different. Both take an experimental approach grounded on past humanities successes, whether that’s DH Twitter or speculative textual experiments like the Ivanhoe game or Prism . The experiments of both projects recognizably demonstrate humanities scholarship and community. Both experiments also demonstrate my key interests. I’m deeply invested in DH design and building as a route to new knowledge, especially through interfaces for public, communal learning. And I’m keenly interested in the larger questions of how we make DH go: infrastructure and process, project management, and community-building. If you visit my LiteratureGeek.com blog, you’ll see that I share everything from theory and analysis, to my software and design decisions, how I get started planning a new project, and how my work space is set up. This is all as a way to think through not just the strategies, but also the daily tactics of successful DH. Infinite Ulysses and the DH Slack are at two ends of the collaboration spectrum, but the bulk of my interdisciplinary collaboration experience actually happens quite differently, in project teams that regularly meet face-to-face (as with a group of graduate students I taught to work with the Shelley-Godwin Archive ) or over Skype (as with the BitCurator team). For example, in my current role as a DH librarian and faculty at Purdue Libraries, I enjoy a close working relationship with my colleagues in both Archives and Special Collections, and in our university press. Together, we’re using DH as the connector to build a workflow for campus DH research and teaching that runs from archival description and discovery, through scholarly communication and publication. I didn’t cover this kind of collaboration in detail in this talk, as I suspect this kind of face-to-face collaboration is the most common and familiar. But, I’ll wrap up by listing on the screen a few of my more typically collaborative DH projects on the screen, and I’m happy to discuss these further in the Q&amp;A or over email. Originally posted at LiteratureGeek.com . Edited 3/6/2017 to link to Brandon’s job talk."},{"id":"2017-03-06-in-out-across-with-collaborative-education-and-digital-humanities-job-talk-for-head-of-graduate-programs","title":"In, Out, Across, With: Collaborative Education and Digital Humanities (Job Talk for Head of Graduate Programs)","author":"brandon-walsh","date":"2017-03-06 06:15:23 -0500","categories":["Digital Humanities","Grad Student Research","Technical Training"],"url":"in-out-across-with-collaborative-education-and-digital-humanities-job-talk-for-head-of-graduate-programs","layout":"post","content":"[Crossposted on my personal blog and the WLUDH blog .] I’ve accepted a new position as the Head of Graduate Programs in the Scholars’ Lab, and I’ll be transitioning into that role over the next few weeks! As a part of the interview process, we had to give a job talk. While putting together this presentation, I was lucky enough to have past examples to work from (as you’ll be able to tell, if you check out this past job talk by Amanda Visconti). Since my new position will involve helping graduate students through the process of applying for positions like these, it only feels right that I should post my own job talk as well as a few words on the thinking that went into it. Blemishes, jokes, and all, hopefully these materials will help someone in the future find a way in, just as the example of others did for me. And if you’re looking for more, Visconti has a great list of other examples linked from her more recent job talk for the Scholars’ Lab. For the presentation, I was asked to respond to this prompt: What does a student (from undergraduate to doctoral levels) need to learn or experience in order to add “DH” to his or her skill set? Is that an end or a means of graduate education? Can short-term digital assignments in discipline-specific courses go beyond “teaching with technology”? Why not refer everyone to online tutorials? Are there risks for doctoral students or the untenured in undertaking digital projects? Drawing on your own experience, and offering examples or demonstrations of digital research projects, pedagogical approaches, or initiatives or organizations that you admire, make a case for a vision of collaborative education in advanced digital scholarship in the arts and humanities. I felt that each question could be a presentation all its own, and I had strong opinions about each one. Dealing with all of them seemed like a tall order. I decided to spend the presentation close reading and deconstructing that first sentence, taking apart the idea that education and/or digital humanities could be thought of in terms of lists of skills at all. Along the way, my plan was to dip into the other questions as able, but I also assumed that I would have plenty of time during the interview day to give my thoughts on them. I also wanted to try to give as honest a sense as possible of the way I approach teaching and mentoring. For me, it’s all about people and giving them the care that they need. In conveying that, I hoped, I would give the sort of vision the prompt was asking for. I also tried to sprinkle references to the past and present of the Scholars’ Lab programs to ground the content of the talk. When I mention potential career options in the body of the talk, I am talking about specific alumni who came through the fellowship programs. And when I mention graduate fellows potentially publishing on their work with the Twitter API, well, that’s not hypothetical either . So below find the lightly edited text of the talk I gave at the Scholars’ Lab - “In, Out, Across, With: Collaborative Education and Digital Humanities.” I’ve only substantively modified one piece - swapping out one example for another. And a final note on delivery: I have heard plenty of people argue over whether it is better to read a written talk or deliver one from notes. My own sense is that the latter is far more common for digital humanities talks. I have seen both fantastic read talks and amazing extemporaneous performances, just as I have seen terrible versions of each. My own approach is, increasingly, to write a talk but deliver that talk more or less from memory. In this case, I had a pretty long commute to work, so I recorded myself reading the talk and listened to it a lot to get the ideas in my head. When I gave the presentation, I had the written version in front of me for reference, but I was mostly moving through my own sense of how it all fit together in real time (and trying to avoid looking at the paper). My hope is that this gave me the best of both worlds and resulted in a structured but engaging performance. Your mileage may vary! In, Out, Across, With: Collaborative Education and Digital Humanities  It’s always a treat to be able to talk with the members of the UVA Library community, and I am very grateful to be here. For those of you that don’t know me, I am Brandon Walsh, Mellon Digital Humanities Fellow and Visiting Assistant Professor of English at Washington and Lee University. The last time I was here, I gave a talk that had almost exclusively animal memes for slides. I can’t promise the same robust Internet culture in this talk, but talk to me after and I can hook you up. I swear I’ve still got it.  In the spirit of Amanda Visconti, the resources that went into this talk (and a number of foundational materials on the subject) can all be found in a Zotero collection at the above link . I’ll name check any that are especially relevant, but hopefully this set of materials will allow the thoughts in the talk to flower outwards for any who are interested in seeing its origins and echoes in the work of others.  And a final prefatory note: no person works, thinks or learns alone, so here are the names of the people in my talk whose thinking I touch upon as well as just some – but not all – of my colleagues at W&amp;L who collaborate on the projects I mention. Top tier consists of people I cite or mention, second tier is for institutions or publications important to discussion, and final tier is for direct collaborators on this work. Today I want to talk to you about how best to champion the people involved in collaborative education in digital research. I especially want to talk about students. And when I mention “students” throughout this talk, I will mostly be speaking in the context of graduate students. But most of what I discuss will be broadly applicable to all newcomers to digital research. My talk is an exhortation to find ways to elevate the voices of people in positions like these to be contributors to professional and institutional conversations from day one and to empower them to define the methods and the outcomes of the digital humanities that we teach. This means taking seriously the messy, fraught, and emotional process of guiding students through digital humanities methods, research, and careers. It means advocating for the legibility of this digital work as a key component of their professional development. And it means enmeshing these voices in the broader network around them, the local context that they draw upon for support and that they can enrich in turn. I believe it is the mission of the Head of Graduate Programs to build up this community and facilitate these networks, to incorporate those who might feel like outsiders to the work that we do. Doing so enriches and enlivens our communities and builds a better and more diverse research and teaching agenda. This talk is titled “In, Out, Across, With: Collaborative Education and Digital Humanities,” and I’ll really be focusing on the prepositions of my title as a metaphor for the nature of this sort of position. I see this role as one of connection and relation. The talk runs about 24 minutes, so we should have plenty of time to talk. When discussing digital humanities education, it is tempting to first and foremost discuss what, exactly, it is that you will be teaching. What should the students walk away knowing? To some extent, just as there is more than one way to make breakfast, you could devise numerous baseline curricula.  This is what we came up with at Washington and Lee for students in our undergraduate digital humanities fellowship program . We tried to hit a number of kinds of skills that a practicing digital humanist might need. It’s by no means exhaustive, but the list is a way to start. We don’t expect one person to come away knowing everything, so instead we aim for students to have an introduction to a wide variety of technologies by the end of a semester or year. They’ll encounter some technologies applicable to project management, some to front-end design, as well as a variety of programming concepts broadly applicable to a variety of situations. Lists like this give some targets to hit. But still, even as someone who helped put this list together, it makes me worry a bit. I can imagine younger me being afraid of it! It’s easy for us to forget what it was like to be new, to be a beginner, to be learning for the first time, but I’d like to return us to that frame of thinking. I think we should approach lists like these with care, because they can be intimidating for the newcomer. So in my talk today I want to argue against lists of skills as ways of thinking. I don’t mean to suggest that programs need no curriculum, nor do I mean to suggest that no skills are necessary to be a digital humanist. But I would caution against focusing too much on the skills that one should have at the end of a program, particularly when talking about people who haven’t yet begun to learn. I would wager that many people on the outside looking in think of DH in the same way: it’s a big list of unknowns. I’d like to get away from that. Templates like this are important for developing courses, fellowship, and degree-granting programs, but I worry that the goodwill in them might all too easily seem like a form of gatekeeping to a new student. It is easy to imagine telling a student that “you have to learn GitHub before you can work on this project.” It’s just a short jump from this to a likely student response - “ah sorry - I don’t know that yet.” And from there I can all too easily imagine the common refrain that you hear from students of all levels - “If I can’t get that, then it’s because I’m not a technology person.” From there - “Digital humanities must not be for me.” Instead of building our curricula out of as-yet-unknown tool chains, I want to float, today, a vision of DH education as an introduction to a series of professional practices. Lists of skills might be ends but I fear they might foreclose beginnings. Instead, I will float something more in line with that of the Scholarly Communication Institute (held here at UVA for a time), which outlined what they saw as the needs of graduate and professional students in the digital age. I’ll particularly draw upon their first point here (last of my slides with tons of text, I swear): graduate students need training in “collaborative modes of knowledge production and sharing.” I want to think about teaching DH as introducing a process of discovery that collapses hierarchies between expert and newcomer: that’s a way to start. This sort of framing offers digital humanities not as a series of methods one does or does not know, but, rather, as a process that a group can engage in together. Do they learn methods and skills in the process? Of course! Anyone who has taken part in the sort of collaborative group projects undertaken by the Scholars’ Lab comes away knowing more than they came in with. But I want to continue thinking about process and, in particular, how that process can be more inclusive and more engaging. By empowering students to choose what they want to learn and how they want to learn it, we can help to expand the reach of our work and better serve our students as mentors and collaborators. There are a few different in ways in which I see this as taking place, and they’ll form the roadmap for the rest of the talk.   Apologies - this looks like the sort of slide you would get at a business retreat. All the same - we need to adapt and develop new professional opportunities for our students at the same time that we plan flexible outcomes for our educational programs. These approaches are meant to serve increasingly diverse professional needs in a changing job market, and they need to be matched by deepening support at the institutional level. So to begin. One of our jobs as mentors is to encourage students to seek out professionally legible opportunities early on in their careers, and as shapers of educational programs we can go further and create new possibilities for them. At W&amp;L, we have been collaborating with the Scholars’ Lab to bring UVA graduate students to teach short-form workshops on digital research in W&amp;L classrooms. Funded opportunities like this one can help students professionalize in new ways and in new contexts while paying it forward to the nearby community. A similar initiative at W&amp;L that I’ve been working on has our own library faculty and undergraduate fellows visiting local high schools to speak with advanced AP computer science students about how their own programming work can apply to humanities disciplines. I’m happy to talk more about these in Q&amp;A. We also have our student collaborators present at conferences, both on their own work and on work they have done with faculty members, both independently and as co-presenters. Here is Abdur, one of our undergraduate Mellon DH fellows, talking about the writing he does for his thesis and how it is enriched by and different from the writing he does in digital humanities contexts at the Bucknell Digital Scholarship Conference last fall. While this sort of thing is standard for graduate students, it’s pretty powerful for an undergraduate to present on research in this way. Learning that it’s OK to fail in public can be deeply empowering, and opportunities like these encourage our students to think about themselves as valuable contributors to ongoing conversations long before they might otherwise feel comfortable doing so. But teaching opportunities and conferences are not the only ways to get student voices out there. I think there are ways of engaging student voices earlier, at home, in ways that can fit more situations. We can encourage students to engage in professional conversations by developing flexible outcomes in which we are equal participants. One approach to this with which I have been experimenting is group writing, which I think is undervalued as a taught skill and possible approach to DH pedagogy. An example: when a history faculty member at W&amp;L approached the library (and by extension, me) for support in supplementing an extant history course with a component about digital text analysis, we could have agreed to offer a series of one-off workshops and be done with it.  Instead, this faculty member – Professor Sarah Horowitz – and I decided to collaborate on a more extensive project together, producing Introduction to Text Analysis: A Coursebook . The idea was to put the materials for the workshops together ahead of time, in collaboration, and to narrativize them into a set of lessons that would persist beyond a single semester as a kind of publication. The pedagogical labor that we put into reshaping her course could become, in some sense, professionally legible as a series of course modules that others could use beyond the term. So for the book, we co-authored a series of units on text analysis and gave feedback on each other’s work, editing and reviewing as well as reconfiguring them for the context of the course. Professor Horowitz provided more of the discipline-specific material that I could not, and I provided the materials more specific to the theories and methods of text analysis. Neither one of us could have written the book without the other. Professor Horowitz was, in effect, a student in this moment. She was also a teacher and researcher. She was learning at the same time that she produced original scholarly contributions. Even as we worked together, for me this collaborative writing project was also a pedagogical experiment that drew upon the examples of Robin DeRosa, Shawn Graham, and Cathy Davidson, in particular.   Davidson taught a graduate course on “21st Century Literacies” where each of her students wrote a chapter that was then collected and published as an open-access book. For us as for Davidson, the process of knowing, the process of uncovering is something that happens together. In public. And it’s documented so that others can benefit. Our teaching labor could become visible and professionally legible, as could the labor that Professor Horowitz put into learning new research skills. As she adapts and tries out ideas, and as we coalesce them into a whole, the writing product is both the means and the end of an introduction to digital humanities. Professor Horowitz also wanted to learn technical skills herself, and she learned quite a lot through the writing process. Rather than sitting through lectures or being directed to online tutorials by me, I thought she would learn better by engaging with and shaping the material directly. Her course and my materials would be better for it, as she would be helping to bind my lectures and workshops to her course material. The process would also require her to engage with a list of technologies for digital publishing.  Beyond the text analysis materials and concepts, the process exposed her to a lot of technologies: command line, Markdown, Git for version control, GitHub for project management. In the process of writing this document, in fact, she covered most of the same curriculum as our undergraduate DH fellows.  She’s learning these things as we work together to produce course materials, but, importantly, the technical skills aren’t the focus of the work together. It’s a writing project! Rather than presenting the skills as ends in themselves, they were the means by which we were publishing a thing. They were immediately useful. And I think displacing the technology is helpful: it means that the outcomes and parameters for success are not based in the technology itself but, rather, in the thinking about and use of those methods. We also used a particular platform that allowed Professor Horowitz to engage with these technologies in a light way so that they would not overwhelm our work – I’m happy to discuss more in the time after if you’re interested. This to say: the outcomes of such collaborative educations can be shaped to a variety of different settings and types of students. Take another model, CUNY’s Graduate Center Digital Fellows program, whose students develop open tutorials on digital tools. Learning from this example, rather than simply direct students or colleagues towards online tutorials like these, why not have them write their own documents, legible for their own positions, that synthesize and remix the materials that they already have found?  The learning process becomes something productive in this framing. I can imagine, for example, directing collaboratively authored materials by students like these towards something like The Programming Historian . If you’re not familiar, The Programming Historian offers a variety of lessons on digital humanities methods, and they only require an outline as a pitch to their editorial team, not a whole written publication ready to go. Your graduate students could, say, work with the Twitter API over the course of a semester, blog about the research outcomes, and then pitch a tutorial to The Programming Historian on the API as a result of their work. It’s much easier to motivate yourselves to write something if you know that the publication has already been accepted. Obviously such acceptance is not a given, but working towards a goal like this can offer student researchers something to aim for. Their instructors could co-author these materials, even, so that everyone has skin in the game. This model changes the shape of what collaborative education can look like: it’s duration and its results. You don’t need a whole fellowship year. You could, in a reasonably short amount of time, tinker and play, and produce a substantial blog post, an article pitch, or a Library Research Guide (more on that in a moment). As Jeff Jarvis has said, “we need to move students up the education chain.” And trust me - the irony of quoting a piece titled “Lectures are Bullshit” during a lecture to you is not lost on me. But stay with me. Collaborative writing projects on DH topics are flexible enough to fit the many contexts for the kind of educational work that we do. After all, no one needs or values the same outcomes, and these shared and individual goals need to be worked out in conversation with the students themselves early on. Articulating these desires in a frank, written, and collaborative mode early on (in the genre of the project charter ), can help the program directors to better shape the work to fit the needs of the students. But I also want to suggest that collaborative writing projects can be useful end products as well as launching pads, as they can fit the shape of many careers. After all, students come to digital humanities for a variety of different reasons. Some might be aiming to bolster a research portfolio on the path to a traditional academic career. Others might be deeply concerned about the likelihood of attaining such a position and be looking for other career options. Others still might instead be colleagues interested in expanding their research portfolio or skillset but unable to commit to a whole year of work on top of their current obligations. Writing projects could speak to all these situations. I see someone in charge of shaping graduate programs as needing to speak to these diverse needs. This person is both a steward of where students currently are – the goals and objectives they might currently have – as well as of where they might go – the potential lives they might (or might not!) lead. After all, graduate school, like undergraduate, is an enormously stressful time of personal and professional exploration. If we think simply about a student’s professional development as a process of finding a job, we overlook the real spaces in which help might be most desired. Frequently, those needs are the anxieties, stresses, and pressures of refashioning yourself as a professional. We should not be in the business of creating CV lines or providing lists of qualifications alone. We should focus on creating strong, well-adjusted professionals by developing ethical programs that guide them into the professional world by caring for them as people. In the graduate context, this involves helping students deal with the academic job market in particular.  To me in its best form, this means helping students to look at their academic futures and see proliferating possibilities instead of a narrow and uncertain route to a single job, to paraphrase the work of Katina Rogers. A sprinkler rather than a pipeline, in her metaphor. As Rogers’s work, in particular, has shown, recent graduate students increasingly feel that, while they experienced strong expectations that they would continue in the professoriate, they received inadequate preparation for the many different careers they might actually go on to have. The Praxis Program and the Praxis Network are good examples of how to position digital humanities education as answers to these issues. Fellowship opportunities like these must be robust enough that they can offer experiences and outcomes beyond the purely technical, so that a project manager from one fellowship year can graduate with an MA and go into industry in a similar role just as well-prepared as a PhD student aiming to be a developer might go on to something entirely different. And the people working these programs must be prepared for the messy labor of helping students to realize that these are satisfactory, laudable professional goals. It should be clear that this sort of personal and professional support is the work of more than just one person. One of the strengths of a digital humanities center embedded in a library like this one at UVA is that fellows have the readymade potential to brush up against a variety of career options that become revealed when peaking outside of their disciplinary silos: digital humanities developers and project manager positions, sure, but also metadata specialists, archivists, and more. I think this kind of cross-pollination should be encouraged: library faculty and staff have a lot to offer student fellows and vice versa. Developing these relationships brings the fellows further into the kinds of the work done in the library and introduces them to careers that, while they might require further study to obtain, could be real options. To my mind the best fellowship programs are those fully aware of their institutional context and those that both leverage and augment the resources around them as they are able. We have been working hard on this at W&amp;L. We are starting to institute a series of workshops led by the undergraduate fellows in consultation with the administrators of the fellowship program. The idea is that past fellows lead workshops for later cohorts on the technology they have learned, some of which we selectively open to the broader library faculty and staff. The process helps to solidify the student’s training – no better way to learn than to teach – but it also helps to expand the student community by retaining fellows as committed members. It also helps to fill out a student’s portfolio with a cv-ready line of teaching experience. This process also aims to build our own capacity within the library by distributing skills among a wider array of students, faculty, and staff. After all, student fellows and librarians have much they could learn from one another. I see the Head of Graduate Programs as facilitating such collaborations, as connecting the interested student with the engaged faculty/staff/librarian collaborator, inside their institution or beyond. But we must not forget that we are asking students and junior faculty to do risky things by developing these new interests, by spending time and energy on digital projects, let alone presenting and writing on them in professional contexts. The biggest risk is that we ask them to do so without supporting them adequately. All the technical training in the world means little if that work is illegible and irrelevant to your colleagues or committee.  In the words of Kathleen Fitzpatrick, we ask these students to “do the risky thing,” but we must “make sure that someone’s got their back.” I see the Head of Graduate Programs as the key in coordinating, fostering, and providing such care. Students and junior faculty need support – for technical implementation, sure – but they also need advocates – people who can vouch for the quality of their work and campaign on their behalf in the face of committees and faculty who might be otherwise unable to see the value of their work. Some of this can come from the library, from people able to put this work in the context of guidelines for the evaluation of digital scholarship. But some of this support and advocacy has to come from within their home departments. The question is really how to build up that support from the outside in. And that’s a long, slow process that occurs by making meaningful connections and through outreach programs. At W&amp;L, we have worked to develop an incentive grant program, where we incentivize faculty members who might be new to digital humanities or otherwise skeptical to experiment with incorporating a digital project into their course. The result is a slow burn – we get maybe one or two new faculty each term trying something out. That might seem small, but it’s something, particularly at a small liberal arts college. This kind of slow evangelizing is key in helping the work done by digital humanists to be legible to everyone. Students and junior faculty need advocates for their work in and out of the library and their home departments, and the person in this position is tasked with overseeing such outreach. So, to return to the opening motif, lists of skillsets certainly have their place as we bring new people into the ever-expanding field: they’re necessary. They reflect a philosophy and a vision, and they’re the basis of growing real initiatives. But it’s the job of the Head of Graduate Programs to make sure that we never lose sight of the people and relationships behind them. Foremost, then, I see the Head of Graduate Programs as someone who takes the lists, documents, and curricula that I have discussed and connects them to the people that serve them and that they are meant to speak to. This person is one who builds relationships, who navigates the prepositions of my title.  It’s the job of such a person to blast the boundary between “you’re in” and “you’re out” so that the tech-adverse or shy student can find a seat at the table. This is someone who makes sure that the work of the fellows is represented across institutions and in their own departments. This person makes sure the fellows are well positioned professionally. This person builds up people and embeds them to networks where they can flourish. Their job is never to forget what it’s like to be the person trying to learn. Their job is to hear “I’m not a tech person” and answer “not yet, but you could be! and I know just the people to help. Let’s learn together.”"},{"id":"2017-03-13-gothic-dh-at-washington-and-lee","title":"“Gothic DH” at Washington and Lee","author":"christian-howard","date":"2017-03-13 05:00:54 -0400","categories":["Digital Humanities","Experimental Humanities","Grad Student Research"],"url":"gothic-dh-at-washington-and-lee","layout":"post","content":"Cross-posted on the Washington and Lee DH Blog Toward the beginning of the semester, I was contacted by Brandon Walsh, a UVA alumnus and the current Mellon Digital Humanities Fellow at Washington and Lee. As part of his fellowship, Brandon has been pairing UVA DH folks with professors at Washington and Lee; the initiative is aimed at integrating new developments in technology with more traditional pedagogical methods practiced at a liberal arts university (read more about it here: http://augustafreepress.com/mellon-foundation-grants-washington-and-lee-funds-for-digital-humanities-study-of-histor/ ). Brandon paired me with Professor Taylor Walle, who is teaching Washington and Lee’s English 232: “Frantic and Sickly, Idle and Extravagant: The Gothic Novel, 1764-2002.” Now I’m no expert in the gothic novel, but after talking with Taylor, it became clear that our interests overlapped in unexpected yet productive ways. That is, one of Taylor’s aims in her course is to trouble the distinction between “highbrow” and “lowbrow” literature. A few of the questions that Taylor set to her students include: “Do you think the gothic deserves its long history of being associated with ‘popular’ (read: trash) fiction? Do you think the distinction between ‘highbrow’ and ‘lowbrow’ is legitimate or useful? How do we determine what counts as ‘high’ and what counts as ‘low’? Are these categories meaningful or arbitrary?” These questions resonated with my own research into what Nick Levey has termed “post-press” literature ( http://post45.research.yale.edu/2016/02/post-press-literature-self-published-authors-in-the-literary-field-3/ ) and other technologically experimental publication platforms, including Twitter and digital comics. Because such self-published literature is not approved by the “gatekeepers” of the publishing market, many people – especially authors who have successfully navigated the publishing market – have denigrated self-published literature. Award-winning author Laurie Gough, for instance, states: “I’d rather share a cabin on a Disney cruise with Donald Trump than self-publish. To get a book published in the traditional way, and for people to actually respect it and want to read it — you have to go through the gatekeepers of agents, publishers, editors, national and international reviewers. These gatekeepers are assessing whether or not your work is any good. …The problem with self-publishing is that it requires zero gatekeepers.” So we can see a current-day example of the tension between so-called “highbrow” and “lowbrow” literature in the form of traditionally-published vs. independent and experimental publication methods. I started looking for post-press and technologically-experimental literature that echoes gothic themes and characteristics. One work that Taylor and her class were discussing particularly caught my interest, namely, Oscar Wilde’s Picture of Dorian Gray . So I began my search for the “digital gothic” using this work as my focal point, and the results were intriguing. Not only has Dorian Gray been a favorite subject of Hollywood, but there have also been multiple graphic novels starring Dorian. Bluewater Comics even published an interactive, digital comic titled Dorian Gray that incorporates sound and movement. Screen shot of Dorian Gray the digital comic, created by Darren G. Davis, Scott Davis, and Federico De Luca. I continued brainstorming and gathering materials, until it was finally time to visit Taylor’s class on Wednesday, March 1, 2017. Despite the relatively early hour (8:30 am), Taylor’s class was enthusiastic and eager to participate. We reviewed the categories of highbrow and lowbrow literature before jumping into a discussion about Wilde’s Dorian Gray, which the students had just finished reading. Wilde’s descriptions and imagery gave us a pivoting point to examine other interpretations of Wilde’s work, particularly these graphic versions. But before we looked at these versions, I had the students watch part of Scott McCloud’s TED Talk, “The Visual Magic of Comics” ( https://www.ted.com/talks/scott_mccloud_on_comics ). McCloud clearly and persuasively lays out the visual resources available to comics, and he further discusses the future of comics in a digital age. Armed with the vocabulary outlined by McCloud, we examined first a (printed) graphic novel and then the digital comic version of Wilde’s novel. Our examination of the visual affordances of graphic novels and digital comics We looked at several other instances of contemporary gothic digital literature, including webcomics (particularly “Bongcheon-Dong Ghost”) and hypertext fiction (Shelley Jackson’s Patchwork Girl, a retelling of Mary Shelley’s Frankenstein featuring the creation of a female monster).   Students examine Shelley Jackson’s hypertext fiction, Patchwork Girl The students were particularly interested in the interactivity and changing role of the reader/user demanded by digital literature. One student observed that the physical reactions to print literature and interactive digital literature are different in that interactivity heightens the physical response, especially during moments of shock or horror (if you want to experience this yourself, check out the webcomic “Bongcheon-Dong Ghost” - http://comic.naver.com/webtoon/detail.nhn?titleId=350217&amp;no=31&amp;weekday=tue ). Another student speculated that it might be precisely because of these visceral reactions that such digital literature is relegated to “horror” or “popular” genres rather than being considered “serious” literature. After the class was over, Taylor expressed her enthusiasm for the material and medial considerations that this foray into the digital had on her class. I know I certainly enjoyed the lively and insightful conversation that emerged from this unlikely pairing of the gothic and the digital, and I am excited to think more about the developing and emerging genres inspired by these new literary forms!"},{"id":"2017-03-15-topic-modeling-twitter","title":"Topic Modeling Twitter","author":"alicia-caticha","date":"2017-03-15 10:10:30 -0400","categories":null,"url":"topic-modeling-twitter","layout":"post","content":"What is topic modeling? Topic modeling is type of statistical model that sorts through a large corpus of writing through language processing algorithms with the purpose of discovering the broad topics under discussion by grouping together words frequently used in tandem.  This method has been used in the past by scholars working on distant reading, a method of studying literature that aggregates and analyzes massive amounts of text with the goal of uncovering the fundamentals of literature on a vast, universal level (the opposite of close reading, which focuses on singular and often exceptional canonized texts).  How did we implement it? Using the program Mallet (Machine learning for language toolkit), I was able to import the text files of tweets and run the program.  The program resulted in two documents: a list of “topics” consisting of keywords, and meta data surrounding these topics.  This meta data included the percentage of tweets that focused one each “topic” and a list of the relevant tweets. What did we learn? Topic modeling is often assumed to be a primary text analysis tool for digital humanists. However, the utility of topic modeling is more ambiguous.  Where topic modeling may be an integral tools for studying an enormous body of law documents, for example, does it have the same utility for a corpus of tweets? Key “topics” revealed by the Kim Kardashian tweet corpus included clusters similar to the ones following:  “kardashian kollection sears dress spring” “excited line tomorrow launch coming sisters” “love guys wow million omg twitter followers congrats” “only fragrance buy free special people” “kendalljenner kyliejenner kylie sister kendall fun proud sis” “back nyc home missed time sleep” “shoes shoedazzle obsessed love” “keeping kardashians tonight season watch watching episode coast tune” “book kardashian today selfish beauty” “mom bruce dad cute kris khloe proud” “fashion love show week paris watching givenchy icon” “city selfie years sex double yeezus saint magic pablo” None of these “topics” are truly revealing.  We know Kim tweets about her family, about fashion, about her personal life (Kanye West frequently appears in multiple “topics”).  We know that twitter is a key place for the Kardashian to promote their show Keeping up with the Kardashians and this is blatantly clear in the multiple word clusters encouraging people to “tune in.”  The majority of the topics were simply variations of the aforementioned topics.  One notable exception was “people armenian power join stand april truth bro real genocide armo pray proud.” We had known that Kim tweeted about her Armenian heritage and that the Kardashian family has been vocal about the Armenian genocide.  What the topic modeling allowed me to do was find every tweet in which Kim mentioned anything relating to her Armenian heritage.  It further allowed me to place these tweets within broader categories.  Half of what I will term the Armenian tweets were also related to charity and activism tweets, using the same language and subject matter as anti-bullying campaigns.  The other half of Armenian tweets were of a social nature, discussing cooking Armenian food, going out to eat, and discussing notable Armenians. Everyone: Please call Speaker Pelosi TODAY at 202-225-0100 and URGE her to schedule a vote on H.Res.252, the Armenian Genocide Resolution — Kim Kardashian West (@KimKardashian) December 9, 2010 Conclusion While topic modeling did not offer much in terms of sorting through the corpus of Kardashian tweets at our disposal, it did allow me to collect and analyze one specific thematic topic.  After this preliminary step, however, the onus to continue to study these tweets and how they might inform our study of the Kardashians public use of their Armenian heritage is on us."},{"id":"2017-03-16-if-you-want-to-master-something-teach-it-digital-humanities-and-the-aha-moment","title":"If You Want to Master Something, Teach it: Digital Humanities and the ‘Aha!’ Moment","author":"nora-benedict","date":"2017-03-16 06:00:17 -0400","categories":["Digital Humanities","Grad Student Research","Technical Training"],"url":"if-you-want-to-master-something-teach-it-digital-humanities-and-the-aha-moment","layout":"post","content":"[Cross-posted to the Washington and Lee Digital Humanities Blog ] I was recently invited to give a guest lecture in Caleb Dance’s Classics Course (“Blasts from the Classical Past: In Consideration of the Ancient Canon”) at Washington and Lee University as part of their Mellon Foundation Grant for Digital Humanities, which provides support for research, workshops, guest lectures, and the general development of Digital Humanities initiatives. As a contemporary Latin American scholar, who works on Jorge Luis Borges and publishing history, I was, at first, quite daunted by the task of teaching about classical canons and antiquity. Lucky for me (and the students), I was asked to work through possible avenues of investigation for their final project, “Book Biographies,” that required students to add their own text to the class’s “canon” and justify their selection with quantitative and visual data. More specifically, Caleb asked me to present students with a series of approaches for the project, including any necessary platforms and databases, and also touch on some potential problems that might arise in the process. Before diving into a discussion of the digital tools for the day, I wanted to pause and parse out what exactly a “bibliographical biography” might be. Or rather, what students understood as “bibliographical” or a “bibliography” more generally. The entire class immediately defined the term to mean a list of works cited and used for research (i.e. enumerative bibliography). I then added to their definition by introducing the concepts of descriptive, analytical, and textual bibliography. We spent the most time walking through descriptive bibliography, or the physical descriptive of books as objects, because I felt that an understanding of this branch of bibliography would best serve students in thinking about the physical data necessary for their projects. I devoted the remainder of the class to presenting students with two avenues of investigation for their digital humanities projects. For fear of selecting a classical work that another student might have already chosen for his/her own project, I used Jorge Luis Borges’s Ficciones (1944) as my example text to add to their class “canon.” First, drawing on my own current digital project, I introduced students to different modes of visualizing data with digital tools. For starters, I asked students what types of questions they might ask their chosen books to gather the necessary data to populate a map or visualizing tool. Together, we formed a list that included the work’s printing history, cost, copies produced, places of publication, languages of publication, and circulation, which would all help students to answer the central question of why their book should be added to the class’s “canon.” Moreover, I continually emphasized the need to accumulate as much physical data as possible about their work, and keep this information in an easily readable format (such as an excel spreadsheet). Next, I showed them a demo project I created with an annotated Google map, which plotted the locations of archival materials, such as manuscripts and correspondence, in green and translations of Ficciones in different languages in yellow. As a class, we added new plot points to track the circulation of Ficciones in libraries in the United States with data we quickly acquired from WorldCat in purple: After we mapped out several locations and entered detailed metadata for each point, I wanted to show students several examples of more advanced data visualization projects. My hope was that as these students explored and experimented with their first digital humanities projects, they would be inspired to work with more complex platforms and programs for future projects. Given my own training in the UVA Scholars’ Lab, their unique program Neatline was the most logical place to turn. In particular, I walked the students through a demo of the project, “Mapping the Catalogue of Ships,” which relies on data gathered from the second book of the Iliad to map the physical route discussed based on the locations named, which seemed most appropriate for a Classics course: While platforms and programs for data visualization allowed the students to see the immediate impact of their selected text in terms of its production and circulation, I wanted to also push them to think about ways to represent the links, connections, and relations between certain authors and works across time and space. For starters, I asked students to think about how many works have been written about their selected texts (in terms of literary references, allusions, critical studies, or even parodies). I then showed them DBPEDIA, which extracts data and datasets from Wikipedia pages for further analysis. Looking to the page dedicated to Jorge Luis Borges, I scrolled to the section of writers that are influenced by him: Thinking about the various names on this list, and the potential writers that might populate the lists for their own selected writers, allowed students to see the possible outcomes of analyzing social networks of impact. I told students that this type of data was not limited to people and could be expanded to think about various historical, social, or even political movements.\nAfter discussing several possible ways to gather data about the social networks related to their own texts, I showed the students a few examples of how their data might visually manifest itself, drawing on sample screenshots from Cytoscape, a platform which helps create complex network visualizations: Walking through a few visual examples of network analysis with digital platforms got students really excited for their own projects and their potential outcomes. I then introduced students to Palladio, which is one specific tool engineered by Stanford Digital Humanities for their “Mapping the Republic of Letters” that they might consider using for their own projects. One of the most intriguing aspects of this tool is the ways in which you can manipulate your data. More specifically, as we saw with the sample dataset, you are able to visualize your information in the form of a map, a graph, a table, or through a photo gallery of the players involved: This variety in format was particularly promising for students that hoped to present their projects in diverse ways and draw on both visualization and social network tools. Even though we experienced some connectivity issues due to a campus-wide network outage, students were able to see the benefits of using digital humanities approaches for their own projects while also getting a feel for a few of these tools with hands-on tutorials. Moreover, instead of panicking about sites and videos that wouldn’t load for the students, I stepped back and saw these connection problems as a teaching moment. In particular, I embraced the slow internet speeds as a catalyst for reflecting on minimal computing and questions of access in certain parts of the world, such as Latin America. In turn, I encouraged the students to think critically about their projected audiences and how they hoped to not only present their ideas digitally, but also how they hoped to preserve them and make them accessible to a wide range of people. As a whole I am eternally grateful to Washington and Lee and Caleb Dance for this opportunity to share some of my favorite digital humanities tools, tips, and tricks with undergraduate students and introduce them to software and platforms that can make many of their imagined projects a reality. With each new tool we discussed, I was overjoyed to see students feverishly writing notes and having “Aha!” moments about their unique projects. Much of my DH fellowship year in the Scholars’ Lab has been about exploration and experimentation that tends to end in failure and a return to the drawing board, but, in the process, I’ve learned an incredible amount and had my own personal “Aha!” moments. Successfully being able to teach these students about data visualization and social network analysis was, quite possibly, the biggest “Aha!” moment of my DH fellowship thus far and a real turning point in my career as a digital humanities teacher-scholar."},{"id":"2017-03-17-fair-use-dh-and-the-kardashians","title":"Fair Use, DH, and the Kardashians","author":"joseph-thompson","date":"2017-03-17 04:21:46 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"fair-use-dh-and-the-kardashians","layout":"post","content":"This week, the Praxis cohort heard from Brandon Butler, UVA’s Director of Information Policy, who gave a fascinating talk on the evolution of copyright law and the meaning of intellectual property. He covered the U.S. Constitution’s intellectual property clause, the rolling boundaries of public domain, and the shift from monetary value to cultural value as a metric in the decision of Fair Use cases. Understanding Fair Use holds important implications for all academics, and the Praxis Program is no exception, especially given the subject of this year’s project: the Kardashians. Considering the Kardashians know a thing or two about how to make money off of selling their likenesses and names, it’s crucial that we understand Fair Use, as the Praxis cohort jumps into studying images and texts created by this family. After listening to Brandon, we learned some key tenets of Fair Use like articulating the scholarly purpose of your work that uses original copyrighted material, proving your work has transformed the original in some capacity, and/or showing that the reproduction of copyrighted material does not inhibit the creator’s ability to profit from that original work. He also hipped us to the Center for Media and Social Impact, which offers great guidelines of best practices regarding Fair Use and the reproduction of copyrighted materials. Over the past few months, we have collected Tweets, Twitpics, transcripts of Keeping Up with the Kardashians, US Weekly articles, fan fiction stories, and Instagram images to create our archive of the Kardashian media ecology. In an effort to maintain transparency and accessibility, we had kicked around the idea of making this archive accessible through the website we’re building. Now, we’re rethinking that goal, but forging ahead with a project that will rely heavily on copyrighted materials. But what’s really at stake in engaging with their copyrighted or trademarked material? Let’s take a quick look at a recent example of Kardashian jurisprudence. Last month, Kylie Jenner lost a legal battle with pop singer Kylie Minogue to trademark the name “Kylie.” For those who don’t know, the nineteen-year-old Jenner rose to fame as one of the stars of Keeping Up with the Kardashians, and is the youngest child of the Kardashian/Jenner matriarch, Kris Jenner. She’s been on a reality television show since she was nine. Who can blame her for thinking that she should trademark her name? And in the mode of the Kardashian family business model, Jenner wanted to turn her name into a licensed brand that she uses to sell her line of Kylie Cosmetics . But Minogue already owns www.kylie.com, and her millions of fans all over the globe know her simply as Kylie. According to a recent Billboard article about the case, Minogue has sold over 65 million albums in a career that dates back to 1988. Clearly, Minogue held something of a precedent regarding the use of the name. In other words, Jenner is influential and carries a certain amount of media power, but she’s no Kylie. Although copyright and trademark laws are certainly different animals, it’s safe to assume that someone who tries to trademark a name might be interested in a project that is digging into and trying to map their family’s media ecology - the very essence of their presence in the world. They are, after all, a family who makes a substantial living monetizing their likenesses and names via copyright and trademark, establishing themselves as the embodiment of celebrity power in the process. And if they are truly the current archetypes of “famous for being famous” based on curated images of themselves, then how do we think critically about the Kardashians without gathering these copyrighted and trademarked materials and, in some cases, reproducing them to further our analysis? Well, we don’t. So we will be reproducing some credited but copyrighted images on our site. Luckily, Fair Use goes before us."},{"id":"2017-03-24-congratulations-to-the-praxis-2016-2017-cohort","title":"Congratulations to the Praxis 2016-2017 Cohort","author":"alicia-caticha","date":"2017-03-24 09:33:14 -0400","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"congratulations-to-the-praxis-2016-2017-cohort","layout":"post","content":"On Tuesday, March 20th the Praxis 2016-2017 Cohort was awarded first place for their project “Dash-Amerikan: Keeping up with Kardashian Media Ecologies” at the 2017 Huskey Research Exhibition, hosted by the University of Virgina Graduate School of Arts and Sciences. They will be presenting their findings again in early May.  Until then, here is the abstract of their presentation to wet your appetite! Dash-Amerikan: Keeping Up with Kardashian Media Ecologies  In 2015, the season premiere of Keeping Up with the Kardashians was the most viewed Sunday cable program, out performing the series finale of the critically acclaimed television drama, Mad Men. Kim and company have never received the critical adulation that Don Draper elicited. Yet the Kardashians’ sheer popularity demands further inspection by scholars who claim interest in the cultural productions that reflect and shape our current historical moment. The Kardashians remain particularly important for their ability to normalize a matriarchal family structure and transform the traditionally private sphere of the home into their center of business, all while maintaining heteronormative assumptions about the objectification of women. Furthermore, their use of Twitter, Instagram, mobile apps, online blogs, and even tabloid coverage work together to engage fans in an unending advertisement for their show. Whether commenting on news events, promoting awareness of the Armenian genocide, or producing extravagant television specials, the Kardashian family has harnessed the 24/7 media landscape in new and unprecedented ways. The 2016-2017 Praxis Cohort examines the Kardashian media empire to reveal the discursive power of celebrities, fans, and their critics. To do so, we created a data set of every tweet written by a member of the Kardashian family, as well as every relevant US Weekly article, by using web scraping, text analysis, and topic modeling techniques. Using these digital humanities methods, we aim to interrogate constructions of race, class, and sexuality in contemporary popular culture and the pervasiveness of social media on our daily lives. Please join me in congratulating Jordan Buysse, Alicia Caticha, Alyssa Collins, Justin Greelee, Sarah Mceleney, and Joseph Thompson."},{"id":"2017-03-29-the-long-and-messy-history-of-privacy","title":"The Long and Messy History of Privacy","author":"shane-lin","date":"2017-03-29 04:14:27 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"the-long-and-messy-history-of-privacy","layout":"post","content":"[Cross-posted to the Washington and Lee Digital Humanities Blog . He came to W&amp;L to give a workshop through a Mellon-funded collaboration with the Scholars’ Lab . More information about this initiative can be found here .] I was invited by Brandon Walsh, the Mellon Digital Humanities Fellow at Washington and Lee and a former Scholars’ Lab Praxis fellow of my cohort, to come to W&amp;L as part of an ongoing collaboration between W&amp;L and the Scholars’ Lab. The program pairs UVA graduate students with W&amp;L undergraduate students studying a relevant subject and gives the former the opportunity to expose the latter to new modes of digital scholarship. I was fortunate to be matched with Brandon’s own English composition class: “Writing in the Age of Digital Surveillance”. My dissertation research is on the history of cryptography and the construction of digital privacy rights from the mid-1970s through the 1990s and involves a text analysis project examining influential privacy-related Usenet newsgroups and mailing list messages, so this seemed like a perfect match. My first instinct was to give a quick rehash of my work, an introduction to modern cryptography, and a few hands-on exercises in code-making and code-breaking. I could also reuse a demonstration I used from the last time I taught undergraduate students: how to anonymously buy cocaine on the darkweb with cryptocurrency. Yet none of these approaches seemed quite appropriate. Brandon had done a good job. The syllabus showed that the students had engaged very closely with the ramifications of modern technology on surveillance, drawing on thinkers like Siva Vaidhyanathan and Clay Shirky. My research, though focused narrowly on cryptography, interfaced with so many of the big ideas that the students had already broached. I didn’t want to bore them with historical detail (“scholarly rigor”!) or to just rehash the broader themes they were already well familiar with. Nor did I think that it was entirely appropriate to give a workshop on cryptography tools or on digital research techniques in a writing class. It didn’t seem very useful to throw prime factorization or Python web scraping libraries at these unsuspecting students for a single class. I was supposed to speak about DH and digital technology, but Brandon assured me that I had wide latitude to choose the topic. So I decided to hardly mention computers at all and talk instead about privacy in the context of the fundamental idea of history: things change. Privacy is not a fixed principle. The abstract notion of privacy rights is a very modern construction and even the practical, everyday conception of physical privacy has radically shifted through history, owing much to the affordances of technologies we may not think of as having much to do at all with privacy. I started by examining evidence of privacy in ancient Greece and how quantitative research on ruins has shown that prioritization of privacy was built into the architecture of Mediterranean homes. We discussed the rise of public bathing and its shifting practices and cultural significance under the Roman empire. And we spoke of the etymology of the word “eavesdrop” and its political connotations during the rein of Henry VIII. This was followed by a tour through the communications infrastructure of the early American republic and the role of the revolutionary and partisan presses and the post office in democratizing privacy by broadening access to both subversive ideas and the means to convey them. Finally, we discussed the dynamic legal conception of privacy, from the Fourth Amendment’s originally weak protections against searches to the landmark 1967  Katz v. United States decision that codified an expectation of privacy based solely in the realm of ideas. Digital technology was the end-point of this crooked journey. Though they have dramatically altered our understandings of privacy and the topography of power that supports such conceptions, the rights that these technologies challenged or championed were forged through centuries of history. Focusing on just the most recent debate wrongly implies that digital technologies are uniquely potent mechanisms and that the shifting landscape of privacy in our tremulous times represents a singular historical moment. I thought it was important to put our modern, contested notion of privacy in broader context, a context that includes the changes wrought by aqueducts, fire pits, chimneys, printing presses, bureaucratic organization, and other earlier technologies of decidedly analog mode."},{"id":"2017-04-04-working-at-scholars-lab-makerspace-pt-1","title":"Working at Scholars’ Lab Makerspace, pt. 1","author":"spyros-simotas","date":"2017-04-04 14:19:25 -0400","categories":["Makerspace"],"url":"working-at-scholars-lab-makerspace-pt-1","layout":"post","content":"Last fall, having no prior experience in 3D printing, I joined the team of Makerspace technologists. Maybe that’s something nobody’s supposed to know, but it’s true. I began assisting at the Makerspace with only Shane ’s super-comprehensive presentation on 3D printing, which is the activity that attracts the most students to the SLab. Shane’s knowledge and competency, both historical and technical, had me wondering what I had gotten myself into. But as he kept speaking, using words I had never heard before, a whole new dimension had started to rise. Words like slicer, .stl file and GCode, layer height and infill, PLA and ABS, extruder, nozzle size, and bed leveling, had started to layer up in the inevitable Z axis! Luckily, the first day on the job was painless. No incidents. No mishaps. No failures. No adhesion or warping problems. No blocked nozzles requiring cold pulling. As a matter of fact, given the uncomplicated nature of my first print, there was no reason for things to go wrong. The command was for a small and uncomplicated cylinder tube. I sliced the .stl file with Cura, then loaded the GCode to the Ultimaker2 and I had basically done my job! It was now up to the machine to translate the GCode into movements in the X, Y, and Z axes by pouring hot plastic in the process. Indeed, after the nozzle was hot enough (over 200ºC) to let the filament come through in liquid form, a motor started to push it into the extruder as it started piling up circle after circle every .2mm. The process may sound tedious, but it is also mesmerizing. There is something exciting and soothing at the same time, to let yourself get lost in the paths of the printer’s head. And its music! So, that first print didn’t last long, as most prints usually do. The actual size of the cylinder tube was a few centimeters long. In less than 30 minutes, the tube was erected on the heated bed, ready to be peeled off. In less than 30 minutes, a small physical object had come into being and was now departing in the hands of my first happy patron. Since that first print, I can’t say the same seamless process reflects Makerspace’s reality. Most commonly, success comes as the result of many aborted attempts, tweaking, and recommendations from more experienced makers. Due to all kinds of unexpected technical failures, with adhesion probably being the #1, I have learned to tame my enthusiasm until the final layer is applied, and to use glue! More importantly, I have learned to observe more experienced makers and absorb their know-how and admirable problem-solving skills. Certainly, 3D printing is not rocket science (although it helps you build rockets, right Duy ?), but it can be quite frustrating if a significant number of parameters don’t line up. There is an artisanal quality to 3D printing that I didn’t expect existed. The attitude is to preserve the right amount of detachment from the final results and keep experimenting. And in your darkest hour of troubleshooting, it is useful to put things in perspective through poetry: “ Hope the voyage is a long one .” For that reason, Makerspace offers exposure, good mentorship, an amazingly friendly and playful learning environment, as long as unlimited stocks of filament!"},{"id":"2017-04-12-are-you-our-new-senior-developer","title":"Are You Our New Senior Developer?","author":"alison-booth","date":"2017-04-12 10:55:03 -0400","categories":["Announcements","Job Announcements"],"url":"are-you-our-new-senior-developer","layout":"post","content":"Note: Applications for the Senior Developer role have closed. Welcome to new Scholars’ Lab Senior Developer Shane Lin ! We’re very excited that Managing Director Amanda Visconti and Head of Graduate Programs Brandon Walsh will be on board by April 24. And we are rebuilding our research team at a time when DH@UVA and a new Certificate in DH will be coming into clearer focus! Please spread the word of this newest job posting for a Senior Developer at the Scholars’ Lab . ( In case that link doesn’t work, you can visit  hr.virginia.edu, click “Jobs”, and search for staff posting 0620730 ). A description of the role follows: The University of Virginia Library is the hub of a lively and growing community of practice in technology and the humanities, arts, and social sciences. As part of that community, the Scholars’ Lab has risen to international pre-eminence as a library-based center for digital humanities. The Scholars’ Lab collaborates with faculty, librarians, and students on a range of projects and tools, including spatial humanities, data visualization, text analysis, digital archiving, 3D modeling, and experimental humanities. Our Praxis Program and Digital Humanities Fellowships are both models for graduate education in Digital Humanities. Further, the Library and the Scholars’ Lab are committed to diversity and safe spaces, and we particularly focus our speaker series and practice on accessibility and social justice in all senses. We welcome curious, critical, and compassionate professionals with integrity and a strong work ethic, and who possess a keen and deep understanding of what it takes to continuously improve and maintain research projects within a major academic research library. We particularly welcome applications from women, people of color, LGBTQ, and others who are traditionally underrepresented among software developers. Responsibilities: The Senior Developer will consult with faculty and students to advance research projects and training; evaluate scholarly needs and define project goals for research projects; provide input on appropriate deliverables and reasonable schedules for completion; write, test, and debug original software code for applications that enable scholars and library users to collect, manage, produce, manipulate, or analyze digital information resources. The Senior Developer will modify existing applications to improve their functioning or achieve broader and more effective use and engage with new technologies to help researchers find their use and interest for research. Qualifications: Required: Graduate degree or equivalent experience Up to 4 years of experience with software development, application development, or systems administration Experience with relational database systems, including Postgresql and MySQL Experience with a number of programming languages, including PHP, Ruby, Python, Java, SQL, JavaScript, shell Experience with version control, including Git and Subversion Systems Ability to scope and implement software in diverse environments Ability to communicate effectively with researchers and fellow developers Ability to encourage and develop a community of users and developers. Preferred: Graduate degree or equivalent experience in the humanities or social sciences Experience with data collections, analysis, visualization, and interpretation Experience with a variety of text analysis or image analysis methods and tools Familiarity with a variety of application frameworks, including Rails, Django, and Zend Experience with TEI, XML, Solr, Cocoon, Tomcat Applications will be accepted until the position is filled. A CV/resume, cover letter, and contact information for three references are required pieces of your application. Email a Friend: jobs.virginia.edu/applicants/Central?quickFind=81600"},{"id":"2017-04-12-raspberry-pi-on-uva-wifi-network","title":"Raspberry Pi on UVa WiFi Network","author":"ammon-shepherd","date":"2017-04-12 08:59:24 -0400","categories":["Makerspace","Technical Training"],"url":"raspberry-pi-on-uva-wifi-network","layout":"post","content":"The easiest way to get your Raspberry Pi connected to the Internet at the University of Virginia is to use an Ethernet cable. If you want to use wireless, pretty much the only option is to use the hidden “wahoo” network. This is a quick tutorial for getting your Raspberry Pi to connect to the “wahoo” WiFi network at the University of Virginia. I’m starting with a clean install of Raspbian OS (follow the link for a great tutorial on installing Raspbian OS). Step 1: Change Password The very first thing you should always do with clean install of Raspbian OS is change the default password. Bad people on the Internet are looking for Raspberry Pis with default passwords so that they can hack in and take over. It’s just always best practice, and common-computer-security-sense to secure your computer with a password only you know. Never settle for the default! :) To change the password, you can use the Terminal. Open that up and type “passwd”, enter the current password “raspberry” (you won’t see any text typed), then hit enter. Now type in a new password (again, you will see no text), hit enter, and type it a second time. You can also change the password using the graphical interface (as shown in the next steps) on the “System” tab. Step 2: Change Localization Settings There are two ways to change these settings: 1. using the graphical interface, or 2. the command line. Pick one option and go with it. 2.1. Graphical Go to the Raspberry Pi menu (the raspberry logo in the top left corner), then to Preferences, then to Raspberry Pi Configuration. Under the Localization tab, you’ll want to change the Locale, Timezone, Keyboard and WiFi Country settings. (As a side note, you can also change the password under the “System” tab.) For our purposes, you need to set the WiFi to “US”. Don’t restart the system, yet. We have more to do. 2.2. Command Line In the terminal, type “sudo raspi-config”. The screen will show a textual display of options. Select Localization Options, and from there change the Locale, Timezone and WiFi Country settings. (Note, you can also change the password using this tool.) Don’t restart the system yet. We have more to do. Step 3: Edit System Config The newest Raspbian OS (Stretch) changed the way the network is handled, so pick the “Stretch” or “Pre-Stretch” (like Jessie, and before) options as necessary. 3.1. Stretch Change the /etc/wpa_supplicant/wpa_supplicant.conf file. You’ll need to open the file with root privileges, so we’ll use the terminal to open a program for editing the file. In the Terminal, type in: sudo leafpad /etc/wpa_supplicant/wpa_supplicant.conf There should only be three lines in there now. We’ll add some lines to let the network system know about the hidden ESSID. The file should now look like this: ```\ncountry=US\nctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev\nupdate_config=1 network={\n ssid=”wahoo”\n scan_ssid=1\n key_mgmt=NONE\n}\n``` The code is pretty self explanatory. The “ssid” is the hidden wahoo.\nThe “scan_ssid” does a direct Probe Request which is required when a network hides the ssid.\nThe “key_mgmt” line lets the network know that there is no password required. UVA’s wahoo network is secured by only allowing a registered MAC address onto the network. You’ll be doing this in Step 4. Save and close that file and you’re just about done. Continue on to Step 4. 3.2. Pre-Stretch Change the /etc/network/interfaces file. You’ll need to open the file with root privileges, so we’ll use the terminal to open a program for editing the file. In the terminal, type in: sudo leafpad /etc/network/interfaces Towards the bottom of the file are two sections for a “wlan0” and “wlan1”. Delete those lines, and in place of them add in three new lines for “wlan0” (indent the third line with four spaces or a tab). auto wlan0\niface wlan0 inet dhcp\n    wireless-essid wahoo What this code does: auto wlan0 = this will automatically turn on the wireless card iface wlan0 inet dhcp = this declares an interface card called wlan0 allowed to connect to the TCP/IP network (the Internet) using DHCP (basically, the router will give your Pi an IP address). wireless-essid wahoo = look for the wireless Extended Service Set Identification named wahoo. The entire file should look like this: ```\n# interfaces(5) file used by ifup(8) and ifdown(8) Please note that this file is written to be used with dhcpcd # For static IP, consult /etc/dhcpcd.conf and ‘man dhcpcd.conf’ Include files from /etc/network/interfaces.d: source-directory /etc/network/interfaces.d auto lo\niface lo inet loopback iface eth0 inet manual auto wlan0\niface wlan0 inet dhcp\n  wireless-essid wahoo\n``` Step 4: Get WiFi MAC Address While you have the Raspberry Pi on and the terminal open. Grab the MAC address for the wireless card. In the terminal type ifconfig Now look for the section for the “wlan0” and on the first line you’ll see information for a “HWaddr” or Hardware Address. It will be in the format aa:11:bb:22:cc:33 The HWaddr is the MAC address. Write the MAC address down on a piece of paper, you’ll need for the next step. Register the MAC Address You will need a computer or mobile device that already has an Internet connection in order to complete this step. In order to get your Raspberry Pi connected to the ‘wahoo’ network, you will first need to register the MAC address with ITS. Follow this link to the ITS Registration page:  https://netreg.itc.virginia.edu/cgi-bin/mac_registration.cgi?alien=1 ITS suggests getting a static IP on the more secure network. You can make that request through ITS help desk . If on the more secure network, your computer must be on that network as well if you want to SSH into the Raspberry Pi. You can connect to that network through the ITS provided  CISCO VPN client . Restart the Pi Now you can restart the Raspberry Pi. When it boots up, it will automatically detect and connect to the “wahoo” network."},{"id":"2017-04-17-endangered-data-week","title":"Endangered Data Week","author":"alison-booth","date":"2017-04-17 06:22:37 -0400","categories":null,"url":"endangered-data-week","layout":"post","content":"A range of Scholars’ Lab and Praxis folks are collaborating with Research Data Services and others in the Library for a series of workshops under the umbrella, Endangered Data, this week. Outline of events: Introduction to Libra Data (Dataverse at UVa) – Monday, April 17th 11am-noon, in Brown Library, Room 133 . Introduction to Git/Github – Tuesday, April 18th, noon-1:30pm, in Brown Library, Room 133 . Introduction to DocNow – Tuesday, April 18th, 2pm-4pm, in Alderman Library, Room 421 . Web Scraping with R – Wednesday, April 19th, 10:30am-noon, in Brown Library, Room 133 . Preserving Artifact and Architecture with Cultural Heritage Informatics – Friday, April 21st, 10:30am-11:30am, in Clemons 3rd Floor VizLounge . Endangered Data Week webinar – Friday, April 21st, 1pm-2:30pm, in Brown Library, Room 133 . Within the UVa Library, these events are being hosted by Research Data Services and the Scholars’ Lab . Kudos to Jeremy, Will, and Arin, and our colleagues in Brown Library, as well as the DLF Webinar. Meanwhile Ronda is at Texas A&amp;M, sharing her expertise in Neatline Training!"},{"id":"2017-05-03-announcing-2017-2018-fellows","title":"Announcing 2017-2018 Fellows!","author":"brandon-walsh","date":"2017-05-03 06:11:38 -0400","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"announcing-2017-2018-fellows","layout":"post","content":"We are thrilled to announce the 2017-2018 Scholar’s Lab fellows for the  Praxis Program, the new Digital Humanities Project Incubator Fellowship, and the  Graduate Fellowship in the Digital Humanities . We are welcoming 14 fellows from 6 disciplines from the  arts, humanities, and social sciences . Our graduate fellows are joining a robust and vibrant community of  past fellows ! Praxis Program We are delighted to welcome 6 diverse disciplinary team members to the 7th year of the Praxis Program: Monica Blair (History) Ankita Chakrabarti (English) Victoria Clark (Music, Critical and Comparative Studies) Tanner Greene (Music, Critical and Comparative Studies) Christian Howard (English) Spyros Simotas (French) Look forward to more details about the Praxis Program’s new project in the fall! Digital Humanities Project Incubator Fellowship This year we will be piloting a set of smaller, short-term fellowships dedicated to boosting nascent digital humanities research in a flexible, collaborative environment. James Ascher and Sarah Berkowitz (English, Summer 2017) Benjamin Gorham (Art and Architectural History, Fall 2017) Ryan Maguire (Music, Composition and Computer Technologies; Fall 2017) Joseph Thompson (History, Fall 2017) These students will work with our R&amp;D team for a single semester to rapidly prototype their project ideas and position themselves for future work in digital humanities. Look for more from them this summer and fall! Graduate Fellows in the Digital Humanities Finally, we are looking forward to working with Julia Haines and Ethan Reed, our 2017-2018 Graduate Fellows in the Digital Humanities. Julia Haines’ (Anthropology) dissertation is titled,  Archaeology at 19th-century Bras d’Eau, Mauritius: Intimate Spaces and Industrial Landscapes of Indentured Laborers . Ethan Reed’s (English) dissertation is titled,  Forms of Frustration: Unrest and Unfulfillment in American Literature after 1934. These fellows will work with our team throughout the year and over the summer on substantial research projects related to their dissertations. We are looking forward to working with all these fantastic students in the coming year!"},{"id":"2017-05-04-1st-annual-makerspace-hackontest","title":"1st Annual Makerspace Hackontest","author":"ammon-shepherd","date":"2017-05-04 06:25:22 -0400","categories":["Makerspace"],"url":"1st-annual-makerspace-hackontest","layout":"post","content":"The Scholars’ Lab staff and Makerspace Technologists completed the first ever annual/biannual/semesterly SLab Makerspace Hackontest. The brain-child of Shane Lin, this contest, in short, was to get SLab staff and Makerspace techs together to create something that would keep an egg from cracking; a spin on the traditional egg drop competitions you may have had in middle or high school. What’s a Hackontest? A hackontest is a contest where you hack together something. (clever, right?) The Rules We (I?) cordially invite you to participate in the 1st biannual (or possibly biennial) Makerspace Employee Hackathon (MEH)! This event is open to all Scholars’ Lab makerspace technologists and full-time employees. As an arguably fun activity, each of you is challenged to complete a design and production challenge using tools and materials found in the makerspace. For this inaugural event, the challenge is a spin on that staple of American primary education science classes: the egg drop. Design a mechanism to allow an American-standard Large size unboiled egg to survive two sequential drops from 2 meters onto a hard floor without cracking the egg. Our twist on this project is that in between the drops, the entire design must bear the compressive force of 1 metric Shane mass at Earth gravity applied sequentially on three perpendicular axes at the longest point (i.e. I must be able to stand on it). If multiple designs allow an egg to survive both drops, the team with the design with the lowest pre-drop weight (in case fuel or reaction mass is expended in the course of the drop) will win the Grand Technical Prize and be temporarily presented with the prestigious SLabby Trophy. After we perform the drops, participants will also vote on a Special Ingenuity Award for designs that incorporate exceptional trickery or superior aesthetics. Teams will be announced by  Monday 3/6  (so please fill out the form by the Sunday before) and have until Monday 3/27 to submit their design and model (we’ll figure out the best time to perform the drop after we assign teams). Additional rules: A. Each participant will be assigned by the organizers to a team to be decided in accordance with our secret agenda (chiefly, fraternization). B. Use of personal computers are allowable, as is any freely-available and Lab-provided software, including 3D designs, but any part that touches the egg must be an original design. C. You may incorporate any materials found in and of the Makerspace in your designs, with the following exceptions: No paper and paper-like materials, including cardboard and card stock. No liquid adhesives or hot glue (glue sticks are okay) No adhesive tape No bubble wrap In the case of any questions or questionable edge-cases, please send me (Shane: ssl2ab@virginia.edu ) an email and I will arbitrate. As a rule of thumb, I really like unorthodox ideas. Good luck! The Judge Shane was the brains and the judge behind the contest. The Competition Three entries made it to the competition: Team Bomb! Team Urchin ![](http://static.scholarslab.org/wp-content/uploads/2017/05/IMG_20170503_154617.jpg) Shane was just helping out. Team Urchin was Spyros and Duy Team DragonSpawn ![](http://static.scholarslab.org/wp-content/uploads/2017/05/IMG_20170503_154024.jpg) Team DragonSpawn: Lauren and Ammon The Winner is… Only one competitor survived all the tests: drop from 2 meters, the full weight of one metric Shane unit on three axis, and a second drop from 2 meters… Team DragonSpawn! Congratulations, you get the first SLabby award!"},{"id":"2017-05-22-introductions-meet-charm-and-wit-or-wit-and-charm","title":"Introductions: Meet Charm and Wit, or Wit and Charm","author":"sarah-berkowitz","date":"2017-05-22 07:17:50 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"introductions-meet-charm-and-wit-or-wit-and-charm","layout":"post","content":"Sarah E. Berkowitz I will be working with James Ascher this summer on a Scholar’s Lab Digital Humanities Project Incubator Fellowship. Our project consists of creating a digital edition of “Characters” from Samuel Butler’s posthumous (1759). James and I are both rising fifth year Ph.D. candidates in the English Department, and we both study the eighteenth century, but there the similarities end. I study primarily novels, and I’m primarily interested in post-structuralist literary criticism. My dissertation focuses on male characters in the late eighteenth-century novel, and I’m wildly interested in Butler’s text because of what I think it can tell us about characters outside of novels in the eighteenth century. Oh, and did I mention? I have absolutely no digital humanities experience. Suffice it to say James’s interests and experiences are very different from mine, but I’ll let him introduce himself. Because we have an Incubator fellowship, our goal is to create a fast and furious prototype by the end of the summer. My job is primarily to provide a critical framework for thinking about character. In the dream version of this project, I would be writing the footnotes that explain and contextualize Butler’s individual characters. But, I still need to learn some basic computer skills and, and because of the truncated time frame I have to learn them fast! I’ve start using a free website   to teach myself basic command line, and have started to go through the built-in Emacs tutorial . Next on the list is learning Git . The hope is that I can master enough basic skills by Memorial Day that I can meaningfully contribute to our conversations about design. This seems like a tall order right now, but I’m working on it a little every day. Then the real fun will begin. James P. Ascher Sarah Berkowitz has kindly provided a gracious introduction to our project and our collaboration, but I’d be remiss if I didn’t write of my reciprocal enthusiasm.  Characteristic of our collaboration, we debated if we were in fact different in our interest in literary criticism—we feel different, but are we?  As we talk, the names we engage with are different: David Brewer, Deidre Lynch, and Catherine Gallagher provide mortar the bricks of Sarah’s thinking and G. T. Tanselle, David Foxon, and James Raven are smeared like honey on goblet of medicine that I try to present the world.  Of course, I think that I’m critiquing literature—as the eighteenth century would have defined it: written language presented to the public in printed form.  Of course, Sarah is right that the burden of understanding how those marks on paper came to be is so large that I seldom get a chance to talk about phenomena like character;–it’s too far to induce from printing history to ideology.  Hence, my excitement in this project.  I feel as though we’re reaching across a gap, trying to find a bridge between the physical facts of items and the theoretical preoccupations of post-structuralist literary criticism.  I think of William Whewell’s inductive tables as a sort of emblem for us ! Whewell proposes the table as a way of visualizing the method behind the slow progress of induction building on the ideas of the past—two separate explanations become one and then that one is combined with another explanation to make an even grander theory.  Repeat forever, as first causes—the ultimate end of such a chain of induction, are unreachable.  In this way, he imagines enacting Francis Bacon’s great instauration: renewing the accumulation of private natural-history knowledge as theories that explain the world.  You collect data, find patterns, write a theory, and make a theory out of those theories.  I think this accurately describes what both of us are doing, but on different parts of the chart that would represent literature.  Sarah builds on the long history of the study of character; I build on the history of printed forms of meaning.  What this project can helpfully show us is the bracket that would correspond to the induced theory that contains both of these chains of induction.  Surely, we wouldn’t bother reading printed works if it weren’t for characters?  Surely, we couldn’t read about as many characters if it weren’t for printed works?  Okay, that means they relate, but how?"},{"id":"2017-05-24-accessibility-online-take-aways-from-the-luis-perez-workshop","title":"Accessibility Online--Take aways from the Luis Perez workshop","author":"sarah-berkowitz","date":"2017-05-24 05:14:49 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"accessibility-online-take-aways-from-the-luis-perez-workshop","layout":"post","content":"One of our aims for our summer project is to build a product that is accessible for all users, but I realized today that James and I have a different idea of what it means for digital humanities to be “accessible.” When I think of making a digital product accessible, I imagine a home bound elderly user who still has an AOL email account. For these users, something that is straightforward and unintimidating is crucial. I think about stressing plain language and a simple interface. When James thinks about accessibility, he imagines a self-taught Linux user in her basement who has no access to software through institutions. For James, open source products and a process that leaves a digital blueprint so that it can be easily recreated by users outside the academy is essential. I’ve been thinking about accessibility because I recently attended a workshop sponsored by The Center for Teaching Excellence and the Office of Accessibility that brought Dr. Luis Perez on grounds to discuss technology and academic accessibility. The workshop stressed the benefits of technology in the classroom, and particularly emphasized the concept of “Universal Design,” the idea that technology should be designed with all users in mind, not just able bodied ones. Dr. Perez’s point was that technology that was helpful for some was necessary for others. For instance, the speech to text feature on a cellphone may be useful for a sighted user, but necessary for a visually impaired one. The academic accessibility workshop got me thinking about how we might apply Universal Design to our summer project. Print can be especially disabling for people with visual impairments and processing disorders. How, then, can we make a mostly text and image based project accessible for all users? The workshop introduced the acronym “SLIDE” to think about designing accessible projects. You can find a more full explanation at Dr. Perez’s website here, but I think they bear repeating. SLIDES stand for Styles, Links, Images, Design and Empathy. Styles refers to the structure of a website. This model stresses consistent uses of headings and subheadings to make websites easier to navigate with text to voice software. Without consistent headings and subheadings, a reading app would have to read an entire page before getting the user to the relevant parts. Links means that links should never be “naked,” that is, should never be presented without a descriptive shortcut. This prevents a voice to text program from reading out the entire (sometimes long and unwieldy) url, and provides succinct information that can be delivered verbally. Images asks that all images have an alternate text description, so visually impaired users can have their computers describe the images rather than missing out on them altogether. Design implies that design of a given webpage should be consistent. “If your web design is consistent,” Dr. Perez said, “it’s like I can see it a little better every time I use it.” If it’s idiosyncratic, it can be hard to get a feel for the page. And lastly, Empathy refers to the idea that websites are designed with all users in mind. Everyone will be disabled eventually, empathy asks that you design a project for the user you will one day become rather than the user you necessarily are. I want to keep these principles in mind as we move forward building our edition. We’re planning to use a lot of visual information, and I would like to make sure our site is accessible to all users. I don’t know if this will possible given the time frame allotted, but I think these are good guidelines to keep in mind!"},{"id":"2017-05-25-visualizing-paper-evidence-using-digital-reproductions","title":"Visualizing Paper Evidence Using Digital Reproductions","author":"james-p-ascher","date":"2017-05-25 04:41:48 -0400","categories":["Digital Humanities","Experimental Humanities","Grad Student Research","Research and Development","Visualization and Data Mining"],"url":"visualizing-paper-evidence-using-digital-reproductions","layout":"post","content":"Digital images both lie to us and tell us truths that exist outside of our normal perception. The lie comes about through both deliberate distortions and distortions produced by limitations in digital and in other reproduction methods. The limitations of reproductions are easy to see for anyone who considers the situation carefully, but understanding the problems that they create in our understanding of the past requires some study. The most coherent summary of the problems of the late 1980s and the problems with reproductions in general was written by G. T. Tanselle in 1989. In this essay, “Reproductions and Scholarship”, Tanselle reminds us that “everyone knows (though many people act as if they do not know) that every form of reproduction can lie, by providing a range of possibilities for interpretation that is different from the one offered by the original.” 1 These reproductions can be useful supplements to the originals, but serious research ought to be checked against the originals to determine where the reproduction may mislead. He summarizes the obligation of those who study the verbal texts of the past, The words that come to us from the past, transmitted by paper and ink, cannot be assumed to reflect accurately what their authors intended; in order to assess how the words that are present in documents came to be there, and indeed to try to make sure that we know what words are in fact there, we must avail ourselves of all the evidence that comes with them. 2 While photocopies, photographs, and microphotographs can place texts before us that we cannot always get to ourselves, they are all several steps removed from the original and each step provides opportunities to alter—intentionally and not—the final product of the reproduction process. Taking up this line of thinking, Bonnie Mak recently wrote about the problematic assumptions in doing literary research from the digitized versions of high-contrast microfilms, created under war-time expediencies.3 Her “Archaeology of digitization” describes the context in which the familiar database of Early English Books Online was produced and how it has come to stand in for access to the original. Echoing Tanselle’s objection, she notes that the expediencies of war-time microfilming and batch digitization lead to unpredictable, but often interpretable, errors that themselves are worthy of study. Rather than bemoaning the poor state of—what is for many students and scholars distant from major libraries—the “archive of record” for English books, she claims that we can study the errors in these reproductions to understand a historical moment that sees verbal texts in a certain way. Digitization, microfilming, and OCR fail us, but we can use “paratextual” clues to determine how a particular digital reproduction came to be and so compensate for actual and potential errors, albeit incompletely. Both of these arguments anticipate specialized tools that reproduce information that would not normally be part of the experience of looking at a book, which is right, but something that I think deserves further investigation. While a high-contrast microphotograph may obscure faint ink, blue pencil, or small dots, it also shows us something that we would not normally be attentive to. Consider this photograph of page 382 from a copy of Samuel Butler’s Genuine Remains (ViU PR3338 .A17 1759 c.2 v.2),      P. 382 Our eyes immediately read the heading “A Knave” and, if you are literately minded, you—perhaps—start reading the text, wondering what a knave might be. But, let me point out the black background, the brown leather of the cover on the left, the transparent fingers holding the page down, and the show-through of the ink from the other side of the leaf in the image. If we thought that those things did not matter, we might have a good reason to alter the digital image by tweaking the colors so that the colors that do not interest us are not visible. But first, we would need to identify which colors interest us. One way of visualizing the range of colors in an image is whats called a color histogram, Histogram for p. 382 The peaks to the left are the dark colors of the background. The peaks to the right are the brown colors of the paper. In between those peaks, the smaller peaks represent ink, discolored paper, the restraining fingers, and other features. If we alter the image by emphasizing those colors that interest us, we destroy the paratextual evidence valued by Mak and obscure the evidence noted by Tanselle, but emphasize what interests us. If we take on the obligation of returning to the original to check our assumptions, this alteration can focus to our investigation on the facts that interest us. For example, Ink emphasized stretching the colors in the middle of the histogram to cover the whole range of reproducible colors eliminates information in the outer ends of the histogram, which turns out to be mostly show-through and portions of the digitization apparatus, other pages, and the binding. Quantitatively, the first image has 10,778 unique colors and the stretched image that emphasizes ink has 26,344. This means, as both Mak and Tanselle have rightly pointed out, that we have fabricated evidence by altering the image. The second image has over twice as many colors which do not occur in the original and it would be wrong to suggest that we have somehow discovered new information with the second image. In fact, we have destroyed evidence to render a digital image more useful for a particular line of investigation: examining only the fully inked bits of the page. A careful observer may also note that the image looks a bit Fauvist with artificial colors. Were I to believe that I was improving the image by altering its color range, I would have to “correct” that error by tweaking the colors further, but as this image is an artificial visualization the unnatural coloring helps to remind me and anyone else who sees this image that it is a deliberate alteration of what one would see if they looked at the book. The stumbling block in understanding the limitations of visualizations like this one is that they foreground what most people look at when they look at a book: the letters that have been deliberately and sufficiently inked and impressed on the page. If we consider a physical codex as a noisy container for works of art made of text, one can imagine—à la Claude Shannon —that this alteration restores the original text by eliminating noise. The problem with this approach is that it eliminates the opportunity to apply our judgment to the full range of evidence available, even in a limited reproduction. A book, and even the digital image of a page of a book, contains far more information than just the sequence of letter-forms that appear sufficiently on its surface. Consider the digitization process itself and what it can capture. 8-Bit Per Channel Versus 16-Bit Per Channel Human vision can normally distinguish a bit more than sixteen-million colors, so modern digital images provide a color range that encompasses that in three channels—typically, red, green, and blue. Each pixel—that is, evenly-colored dot—in the image has three numbers associated with it that correspond with the amount of red light, green light, and blue light that must be mixed together to produce the right color on an illuminated screen. (There are many variations of this in different color spaces, CMYK is more useful for printing, Munsell is better for colorimetry; but this particular explanation is sufficient for this post.) A quick bit of math shows that eight bits gives 256 intensities to each channel (red, blue, or green) and that these three channels together give 256 times 256 times 256 colors, that is 16,777,216 different colors. Thus, it is simply a matter of calibration and choosing the right range of intensities for an eight-bit per channel image to reproduce the entire range of colors that can be seen. However, image detectors can distinguish more than this and digital photographers can use this extended range to mimic the large range of colors that can be perceived in the world. Let me explain how we can perceive more colors than we can see. When we look at a dark alley in the sunlight, our eyes adjust the intensity of the light as our gaze passes over the scene. When we look at the clouds around the sun, our irises constrict, reducing the amount of light that enters our eye so that we can see clouds as white on blue backgrounds rather than as just bright white light. When we stare into the alleyway, our irises expand to take in more light, so that we can see the red bricks on the side rather than just a dark alley. Normally, our eyes move over a scene quickly and take in different intensities of light and our brain combines them together into one image in our mind. Based on contextual clues, we can perceive a wider range of colors than our limited sixteen-million-color vision can see because of how our eyes adjust and our intuition about how light functions in the world. (Josef Albers explains how this visual intuition is not truly a deeper knowledge about color, but an intuition that can be manipulated into seeing colors that are not really there in his pioneering Interaction of Color 3 ) Digital detectors, as of yet, do not operate quite in this way, but can mimic the process. One way is to take several digital photographs at different brightness levels and stitch them together. In our example, we could take a too-bright photograph to capture the red brick of the alley and a too-dark photograph to capture the white and blue of the clouds. Photographers call this high-dynamic-range imaging, and it essentially extends the detection of color beyond the sixteen-million limit. As this method has existed for awhile, detectors have been built that can detect colors outside of the normal range to save the trouble of taking multiple exposures. (Thanks to Shane Lin for helping me to understand this.) The commercial purpose of these detectors is to help in high-dynamic-range photography, but the result is that they can detect sixteen-bits of information per red, blue, and green channel (or equivalent) giving 65,563 colors per channel and giving trillions (281,823,012,408,547) of possible colors. One way of considering how this extended range might improve digital reproductions, is that sixteen-million times more evidence captured. If the color of a page means something—and it certainly does since color is the way we distinguish the letter-forms that make up the text—then we have much, much more information in a sixteen-bit per channel image. The Digital Production Team at the University of Virginia graciously provided these higher-bit-per-channel images. (A special thanks belong to Christina Deane who approved what must have seemed like a plan hatched by the Mad Hatter and Sam Pierceall who joined the tea party and helped us to understand his workflow—and possibilities for digitization using his equipment.) Looking back at the histogram earlier in the article, you can see that the broadest peaks fall on the right. These correspond to the brownish color of the paper and, in the sixteen-bit per channel images, cover a range of around eleven-million colors. These eleven-million colors fall into around ten-thousand distinguishable colors under normal circumstances. However, using a similar Fauvist-like visualization to the one above, we can uncover more information. For this experiment, we focused on a traditionally bibliographical object of study, paper. Finding the Color Range for a Typical Image of a Piece of Paper in a Book The first problem was to uncover the typical color of the paper in the images of the book. If you look closely at any sheet of paper, or any image of a sheet of paper, you see a range of colors. For example, Close-up of p. 382 you can see the fragments of dark-brown wood and whitish fibers along with yellow-brown linen fibers that have a range of shades. Furthermore, since we are exploring shades of brown beyond normal human vision, we cannot assume that what appears—using human vision—to be a uniform brown paper throughout is actually uniform. We needed to find the average shade across all five-hundred and thirty images of the pages of the body of the book without relying on our eyes. Luckily, we have ImageMagick, a tool aggressively developed by a range of experimenters who work not only with sixteen-bit per channel color, but even higher and stranger bit depths. After recompiling the tool to work with higher bit-per-channel images, we could create a running average of every ten pages, which looks like semi-transparent pages on top of each other, Mosaic of averages (converted to 8-bit per channel here) This visualization—at this point no one would think that this tries to accurately reproduce the look of a book—has some interest in itself. Were we doing distant-reading, we could note that the book seems to be mostly in prose, with some footnotes, and a table of contents at the front. But, if we average these fifty-three images together (we leave, as a trivial exercise to the reader, the fact that averaging ten images at a time and then averaging all those together is equivalent to averaging all five-hundred and thirty images together, but is far less computationally intensive) we get what we are aiming at—the average of all the images of pages of our text, including the average color of the paper, Average page Using this, we can identify the range of colors in the images of paper from the book as a whole and stretch those eleven-million or so colors, that would normally look like only ten-thousand, across the whole range of human vision. We should be able to visualize subtle shades of brown that we could not normally see. Arranging the Images Altering each image with ImageMagick is a relatively simple exercise, but leaves five-hundred and thirty Fauvist-style, blue-tinted images. Some detail can be made out, such as wirelines, but these visualizations still focuses on the page as a unit of inquiry while the paper in a book is not printed by page, but by gathering. Summarizing the principles of collation is beyond the scope of this post, but for those who understand the fundamentals of descriptive bibliography, the book collates thus, 8o: A4 B-2K8 [$1-4 (-A1,3,4) signed] 260 leaves, pp. [ 8 ] [1-2] 3-512 (4=iv, 14-15=15-14, 474=74). In other words, it is printed in octavo—that is eight pages to each side of a large sheet of paper—and folded down into gatherings of eight leaves and sixteen pages, although gathering A is slightly different. This means that we can use imposition schemes from Gaskell to recreate the layout of how the sheets of paper looked as they came off the printing press.5 Using ImageMagick to flip and arrange the digitized images (slightly cropped, so that there is not too much of the digitizing apparatus visible and leaving out the irregular A), we get, Sheets of the Book In this visualization—it cannot be a reproduction since the original sheet form of the book no-longer exists, though it might be called a “simulation” of the original form—the white lines represent the boundaries of each sheet of paper and the yellow lines the front and back of each sheet. That is, the eight pages to the left of each yellow line represent the outer forme—as identified by Gaskell—and the eight pages to the right of the yellow line represent the inner forme of the sheet. Looking at the image, things do not look promising. Each sheet of paper should be a slightly different shade of brown because many should come from separate batches: the printer would have bought paper for the whole book, but printed off each forme in sequence so that the sheet of paper produced immediately before or after any particular sheet of paper in this book is probably not in this copy. Furthermore, the finished paper coming from the paper mill may have been shuffled along the way, or at the printing shop, so even if we knew the exact sheet that was printed after a given sheet (in another copy of this book, or perhaps another book altogether, or some broadside ballad), we still would not know if it was produced in the same batch. It is likely that many of these sheets came from different batches, but there is no way of knowing because there are only subtle differences of the shade of brown to distinguish them. Yet, by stretching the color range of the paper to the whole range of human vision, we can perceive another pattern. Visually Altered Sheets of the Book ( Download the large version ) Each sheet of eight pages per side, seems to have a different shade of blue. In the upper left, you see a darker, speckled sheet. In the middle, pale blue sheets that might be the same batch. In other places, you can see brown smears across the surface of the sheet itself or brown fingerprints on the individual pages. This visualization shows some of the evidence contained in the physical copy of the book that can be captured with a higher bit-depth image. The deviation of color from the average happens over whole sheets, portions of one side of a sheet, on facing pages, or on single pages. Each of these discolorations—relative to the average color—is evidence of different sorts things that happened at different times in the history of this book. Whole Sheet Deviation Paper is made in vats of floating fibers that are periodically refilled with more fibers, so no two sheets are quite alike. At the end of a day of work, a vat would be left and refilled the next day, perhaps with new fibers. So, that two sheets do not have quite the same color makes sense: they have different combinations of fibers. Two sheets made one right after the other should be somewhat similar, like the sheets in the middle of this book, but sheets made by different producers, with different fibers, or on different days would likely look totally different. Thus, when we see both sides of a sheet of paper colored the same as each other, but differently colored than their neighbors, it provides evidence of the paper-making process from the vat where discoloration would be evenly distributed. It is possible that while the sheets were stored, dampness or other factors may have discolored the whole sheet, but as a physical object in the world, the source of the discoloration is more likely to come from one side or the other, and is likely to be localized to only one part of the sheet. Portions of One Side of a Sheet Discoloration Single-sided discolorations can be seen throughout, in some places with an apparently matching discoloration on the adjacent sheet. These suggest evidence that comes about during the sheet-phase of the book’s life. Either when it was paper being stored and transported, or newly printed sheets drying (or being stored or transported) before they were rough folded. As such, the matching brown smears on the two sheets in the middle suggest that they might have been stored one on top of the other with some moisture or other discoloring substance between them. In other cases, the discoloration may come from the unfolded or partially folded sheets being stored one on top of the other with felts between them. Another source of discoloration could be fingerprints from the papermakers, printers, or warehouse employees handling the pages before they were folded. These can be seen on the outer edges of the sheets of paper, where you would grab these if you were picking them up. Fingerprints and small marks in the middle of the sheet suggest something else. Facing Page Discoloration During the page-phase of the book’s life, after it was bound or rough-folded into a codex form, dirty fingers could still leave marks on pages. Marks on the edges of page units, particularly those that would have fallen in the middle of a sheet, rather than marks on the edges of sheets suggest that handling soiled them. If a substance that discolored the page was left on it, when the book was closed the substance could also transfer to the adjacent page. It follows that a mirror-image of the discoloration might be found on the facing page. Look in the lower right, a normally-visible water-stain becomes black on both facing sheets. Some of the faint smudges in the middle have even fainter smudges on the page facing them. These marks suggest things that happened to the book as it was read or handled in codex form. We can note the more heavily soiled pages as a way of suggesting—perhaps bolstered with evidence from other copies—the pages that were most read, at least by people with dirty fingers. Single Page Discoloration A few pages stand out as different from their neighbors, and differing from the page opposite them on the same leaf. These discolorations must have come about during the codex phase of the book’s life, but how it was isolated to one page is unclear to me. In some cases, it may merely be a smudge, or ink mark, that had time to dry and so was not transferred to a neighbor, but was not wet enough to penetrate the thin page to the opposite side. In other cases, it could be the result of an inconsistency in the digitization process (a light started to flicker, the software had a bug that altered the color differently) or image manipulation process. A more remote possibility is that the one page is somehow different—say a cancellans, a term coined by R. W. Chapman to name the replacement sheet inserted in a book to correct an error or add to a text6—but that it is glued to an original page. Or, the page could be a full cancellans from a stock of paper that was stored differently and was discolored then. Lastly, it could be soiling from a time when the book was stored as rough-folded gatherings. In any of these cases, I think of Tanselle’s suggestion to check the reproduction “against the original”7 when proof-reading. This check becomes particularly crucial here because our visualization is artificially colored and does not even attempt to try to realistically reproduce the characteristics of the original object. The pages are not blue, so these images do not reproduce these pages and the artifact itself may provide more insight. These single page discolorations suggest that further research into particular pages needs to be done, something that this whole exercise has already suggested. The Importance of the Original Object This post, I hope, demonstrates two things. First, that information about the paper in books, and how it was handled, can be partially uncovered using digital visualization techniques. While the Fauvist-like, blue-book imposition scheme is totally artificial and does not reproduce anything in the real world, it corresponds with the features of a real artifact. We can always learn more about the book as a mute witness to history by collecting and arranging detectable data. Second, I hope this post demonstrates that a digital reproduction is not only an insufficient substitute for an artifact, but that one digital reproduction is not equivalent to another. Only a few years ago, we would not have been able to produce the sixteen-bit per channel image needed for this work, yet the information was still contained in the artifact, waiting to be uncovered. Who is to say that more subtle and powerful digitization methods will not be discovered? Too often, requests for digitization are met with “it’s already in Google Books”, but I think this project shows that particular digital reproductions reproduce particular characteristics that do not encompass all the characteristics that can be captured using other digitization techniques. Multi-spectral imaging and higher-quality spectroscopy promise to generate even better visualizations, but if the pace of technical progress continues, these will just be another waypoint along the way of continuing technological improvement. Next decade’s imaging may enable visualization of information contained in original artifacts that we have not even considered, another reason to preserve the original artifact after it has been digitized and to avoid damaging materials while studying them through digitization. While our books bear silent and eloquent witness to their own production history, technology keeps changing and tomorrow’s technology might let us know a little bit more of the history that books embody. For this project, this visualization suggests the range of evidence to be found in particular copies. Returning to the author’s manuscript, where available, or attempting to infer what the author’s manuscript must have looked like, has a great deal of value as a coherent aim for historical investigation, but—so too—has the recovery of the history of printed artifacts which first exposed readers to a composite of production processes that take an author’s manuscript, edit it, revise it, order it, format it, and impress it on paper. Studying a particular edition, while not replacing editing for authorial intention, recovers the intention of compositors and correctors who also had a say in the texts that the past left to us, and so should have a voice it recovering their history. G. Thomas Tanselle, “Reproductions and Scholarship,” in Literature and Artifacts (Charlottesville: The Bibliographical Society of the University of Virginia, 1998), 33.  &#8617; Ibid., 38–39.  &#8617; Josef Albers, Interactions of Color, Rev. ed. (New Haven: Yale University Press, 1975).  &#8617;"},{"id":"2017-05-31-why-to-teach-students-to-not-read-novels","title":"Why To Teach Students to Not-Read Novels","author":"james-p-ascher","date":"2017-05-31 04:57:35 -0400","categories":["Digital Humanities","Experimental Humanities","Grad Student Research","Visualization and Data Mining"],"url":"why-to-teach-students-to-not-read-novels","layout":"post","content":"Scholars’ Lab Fellow James Ascher went to Washington and Lee University to give a workshop in Prof. Taylor Walle’s ENGL 335 course through a Mellon-funded collaboration with the Scholars’ Lab  in the UVA Library. More information about this initiative can be found here . His post is cross-listed on the W&amp;L blog . This post has a simple argument: if you teach novels, you should teach students to not-read novels. Now, before you get concerned, I’m not arguing against teaching literature or avoiding novels altogether; the hyphen in “not-read” means a method, not a rejection of reading. Indeed, my whole argument is based on the idea that students need help returning careful attention to texts, but faced with a deluge of texts, we teachers ought to show them how some professional literary critics not-read, or as others call it “distant read.” What is a novel anyhow? A reasonable question to ask your students as a semester goes along, since it seems to be a long form of prose that comes to dominate what we now consider literature. If one is to believe Amazon Rankings, then the most read forms of electronic books and the most purchased books remain novels (at least when I wrote this, but I’d be surprised if it changes). This wasn’t always the case and part of the history of the novel is its commercial success. Franco Moretti makes the case clear in his Graphs, Maps, and Trees where he argues that the rise of the novel can be studied using charts, as he does in his previous work Atlas of the European Novel ( Moretti’s response, which is more useful. ) Building on the idea of charting large-scale phenomena over time, he notices that novels rise (fig. 1). Fig. 1 rises of novels from Graphs, Maps, and Trees Something about the form, the markets, the people, or something else means that this particular literary form comes to dominate, so whatever time period you consider, it’s worth considering what was going on. Its well established that the English novel rose first—the form seems to have been invented there—so even in a non-English classroom, it’s worth considering how those novels were imported into a broader literary discourse. Luckily, the text of most eighteenth-century English novels are freely available online. We have a fortuitous confluence of the intellectually important materials that have become technologically available (a careful reader will note that this is one possible explanation of the novel’s rise as well). But, how can you study the features of a whole genre? An easy way is to read them by putting them on your syllabus, which I encourage, but after reading them you can look back at the whole syllabus and chart the topics that come up. Fig. 2 Novel topics To test this idea, I presented for Taylor Walle in her English 335 “Radical Jane: the Politics of Class, Gender, and Race in Austen’s ‘Polite’ Fiction.” Her course asked students to think about how English novels formed identities and related to the growing issues of British society. It seemed like a great chance to try some topic-modeling across her entire syllabus, the chart was produced with ten topics across the whole class (fig. 2). You can see the lesson here if you want to reproduce the work. After demonstrating how the method works, we turned to this chart and looked for topics that crossed the texts. The works read for the class and on the chart—from left to right—are Emma, Northanger Abbey, Pride and Prejudice, Sense and Sensibility, A Sentimental Journey, A Sicilian Romance, and A Vindication of the Rights of Woman . You can see, and the class immediately saw, that the topics broke at the boundaries of books. You can see that many of the topics to detect specific books, but a few cross boundaries. Now, the topic used in topic modeling isn’t quite the normal sense of the word “topic.” It means a list of words with probabilities that, when they occur, signal the topic that is that list of words. The topics by their top words are, &lt;code&gt;time made heart letter moment feelings mind spirits happiness\npresent long felt thought affection left hope return day love\nsituation ...\n\nelizabeth darcy bennet jane bingley wickham collins mrs sister\nlydia catherine lady lizzy longbourn gardiner father family\nnetherfield kitty charlotte ...\n\nmiss mrs good great dear young make time room house give day\nthought friend heard man home replied pleasure hear ...\n\njulia marquis door ferdinand madame hippolitus castle heart duke\nheard marchioness length appeared light night discovered time part\ncount scene ...\n\nman life love woman character mind world society sense opinion\ngreat beauty good present taste nature understanding husband\ndegree subject ...\n\nemma harriet weston mrs knightley elton thing jane woodhouse miss\nfairfax frank churchill body hartfield bates highbury father sort\nharriet's ...\n\nfleur paris monsieur poor hand count thou man set told madame good\nfrench thy heart lady tis put made nature ...\n\nelinor marianne mrs dashwood edward jennings sister willoughby\ncolonel lucy john mother thing brandon ferrars barton middleton\nmarianne's lady town ...\n\ncatherine tilney isabella thorpe morland allen general henry bath\neleanor catherine's brother james father street hour northanger\nabbey john captain ...\n\nwomen men reason virtue sex respect mind duties affection make\nheart children power render human virtues true allowed till duty\n...&lt;/code&gt; As far as the course goes, the first topic seems to cover what all these texts have in common, but notice everything isn’t perfectly lined up by novel. The topic beginning “julia marquis door” clearly comes from Sicilian Romance, but also hits on some later chapters of Northanger Abbey . Why would that be? Well, if you know the texts, you realize that some of the same Gothic themes occur in both texts and they use the same words.—“Dear students, can anyone bring us to where in the text this happens?” And, we enter the realm of the normal literature classroom. By presenting a broad view of the texts, built by a computer algorithm, but out of the words of the text, we invite students to re-read works. Not-reading becomes re-reading and presenting words across the entire corpus puts students into partnership with technology to ask what it is about the form of novelistic prose that makes it popular and speak to social issues. Furthermore, we also encourage students to be critical of the results of computerized analysis. Several students noted that these topics were obvious, having read the works, and that they could have come up with them by hand, which is—of course—true. It’s easy to scale these up beyond what you could do by hand, but seeing how they reflect what is accurately in the text shows that they provide some purchase on truth and also suggests what might be going on with other computerized analyses. One way we imagined it was that the computer applied an obvious rule at a fine level of detail. If we follow the same method, but only for Emma by paragraph, we get a much messier chart (fig. 3), but seeing that chart, students can begin to engage with both literary texts and computers that help them to not read—to ask what it means and what can be done with it. Fig. 3 Emma by paragraph"},{"id":"2017-06-05-what-should-you-do-in-a-week","title":"What Should You Do in a Week?","author":"brandon-walsh","date":"2017-06-05 07:00:10 -0400","categories":["Digital Humanities","Technical Training"],"url":"what-should-you-do-in-a-week","layout":"post","content":"[Cross-posted to my personal blog .] For the past several years, I’ve taught a Humanities Programming course at HILT . The course was piloted by Wayne Graham and Jeremy Boggs, but, these days, I co-teach the course with Ethan Reed, one of our DH fellows in the Scholars’ Lab. The course is a soup-to-nuts introduction to the kinds of methods and technologies that are useful for humanities programming. We’re changing the course a fair amount this year, so I thought I’d offer a few notes on what we’re doing and the pedagogical motivations for doing so. You can find our syllabus, slides, resources, and more on the site . We broke the course down into two halves: Basics: command line, Git, GitHub, HTML/CSS Project: personal website Programming concepts: Ruby Project: Rails application deployed through Heroku and up on GitHub In the first half, people learned the basic stack necessary to work towards a personal website, then deploying that site through GitHub pages. In the second half, students took in a series of lessons about Ruby syntax, but the underlying goal was to teach them the programming concepts common to a number of programming languages. Then, we shifted gears and had them work through a series of Rails tutorials that pushed them towards a real-life situation where they’re working through and on a thing (in this case a sort of platform for crowdsourcing transcriptions of images). I really enjoyed teaching the Rails course, and I think there was a lot of good in it. But over the past few years it has raised a number of pedagogical questions for me: What can you reasonably hope to teach in a week-long workshop? Is it better to do more with less or less with more? What is the upper-limit on the amount of new information students can take in during the week? What will students actually use/remember from the course once the week is over? To be fair, week-long workshops like this one often raise similar concerns for me. I had two main concerns about our course in particular. The first was a question of audience. We got people of all different skill levels in the course. Some people were there to get going with programming for the first time. These newcomers often seemed really comfortable with the course during the first half, while the second half of the course could result in a lot of frustration when the difficulty of the material suddenly seemed to skyrocket. Other students were experienced developers with several languages under their belt who were there specifically to learn Rails. The first half of the course seemed to be largely review for this experienced group, while the second half was really what they were there to take on.  It’s great that we were able to pull in students with such diverse experiences, but I was especially concerned for the people new to programming who felt lost during the second half of the course. Those experienced folks looking to learn Rails? I think they can probably find their way into the framework some other way. But I didn’t want our course to turn people off from programming because the presentation of the material felt frustrating. We can fix that. I always feel as though we should be able to explain these methods to anyone, and I wanted our alumni to feel that they were empowered by their new experiences, not frustrated. I wanted our course to reflect that principle by focusing on this audience of people looking for an introduction, not an advanced tutorial. I also wondered a lot about the outcomes of the course. I wondered how many of the students really did anything with web applications after the course was over. Those advanced students there specifically for Rails probably did, and I’m glad that they had tangible skills to walk away with. But, for the average person just getting into digital humanities programming, I imagine that Rails wasn’t something they were going to use right away. After all, you use what you need to do what you need. And, while Rails gives you a lot of options, it’s not necessarily the tool you need for the thing in front of you - specially when you’re starting out. So we set about redesigning the course with some of these thoughts in mind and with a few principles: Less is more. A single audience is better than many. If you won’t use it, you’ll lose it. I wondered how we might redesign the course to better reflect the kinds of work that are most common to humanists using programming for their work. I sat down and thought about common tasks that I use programming for beyond building apps/web services. I made a list of some common tasks that, when they confront me, I go, “I can write a script for that!” The resulting syllabus is on the site, but I’ll reiterate it here. The main changes took place in the second half of the course: Basics: command line, git, GitHub, HTML/CSS Project: personal website Programming concepts: Python Project(s): Applied Python for acquiring, processing, and analyzing humanities data The switch from Python to Ruby reflects, in part, my own changing practices, but I also find that the Pythonic syntax enforces good stylistic practices in learners. In place of working on a large Rails app, we keep the second half of the course focused on daily tasks that programming is good for. After learning the basic concepts from Python, we introduce a few case studies for applied Python. Like all our materials, these are available on our site . But I’d encourage interested folks to check out the Jupyter notebooks for these units if you’re interested. These are the new units on applications of Python to typical situations: Working with CSV files Getting data from API’s Introduction to Web Scraping Basic Text Analysis In the process of working through these materials, the students work with real, live humanities data drawn from Project Gutenberg, the DPLA, and the Jack the Ripper Casebook . We walk the students through a few different options for building a corpus of data and working with it. After gathering data, we talk about problems with it and how to use it. Of course, you could run an entire course on such things. Our goal here is not to cover everything. In fact, I erred on the side of keeping the lessons relatively lightweight, with the assumption that the jump in difficulty level would require us to move pretty slowly. The main goal is to show how situations that appear to be much more complicated still boil down to the same basic concepts the students have just learned. We want to shrink the perceived gap between those beginning exercises and the kinds of scripts that are actually useful for your own day-to-day work. We introduce some slightly more advanced concepts along the way, but hopefully enough of the material will remain familiar that the students can excel. Ideally, the concepts we work through in these case studies will be more immediately useful to someone trying to introduce programming into their workflow for the first time. And, in being more immediately useful, the exercises might be more likely to give a lasting foundation for them to keep building on into the future. We’ve also rebranded the course slightly. The course description has changed, as we’ve attempted to soften jargon and make it clear that students are meant to come to the course not knowing the terms or technologies in the description (they’re going to learn them with us!). The course name has changed as well, first as a joke but then in a serious way. Instead of simply being called “Humanities Programming,” the course is now “Help! I’m a Humanist! - Programming for Humanists with Python.” The goal there is to expose the human aspect of the course - no one is born knowing this stuff, and learning it means dealing with a load of tough feelings: anxiety, frustration, imposter syndrome, etc. I wanted to foreground all of this right away by making my own internal monologue part of the course title. The course can’t alleviate all those feelings, but I hoped to make it clear that we’re taking them into account and thinking about the human side of what it means to teach and learn this material. We’re in it together. So. What can you do in a week? Quite a lot. What should you do - that’s a much tougher question. I’ve timed this post to go out right around when HILT starts. If I figure it out in the next week I’ll let you know."},{"id":"2017-06-21-lami-summer-fellows-2017","title":"LAMI Summer Fellows 2017","author":"brandon-walsh","date":"2017-06-21 07:30:46 -0400","categories":["Digital Humanities","Technical Training"],"url":"lami-summer-fellows-2017","layout":"post","content":"For the third year in a row, the Scholars’ Lab and the University of Virginia Library are helping host summer fellows from the Leadership Alliance Mellon Initiative (LAMI) at UVA. The students will pursue original research this summer at UVA in consultation with a faculty mentor. For our part, the Scholars’ Lab and the Library have worked with Keisha John, Director of Diversity of Programs in the Office of Graduate and Postdoctoral Affairs, to organize a weekly series of workshops introducing the students to digital humanities and library research methods. They’ll be getting a broad introduction to digital research and the resources of the library as they think towards graduate school, and we’ve also coordinated weekly board game sessions over lunch (for SLab-style bonding). In addition to introducing these students to the resources available at UVA and in the library system, the program aims to increase the number of demographically underrepresented students pursuing graduate work and careers in the academy. You can find more information about the program in a 2015  press release put out by UVA Today when our first cohort was in residence. Two of our own graduate fellows, Jordan Buysse and Sarah McEleney, are serving as dh mentors. These are the students that you might meet if you happen to be around the Scholars’ Lab this summer. Look for more information about them and their projects by clicking through to their bios! Je’lon Alexander Valeria Arce Sara Castro Madison Choi Matt Ford Victoria Juarez Kaitlin Mitchell Ryan Russell Gabriela Trinidad-Perez They’re a fantastic group, and we’re excited to work with them this summer. Thanks to all of our colleagues at UVA, the Library, and the Scholars’ Lab for their participation in the program."},{"id":"2017-06-27-job-opening-curious-about-focusing-on-dh-development","title":"Job Opening: Curious about focusing on DH development?","author":"amanda-visconti","date":"2017-06-27 10:24:54 -0400","categories":["Announcements","Job Announcements"],"url":"job-opening-curious-about-focusing-on-dh-development","layout":"post","content":"Note: Applications for this position have closed. Welcome to new Scholars’ Lab DH Developer Zoe LeBlanc ! You might have seen our opening for a Senior Developer —we’re now seeking an additional colleague for our R&amp;D team: DH Developer!  Apply here  (posting number #0621212), or read on for more information. We welcome applications from women, people of color, LGBTQ, and others who are traditionally underrepresented among software developers. In particular, we invite you to contact us even if you do not currently consider yourself to be a software developer. We seek someone with the ability to collaborate and to expand their technical skill set in creative ways. This is a full-time position, with flexibility to help you achieve a healthy work-life balance. Like all our team members, this role includes 20% of your time devoted to your own research initiatives and professional development. This “personal R&amp;D” time includes  access to our experimental humanities makerspace, other high-end facilities, and expert collaborators and mentors. About us The University of Virginia Library is the hub of a lively and growing community of practice in technology and the humanities, arts, and social sciences. As part of that community, the Scholars’ Lab has risen to international pre-eminence as a library-based center for digital humanities. The Scholars’ Lab collaborates with faculty, librarians, and students on a range of projects and tools, including spatial humanities, interface design, innovative pedagogy, data visualization, text analysis, digital archiving, 3D modeling, virtual reality and gaming, and other experimental humanities approaches. The Library and the Scholars’ Lab are committed to diversity, inclusion, and safe spaces, and we have focused recent speaker series and practice on accessibility and social justice ( check out our team-authored charter for more on our values ). We welcome curious, critical, and compassionate professionals who are keenly interested in the overlaps between technology and the humanities (literature, history, art, cultural heritage, and related fields). The Scholars’ Lab currently consists of 11 staffers (plus a senior developer role), as well as an amazing cohort of graduate fellows and makerspace technicians. **Anticipated salary range: **$65,000-75,000, plus benefits such as professional development/conference travel funding, health insurance, and retirement savings plan Responsibilities Under the direction of the Head of R&amp;D for the Scholars’ Lab in the UVA Library, the DH Developer works with scholars from the humanities and social sciences to understand their needs and define project goals provides professional opinions on appropriate project deliverables and reasonable schedules for completion of projects collaborates on building applications that enable scholars and library users to collect, manage, produce, manipulate, or analyze digital information resources in interesting ways writes original code, and tests and improves on existing code learns about and engages with new technologies toward widening and deepening the Scholars’ Lab’s pool of staff expertise creates documentation for both internal Lab and external non-technical audience use Qualifications   Minimum requirements: Experience equivalent to one full-time year with either a programming language (such as—but not limited to—PHP, Ruby, Python, Java), or HTML, CSS, and JavaScript 2 years of web development experience, with tech skills demonstrated in an accessible portfolio of work. Familiarity with a code version control system such as Git. Ability to work and communicate with technical and non-technical collaborators. Either education through the master’s level or equivalent experience through your job, hobbies, or other activities, preferably in the humanities or library/information science. Interest in the humanities (literature, history, art, cultural heritage, etc.) Preferred: Graduate degree or equivalent professional or other experience in the humanities or social sciences. Knowledge of and interest in the digital humanities. Experience with collaborative project work. Experience with any of the following: data collections, analysis, visualization, and interpretation; front-end web development and design; back-end web development; systems and database management; text analysis or image analysis methods and tools; frameworks such as Rails, Django, and Zend; TEI, XML, Solr, Cocoon, Tomcat. Experience taking initiative to suggest or begin new projects, and to carry out projects with little supervision. Interested? You can apply here  (posting number #0621212), but please feel free to reach out with any questions—for yourself or a friend—by emailing visconti@virginia.edu or tweeting @scholarslab  or @literature_geek . In particular, we’re very happy to talk with anyone who’s interested, but not sure whether they have the required technical background. All job discussions will be treated as confidential."},{"id":"2017-07-05-neatline-2-5-2","title":"Neatline 2.5.2","author":"ronda-grizzle","date":"2017-07-05 09:15:00 -0400","categories":["Announcements"],"url":"neatline-2-5-2","layout":"post","content":"New release! First, a huge thank you to Jamie Folsom and Andy Stuhl from  Perfomant Software Solutions LLC, who did the heavy lifting on the coding for this release. We couldn’t have done it without them. We’re grateful, as well, to Neatline community member Adam Doan ( @doana on Github) from the University of Guelph, whose code contributions made Neatline’s first accessibility functionality possible. What’s Fixed: Google Maps API issues. We originally embedded the API key for Google Maps directly in the Neatline code, but Google changes the way apps should connect to their codebase fairly regularly, and with little or no warning. It’s just easier for everyone if you can directly configure an API key for your specific installation of Neatline, so that’s what we’ve done. Updated installation and configuration instructions (with screencaps!) are available on our documentation site  . WMS map layer issues. We thought we had this one squished, but it came back again because of issues with our implementation of OpenLayers 2.0 and conflicts with the way that MapWarper passes data via URL. MapWarper WMS layers will now render properly as exhibit items and as base layers for an exhibit. What’s New: Accessibility. Thanks to Neatline community member @doana, you can now specify a URL to an accessible version of your Neatline exhibit in the exhibit’s settings. If the accessible URL exists, a hidden link will be rendered at the top of the public exhibit page directing users of assistive technology to the alternative page so that their screen reader can render the page for them. This feature relates specifically to Guideline 1.1 of WCAG 2.0 . Our documentation of this new feature will be available on docs.neatline.org by July 10, 2017. For more detail on this update, check out the Changelog . Ready to download? Get the latest release from the Omeka Add-Ons Repository . Encounter an issue? ask a question on the Omeka Forums  or submit an issue, or feature request, directly to us on our issue tracker ."},{"id":"2017-07-05-transcribing-typography-with-markdown","title":"Transcribing Typography with Markdown","author":"james-p-ascher","date":"2017-07-05 14:28:23 -0400","categories":["Digital Humanities","Experimental Humanities","Grad Student Research","Research and Development"],"url":"transcribing-typography-with-markdown","layout":"post","content":"Digital technologies are not new solutions to our old problems, but are new problems asking for us to return to old solutions. People have been transcribing texts for as long as there have been texts. So it is no surprise that some of the earliest applications for computers were concerned with transcribing texts. These applications built on ideas based on previous ideas which were themselves based on yet earlier ones—the genealogical chains of some going back centuries. These genealogies have their own fascinating histories, but the problem that concerns this post is finding ways to use those centuries of accumulated ideas to reproduce texts in a computerized platform that has developed its own sense of how texts should function. That is, how do we digitally transcribe using the techniques that already exist? Techniques developed in prior decades can be difficult to use with computer-based methods because these computer-based methods have only recently developed and have not yet been applied to as wide a range of sources as traditional transcription methods. The solutions that computer applications first stumbled across for one problem initially seem to be right for every problem, but on careful study it turns out they often obscure important details. We do not yet have an extensive repertoire of computer transcription to consider, so it naturally seems like the solution that exists, however idiosyncratic, is the solution for all situations. For example, &lt;em&gt; is widely used to indicate both italics and emphasis, which seems fine, but conflates two different sorts of text forms: The former could indicate poetry, summaries, Latin or citations, while the later suggests something that is an exception—a key glossary term or foreign phrase. The conventional computerized scheme is to treat the stylistic choice of italic as separate from the semantic choice of emphasis. For a non-emphasis use of italic, a style could be applied to distinguish that particular semantic meaning, but these semantic meanings would have to be carefully integrated into the existing scheme without conflicting with any other components. This integration becomes increasingly complicated as the schemes become more complete because there are more opportunities for conflict. New standards for HTML and TEI make the situation more flexible, but fundamentally transcriptions on computers must either force their intellectual goals into an existing framework—i.e. marking a distinct convention of italics as though it was the same as emphasis—or extend the framework itself—i.e. inventing a tag that distinguishes all the semantic types of italic used in the work. When I consider what digital tools could bring to the humanities, I envision new forms of writing which challenge us to reimagine traditional questions in systems with new capabilities but which accept their own limited application and eventual obsolescence in a long history of writing technologies. If a writer recognizes that eventually all technologies become obsolete, they cannot reasonably ignore the wisdom recorded by the technologies of the past. Furthermore, writing serves a wider range of uses when it collaborates with as many conventions as possible, encoding the wisdom of old and new systems and clearly seeing the goals of past writers as well-worn traditional tools, and not creating new tools when a better old one remains to be understood. Timothy Morton’s idea of “ecology” serves as well as any other metaphor here: he invites a move from placing nature on a pedestal to thinking of ecology as a sort of ambient poetics . Rather than asking how nature writing is done, we ask about the contexts in places, space and material that surround both nature and the writing about nature. I propose that his idea of ecology can be extended to the way we think of computer tools and their relationship to the wider range of writing tools—the more widely we share tools and link to other ideas, the more comprehensible will be our approach which also sets us up for the next innovation. We ought not put a particular scheme on a pedestal but instead recognize how it interacts with all other schemes. 1 Translating the physical and intellectual features of a text into a system of transcription requires judgment and I think adapting the most established technologies to the task does more good than taking on the unproven baggage of complicated new systems. A note on the scope of technology I’m talking about here: By established, I don’t mean HTML or even ASCII, I mean conventions that span hundreds or even thousands of years: punctuation, glossing, words in sequence, and that sort of thing. The stability of the technology of punctuation means that it has been adopted in more recent technologies and could be used without careful consideration, but I think that people concerned with the past need to ask what questions the past has already answered well enough. Questions of how to transcribe have two components: the intellectual work of selection and description that needs to be done and the conventions available for encoding. The former requires developing judgment, the later only familiarity with a particular system. Adapting the ethos of the 1999 essay by David L. Vander Meulen and G. Thomas Tansell, “A System of Manuscript Transcription,” which broke new ground by adhering as closely to common sense as possible rather than breaking new ground with a byzantine new system, we can write to foreground the historical text and our judgments about it. 2 Their approach admits that a transcription re-forms a text, requiring decisions and highlighting where those decisions are made. Consider a hypothetical example in Markdown using something like the Vander Meulen-Tanselle approach: &lt;code&gt;The [?funny *conceled*] cat ran after the [*next two words\ninserted*] big brown dog.&lt;/code&gt; Which renders as: “The [?funny canceled ] cat ran after the [ next two words inserted ] big brown dog.” The convention of using square brackets to mark later insertions is so well established that it requires little explanation to understand that these are editorial comments and that the text that the editor believes is correct is outside of the brackets. 3 Enclosing italic text in asterisks is a Markdown convention, but is relatively familiar to most readers of digital texts. In contrast, one way to mark the same text in TEI would be: &lt;code&gt; &lt;p&gt;The &lt;del&gt; &lt;unclear&gt; funny &lt;/unclear&gt; &lt;/del&gt; cat ran after the\n &lt;add&gt; big brown &lt;/add&gt; dog &lt;/p&gt;&lt;/code&gt; But those more familiar with TEI might be tempted to write: &lt;code&gt;  &lt;p&gt;The &lt;del rend=\"strikethrough\" cert=\"medium\"&gt; &lt;unclear\n  reason=\"smudged\"&gt; funny &lt;/unclear&gt; &lt;/del&gt; cat ran after the &lt;add\n  place=\"above\"&gt; big brown &lt;/add&gt; dog &lt;/p&gt;&lt;/code&gt; These tags instantiate collaboratively delineated concepts intended to be broadly applicable to a wide range of texts, which aspire to encode the document as it is. Various style sheets can cause these semantic meanings to render as floating notes, in brackets, as plain text or in nearly any other way imagined by the designer of the particular style sheet. However, there is no default rendering and no rule about which details to record. TEI users are tempted to imagine that they merely record what they see rather than applying their judgment. Does knowing that the certainty of the word “funny” is medium, that it was canceled with a strike, and that “big brown” was above the line add to the meaning of this text? I think, typically, it would not. But, if in a project the method of cancellation mattered, it could easily be included in the editorial comments in the Vander Meulen-Tanselle system in plain English. Indeed, if the method of cancellation mattered, writing it in English would emphasize it rather than hide it in the appearance of the display. Another difference between a TEI-like approach and an approach like Vander Meulen and Tanselle’s is whether or not the system is expanded using the resources of plain English or by using the coding expertise of a specific community, and, by implication, whether either system encourages developing judgment appropriate to the task at hand and demonstrating where that judgment is applied—or if it encourages outsourcing judgment to experts unfamiliar with your particular problem and hiding that application of judgment under the aegis of an official standard. In one case, capable readers of English can understand the markup, while in the later case, you would need access to the TEI standards and discussions to determine how to express and understand a feature correctly according to people probably not involved with your particular project. The plain English system can be read by competent and informed readers; the TEI system requires specialist expertise. Now, I do not mean to pick on the members of the TEI Consortium, who do good work, but to outline the sort of thinking that influences the use of digital technology in transcription. If we care to write about a text in history, we must think about which of the innumerable facts about that text apply to the project at hand. This work can be shortcut by adhering to any present or future standard that displaces judgment onto someone else, but at the expense of the system working for your project. A system needs to grow from the problems in the project. For this project, we aim to recreate, digitally, the experience of reading eighteenth-century texts for commonplace entries. Having looked at several eighteenth-century books that survive and thinking about manuals on preparing commonplace books (Locke has a particularly good one ), we realized that the physical layout of the text affected reading. Since texts occur in some layout which presents their meaning, it is obvious that the layout links to meaning, but you would not think so based on text encoding standards that ignore white space and line breaks. Furthermore, commonplacing readers may read with a pen in hand to mark the text itself in the margin. Conventional computer document formats are not well suited to those approaches that remain attentive to the layout of the page of the text they comment on. Recently, Ted Nelson began a new lecture series describing how current computer systems reflect arbitrary compromises that stuck. One example he mentions is that most modern document formats include no way to indicate marginal notes. For centuries, people commonly commented on texts with marginal notes, but because the early designers did not use those sorts of notes in their own day-to-day work, they were not included in most modern standards for texts. We aim to capture features like white space and line breaks that are not currently recorded in transcription systems, but which would have been part of the eighteenth-century text’s features. But, how can we wrestle the strangeness of old texts to the modern systems we know? One approach draws on improv theater—just start doing it and then look over the work to see which parts are useful. For this portion, three of us—Sarah Berkowitz, Elizabeth Ferguson and James P. Ascher—each transcribed a few pages of the printed text we have been studying—Samuel Butler’s Characters —after talking about the aims of the project. We compared our approaches to find what did and did not work. These solutions, like other good transcription solutions, were developed collaboratively but are quite specific to this project. Our hope is that the reasoning here, while it should not be used to answer your specific questions, should demonstrate an approach to thinking about your own project. Headings Within Butler’s Characters, each character has its own distinct heading, so they are important to record because they structure the text and name the theme for each part. On the printed page, these headings have a different appearance from the main text: They are centered, in a larger face, use capitals and small caps, and have rules above and below them. One approach might be to transcribe thus: &lt;code&gt;{Double Rule}\n\nA {Small Caps}\n\nHUFFING COURTIER&lt;/code&gt; Or, with kerning between the letters &lt;code&gt;{Double Rule}\n\n   A\n\nS M A L L  P O E T&lt;/code&gt; Both of these approaches recreate the layout of the text in the document and preserve the capitalization. Another approach could ignore the layout and kerning: &lt;code&gt;[*two rules*]\n\nPREFACE.&lt;/code&gt; Or, following the Markdown convention for signaling a heading, we could write: &lt;code&gt;[*two rules*]\n\n# AN\n# ALDERMAN&lt;/code&gt; All four approaches, although they may seem somewhat different, are shaped by two consistent categories of decision making: first, the selection of the elements worthy of notice and, second, the system of recording those elements that have been noted. In each example, the heading is marked as different: in two cases by giving the typography and in two other cases by interpreting the typography as bearing semantically equivalent meaning to a heading. Going back to the aim of the project, recreating the reading practices of the past, since the typography of headings is fairly consistent, it seems right to merely apply semantic judgment to the text and save the reader of the transcription time—as long as we’re confident of our interpretation. Sure, they could figure out that centered small caps words are headlines, but instead we could tell them every headline is like that in an introduction and just note any variation. All four transcriptions, however, regard the presence of the rules as significant. The rule seems like an element to notice since a reader of the text would come to expect the pattern and the rules interrupt the purely alphabetic text before and after. The matter of how to transcribe is more about convenience for converting the file and will be treated later. Initial Letters Each section also begins with what is called a “drop cap,” a capital letter that is bigger than the adjacent letters, and which drops down several lines. We have a few examples of how to record this as well (unrelated portions of the transcription are omitted): &lt;code&gt; [*two line initial*]HAS taken his Degree in Cheating [...]\n [*two line initial*] the higheſt of his Faculty [...]&lt;/code&gt; Another: &lt;code&gt; [I]s one, that would fain make [...]\n [ ]which Nature never meant him; [...]&lt;/code&gt; Another: &lt;code&gt; [Two Line initial*]Is a Cypher, that has no Value himsself, but\n                    from the Place he stands in.  All his Hap-&lt;/code&gt; Another: &lt;code&gt; T^2^HE writing of Characters [...]\n much in Faſhion [...]&lt;/code&gt; The first three mark the lines that a the drop cap initial spreads to. This spread may be significant as drop caps in many French books of this same time period go upwards rather than downwards, extending above the line. Furthermore, the word on the next line might be confused as relating to the drop capital; resolving this confusion could be part of the reading practice. Note also, that the first and third example preserve the space between the capital and the text of the second line, the second does not (but that seems fine because the extension of the capital is already blank). The last transcription, which is based on the suggestion of Fredson Bowers in his Principles, uses a superscript in Markdown to signal the initial, but does not record whether or not it goes up or down. 4 This choice may be fine if we are restricted to English books of a certain type where these always seem to go down, but might be a problem if developing a transcription standard that covers all types of books. Since in our case the practice is uniform across the text and merely serves to signal the reader that they have arrived at the beginning of a new section, Bower’s approach seems to best preserve the readability of the text. It would render “T2HE writing of Characters.” While the superscript 2 is somewhat confusing, a note could explain and the superscript follows a convention developed from early twentieth-century work on incunabula that has been used in certain serious scholarship for over a century. Long ‘s’ characters Eighteenth-century printers used both the long s—i.e. ‘ſ’—and the short s—i.e. ‘s.’ Typically, the long s is used in the middle of a word and the short one at the beginnings and endings, but the practice is hardly consistent. It is not until 1785 that the short s really dominates mainstream printing as it does now. For this reason, most transcriptions note the presence of a long s in some way. We have three examples in our transcriptions: as “ſ,” as “ss” or as “s*.” Each transcriber seems to want to include the letter as distinct from ‘s,’ which seems like the right choice since long s at the very least could potentially be confused with an ‘f’ by current, or past, readers. However, each mode of transcription emphasizes the long s in different ways. On the one hand, one marks it with an asterisk, suggesting that something unusual is going on; the asterisk is the convention for notae and footnotes, so this could work to mark and make visible all the places that this unusual letter occurs. Consider this passage, &lt;code&gt;    He has found out a Way to s*ave the Expence\n of much Wit and Sens*e: For he will make\n les*s than s*ome have prodigally laid out upon\n five or s*ix Words s*erve forty or fifty Lines.\n This is a thrifty Invention, and very eas*y; and,\n if it were commonly known, would much in-\n creas*e the Trade of Wit, and maintain a Mul-&lt;/code&gt; It makes it extremely clear that the long s occurs throughout in the beginning and middle of words, pointing out that it deviates from the convention I proposed above. On the other hand, using “ss” blends in and seems to be another spelling of a word. Since spelling was mostly normalized in this time period, a deliberate misspelling can signal special typography, but we can never be totally certain that that something might not be an error or a word we do not know. Consider “paſs” and “pasſ,” which would both be transcribed in this system as “passs,” which seems to conflate the common situation with the odder one. Using the Unicode character, ‘ſ,’ seems to both mark the letter as exceptional and to leave a readable text. Those familiar with the conventions of eighteenth-century printing will be not be surprised by it, but for those who are not familiar with the conventions, it would recommend itself for further study. Footnotes, Quotations and Certainty Brief, conventional quotations provide no special problems since the quote mark in ASCII and Unicode serves well enough, but two kinds of typographic style associated with references require special attention: footnotes in smaller type at the foot of the page and running quotes. An example that combines both (slightly altered so that only those two conventions are apparent), &lt;code&gt;if it were commonly known, would much in-\ncrease the Trade of Wit, and maintain a Mul-\n\nWe read that Virgil used to make, &amp;c ] This alludes to a Passage\nin the Life of Virgil ascribed to Donatus. \" Cum Georgica scrie-\n\" traditur quotidio meditatos mane plurianos versus dic_tare so-\n\" litus, [---Illegible need to check original copy (sarah)]\"&lt;/code&gt; While the footnote text in the original is smaller, this transcription does not document that fact, reasonably since the bracket and the verbal text itself alerts us that we are in a footnote. Additionally, skipping one line and continuing on the same page preserves the ability to make line references to both the main text and the footnotes in the same system. Yet, in comparison to our treatment of headings, it seems like we could provide some signal to the reader that the text before them has special semantic value. Following the convention of square brackets, we can adopt the Markdown notation, “ [^1] ” and “ [^1]:,” to signal, respectively, the location in the source text that is marked and the lines of the text that comment on it. The problem we run into using this notation with this particular footnote is that it comments not on the page it occurs on, but on the facing page. If several footnotes occur, the number can be incremented, but—as far as I know—footnotes commenting on a text in another file are uncommon enough as to not have been dealt with. It seems that such a linkage needs two pieces of information: a linking symbol (i.e. the footnote mark or passage footnoted) and the file that contains the proper footnote. Markdown provides such a mechanism in the “reference link”; i.e. our source passage could be [words][^1] and the note could read [^1]: otherfile.md . The problem is that to Markdown, this reference notation means the word “words” becomes a hyperlink to file “otherfile.md,” which isn’t quite the right linkage. A simple extension of this scheme would be to include a statement of the location at both ends of the footnote viz., for page 22, &lt;code&gt;[...]\nThe words were found in the notes. \\*[^1]\n\n[^1]: [*the footnote occurs on page 23*]&lt;/code&gt; Page 23 has an additional footnote but still refers back: &lt;code&gt;[...]\nSomeunrelated text with its own note, \\*[^1]\nthat doesn't relate to the wrong note.\n\n[^1]: This is a note for this page, so no comment.\n[^2]: [*referring to the note on page 22*] I here note.&lt;/code&gt; Notice a few aspects of this approach. Since the numbering for footnotes in the brackets is not part of the transcription, but merely an aid for the abstract structure, it only matters that the numbers are consistent within one document. Footnote “1” on one page could very well be footnote “2” on another page while preserving the enumeration or symbols provided by the printer. In this imaginary example, we prefixed the asterisk with a backslash so that any computerized parser would see that it is a symbol not a special character indicating an italic font. In the original example, note also that the running quotation marks are simply transcribed in the margin with their apparent spacing. This seems right as the aim is to preserve the reading experience of the page which would have these marked out by quotes. Lastly, note the final editorial comment that expresses the uncertainty of the transcription. While a uniform language for expressing uncertainty is desirable, the nature of uncertainty is so various that providing free-reign to the editor to explain what’s going on seems the most prudent. Italics, Small Caps and Other Type Changes Transcribing italics is both conventional and is part of the procedure used by all three of the transcribers in this project. Options include tagging the text with [i] or {i} or &lt;i&gt; as well as using * to enclose the passage. In each case, the transcription identifies the moment in setting type where the compositor would have switched from pulling letters out of the physical case of roman type to pulling letters out of the physical case of italic type and vice versa. Whichever sign is used, the aim is to note the presence of another style of typeface with the same body size. As Markdown understands asterisks to mean italic, something like *this* seems to be the right approach only because it makes the text easier to parse with conventional tools and does not misstate the situation. Following the same technique, small capitals often signal different sorts of information in the text. Markdown does not provide a solution that is quite as elegant as the one for italic, but these type changes are a bit less common. The convention is to write &lt;span style=\"font-variant:small-caps;\"&gt;In Small Caps&lt;/span&gt; for a passage with I, S and C in full caps with the remaining letters in small caps. Aside from being unwieldy, this captures exactly what is happening and provides a pattern for other changes between type cases that warrant note in the transcribed text: Simply alter “small-caps” to the appropriate font-variant of interest for a particular project. It is worth remembering, however, that the goal is to digitally recreate a reading experience, so for cases of different type sizes indicating a heading, it seems sufficient to use the mark for heading. For cases of different type sizes indicating footnotes, the semantic marking for footnotes seems sufficient. Lastly, we may come across a broken or damaged letter. One example transcribes a t that is damaged as &lt;t&gt;, which follows the old tradition of using angle-brackets to indicate portions of the text that have been mutilated but which the editor can recover. However, given the inconsistent use of different kinds of brackets in different kinds of editions, this might be confusing. Another option is to use editorial notes, i.e. t [*previous letter shifted*] which interrupts the text to announce the damage. A further option would be to note the t plainly and include a note at the bottom of the page, such as this one: &lt;code&gt;[*letter t on line 8 shifted*]&lt;/code&gt; This last approach emphasizes that the situation with the letter t is totally comprehensible—it is just shifting type—and would not interrupt the reading experience. It seems that the choice between these last two approaches has to do with how prominent the mistake seems to be. Material in the Skeleton: Headlines, Page Numbers and Signatures A page of text includes not just the text of the work, but also what Genette would call paratexts which indicate the subject of chapters, location in the book or instructions to a binder. These occur in type imposed into a forme, but have a different sort of relationship to the text set in a continuous operation by the compositors. Only in unusual circumstances would an author expect a printer to follow their page numbers, so compositors normally just provide the body text and footnotes; when these are imposed, a skeleton forme of page numbers, signatures and headlines from the previous imposition can be moved over and the details corrected for the new set of pages. A full description of a book should account for all the textual elements of a page, but it makes perfect sense to segregate the information that was inserted as a guide surrounding the main text from the text itself. Since this project includes a conventional bibliographical description, this information can be put there while the text transcriptions can focus on the text that forms the work of the compositors before the material was imposed in a skeleton forme. Spacing Between Letters and Around Punctuation Eighteenth-century punctuation often used spacing differently than we do now. A semicolon might have a thin space before it and a thick space after it. To fit some punctuation into a line, there might be no spaces after a period but before the next sentence begins. In another line, the space after a period might be exceptionally large, or the spaces between words exceptionally large. A compositor may put extra spaces between the letters of a word to give it emphasis as a heading, which seems like a semantic choice rather than one that aims to preserve the justification of the line. That is, there seem to be two possible reasons to have a noticeable variation in space: either the need to provide a line of type of a certain width or to indicate a type of semantic information. Experience must be the guide in distinguishing between these two, but the situation should generally be clear after some study. The problem becomes one of how to represent variations in spacing widths if it is decided that they represent semantic meaning. Since each line is justified separately, the unit of analysis must be the line. Different widths of spaces between two different lines almost certainly represent the need to justify that particular line, but different widths within a line may—if the editor judges it to have meaning—be transcribed within that line. To encode this, Unicode provides a range of spaces of different widths. The characters “HAIR SPACE,” “PUNCTUATION SPACE,” “EM SPACE,” “THREE-PER-EM SPACE,” “FOUR-PER-EM SPACE,” “SIX-PER-EM SPACE” cover a wide variety of types of spaces and widths of spaces (the Unicode Standard itself covers these in far more detail). The most sensible treatment of space, since a compositor would not really be distinguishing a space used for punctuation from a similarly narrow space, would be to follow Peter Blayney’s approach in the Appendix to his Texts of King Lear 5 When different sizes of space appear in the same line, simply use different sizes of space in the transcription to indicate that. The only modification for our project is that those spacing elements ought—in the judgment of the editor—to bear some sort of semantic meaning. An Example Page This post has discussed a wide range of choices in transcribing a text to preserve the reading experience from a printed book that can be summarized simply: use Markdown and Unicode and make judgments clear in square brackets when alterations are needed to use Unicode and Markdown. Yet, it can be useful to have an example—however fabricated—that brings these elements together, &lt;code&gt;[*two rules*]\n\n#AN\\\nEXAMPLE PAGE\n\nT^2^he *text* on this page isn't in any book, but &lt;span\nstyle=\"font-variant:small-caps;\"&gt;Demonstrates&lt;/span&gt; some\ntechniques you might use to tranſ-\\\nscribe texts as you see them.  Note that each line breaks with a\\\nbackſlash before the newline. [^1]  This signals the difference\nbetween\\\na newline needed to fit the text on one screen and one which rep-\\\nresents an actual line break.  \"We find that the quotes run\\\n\" along the side for extended quotes.  Just as they do in\\\n\" eighteenth century texts.\" And , that punctuation spaces can be\\\ncoded as such .  What a text! [*last word poorly inked, could be \"hex\"*]\n\n[^1]: before the newline ] a newline is a special sort of chara-\\\ncter that means you begin a new line of text.\n\n[*the letter t in \"techniques\" on the first line of the text is\nshifted upward*]&lt;/code&gt; One way this would render by default would be: [ two rules ] E X A M P L E P A G E T2he text on this page isn’t in any book, but Demonstrates some techniques you might use to tranſ-\nscribe texts as you see them. Note that each line breaks with a\nbackſlash before the newline. 6 This signals the difference between\na newline needed to fit the text on one screen and one which rep-\nresents an actual line break. “We find that the quotes run\n“ along the side for extended quotes. Just as they do in\n“ eighteenth century texts.” And , that punctuation spaces can be\ncoded as such . What a text! [ last word poorly inked, could be “hex” ] [ the letter t in “techniques” on the first line of the text is shifted upward ] cter that means you begin a new line of text. Timothy Morton, Ecology Without Nature: Rethinking Environmental Aesthetics (Cambridge, Mass.: Harvard University Press, 2009).  &#8617; David L. Vander Meulen and G. Thomas Tanselle, “A System of Manuscript Transcription,” Studies in Bibliography 52 (1999): 201–12.  &#8617; For those unfamiliar, brackets are a standard symbol used to indicate editorial additions and italics distinguish descriptions and explanations of the roman text: since “big brown” is conjectured to belong to the final text, the words are placed outside of the brackets; since “funny” is conjectured to belong to an earlier version, the word is place inside the brackets.  &#8617; Fredson Bowers, Principles of Bibliographical Description (New York: Russell &amp; Russell, 1962).  &#8617; Peter W.M. Blayney, The Texts of King Lear and Their Origins (Cambridge: Cambridge University Press, 1982); Blayney is studying the recurrence of types so chooses to transcribe both semantic meaning and what evidence he finds of the typographical habits of the compositors.  &#8617; before the newline ] a newline is a special sort of chara-  &#8617;"},{"id":"2017-07-25-what-can-digital-humanities-tell-us-about-character","title":"What can digital humanities tell us about Character?","author":"sarah-berkowitz","date":"2017-07-25 12:31:11 -0400","categories":["Grad Student Research","Research and Development"],"url":"what-can-digital-humanities-tell-us-about-character","layout":"post","content":"My part of the collaboration with James has been thinking through what this text has to tell us about “Character” as a literary category and to consider how digital tools can help modern users interact with eighteenth-century characters. There’s been a learning curve for me as I find out more and more about what digital formats can and can’t do. I think my biggest challenge has been learning to think about digital material spatially—in order for something to exist in our final product we have to think about where it goes and how to attach it. Our original plan was to preserve every page in three separate files—one with the image of the text, one with a transcription of the text, and a third that contained commentary for that page. The hope was that we could sync every file by line and thus create a no frills edition that could be accessible and transparent for all users. We’ve been forging full steam ahead with the transcriptions, and I’ve learned a great deal about how to preserve physical features on page in a digital translation. I began to realize that I think of character conceptually, not spatially, and thus finding a way to break down what this text can tell us about Character by page began to seem less and less feasible—let alone breaking it down by line! A line by line commentary is useful to explicate specific things in the text—allusions that would escape a twenty first century reader, say, or translating Latin phrases into English. Each of these things occur at a specific place in the text, and are thus well suited to line by line annotation. We’ve shied away from doing that kind of annotation—not because it’s not useful, but because it’s already been done, and done well, first by Robert Thyer for the 1759 edition and for modern audiences by Charles Daves in 1970. Butler’s work is a collection of Theophrastan Characters—a genre of writing that enjoyed a revival when Butler was writing in the late seventeenth century, but which had fallen out of fashion by the time the collection was published posthumously in 1759. Theophrastan Characters are an odd genre. They break down characters into general “types” and give a description that ostensibly describes every person that falls under that category. For instance, when Butler writes about “An Amorist” that “His Passion is as easily set on Fire as a Fart, and as soon out again.” We are meant to assume that 1) this is true of all Amorists, and 2) if we ever meet somebody whose passion is, err, easily stirred and just as quickly extinguished, that person is an Amorist. We’re used to breaking down literary characters into round and flat characters, or individuals and types. Theophrastan Characters dwell completely on the side of types, which, when you think about it is kind of nuts. We tend to think of people specifically, not generally. If I were to ask you to imagine a lawyer, you would probably think of a lawyer you know, or a famous lawyer you’ve seen in the news or in pop culture—Elle Woods, say, or Johnny Cochran. But Butler asks us to imagine a generic lawyer, someone whose “Opinion is one thing while it’s his own, and another when it is paid for,” a figure who represents all lawyers everywhere. This is familiar to us when we think about type—who doesn’t love a good lawyer joke? But it’s strange when we consider this figure as a “Character.” In literature, even type characters require a modicum of specificity, which is dictated by their literary surroundings. When a lawyer appears in Bleak House, even though that lawyer is just a flat, type character, we still imagine a single figure in Chancery during 1852 litigating Jarndyce vs Jarndyce; it could not be Elle Woods, or Johnny Cochran or your college friend who went to law school. But Butler’s characters are devoid of context—his lawyer is at once every lawyer and no lawyer at all. I’m hoping this project will be able to tell us two things. First, what tools do you use to create a general character? Just a surface read through shows us that Butler seldom uses traits or characteristics to describe his characters—they’re too individualizing. Instead he writes largely with metaphors. An Amorist is “like an Officer in a corporation” and a Lawyer is “like a French duelist”—which of course begs the question, what are the officers of corporations and French duelists like? Are there other devices that Butler uses? Does he use the same devices for every character? My plan is to run the text through Stylo to see if we can learn anything about how Butler creates his types. Second, what will it take to find examples of Butler’s characters? What does it take to fit a specific person into a general description? Could we argue that perhaps Butler is describing Johnny Cochran, even if he is not describing Elle Woods? How would we show that Cochran fits into Butler’s category? By looking at what he’s done? How he acts? Who he is? Leaving aside lawyers, would we be able to find examples of Henpect Men or Fifth Monarchy Men in today’s world—or are types too dependent on their political and cultural context to translate? Now that we have a good number of transcriptions we can begin to create a corpus, which I hope will be able to answer some of these questions."},{"id":"2017-08-03-fall-2017-gis-workshops","title":"Fall 2017 UVa Library GIS Workshop Series","author":"chris-gist","date":"2017-08-03 10:10:09 -0400","categories":["Announcements","Events","Geospatial and Temporal"],"url":"fall-2017-gis-workshops","layout":"post","content":"All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be held on Tuesdays from 3PM to 4PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up! September 12th Making Your First Map with ArcGIS Here’s your chance to get started with geographic information systems software in a friendly, jargon-free environment.  This workshop introduces the skills you need to make your own maps.  Along the way you’ll get a taste of Earth’s most popular GIS software (ArcGIS) and a gentle introduction to cartography. You’ll leave with your own cartographic masterpieces and tips for learning more in your pursuit of mappiness at UVa. September 19th Georeferencing a Map - Putting Old maps and Aerial Photos on Your Map Would you like to see historic map overlaid on modern aerial photography?  Do you need to extract features of a map for use in GIS?  Georeferencing is the first step.  We will show you how to take a scan of a paper map and align in it in ArcGIS. September 26th Getting Your Data on a Map Do you have a list of Lat/Lon coordinates or addresses you would like to see on a map?  We will show you how to do just that.  Through ArcGIS’s Add XY data tool and Geocoding (address matching), it is easy to take your tabular lists and generate points on a map. October 10th Points on Your Map: Street Addresses and More Spatial Things Do you have a list of street addresses crying out to be mapped?  Have a list of zip codes or census tracts you wish to associate with other data?  We’ll start with addresses and other things spatial and end with points on a map, ready for visualization and analysis. October 17th Taking Control of Your Spatial Data: Editing in ArcGIS Until we perfect that magic “extract all those lines from this paper map” button we’re stuck using editor tools to get that job done.  If you’re lucky, someone else has done the work to create your points, lines, and polygons but maybe they need your magic touch to make them better.  This session shows you how to create and modify vector features in ArcMap, the world’s most popular geographic information systems software.  We’ll explore tools to create new points, lines, and polygons and to edit existing datasets.  At version 10, ArcMap’s editor was revamped introducing new templates, but we’ll keep calm and carry on. October 24th Easy Demographics Need to make a quick demographic map or religious adherence?  This workshop will show you how easily navigate Social Explorer.  This powerful online application makes it easy to create maps with contemporary and historic census data and religious information. October 31st Introduction to ArcGIS Online With ArcGIS Online, you can use and create maps and scenes, access ready-to-use maps, layers and analytics, publish data as web layers, collaborate and share, access maps from any device, make maps with your Microsoft Excel data, customize the ArcGIS Online website, and view status reports. November 7th Expanding Content in ArcGIS Online You can also use ArcGIS Online as a platform to build custom location-based apps.  You can create stories and context around online maps for things like storytelling, tours or map comparisons.   Many of these applications have templates that make for easy viewing on mobile devices."},{"id":"2017-08-15-walt-whitmans-jack-engle-and-lola-montez-new-from-collective-biographies-of-women","title":"Walt Whitman's Jack Engle and Lola Montez: New from Collective Biographies of Women","author":"alison-booth","date":"2017-08-15 12:39:39 -0400","categories":["Digital Humanities","Research and Development"],"url":"walt-whitmans-jack-engle-and-lola-montez-new-from-collective-biographies-of-women","layout":"post","content":"This is the first part of a short essay I posted on the blog of Collective Biographies of Women and elsewhere on August 15, 2017.  See https://pages.shanti.virginia.edu/CBW_Blog/?p=441&amp;preview=true#_ftn6 for the entirety, with additional notes and references. In February, 2017, there was some exciting news of the kind that gratifies literary scholars everywhere.  Graduate student Zachary Turpin had discovered a lost short novel that Walt Whitman serialized anonymously in New York’s Sunday Dispatch in 1852.  The Life and Adventures of Jack Engle, as narrated by a young clerk of that name, gives impressions of New York life as Whitman experienced it before he became revered as the Good Gray Poet.  I am no Whitman scholar and have little to add to the discussion of US periodicals in the 1850s.  But as I quickly devoured the news and the novel itself, I was taken with a minor character closely related to my own research: the Spanish dancer, Inez.  Could this be a version of Lola Montez? The improbable “auto-biography” of Jack Engle now attributed to Whitman claims in the preface to be a “true story” about “familiar” people; “the main incidents were of actual occurrence,” giving “the performers in this real drama, unreal names” (Whitman, Engle 3). Clearly, the “life and adventures” of the quasi-Dickensian hero differ from Whitman’s (Walt was no orphan, for example). But Whitman might have given an unreal name to the real Lola Montez, Spanish dancer, whom I have long featured in my digital project on women’s biographies, Collective Biographies of Women or CBW (Booth).  The Irish-born adventuress who became the Countess of Landsfeld, who was buried in New York as Eliza Gilbert in 1861, has received many full-length and brief biographies. Whitman’s connection to this celebrity is not unknown, though little remarked.  She was in New York during the production of Whitman’s novella, Jack Engle .  On January 5, 1852, weeks into her first star turn in New York, she danced in Un Jour de Carneval à Seville in the role of Donna Inez (Morton 205). 1 Then, after controversial appearances in Boston, Hartford, and elsewhere, she appeared at the Broadway Theatre in Lola Montez in Bavaria, a play in five acts recounting her famous alliance with King Ludwig I and the rebellions and backlash that led to the king’s abdication (“The Danseuse, the Politician, The Countess, the Revolutionist and finally the Fugitive”; Morton 218). Whitman could easily have seen her reprise of this play at the Bowery Theatre on 28 June, or could have attended one of her benefit performances that spring, as Jack attends Inez’s benefit performance in the novella. Certainly Whitman and Montez coincided when she was back in New York six years later and they frequented Pfaff’s, Whitman’s bohemian hangout after first publication of Leaves of Grass (Lehigh University). These enterprising mid-century figures have more interesting qualities in common than coinciding in New York in certain years.  Her defiant self-making is not out of keeping with his celebration of the body.  Notably, during their shared New York-bohemian years, both published highly gendered self-help.   Manly Health and Training, an advice book by “Mose Velsor,” was serialized in the New York Atlas in 1858, and Zachary Turpin recently discovered Whitman’s authorship (Velsor).  The Arts of Beauty: or Secrets of a Lady’s Toilet (New York: Dick &amp; Fitzgerald, 1858) capitalized on Lola Montez as the famous author, drawing upon her series of popular lectures in New York, London, and elsewhere (Montez). Whitman left unacknowledged his authorship of the episodic entertainment, Jack Engle .  We might then allow a canonical poet to steer clear of a notorious entertainer whose vocational tag in the Oxford Dictionary of National Biography is “adventuress.”  To follow through on my first impulse to post that “Whitman’s Inez is Lola Montez,” it would take more than the known connections in 1858; the novel, again, was churned out topically and serially in 1852.  The Whitman scholars I contacted were less than convinced that Inez resembles Montez.  I share their opinion that Inez can be a composite of Spanish dancers Whitman might have known in New Orleans (she was there in 1853, he in 1848) or New York, as well as some features of George Sand and others whom Whitman admired.  The fictional Spanish dancer has no exalted political past and, like other characters in the novel, she derives a great deal from the conventions of romance and melodrama. But it is certain that Lola Montez was big news in New York in the early months of 1852, and there are interesting connections with Whitman’s plotline of the hero’s growing intimacy with a belle of the town. 2 Though “Spanish” connotes hot-blooded, it also connotes veiled and hard to get. The portrayal of the novel’s Spanish dancer points to significant features of the well-educated, entrepreneurial celebrity. Whitman’s version also renders the performer more bourgeois and less interesting than the real thing, downplaying Montez’s kinky suggestiveness. The differences are a measure of the fictional purpose of this minor character.  The hero rises from street life to office work and a brief escapade outside the law that ends happily, all the more because he was never in real danger of falling in love with Inez. Lola Montez in a daguerreotype (color added), 1851, by Southworth &amp; Hawes             Inez and Lola: Not Cheap You know the type: “Spanish,” “dancer”; theaters would be places to find all sorts of accessible women.  But Whitman’s Inez and the real Lola Montez might be called, in hard-boiled speak, classy dames. I intentionally hit on the sore point of typecasting, because it is almost inescapable, even in fact-based historical biography.  The surprise is not the higher quality of love object implicit in the reputations of Inez and Lola, and not even that they evince manners and education, but that they are businesswomen, capitalists.  In Jack Engle, the narrator is a reluctant young apprentice in Covert’s law office, where he notices a young lady client.  Covert is advising her on a doubtful purchase of shares (happily, it turns out she never buys into the fake scheme).  “She had the stylish, self-possessed look, which sometimes marks those who follow a theatrical life. Her face, though not beautiful, was open and pleasing, with bright black eyes, and a brown complexion. Her figure, of good height and graceful movement, was dressed in a costly pale colored silk” (27).  She calls out to the pet dog, also named Jack, who jumps up and muddies her dress.  Inez is annoyed, and then laughs it off—a preview of her responses to drooling men and to Jack himself.  In chapter six of Jack Engle, Inez appears “really fascinating” on stage in the “short gauzy costume of a dancing girl. Her legs and feet were beautiful, and her gestures and attitudes easy and graceful” (29). These characterizing details correspond somewhat with the historical Montez.  Montez was fair, with striking blue eyes, unlike Inez.  She was frequently depicted in association with animals.  Contemporaries range between calling Montez altogether beautiful, or merely fascinating with a face that was not beautiful.  But then of course there was her figure.  Accounts usually disparage Montez’s performing ability, but those who were not too scandalized avidly praised the legs and the costume.  Images in newspapers always emphasize the tiny waist, ballooning bosom, and short skirts…. Notes Read more at https://pages.shanti.virginia.edu/CBW_Blog/?p=441&amp;preview=true#_ftn6 Kirsten Greusz suggests Inez was a common name for the Spanish-beauty type, as in antebellum novels “Inez the Beautiful, or, Love on the Rio Grande” (Harry Hazel, 1846) or Augusta Evans Wilson’s “Inez, A Tale of the Alamo” (1850).  I also consulted with Ed Whitley, Ken Price, and Ed Folsom.  &#8617; Ed Folsom and Ken Price, in their article on Whitman for The Walt Whitman Archive, indicate Whitman’s affiliations with women activists Abby Price, Paulina Wright Davis, Sarah Tyndale, and Sara Payson Willis (Fanny Fern), as well as the “queen of Bohemia” Ada Clare.  CBW includes only Fanny Fern of these women, though abolitionists and activists for women’s rights do appear in some collections listed in our bibliography.  &#8617;"},{"id":"2017-08-15-welcome-senior-developer-shane-lin","title":"Welcome Senior Developer Shane Lin!","author":"jeremy-boggs","date":"2017-08-15 09:32:59 -0400","categories":["Announcements"],"url":"welcome-senior-developer-shane-lin","layout":"post","content":"The Scholars’ Lab team is thrilled to welcome Shane Lin as our new Senior Developer! Shane first joined the Scholars’ Lab as a Praxis Program graduate fellow in 2012. Since then, he’s served as a Technologist in our Makerspace, where he’s provided invaluable guidance on research and pedagogy related to desktop fabrication and physical computing. This past academic year, Shane was a Digital Humanities graduate fellow and worked on software to study networks of information exchange related to cryptography on Usenet lists. That fellowship work contributes to his doctoral work in History at UVA, and his dissertation on the history of cryptography and evolving notions of privacy since 1975. In addition to being an incredible developer and scholar, Shane is a talented photographer, and has taken nearly all the photos of our staff and students. Come by the Lab to say hi to Shane, or welcome him via email at ssl2ab at virginia.edu."},{"id":"2017-08-24-cfp-pmla-special-issue-varieties-of-digital-humanities","title":"CFP: PMLA Special Issue, Varieties of Digital Humanities","author":"alison-booth","date":"2017-08-24 09:08:30 -0400","categories":null,"url":"cfp-pmla-special-issue-varieties-of-digital-humanities","layout":"post","content":"I want to call attention to the opportunity to publish your work in the leading journal in literary studies.  Miriam Posner and I will be co-editing a special issue on digital humanities, and we very much welcome varieties of approaches as well as topics.   PMLA has a very strenuous and blind peer review process that gives ample feedback–usually, it’s well worth this feedback even if excellent work, in the end, doesn’t make the difficult cut.  But that also means, in other words, that it’s not just up to the two of us to decide what will actually appear in the journal.  We would be happy to advise on the kinds of submissions you might send in.  Feel free to reach out at booth@virginia.edu.  Here is the CFP wording that appears at the above site, where you may find instructions on how to prepare and submit the 9000-word-maximum document file. Deadline for submissions: 12 March 2018 Coordinators: Alison Booth (Univ. of Virginia) and Miriam Posner (Univ. of California, Los Angeles) Digital humanities (DH) may not be a full-fledged discipline, but it has advanced beyond “the next big thing” to become a reality on many campuses. Like many fields that have received a great deal of attention, DH derives energy from internal combustion and external friction—dissenters, supporters, and detractors see different sides of what may after all be too large a variety of practice to cohere as a field in the future. This moment, then, seems a good time to ask, What is next for DH? And what can we learn from what has come before? PMLA invites essays that will help assess the past of DH, outline its current state, and point to its future directions among diverse participants, allies, and critics. The special issue welcomes well-informed critical essays that articulate varieties of digital experience with DH as it is commonly understood and as it is practiced in a more expansive, even contested, way, including but not limited to the following topics: game studies; digital narrative and poetry; social media and blogging; digital arts, including music and theater; digital pedagogy in languages, literatures, and writing (teaching with technology, e-portfolios, immersive technology, mapping assignments); textual editing; edited digital archives of manuscript or print materials; natural language processing and textual analysis of large corpora such as historical newspapers or a genre or a literary era; prosopographies, from ancient to modern; 3-D printing or modeling; virtual reality and photogrammetry documenting cultural heritage sites or artifacts; mapping and time lines to visualize trends in cultural or literary history; issues of copyright and commercial databases; theories and histories of digital technologies and their industrial and cultural impact; the growing field of criticism on digital scholarship and institutional change; advocacy or cultural criticism oriented toward new media and transformative practice. The PMLA Editorial Board welcomes collaborative or single-author essays that take note of digital humanities of these or other varieties, whether centered on education or other spheres, whether ephemeral or long-standing. Submissions that consider a specific project should go beyond reporting on its methods and findings and emphasize its implications for digital literature and language scholarship. Of particular interest are reflections on DH as practiced beyond North America and Europe. Issues and themes might include accessibility, sustainability, standards of evidence, transforming the academic career, changing or pursuing further the abiding questions in the discipline. Histories, predictions, and manifestos may be welcome, but all essays should be accessible and of interest to the broad PMLA readership. https://www.mla.org/Publications/Journals/PMLA/Submitting-Manuscripts-to-PMLA"},{"id":"2017-08-30-2017-virginia-higher-ed-gis","title":"2017 Virginia Higher Ed GIS Meeting","author":"chris-gist","date":"2017-08-30 08:13:41 -0400","categories":["Announcements","Geospatial and Temporal"],"url":"2017-virginia-higher-ed-gis","layout":"post","content":"2017 Virginia Higher Ed GIS Meeting November 2, 2017 – 10am to 3pm (check in begins at 9:30am) Scholars’ Lab, Alderman Library - University of Virginia – Charlottesville, VA A meeting of all Virginia higher education Esri/GIS representatives and other GIS support people This meeting is for Esri designates and other GIS support staff to come together to discuss common needs and solutions.  We will kick off with a plenary talk from an Esri representative.  Then in an “unconference” format, the group will decided the topics for the remainder of the day.  Depending on interest and need, we will break into groups for further discussions. Registration (required): https://tinyurl.com/VAgis2107 Schedule 9:30am – Check in Begins 10am – Plenary Session w/ Esri Education Account Manager - Ridge Waddell (tentative) 11am – Group Topic Discussion Decision Making 11:30am – Lunch Noon – Topic Discussions – break-outs if necessary 2:45pm – Group Next Steps 3pm – Adjourn NOTE:   Lunch is being provided by the UVa Library’s Scholars’ Lab.  Because of this, we ask that everyone register in advance.  It is assumed that everyone will drive in for the day and not stay in Charlottesville.  However, we are happy to provide hotel information.  More details on parking, etc. to follow to registered participants.  If you have questions about anything, please feel free to contact Chris Gist at cgist@virginia.edu ."},{"id":"2017-09-01-digital-humanities-fellows-applications-2018-2019","title":"Digital Humanities Fellows Applications - 2018-2019","author":"brandon-walsh","date":"2017-09-01 10:36:56 -0400","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"digital-humanities-fellows-applications-2018-2019","layout":"post","content":"[Read closely: our menu options have changed. Note especially the changes to the application timeline, eligibility, and funding structure of the fellowship. Questions should be directed to  Brandon Walsh, Head of Graduate Programs for the Scholars’ Lab.] We are now accepting applications for the 2018-2019 DH Fellows Cohort! Applications are due  Wednesday, November 1st . The Digital Humanities Fellowship supports advanced doctoral students doing innovative work in the digital humanities at the University of Virginia. The Scholars’ Lab offers Grad Fellows advice and assistance with the creation and analysis of digital content, as well as consultation on intellectual property issues and best practices in digital scholarship and DH software development. Fellows join our vibrant community, have a voice in intellectual programming for the Scholars’ Lab, make use of our dedicated grad office, and participate in one formal colloquium at the Library per fellowship year. As such, students are expected to be in residence on Grounds for the duration of the fellowship. Supported by the Jeffrey C. Walker Library Fund for Technology in the Humanities, the Matthew &amp; Nancy Walker Library Fund, and a challenge grant from the National Endowment for the Humanities, the highly competitive Graduate Fellowship in Digital Humanities is designed to advance the humanities and provide emerging digital scholars with an opportunity for growth. The award provides living support in the amount of $20,000 for the academic year, as well as full remission of tuition and University fees and the student health insurance premium for single-person coverage. Living support includes wages for a half-time graduate teaching assistantship in each semester.  A graduate instructorship, particularly one with a digital humanities inflection, may be substituted for the GTA appointment based on availability within the fellow’s department. Applicants interested in such an option should indicate as such in their application and discuss the possibility in advance with Brandon Walsh . See past fellowship winners  on our People page . The call for applicants is issued annually in August. Eligibility, Conditions, and Requirements Applicants must be ABD, having completed all course requirements and been admitted to candidacy for the doctorate in the humanities, social sciences or the arts at the University of Virginia. The fellowship is provided to students who have exhausted the financial support offered to them upon admission. As such, students will typically apply during their fifth year of study or beyond for a sixth year of support.* Applicants are expected to have digital humanities experience, though this background could take a variety of forms. Experience can include formal fellowships like the  Praxis Program,  but it could also include work on a collaborative digital project, comfort with programing and code management, public scholarship, or critical engagement with digital tools. Applicants must be enrolled full time in the year for which they are applying. A faculty advisor must review and approve the scholarly content of the proposal. How to Apply A complete application package will include the following materials, all of which should be emailed directly to  Brandon Walsh : a cover letter, addressed to the selection committee, containing: a summary of the applicant’s plan for use of digital technologies in his or her dissertation research; a summary of the applicant’s experience with digital projects; and a description of UVa library digital resources (content or expertise) that are relevant to the proposed project; a  Graduate Fellowship Application Form; a dissertation abstract; and 2-3 letters of nomination and support, at least one being from the applicant’s dissertation director who can attest to the project’s scholarly rigor and integration within the dissertation. Questions about Grad Fellowships and the application process should be directed to  Brandon Walsh . Applicants concerned about their eligibility, for whatever reason, are strongly encouraged to write as well. Please note that, per University policy, a student who has undertaken affiliate status and ceased to enroll full time is not eligible to resume full-time enrollment or hold a graduate teaching assistantship.  Because GTA appointments are a component of the DH Fellowship, students who have already undertaken affiliate status are not eligible to be considered for this award."},{"id":"2017-09-06-hello-world-4","title":"Hello World!","author":"spyros-simotas","date":"2017-09-06 08:47:33 -0400","categories":["Digital Humanities","Experimental Humanities"],"url":"hello-world-4","layout":"post","content":"My name is Spyros Simotas and I am a PhD candidate at the French Department at UVa. This year, I am also a Praxis fellow at the Scholars’ Lab. In this first blog post I would like to briefly introduce myself honoring Brandon’s ice-breakers. Brandon always comes to our meetings with an ice-breaker. Here are the three we have had so far: Which is your favorite animal? Which is your favorite plant? Who would you like to have dinner with, dead or alive? My favorite animals are elephants, my favorite plants are palm trees and if I could have a meal with anyone dead or alive, I would like to have coffee with David Lynch. I like elephants because they are big, they make the sound of a trumpet and they care about each other. Despite their size, elephants do not pose a threat to other beings. They are also smart and they can paint . Has anyone ever calculated the size ratio between an elephant and an average-sized bug? Bugs are the most common wild life form we are stuck with in the industrialized and post-industrialized world. Domesticated farm animals that we use for food or pets don’t count. We are stuck with bugs both literally and metaphorically. Unfortunately, I have never seen an elephant hanging from the wall, or lurking inside a piece of software. I have seen palm trees! The reason I like them is because of their simple shape. Their trunk doesn’t branch out, it only ends with a crown of leaves, like a messy toupee. Palm trees are easy to draw. When I lived in California, I remember that sometimes, their tops would disappear in the early morning mist. Also, three cut out palm trees figure on the cover of The Cure’s Boys Don’t Cry as a fine representation of their iconic hair style. Which brings me to David Lynch and his own impeccably messy hairdo. Having begun his career with Eraserhead, it is hard to tell whose, his character’s or his own hair, is the source of inspiration for this electrified spiky hair style. Since then, he has created a lot of strange and heartbreaking characters. Joseph Merrick’s story, better known as The Elephant Man, The Staight Story, not to mention all the characters from his early 90’s TV series Twin Peaks revived recently, 25 years later, for a third and final season. Thanks to his book on meditation, consciousness and creativity, I was also introduced to TM . It is a small book, called Catching the big fish, very easy to read and highly recommended. As an ending to this post, I chose the following excerpt from the chapter “The Circle” where Lynch refers to the feedback loop between an art work and its audience. “I like the saying: “The world is as you are.” And I think films are as you are. That’s why, although the frames of a film are always the same—the same number, in the same sequence, with the same sounds—every screening is different. The difference is sometimes subtle but it’s there. It depends on the audience. There is a circle that goes from the audience to the film and back. … So you don’t know how it’s going to hit people. But if you thought about how it’s going to hit people, or if it’s going to hurt someone, or if it’s going to do this or do that, then you would have to stop making films.” 1 I think the same can be said about digital humanities. Our public scholarship, experiments, code, teaching, and service, also reflect who we are and reverberate with our audience. In our first Praxis meeting, we talked about impact, trying to pinpoint the idea of success. But ultimately, we don’t know “how it’s going to hit people.” In which case, it is always useful to remember the well-known Marshall McLuhan scheme of technology as an extension of certain urges or desires. It is important to understand what is the urge that we are trying to extend because technology, according to Jonathan Harris (who also came up in our first discussion), can have “dramatic effects” on people. That’s why, he calls for “a self-regulated ethics that comes from the mind and the heart of the creator.” Finding our own common interests and desires as a team will help us define the direction we want our project to go. At this early stage, we only know that we want to work with data from the Library, using technology to create new interactions with the archive. But it is with the principles of love, care and good intentions that we embark on this year’s Praxis adventure. David Lynch, Catching the Big Fish: Meditation, Consciousness, and Creativity, 2016.  &#8617;"},{"id":"2017-09-10-2-about-my-research-computers-and-digital-humanities","title":"About my research, computers and Digital Humanities","author":"spyros-simotas","date":"2017-09-10 17:24:42 -0400","categories":null,"url":"2-about-my-research-computers-and-digital-humanities","layout":"post","content":"In my inaugural post a few days ago, I introduced myself to the world in kind of an oblique way. Some people may wonder what I am studying or what my research interests are. This post is here to mend this omission. In large brush strokes, I will talk about my dissertation and then about some general research interests that connect me to digital humanities. Coincidentally, a brief mention of a computer prototype from the late 60’s will echo for the Praxis folks our last meeting (Sept. 5, 2017) and the lesson on the history of computers. My current project focuses on three French contemporary authors who are using new technologies to create and disseminate their work, as well as connect with their audience. More specifically, I am looking at the ways in which new technologies expand the boundaries of literature to include practices often reserved to other artistic disciplines. I am also interested in the new online literary communities clustering around the websites of my corpus and in the margins of the print and prize-driven French literature. Having escaped the pages of the book, literature meets with visual arts, with sound and performance, in new poetic hybrids. The book is always a place where textual content can return to, but it is not the only option. Moreover, various acts of transcoding, made possible through digital technologies, have liberated writing from its exclusive attachment to text. Our contemporary “associated technical milieu” has made the creative gesture a practice available to anyone with a computer connected to the Internet. “Rather than dissociating consumption from production, as did broadcast mass media (from phonography to global real-time television), today’s microtechnologies and the social networking practices they facilitate connect them: if you can use these technologies to consume, you can also use them to produce.” 1 Interestingly enough, the gap between amateurs and professionals is narrowing, which revives Jean Dubuffet’s concept of “art brut” (i.e. art made by people without formal training). Under these circumstances where everything is created by everybody, how does a contemporary author find her place? How does she define her space and the value of her work? Kenneth Goldsmith dubbed these practices “uncreative writing” and traced their origin to some French avant-garde techniques such as those invented by the Situationists ( détournement, psychogeographical drifts) and Oulipo. “Oulipo, short for Ouvroir de littérature potentielle, or ‘Workshop for Potential Literature’ was founded fifty years ago, in 1960, by the writer Raymond Queneau and the mathematician François Le Lionnais with the purpose of exploring the possible uses of mathematics and formal modes of thought in the production of new literature. Oulipo sought to invent new kinds of rules for literary composition, and also to explore the use of now-forgotten forms in the literatures of the past. ” 2 Georges Perec, one of the most popular authors among the Oulipo group ( the star! ), has experimented with algorithmic writing, imitating the inner workings of a computer program, in The art and craft of approaching your head of department to submit a request for a raise, 3 or with extreme self-imposing lipogrammatic constraints in A Void (exclusively composed of words that don’t contain the letter “e”), 4 has also written a a very brief enthusiastic text about computers. Published at a time where computers were still the size of a room, Perec anticipated their everyday personal and social use. “Why not us?” he asks, claiming a programmable machine for creative purposes at home, a place already targeted by a horde of appliances: washing machines and toasters, coffee makers and vacuum cleaners, TV sets and food processors. A dynamic medium for creative thought: the Dynabook Around the same time, at the Palo Alto Xerox PARC Alan Kay and Adele Goldberg were working on a prototype computer strikingly similar to a today’s tablet. They called it Dynabook (portmanteau for dynamic book) and they imagined it as “a self-contained knowledge manipulator in a portable package the size and shape of an ordinary notebook. Suppose it had enough power to outrace your senses of sight and hearing, enough capacity to store for later retrieval thousands of page-equivalents of reference materials, poems, letters, recipes, records, drawings, animations, musical scores, waveforms, dynamic simulations and anything else you would like to remember and change.” 5 Dynabook, unlike any other computer of its generation, was not targeting the military or corporate business. It was designed “for kids of all ages”, people who would use it to enhance their learning and creativity. I want to emphasize the last words here: “to remember and change”. If the computer was to become personal, it was not only because of its capacity to store information, archiving one’s files, and consequently exteriorizing and extending one’s memory but also by offering new techniques to process the information stored and eventually to create new. Technology has always been about extending human capabilities. “The human evolves by exteriorizing itself in tools, artifacts, language, and technical memory banks. Technology on this account is not something external and contingent, but rather an essential—indeed, the essential—dimension of the human.” 6 As a matter of fact, the idea of a mechanical memory storage was not new. Vannevar Bush in his well-known article “As we may think” published in 1945 had already introduced a mechanical memory (memex) for individual use 7 . Beyond the scope of the Universal Turing Machine –a machine that could simulate other machines– Alan Kay and Adele Goldberg’s ambition was to create a Universal Media Machine, a machine that could simulate all other media forms, from books to images to films. “For educators, the Dynabook could be a new world limited only by their imagination and ingenuity. They could use it to show complex historical inter-relationships in ways not possible with static linear books. Mathematics could become a living language in which children could cause exciting things to happen. Laboratory experiments and simulations too expensive or difficult to prepare could easily be demonstrated. The production of stylish prose and poetry could be greatly aided by being able to easily edit and file one’s own compositions.” 8 But in order to achieve this goal of becoming a ”platform for all existing expressive artistic media”, Dynabook had to exceed its function as a storing machine, by adding a new structural level on top of the hardware allowing an easy interaction with the machine. Hence, GUI was born with tools and icons that could help the user perform the same actions across applications, without needing to know the underlying programmatic commands. “Putting all mediums within a single computer environment does not necessarily erase all differences in what various mediums can represent and how they are perceived—but it does bring them closer to each other in a number of ways. Some of these new connections were already apparent to Kay and his colleagues; others became visible only decades later when the new logic of media set in place at PARC unfolded more fully; some may still not be visible to us today because they have not been given practical realization. One obvious example of such connections is the emergence of multimedia as a standard form of communication: web pages, PowerPoint presentations, multimedia artwork, mobile multimedia messages, media blogs, and other communication forms which combine multiple mediums. Another is the adoption of common interface conventions and tools which we use in working with different types of media regardless of their origin: for instance, a virtual camera, a magnifying lens, and of course the omnipresent copy, cut and paste commands. Yet another is the ability to map one media into another using appropriate software—images into sound, sound into images, quantitative data into a 3D shape or sound, etc.—used widely today in such areas as DJ/VJ/live cinema performances and information visualization. All in all, it is as though different media are actively trying to reach towards each other, exchanging properties and letting each other borrow their unique features. ” 9 The success of the personal computer was therefore due to its structural coupling with software that led –so far– to three major shifts in the way we interact with media. Word processors to movie editors, allowed the user to mix, juxtapose, cut and paste, alter, and eventually produce new media. Using the same machine to perform changes in the stored contents was an empowering new form of grammatization. Return to kindergarten I borrow the concept of grammatization from Bernard Stiegler. Derrida’s former student, Stiegler calls grammatization every flow that becomes a process through a series of discrete marks, grammés, that can form a code (grammar) and can be endlessly reproduced in all sorts of combinations. Writing, for example, is the grammatization of speech and it is made possible by the invention of the letters ( grammata ) of the alphabet. Alphanumeric linear writing, up until personal computers came along, was the dominant form of recording, from facts (history) to thoughts and ideas (literature). So much so that the activities of learning to read and write were the main literacy focus of a certain humanistic tradition, from grade school to the academy. In his seminal book Does Writing Have A Future?, Vilém Flusser speculates on the disruption of this tradition brought forth by the computers and their new ways of writing through digital recording and digitization. Without discarding the value of the alphanumeric writing he embraces the possibility of new forms of writing that could lead to a progressive replacement of “the alphabet or Arabic numerals”. What was once written can now be conveyed more effectively on tapes, records, films, videotapes, videodisks, or computer disks, and a great deal that could not be written until now can be noted down in these new codes. … Many people deny this … They have already learned to write, and they are too old to learn the new codes. We surround this … with an aura of grandeur and nobility. Flusser foresees with a great clarity what is yet to come when he publishes his book in 1987. What may seem as a radical stance, results from his position not to resist or reject the new technologies, but to discover their creative and pedagogical potential altering and adding new avenues to the the millennia old practices of reading and writing. But the newness of these tools, their sometimes complex inner workings call for a return to kindergarten. We have to go back to kindergarten. We have to get back to the level of those who have not yet learned to read and write. In this kindergarten, we will have to play infantile games with computers, plotters, and similar gadgets. We must use complex and refined apparatuses, the fruit of a thousand years of intellectual development, for childish purposes. It is a degradation to which we must submit. Young children who share the nursery with us will surpass us in the ease with which they handle the dumb and refined stuff. We try to conceal this reversal of the generation hierarchy terminological gymnastics. While we’re about this boorish non-sense, we don’t call ourselves Luddite idiots but rather progressive computer artists. 10 Isn’t it the “digital turn” that Flusser anticipated with his “infantile games with computers”? And isn’t it Flusser’s kindergarten spirit that lives in labs and DH centers across the academy? Similarly, most recent “making turn” also happens in the same centers and labs. ”As the historian David Staley explains, the “maker turn” introduces “an approach to the humanities that moves our performances off the page and the screen and onto the material world, a hermeneutic performance whereby humanists create non-textual physical objects.” 11 Inspired by Patrick Jagoda’s recent article on “Critique and Critical Making”, this year’s Praxis cohort is set to explore the intersection of DH and the bricolage of physical computing. Taking the cue from Pierre Bayard’s How to talk about books you haven’t read, 12 we have been wondering “how to make books you haven’t read talk!” But more about it in the next post. Stay tuned! Works Cited Excerpt from Mark B. N. Hansen’s introduction to Bernand Stiegler’s chapter on Memory published in W. J. T Mitchell et Mark B. N Hansen, Critical Terms for Media Studies (Chicago; London: The University of Chicago Press, 2010).  &#8617; David Bellos in his introduction to Georges Perec’s The art and craft of approaching your head of department to submit a request for a raise, (London; New York: Verso Books, 2011).  &#8617; Georges Perec, The art and craft of approaching your head of department to submit a request for a raise, trad. par David Bellos (London; New York: Verso Books, 2011).  &#8617; Georges Perec, A Void (London: Harvill, 1994).  &#8617; Lev Manovich, Software Takes Command: Extending the Language of New Media, International Texts in Critical Media Aesthetics 5 (New York, NY: Bloomsbury, 2013).  &#8617; Mark Hansen’s Introduction to Bernard Stiegler’s article on Memory, in W. J. T Mitchell et Mark B. N Hansen, Critical Terms for Media Studies (Chicago; London: The University of Chicago Press, 2010).  &#8617; Consider a future device for individual use, which is a sort of mechanized private file and library. It needs a name, and, to coin one at random, “memex” will do. A memex is a device in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory.  &#8617; Personal Dynamic MediaAlan Kay, Adele Goldberg http://www.vpri.org/pdf/m1977001_dynamedia.pdf   &#8617; Lev Manovich, Software Takes Command: Extending the Language of New Media, International Texts in Critical Media Aesthetics 5 (New York, NY: Bloomsbury, 2013). Emphasis mine.  &#8617; Vilém Flusser et Mark Poster, Does Writing Have a Future?, Electronic Mediations, v. 33 (Minneapolis: University of Minnesota Press, 2011).  &#8617; Patrick Jagoda, « Critique and Critical Making », PMLA 132, no 2 (1 mars 2017): 356‑63, doi:10.1632/pmla.2017.132.2.356.  &#8617; Pierre Bayard, How to talk about books you haven’t read (New York, NY: Bloomsbury USA : Distributed to the trade by Holtzbrinck Publishers, 2007).  &#8617;"},{"id":"2017-09-12-etcrc-local","title":"/etc/rc.local","author":"christian-howard","date":"2017-09-12 09:32:11 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"etcrc-local","layout":"post","content":"Hello again, my fine digital-humanist friends! It’s a delight to be back in the Scholars’ Lab this year! For those who don’t know me, my name is Christian Howard, and I am a PhD Candidate at UVA in English literature and one of the 2017-2018 Praxis Fellows. If you do happen to know me, you might also know that I was fortunate to work in the Makerspace of the Scholars’ Lab last year. In any case, I’m excited to combine the knowledge that I gained there working on hands-on, material projects with finer computer skills and the even greater conceptualizations into which I expect our Praxis team will delve. I’ve recently been rereading Johanna Drucker’s Graphesis: Visual Forms of Knowledge Production, and I want to reflect briefly on one of Drucker’s points, which I think is especially central to our Praxis team this year. Drucker brilliantly exposes “data” as constructs, constructs that cannot “pre-exist their parameterization.” As such, Drucker opts for the alternative term, “capta,” stating: “ Data are capta, taken not given, constructed as an interpretation of the phenomenal world, not inherent in it” (128). Capta comes from the Latin verb capio, capere, which, translated literally, means “to capture, take, seize.” Yet in a more figurative sense, capere could also mean “to take in, understand.” It is partly because of this pun that I find Drucker’s redefinition particularly apt, for it is precisely the act of “capturing” information that facilitates our understanding of that information. In other words, every decision to define the parameters under which “data” will be taken is itself an interpretive strategy. So what does this mean for humanists, and digital humanists in particular? I’ll quote Drucker again, this time at length: “To expose the constructedness of data as capta a number of systematic changes have to be applied to the creation of graphical displays. That is the foundation and purpose of a humanistic approach to the qualitative display of graphical information. That last formulation should be read carefully, humanistic approach means that the premises are rooted in the recognition of the interpretive nature of knowledge, that this display itself is conceived to embody qualitative expressions, and that the information is understood as graphically constituted ” (128-129). It is this recognition – namely, in the fundamentally interpretive nature of data-as-capta – that distinguishes the humanities as a discipline. As a Praxis cohort, we are still working to define the shape that our project will take; nonetheless, in developing our charter or mission statement, we have unanimously agreed that transparency is of the utmost importance to us. As such, we are committed not only to sharing the result of our collaboration with the public, but also to showing the processes through which our project develops, thereby enabling anyone to trace the interpretations and assumptions underlying our own work. Well, that’s all the heavy-lifting for today. For those of you who found this introductory post too lengthy, I’ve provided a handy summary for you below: TL;DR: Born at a young age, I have pursued my education in order to justify my caffeine-dependency. Most recent greatest achievement? I’ve just beaten my all-time personal record of most consecutive days lived! Time to celebrate with some coffee and chocolate. Drucker, Johanna.  Graphesis: Visual Forms of Knowledge Production . Cambridge: Harvard University Press, 2014."},{"id":"2017-09-18-welcome-new-dh-developer-zoe-leblanc","title":"Welcome new DH Developer Zoe LeBlanc!","author":"amanda-visconti","date":"2017-09-18 04:51:03 -0400","categories":["Announcements"],"url":"welcome-new-dh-developer-zoe-leblanc","layout":"post","content":"We are delighted to announce that Zoe LeBlanc has accepted our DH Developer position ! Zoe rose to the top of an extremely strong pool of over 60 applicants. A History ABD at Vanderbilt University, she focuses on post-colonialist movements and media in Cairo and other capitals. She brings solid technical experience in the areas of front-end web design, text and image analysis, and mapping and data visualization, with skills including React, Redux, Elixir, and Postgres, and fluency in French and Arabic. Zoe is a rising junior DH scholar, presenting on network analysis at a well-attended panel at DH2017 in Montreal, as well as through a DH2017 poster on an archival research app she learned to build in response to archival research challenges. Her particular expertise and passion for making technically difficult DH methods accessible and enjoyable to all complements the SLab’s emphasis on pedagogy and mentorship. She balances the SLab’s literature scholars and complements our history scholars, both diversifying our areas of work to the Middle East and adding new expertise in archival research in countries with different archival practices and challenges from the U.S. Come by the Lab once Zoe joins us in mid-October to say hi!"},{"id":"2017-09-19-crafting-our-charter-praxis-program-2017-2018","title":"Crafting Our Charter - Praxis Program 2017-2018","author":"christian-howard","date":"2017-09-19 07:28:11 -0400","categories":null,"url":"crafting-our-charter-praxis-program-2017-2018","layout":"post","content":"As a historian, when I think of charters, the first things I think of are royal charters. The first result when you Google charter, on the other hand, is Charter Telecommunications Company because of course . But as members of the new Praxis Fellowship cohort, my fellow fellows and I tried to chart (I’m sorry) a very different path. The result of our work, The Praxis Charter, 2017-2018, is the first thing we ever created together. http://praxis.scholarslab.org/charter/charter-2017-2018/ Transparency is one of our core values, so I am going to use this post to reveal the process by which we made this document. Our charter’s first draft was written in a jam session in a Scholars’ Lab meeting room, and the fact that we are all teachers was readily evident. We privately brainstormed, we paired and shared those ideas, and then we had a class discussion with Christian at the technological helm. I often think of grad school as a lesson in liminality. “Piled Higher and Deeper” by Jorge Cham www.phdcomics.com That was on full display as we drew on the techniques we use to facilitate classroom discussions to jumpstart our own collaborative work. The liminality of grad school isn’t always to its credit, but in this case, the results were lovely. As a teacher and a historian of education, I spend a lot of time thinking about pedagogy. The pedagogy modeled here made my heart happy! We melded the skill sets of both teachers and students pretty seamlessly to create a productive partnership. Our conversation always seemed to come back to values. Values are, I think, the core of this document. Of course, for every positive value, there is an equal and opposite disvalue. The opposite of humility is egotism. The opposite of flexibility is rigidity. The opposite of transparency is obfuscation. I think this connects to a comment my fellow Torie made, that writing this charter was almost cathartic, because we could list every problem we had encountered with group work and essentially say: not that . This, of course, points to the idea of conflict. As our joyful leader Brandon Walsh noted, past Praxis cohorts have tended to avoid naming conflict in their charters in the hopes that their silence would prevent it from ever rearing its ugly head. Think of conflict as the he-who-must-not-be-named of group work, if you will. Ignoring conflict didn’t really work out for the Ministry of Magic though, and I doubt that the academy fairs much better. My hope is that by setting out clear goals, values, and strategies for coping with conflict we will enable our future selves to handle disagreements with aplomb and grow from them, rather than shrink from them. Perhaps the most radical value embodied in our charter is our commitment to “the creation of a participatory democracy.” Participatory democracy is an idea coined by one of my favorite historical figures, civil rights and feminist icon Ella Baker. Participatory Democracy embraces two ideas, “ a decentralization of authoritative decision-making and a direct involvement of amateurs or non-elites in the political decision-making process. “ Participatory democracy seems like the perfect fit for the Praxis Program as we are all relative amateurs in the digital humanities, and we have been given the task of working and learning together. It also just seems to fit our collective personality. When we talked about past Praxis strategies, we decided we didn’t want to divide and conquer the tasks ahead like many previous years had. We wanted to work on individual elements of our project together so that we could get the most out of our training. This would also allow us to commit to a truly shared vision. In so many ways, a charter is a reminder of our deeply held values. We all carry around ideals of honesty and creativity, kindness and diversity, but writing out a charter makes you actually reflect on those values and why you hold them dear. Writing a charter allows you to reflect on what it is you like about collaborative work - and what it is you don’t, and then make a promise to yourself and to others to try and embody the best of what collaboration has to offer. As for our radical experiment in participatory democracy, I can already hear people asking, is that practical? The true answer is: I don’t know. But Praxis seemed like just the place to try it out."},{"id":"2017-09-21-how-to-make-books-you-havent-read-talk","title":"How to make books you haven’t read, talk.","author":"spyros-simotas","date":"2017-09-21 07:02:18 -0400","categories":["Digital Humanities"],"url":"how-to-make-books-you-havent-read-talk","layout":"post","content":"As promised in my previous post, here is an idea for this year’s Praxis Program. It is uncertain at this early stage of brainstorming whether it will be retained as the one uniting everybody’s creative forces and ingenuity, but I believe it has a lot of potential of unfolding into a project where everybody’s common interests meet: library’s holdings, global culture and world languages, power and inequality, literature and sound. At its core, it is as humanistic as it can be, and its execution requires the use of common digital humanities and critical making techniques, that we are here to train for. But before I get to the idea, let me take you to a journey where books are no longer written, nor pressed into rectangular objets made out of ink and paper and they are by no means meant to be read. The end of books More than a hundred years ago, at the turn of the 19th century, Octave Uzanne, a French bibliophile and journalist, conceived The End of Books 1 ( audio file ) in one of his most cited short nonfictional works. His prediction, mid way between pure speculation and prophecy was that the new media of his time, the rise of electricity and phonography, would soon replace the old Gutenberg’s invention. “I do not believe (and the progress of electricity and modern mechanism forbids me to believe) that Gutenberg’s invention can do otherwise than sooner or later fall into desuetude as a means of current interpretation of our mental products.” “our grand-children will no longer trust their works to this somewhat antiquated process, now become very easy to replace by phonography”. The leap was enormous. Uzanne’s reverie, not only depicted books as a dying medium with no future, but shifted their inherent mutism to the vivacity of the audio recording. It is important to notice, and Uzanne himself insists on the matter, that books don’t have to put a strain on our eyes and bodies anymore, keeping us immobile, squint and hunched over the small print of the page. “You will surely agree with me that reading, as we practice it today, soon brings on great weariness; for not only does it require of the brain a sustained attention which consumes a large proportion of the cerebral phosphates, but it also forces our bodies into various fatiguing attitudes.” “Our eyes (…) have been too long abused, and I like to fancy that some one will soon discover the need there is that they should be relieved by laying a greater burden upon our ears.” What is in the book that can not live in another recorded medium? Ideas, scientific knowledge and scholarship, literary work, can all exist in an audible format. For Uzanne, phonography not only can afford the contents of the book, but this change of reception through another sensory organ, the ear instead of the eye, has clear benefits for the overall mental and physical health of the listener. “Hearers will not regret the time when they were readers; with eyes unwearied, with countenances refreshed, their air of careless freedom will witness to the benefits of the contemplative life.” “At home, walking, sightseeing, these fortunate hearers will experience the ineffable delight of reconciling hygiene with instruction; of nourishing their minds while exercising their muscles for there will be pocket phono-operagraphs, for use during excursions among Alpine mountains or in the canyons of the Colorado.” It is obvious that Uzanne not only imagined the audiobook but also a prototype portable device that would play it back. It is worth noticing then, that before Sony’s Walkman, or Apple’s iPod “a pocket apparatus (…) suspended by a strap from the shoulder” was not designed to accommodate “a thousand songs in your pocket” (Steve Jobs) but a portable device to liberate the bibliophile’s body from the immobility of the study room. Uzanne’s intuitions, albeit prophetic for the most part, failed to envision a future where both printed and audiobooks exist without posing a threat to each other. New technologies first thought as replacement to the old ones end up coexist offering alternative options of engagement. Audiobooks didn’t replace print books and certainly listening didn’t replace reading. The impossible task of reading However, reading, despite being an unhealthy activity as we just saw, heavily taxing one’s eyes and body, forcing its muscles to atrophy, is an overall impossible task. Too much to read, too little time. “When Brandon was entering graduate school, an older student once summed up one of life’s problems as a sort of equation: There is an infinite of material that one could read. There is a finite amount of time that you can spend reading. The lesson was that there are limits to the amount of material that even the most voracious reader can take in. One’s eyes can only move so quickly, one’s mind only process so much. This might sound depressing, as if you’re playing a losing game. But it can also be freeing: if you cannot read everything, why feel the need to try to do so? Instead, read what you can with care.” 2 The sentiment is not new. Today’s readers may feel completely crushed under the weight and the abundance of reading material, but so did the erudite from the early modern era. Compiling methods (common place books, anthologies, florilegia) were thus put in place to compress books within books and save the reader from the folly of having to read everything in extenso . Pierre Bayard in his first chapter of his now classic How to Talk About Books You Haven’t Read 3 addresses the issue by suggesting a few methods of non-reading. “Reading is first and foremost non-reading. Even in the case of the most passionate lifelong readers, the act of picking up and opening a book masks the countergesture that occurs at the same time: the involuntary act of not picking up and not opening all the other books in the universe.” 4 The paradoxical nature of reading as non-reading, leads Bayard to an important insight: the contents of the book don’t really matter. They can be interchangeable even. 5 After all, one’s memory of the books read, will inevitably boil its intricate details to a mush. “ The interior of the book is less important than its exterior, or, if you prefer, the interior of the book is its exterior, since what counts in a book is the books alongside it.” 6 Don’t lose the forest for the trees is what Bayard basically saying. A library is a whole ecosystem that invites the “truly cultured to tend toward exhaustiveness rather than the accumulation of isolated bits of knowledge.” 7 There is a whole network of connections between one book and the totality of books which is undermined when the attention is only given to each book’s singularities. “It is, then, hardly important if a cultivated person hasn’t read a given book, for though he has no exact knowledge of its content, he may still know its location, or in other words how it is situated in relation to other books.” 8 This “topographical approach” that values location over content, or content as location, and the nature of connections that one book enjoys with others is what Bayard calls collective library . Books are in dialogue with each other and the way to get even a faint echo of their conversations is movement. Moving around the library is preferable to the stasis over one particular location-book. The invention of hypertext as “an ongoing system of interconnecting documents” (Ted Nelson) was an attempt to establish the dynamic of movement to what had long seen as a static material. Only, one, still has to read… The problem with languages. Books are not only innumerable, there are also written in different languages, which is another reason inhibiting from reading them (all). Before I move on, I would like to share with you a 1m07” clip from a recent episode of Twin Peaks The Return. In this scene special FBI Agent Gordon Cole, (played by David Lynch himself) after sending off his date (a French woman) to the bar, turns to his colleague Albert with a joke… https://www.youtube.com/watch?v=BemdreTqBA0 Lynch leaves the question linger in silence. It is a way to acknowledge the alarm that just went off: over six thousands languages! 9 Despite their exact number, languages exist, people who speak them exist, and their world views and perspectives are as worthwhile as any. Languages are not a property of any particular population living on a specific location, they spill over borders, they travel like wild fire. But they are also used as means of oppression, when powerful cultural systems impose their monolingualism and consequently their world view to others. What does that make of the idea of the collective library ? Who’s part of the collective and who’s not? How far can this collective be stretched to be a really inclusive collective and not just a club for the happy few? It seems to me that World Languages are the blind spot in the discussions about global culture and diversity, about inequalities and web accessibility. Who gets the joke when it’s a word play intended to the speakers of the same language? Indeed, nobody laughs. Listen, with all your ears, listen! After this rather long detour, I am finally arriving to my proposal. I think it is time to consider Uzanne’s sensorial shift from the eye to the ear and pair it with Bayard’s dialogical relations of books in the collective library . Technically speaking this coupling would take the form of an exploratory device (or app). Its user would then be able to explore the stacks, through a bibliophilic auditory flânerie . Following a fortuitous trajectory inside the library 10, the user will not only experience the books coming to life, but also the vast range of world languages in which these books are written 11 . As the user moves from one section of world literature to another, preinstalled sensors would capture her movement and send new content to her device, interfering with, or completely altering her soundscape. But, it is preferable not to discuss such technical details extensively, without a working prototype at hand. Finally, the purpose of such a device, as mentioned before, is to offer a new experience of exploring the library other than having to look for a specific book, related to a specific topic often suggested by some course syllabus. I want to believe that a university library has much more to offer than a business-like exchange model. Despite not having an immediate benefit for the user, such a serendipitous bibliophilic auditory flânerie through a vast range of world languages may function as catalyst for awakening the desire to learn a new language. 12 And when the user decides to stop her flânerie she will receive a prompt asking her if she wants to borrow a book, most likely one of which she has never heard before. Octave Uzanne, The End of Books, consulté le 21 septembre 2017, https://ebooks.adelaide.edu.au/u/uzanne/octave/end/.  &#8617; « Distant Reading », Introduction to Text Analysis, consulté le 15 septembre 2017, https://walshbr.com/textanalysiscoursebook/book/reading-at-scale/distant-reading/.  &#8617; Pierre Bayard, How to talk about books you haven’t read (New York, NY: Bloomsbury USA, 2007).  &#8617; Ibid. 17  &#8617; Pierre Bayard, Et si les oeuvres changeaient d’auteur ? (Paris: Les Editions de Minuit, 2010).  &#8617; Bayard, How to talk about books you haven’t read., 30  &#8617; Ibid., 27  &#8617; Ibid., 30  &#8617; A more comprehensive view on the languages spoken today in the world is offered by Ethnologue: « How many languages are there in the world? », Ethnologue, 3 mai 2016, https://www.ethnologue.com/guides/how-many-languages.  &#8617; The extent of fortuity can also be configured with a set of questions, allowing the user to add her parameters to the game.  &#8617; For prototyping purposes public domain librivox recordings will be used.  &#8617; I happen to fall in love with French from something I heard on the radio.  &#8617;"},{"id":"2017-10-16-isam-2017-libraries-are-for-making","title":"ISAM 2017 - Libraries are for making","author":"ammon-shepherd","date":"2017-10-16 10:47:00 -0400","categories":["Experimental Humanities","Makerspace","Research and Development"],"url":"isam-2017-libraries-are-for-making","layout":"post","content":"I recently participated in the International Symposium of Academic Makerspaces. I presented a paper, co-authored by Jennifer Grayburn (formerly a Makerspace Technologist, and now at Temple University’s Digital Scholarship Center ). I present here the slides and talking notes of the 7 minute presentation, and a link to the full paper [ Link to PDF ]. Good morning, and thank you for coming. My name is Ammon Shepherd. My paper, co-authored by Jennifer Grayburn, looks at how libraries are uniquely suited to provide makerspaces for traditionally book-bound disciplines. Jen Grayburn works at the Digital Scholarship Center, located in Paley Library at Temple University in Philadelphia. I am located at the Scholars’ Lab in the Alderman Library at the University of Virginia in Charlottesville, Virginia. We both come from humanities backgrounds, so this paper is light on empirical research and heavy on anectodal evidence, but we are both working on tracking data and analyzing that with research questions in mind. To wit, our main question we sought to address with this paper is, How can we get more humanities researchers into our library makerspaces? In the paper we posit that libraries fulfill a unique roll at universities because they are typically departmentally agnostic. Libraries, in general, cater to all faculty, staff, students, and even members of the community. With that in mind, Jen and I looked at both of our spaces (Yet Another Cross-space Comparison) and found four comparable attributes of how we attract and support research from humanities researchers. In this paper we look at four attributes: accessibility, contextualization, collaboration, outreach Both our spaces seek to piggy back on the aforementioned phenomenon of Libraries as an academically neutral space. But adding technology normally only seen in the STEM fields proves to be a mental barrier to humanities researchers. To address this, both spaces first sought to break down any physical barriers to entry. We are both located in open spaces in the main library on campus. The Scholars’ Lab space is in a prime study and group-work area with great natural lighting. Physically open access is relatively easy to address, but mental barriers take more detailed planning. The remainder of the comparison points, and some take aways at the end, help to address the issue of breaking down mental barriers. The major issue facing humanities research is the mental frustration with technology; usually the reason they give for picking the humanities in the first place. How then to ease that burden? Both the DSC and SLab are staffed with individuals from very diverse backgrounds and skill levels. The DSC has full-time library staff, post-docs and graduate students from departments ranging from science, architectural history, and engineering to business. The SLab has 3 full-time staff with library and history degrees, and graduate and undergraduate paid, part-time student employees from language, engineering and chemistry backgrounds. This broad academic background encourages students from all fields to use our spaces. One anecdotal account comes from a bio-med student who felt more comfortable prototyping in our space because she didn’t feel an inferiority complex. She probably thought, they’re just historians, what do they know? :) Encouraging collaboration enriches both staff and users, and both spaces encourage staff to work on personal research and collaborate with others. The DSC partnered with the Ginsburg Library to offer free 3D printing for research, educational or clinical purposes. The 3D print of a pelvis from a CT scan is such a result. They also partnered with the Center for Advancement of Teaching to provide grants to faculty ranging from $500-$3500. The SLab provides short term fellowships to humanities grad students for prototyping ideas. We have provided support for students to use 3D prints for presentations, and are helping a cardiovascular medical researcher print exercise equipment for mice. More examples are in the paper. Collaboration with all departments expands the usefulness of the space beyond the physical location and engages the entire university, even humanities scholars. Finally, outreach plays a major role in attracting any makers, especially interested humanities scholars. The DSC provides workshops and training for all their equipment. They also encourage staff and users to blog about successes and failures, and to publish results in journals. The SLab holds workshops, has a prominent display case, and is a major stop on all the mandatory freshman library tours. We also encourage users and staff to post their making on our blog. Publishing about the making, both successes and failures, encourages others to try; especially humanities researchers who may be afraid to fail. I would like to conclude with four take aways that can help libraries make their makerspaces more approachable to humanities researches. 1st, have a passionate staff person in the makerspace. Skill level is less important, you can hire out or encourage student volunteers to bring in skill. But without excited library staff support, the space will flounder. 2nd, make the space physically accessible. Also think about how you can address mental and social barriers. 3rd, provide incentives to use the technology and space. Team up with Teaching and Learning centers. Provide free supplies and/or money. Finally, use your library liaisons. They know your faculty and students, and they can proselytize the space. Bring them in for training on the equipment. Work with them on projects so they know what the space can provide and the tools can do."},{"id":"2017-10-26-gis-day-wednesday-november-15","title":"GIS Day - Wednesday, November 15","author":"chris-gist","date":"2017-10-26 09:41:34 -0400","categories":["Announcements","Geospatial and Temporal","Technical Training","Visualization and Data Mining"],"url":"gis-day-wednesday-november-15","layout":"post","content":"Mark your calendars, Wednesday, Nov. 15 is GIS Day (http://www.gisday.com/).  To celebrate, all are invited to the University of Virginia Library’s Scholars’ Lab for an afternoon of events. 1PM – Presentation: ArcGIS Pro for ArcMap Users\n2PM – Lightning Round Talks\n3PM – GIS Day Cake Location: Alderman Library Electronic Classroom (ALD 421, just off Scholars’ Lab) Presentation: ArcGIS Pro for ArcMap Users At 1PM, join us for a session on making the switch to ArcGIS Pro by our own Drew Macqueen.  This is a quick and dirty overview of the major differences between ArcMap and ArcGIS Pro. Accept your fate as we delve into the future of desktop GIS. Spoiler alert, it’s totally worth it! Lighting Talks Starting at 2PM, our annual tradition of lightning talks continues.  If you have never seen lightning round talks, they can be pretty entertaining: a rapid fire succession of speakers given a set, short amount of time and PowerPoint slides.  In previous years, many great presenters have shown the incredible breadth of disciplines and fields in which GIS is used in meaningful ways. We encourage everyone, including students (UVa, PVCC and high school), researchers and practitioners in the greater Charlottesville community to contribute. In this year’s round, each speaker will be given five to ten minutes (depending on number of presenters) with a maximum of ten slides.  It is a fairly easy task to create and give a lighting round talk.  Help make this year’s event special by participating in the talks.  You can present on anything spatially related you like.  It could be about a project you have worked on, things going on at your office or just something of personal interest. If you have any interest in participating in the lightning round talks, please email us at uvagis@virginia.edu as soon as possible. GIS Day Cake Another great tradition continues.  Please join us for the GIS Day cake unveiling and partake in the feeding frenzy."},{"id":"2017-11-14-3d-printed-enclosures-with-openscad","title":"3D Printed Enclosures with OpenSCAD","author":"chris-gist","date":"2017-11-14 07:35:03 -0500","categories":["Digital Humanities","Grad Student Research","Makerspace","Technical Training"],"url":"3d-printed-enclosures-with-openscad","layout":"post","content":"This is a tutorial on how to use OpenSCAD to design a 3D object via code instead of using a WYSIWYG editor like Tinkercad, Fusion360, etc. We are currently creating a customized media player to allow people to interact with MP3 artifacts. We’ve been working in Python to prepare the audio and wanted to generate the enclosure programmatically as well, ideally using open source software. OpenSCAD is a great open source solution for CAD and 3D printing projects. Modules In OpenSCAD, you can quickly build duplicates of small parts into more complex designs using “modules”. By assigning variables to parameters, you can vary the size and location of these objects easily. Modules also help break a larger job into more manageable parts and keep the code nice and clean. The four modules below construct the main body of the enclosure, arrange the holes in the enclosure for our electronic components, add a texture to the enclosure, and assemble all the pieces together. After calling those four modules, all that is left to do is split the enclosure in two and render the halves as separate STL files for printing. Main Enclosure Body &lt;code&gt;\n/* This module constructs the main body of the enclosure. First, we name the module: */\n\nmodule enclosure() {\n\n/* Next, we call the difference function. This specifies that we will be subtracting the second object we call from the first. We will use this to make our cube hollow. */\n\ndifference() {\n\n/* The first object will be our main cube. to give the cube rounded edges, we call minkowski, which will trace the shape we specify around the edges. We will use a sphere, so that the hard edges of the cube will take on the shape of the sphere. */\n\nminkowski()\n{\n\n/* Lastly, I am calling difference again here because I wanted to add a small indentation to the bottom of the cube so that it would be more comfortable to hold. Again, difference subtracts the second object from the first, so here, we see a cube; and then an offset (translated), smaller cube(); */\n\ndifference()\n{\ncube([60,40,15], center=true);\ntranslate([-15,-10,-8])\ncube([30,20,1.5]);\n};\n\n/* Having constructed the main box, we can now specify the size of the sphere that we will use to round the edges. */\n\nsphere(2);\n};\n\n/* Having specified our main enclosure body with rounded edges and an indentation on the bottom, we finally hollow it out. */\n\ncube([61.5,41.5,16], center=true);\n}\n}\n&lt;/code&gt; Making Holes for Electronics Components The second module creates all of the holes that we will place in the enclosure for our electronics components. &lt;code&gt;\nmodule enclosureHoles() {\n\n/* This section of the code constructs all of the independent holes and joins them into a uniform object. */\n\nunion() {\n\n// Screen\ntranslate([-13.75,-11,5.5])\ncube([27.5, 19.375, 5]);\n\n// LED Backlight\ntranslate([-14.6875,10,5.5])\ncube([29.375, 8.75, 5]);\n\n// Volume Pot\ntranslate([0,-15.75,5.5])\nrotate([0,0,0])\ncylinder(r=1.25, h=5);\n\n// Pushbutton #1\ntranslate([21.5,0,5.5])\nrotate([0,0,0])\ncylinder(r=4.75, h=5);\n\n// Pushbutton #2\ntranslate([23.5,-12,5.5])\nrotate([0,0,0])\ncylinder(r=4.75, h=5);\n\n// Pushbutton #3\ntranslate([-21.5,0,5.5])\nrotate([0,0,0])\ncylinder(r=4.75, h=5);\n\n// Pushbutton #4\ntranslate([-23.5,-12,5.5])\nrotate([0,0,0])\ncylinder(r=4.75, h=5);\n}\n}\n&lt;/code&gt; Adding a surface texture The next module creates a texture on the surface of our enclosure from an image file. We wanted to use an image of JPEG artifacts for our project, but you could use anything you’d like, or skip this step entirely. Be sure to keep your PNG files very simple here, otherwise you will run into problems when trying to render. When our PNG file was 31kb it took many hours to render and resulted in a huge STL file that was impossible to print. We needed to get our PNG down to 6kb to make it render in a reasonable amount of time. This resulted in a 5mb STL file. Still kind of big, but reasonable. Below, we call the translate() function so that it sits right on the surface of our enclosure. &lt;code&gt;\nmodule texture() {\ntranslate([0,0,9])\nscale([.41,.36,.006]) surface(file=\"/Users/YourUsername/Path/To/Your/File/fileName.png\",\ncenter=true);\n}\n&lt;/code&gt; Bringing it all together The final module assembles the previous three modules together. &lt;code&gt;\nmodule concat() {\n\n/* Difference subtracts the second object from the first */\n\ndifference() {\n\n/* Our first object is the Union of two objects. Here, union attaches the texture to the enclosure. */\n\nunion() {\ntexture();\nenclosure();\n};\n\n/* the semicolon signals that that is a complete object. Now the second object is the one we made from the various holes. */\n\nenclosureHoles();\n}\n}\n&lt;/code&gt; Rendering and Printing Now all we have to do is render using concat() and save as an STL! &lt;code&gt;\n/* To render the entire design, run: */ \n\nconcat();\n\n/* To actually print, we’ll need to render it in two separate halves which we will attach later. So, comment out the above concat() command and instead run the below code to render the top only */\n\ndifference() {\nconcat();\ntranslate([0,0,-8.5])\ncube([65,44,2], center=true);\n}\n\n/* then, comment the above out and run the following code to render the bottom only */\n\ndifference() {\nconcat();\ntranslate([0,0,2])\ncube([65,44,16], center=true);\n}\n&lt;/code&gt; That’s all there is to it! With the two halves rendered, all you have to do is save them as STL Files and then use your favorite 3D printing prep software to print. If you’d like to learn more about OpenSCAD, here is a link to a great cheat sheet ."},{"id":"2017-11-15-measured-unrest-in-the-poetry-of-the-black-arts-movement","title":"Measured Unrest in the Poetry of the Black Arts Movement","author":"ethan-reed","date":"2017-11-15 10:07:10 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"measured-unrest-in-the-poetry-of-the-black-arts-movement","layout":"post","content":"As one of the graduate fellows at the Scholars’ Lab this year, I am working on a year-long digital project (that’s also a chapter of my dissertation) in collaboration with the folks at the SLab. To sum it up in a sentence, the project hopes to offer a proof-of-concept for performing sentiment analysis on some of the most politically and affectively charged poetry of the 20th century, that of the Black Arts Movement of the 1960s and 70s. Today I wanted to post a brief overview and introduction to what I’m working on. For some context, my research investigates theories of affect as they relate to race, class, and gender in American literature. I focus in particular upon the provocation and articulation of emotions like frustration, anger, and discontentment within recent US literary history as they relate to systemic injustice. An agitprop play that ends with shouts for workers to unite in class revolution; a poetic broadside that vents frustrations against white supremacy in America; a novel that indulges in a revenge fantasy against America’s colonial history. Unlike plays, poems, or novels that seem to obscure, submerge, or confound their own political dimensions, these works wear their hearts on their sleeves: they are frustrated, pissed off with how things are, and unafraid to speak truth to power in a direct, seemingly “un-literary” way. At a certain level, then, this is a question of how, where, and to what ends aesthetics and politics meet in a work of literature. To offer a tidy narrative of this prickly history, this sensibility that mobilizes aesthetic objects to address political injustice has posed all kinds of unexpected, even contradictory problems for literary study. On the one hand, the cool detachment of aesthetic mediation keeps experimental works like John Dos Passos’s Communist-leaning U.S.A. trilogy from being seen as mere propaganda, but runs the risk of appearing elitist or self-indulgent. On the other hand, the red-hot political outrage of a protest poem by Amiri Baraka or Sonia Sanchez grounds itself in the present, but may be attacked for subordinating aesthetic sophistication to political agendas. “Anger is loaded with information and energy,” says Audre Lorde in a 1981 speech on its political uses—but the nature of this affective information, sparked by a given political present, becomes highly vexed when articulated by different groups through aesthetic objects. Building on recent scholarship (like the work of Lauren Berlant and Sianne Ngai) suggesting that feeling gives structure to cultural formations, I argue that a history of unrest in America reveals a pattern of artistic response, a sensibility, precipitated by specific historical moments but translated into aesthetic practice through a stable constellation of affective structures. To this end, I examine continuities between politically-engaged aesthetic projects from three periods of discontent in American history: radical journals like Partisan Review in the 1930s; the revolutionary poetry of the Black Arts Movement in the 60s; and contemporary revenge-driven novels drawing from the Red Power movement. My digital project as a graduate fellow is the second of those three chapters. In it I hope to ask two questions in particular: first, how are the feelings associated with injustice in the 1960s and 1970s coded in terms of race and gender? The Black Arts Movement first took shape at the height of the Black Power Movement with the foundation of the Revolutionary Theatre by Amiri Baraka in 1965. As Larry Neal—one of its principal theorists—says in a 1969 manifesto, the “Black Arts movement seeks to link, in a highly conscious manner, art and politics” toward “the liberation of Black people.” Moreover, the movement’s “black esthetic” is famous for its affective dimensions, often exploring the limits and political uses of anger, frustration, and poetic rage. But while BAM writers sought to link art and politics through explicitly racial terms, many—though by no means all—were marked by a failure to attend to the intersections of gender with racial injustice. This leads to my second question: what can natural language processing techniques like sentiment analysis show us about the relations between different dimensions of poetry—like affect and gender—given that poetry, unlike movie reviews or customer feedback, is highly figurative and notoriously difficult to quantify in terms of sentiment or opinion? How can we combine the powerful scale of sentiment analysis with the granularity of close reading to explore the intersections of feeling, gender, race, and injustice in the radical poetry of this period? Moreover, by employing an interpretive method that is in part suspect from a revolutionary perspective—a distanced, potentially de-contextualized computational analysis—I wonder: what limits might these methods have in reading texts that are themselves shaped by the experience of an intense surveillance culture fearful of radical thought? The already vibrant conversations on sentiment analysis and NLP more generally have been illuminating in forming my questions. The discussion between Matthew Jockers and Annie Swafford on the Syuzhet package and “archetypal plot shapes” has helped me not only to explore the current possibilities and limitations of sentiment analysis as applied to literary corpora, but also to think through the kinds of results we expect from digital projects and how we verify those results as an academic community. With regards to poetry and NLP more specifically, Lisa Rhody’s topic modeling of highly figurative ekphrastic poetry is a great model for how unexpected failures in textual analysis can also be productive, prompting us towards new questions as well as new understandings of familiar methods like close reading. So far I have been working in collaboration with folks at the Scholars’ Lab to work through the NLTK handbook, building and prepping my corpus, and beginning to implement some NLP techniques with TextBlob on what I have so far. Another post on those first forays into NLP and sentiment analysis coming soon! In the meantime, if you have any questions about the project, texts or tools I should check out, or just find it interesting and want to talk about it, send me an email! I’ll be posting about my progress over the course of the coming months and aiming to keep my process as open as possible to new ideas, feedback, and inspiration from unexpected places."},{"id":"2017-11-21-learning-to-augment-reality","title":"Learning to Augment Reality","author":"christian-howard","date":"2017-11-21 09:35:21 -0500","categories":["Digital Humanities","Experimental Humanities","Grad Student Research"],"url":"learning-to-augment-reality","layout":"post","content":"The Praxis team is in the midst of defining its project, and for the past few weeks, we’ve been playing around with augmented reality (AR), specifically by using Vuforia and Unity . Learning about AR has been fascinating and, admittedly, a bit frustrating. I won’t go through the process of getting Vuforia and Unity to work with one another (here’s a great intro video if you’re interested!), but I will briefly discuss some of the challenges and implications of trying to augment reality. First, the target image. The target image is the image that you augment, such that when you point your phone/camera at said image, the 3D figure that you have virtually “added” to the image appears on your screen. But the target image can be tricky. That is, Vuforia scans the target image for certain key features, by means of which the program can identify when your phone/camera is pointed at the target image. I’ve taken some screen shots of a few of the items that I augmented, which Vuforia ranks in terms of “augmentability.” Images 1, 2, &amp; 3: The Scholars’ Lab sign received an augmentable rating of one star, meaning its identifiable features are minimal. The cover of Vi Khi Nao’s book, Fish in Exile, has four stars, and the “cowboy” lunchbox residing in the Scholars’ Lab received an augmentable rating of five stars. The yellow crosses indicate the identifying features and patterns that Vuforia recognizes. Not only does the target image need to have enough unique features to be easily identifiable, but the image should be properly edited so that nothing appears in the background. When the image is uploaded with a background, Vuforia will assume that the background is part of the target image, and it will identify features of the background as part of the patterns it is to look for. This will make it difficult if not impossible for your camera/device to recognize the image unless it appears with the exact same background. Image 4: Cover of Fish in Exile against a mesh chair. The yellow crosses have primarily identified features of the chair – rather than the cover of the book – as unique features, and the “augmentability” of the image has declined to two stars. Another problem that we ran into has to do with subject matter. We’re currently experimenting with items on or around UVA’s grounds. So we’ve been taking photos of items from the Small Special Collections, buildings, memorials, and even lunchboxes sitting around in office spaces. But this becomes problematic when the photos we take are affected by the environment. For instance, I tried taking a photo of the segment of the Berlin Wall that stands on UVA’s grounds, and here’s how it turned out: Image 5: A photo of the Berlin Wall at UVA. Encased in glass, the Berlin Wall is nearly effaced by the reflection of Small Library opposite it. Even, then, if I use a “clean” shot of the Berlin Wall taken from the Internet as my target image, my augmentation of the image will not be identifiable or reproducible if someone were to point their camera/phone at the actual Wall on grounds. So needless to say, our work with AR is still very much in progress. But as we continue developing our AR ventures, considerations of target image complexity and environmental factors will, it seems, help shape the scope of our project. And on this parting note, I’d like to include a couple fun pictures of the fruits of our augmentation experiments thus far. Enjoy! Images 6-9: Augmentations of Fish in Exile and the Cowboy lunchbox."},{"id":"2017-11-30-all-of-the-questions-a-recap-of-the-2017-bucknell-university-digital-scholarship-pre-conference","title":"“All of the Questions:” A Recap of the 2017 Bucknell University Digital Scholarship Pre-Conference","author":"kelli-shermeyer","date":"2017-11-30 05:31:07 -0500","categories":null,"url":"all-of-the-questions-a-recap-of-the-2017-bucknell-university-digital-scholarship-pre-conference","layout":"post","content":"In early October I was sent to represent the Scholars’ Lab at the Bucknell University Digital Scholarship Conference and the pre-conference meeting. This conference brings together an interdisciplinary group of students, teachers, scholars, librarians, and instructional technologists for a weekend of conversation about many aspects of digital scholarship including pedagogy, community outreach/social justice, and institutional best practices. This year’s conference was called “Looking Forward, Looking Back: The Evolution of Digital Scholarship” and featured keynotes by Stephen Cartwright, Kalev H. Leetaru, and UVA’s on A.D. Carson. Pre-conference plan: How do we engage students in digital scholarship and support instructors as they incorporate DH or DS practices in their traditional classes? The BUDSC pre-conference was initially convened around these concerns and charged with the task of developing a “DS Cookbook” featuring ideas, best practices, and resources for instructors looking to include digital projects within their courses. We were initially asked to reflect on questions about our own experiences: What would have been helpful to know the first time we attempted to use digital scholarship in the classroom? How can we engage students in digital scholarship with limited budget, resources, or support? Participants: • Lee Skallerup Bessette, University of Mary Washington\n• Joshua Finnell, Colgate University\n• Sarah Hartman-Caverly, Delaware County Community College\n• Aaron Mauro, Penn State, Erie\n• Megan Mitchell, Oberlin College\n• Courtney Paddick, Bucknell University\n• Carrie Pirmann, Bucknell University\n• David Pettegrew, Messiah College\n• Kelli Shermeyer, University of Virginia\n• Emily Sherwood, Bucknell University What actually happened… After a fortifying breakfast of coffee and donuts, our pre-conference group proceeded to make a list of all of the questions and concerns we were stewing over in our work as scholars, teachers, librarians, and instructional technology specialists. This white board was the result: Some of these issues had to do with the intended purpose of the pre-conference – creating a guide for those interested in engaging students with digital scholarship (early concerns included: how do we scaffold or assess digital projects? What does it mean when administrators want students to have “digital literary” or “digital fluency?”) But it became immediately apparent that the interests of this group had a much wider scope. Our morning session consisted of sorting all of the issues raised on this initial whiteboard into categories that we could work with more easily, as well as discussing and sharing resources that we all had at hand. In our afternoon session, we broke up into small groups to work on articulating major questions, a list of best practices, and a set of helpful resources for approaching these topics in a variety of contexts. The results of our work were presented at the pre-conference recap session of BUDSC (which we re-titled “All of the Questions”) and will be published online forthcoming, but for now, here are some highlights: Communicating with Stakeholders: This group provided strategies for talking with administrators and other stakeholders about the value of collaborative digital scholarship, how to find funding for cross-disciplinary work, and how to communicate about DH work as part of promotion and tenure. They suggested that A Short Guide to the Digital_Humanities can be used as a helpful introduction to digital scholarship for administrators and faculty who are unsure of what they may be getting themselves into. MLA also has some guidelines for evaluating digital scholarship for P&amp;T purposes. Data Security &amp; Privacy: This group explored a whole set of questions that I, frankly, had never thought about in any great depth. They asked us to consider, “What exactly is data, anyway? What do we consider to be data in the context of digital scholarship? As we delve more into the world of digital scholarship, it’s become evident that so much of what we do is based on some form of data – be that numerical data, textual data, geospatial data, audiovisual data, etc. With that in mind, how do you ensure ethical, responsible creation and maintenance/preservation of datasets?” The Data Curation Centre can supply researchers with expert help on this topic. This group also suggested Purdue’s Digital Retention Policy as a model document for schools or departments wishing to develop their own protocols regarding data. Digital Pedagogy: Our group assembled a slew of resources for teachers wanting to engage with digital projects in their classrooms. We asked: “What are we assessing when we ask our students to complete digital assignments and how do their outcomes interface with the goals of traditional scholarship? How do we encourage them to value the process over the product?” The resource list includes many sample assignments and assessment ideas, as well as a collection of what we called “easy wins” – plug and play tools to work with in the classroom, including: Timeline js - make a simple, multimedia timeline Voyant - beginning large text analysis Prism - annotate your text Twine - create interactive fiction Juxta Commons - Compare texts IMJ - Large image visualization IP/OA/Fair Use: This group explored how to approach fair use and copyright as our students use, remix, and edit online content for their own projects. We can begin by assessing our own/our institution’s tolerance for risk. Very important take-away point: No one is carting you off to jail for remixing something – the worst that will happen is a take-down notice. There’s also an increasing amount of legal precedence for going a little cowboy with fair use, as demonstrated by this video which not even Disney was able to successfully remove: A Fair(y) Use Tale https://www.youtube.com/watch?v=CJn_jC4FNDo . Sustainability: The questions of project management, project charters, sunsetting, hosting, institutional repositories, and archiving looked like a separate category for us at first, but discussions of these issues were interwoven throughout the other four categories, rather naturally. Shout outs here went to Reclaim Hosting and Miriam Posner’s blog post on Project Charters . N.B. quotations are from the co-authored pre-conference documents."},{"id":"2017-11-30-first-steps-with-nlp-and-a-collection-of-amiri-barakas-poetry","title":"First Steps with NLP and a Collection of Amiri Baraka's Poetry","author":"ethan-reed","date":"2017-11-30 10:27:04 -0500","categories":null,"url":"first-steps-with-nlp-and-a-collection-of-amiri-barakas-poetry","layout":"post","content":"Amiri Baraka’s Black Magic, 1969 In this post I’ll discuss my initial foray into natural language processing (NLP)—cleaning up a corpus and prepping it for some basic text analysis techniques. I want to begin, however, with a note on the small textual corpus that I’m using in these preliminary explorations— Black Magic, a 1969 collection of three books of poetry by Amiri Baraka. In a prefatory note to the collection, Baraka offers an “Explanation of the Work” that touches on the three books of poetry contained within. “ Sabotage,” he writes of the first book, “meant I had come to see the superstructure of filth Americans call their way of life, and wanted to see it fall. To sabotage it,” in a word. The second book, he argues, takes this intensity even further: “But Target Study is trying to really study, like bomber crews do the soon to be destroyed cities. Less passive now, less uselessly ‘literary.’” If these comments are any indication, the poetry of Black Magic has a certain level of emotional and political intensity. These poems articulate rage—they thunder, fulminate, and protest, venting a vindicated anger at racial injustice in America. Others simmer with a more restrained heat, but still tend to employ an often unsettling rhetorical violence. Consider, for example, the conclusion of a poem from Sabotage titled “A POEM SOME PEOPLE WILL HAVE TO UNDERSTAND”: We have awaited the coming of a natural phenomenon. Mystics and romantics, knowledgeable workers of the land. But none has come. (repeat) but none has come. Will the machinegunners please step forward? Though startling, this final image punctuates a familiar narrative: the mounting of frustration, the boiling over of feeling while waiting and waiting for justice. The speaker’s closing remark seems to respond to the question asked in Langston Hughes’s poem “Harlem” —“What happens to a dream deferred?”—but raises the ante of the inquiry, and shifts from Hughes’s suggestive but still open-ended conclusion (“ Or does it explode? ”) to an unsettling direct request (“Will the machinegunners please step forward?”). The poem also, however, seems aware of its high dramatic tone: it conveys the gravity of this deferred deliverance with somewhat formal rhetoric like “We have awaited” and “But none has come”, but highlights—and perhaps undercuts—its own theatricality by embedding a stage direction in the poem, “(repeat)”. We’ve waited for long enough, the poem seems to argue, but stages this claim in such a way that the final line’s delivery hangs suspended somewhere between deadpan and dead serious. In short: a heightened revolutionary rhetoric permeates the poems in this collection. Many have noted, however, that a troubling violence permeates them as well. For example, one scholar describes “Black Art”—one of the most graphic but also most well-known poems from this collection—as “a difficult poem in its race and gender violence, in its violence against peoples.” In the 1991 The Leroi Jones/Amiri Baraka Reader, editor William J. Harris describes Black Magic as a collection in which Baraka “traces his painful exit from the white world and his entry into blackness,” an “exorcism of white consciousness and values [that] included a ten-year period of professed hatred of whites, and most especially jews [sic].” Baraka looks back at this period in his 1984 autobiography at a remove from the red-hot intensity of the poems themselves: “I guess, during this period, I got the reputation for being a snarling, white-hating madman. There was some truth to it, because I was struggling to be born, to break out from the shell I could instinctively sense surrounded my own dash for freedom.” From this perspective, this is the violence of escape, of “struggling to be born” from within a constricting “shell”—a version, perhaps, of the violence of the deferred dream that explodes at the end of Langston Hughes’s poem “Harlem.” Initial Steps with NLP As a scholar interested in articulations of anger, resentment, and frustration with injustice—particularly injustice of a systemic and institutional nature—as well as digital methodologies, I thought these texts in particular might be worth looking at more closely with NLP techniques. As a graduate student working in a period that is almost entirely still in copyright, however, Black Magic also interested me because it is a small corpus of works—three books of poetry—to which I currently have access through UVA. Though conceptually unglamorous, basic questions of access have played an enormous role in determining the initial paths in my scholarly decision-making process. In this sense, though assembling workable data is always a challenge, scholars interested in literary texts prior to the early 20th century have more options for readily accessible textual corpora. For 20th- and 21st-century scholars interested in textual analysis, however, questions of copyright have made finding openly available textual data from which a corpus could be built an extremely difficult task: while able to share results of analyses through transformative, non-consumptive use, scholars of these periods cannot share the corpora from which these insights are drawn. This presents additional challenges in terms of reproducibility as well as in the already long, labor-intensive task of assembling, cleaning, and prepping a corpus prior to any actual application of NLP techniques. If texts aren’t already available as text files through a university or institution, they either have to be typed out by hand or scanned page by page, run through optical recognition software that transforms the page image into text, then also ultimately cleaned and corrected by hand. In short: no preexisting corpora means no experiments, prototypes, or conceptual ventures without surmounting certain barriers to entry that often prove time- or cost-prohibitive. In the case of this project, even though UVA has access to the 1969 edition of Black Magic: Collected Poetry 1961-1967, the text isn’t ready for NLP out-of-the-box. The page contained a lot of text beyond that of the literary work in question: page numbers, line numbers, bibliographical information, headers and footers, all kinds of weird punctuation, and so on. For example, the title of the first poem in Sabotage, “Three Modes of History and Culture,” appeared in this electronic edition as follows: Baraka, Imamu Amiri, 1934- : Three Modes of History and Culture [from Black Magic: Collected Poetry 1961-1967 (1969), The Bobbs-Merrill Company ] To perform sentiment analysis on Sabotage, then, I first needed to get the raw text. By “raw text” I mean a big bag of all of Sabotage ’s words. My goal initially was to get this bag of words with no line numbers, no punctuation, no capitalized first letters (otherwise Python would think they were two different words), and no spaces. As someone doing this work for the first time, I felt like I could handle writing a program that would remove capital letters, get the txt file into the correct file-type, maybe even get rid of the line numbers. But what about all this clutter surrounding the title of each poem? I considered how I might remove this with a program, but even something as small as irregular line breaks means the words would be chopped up in slightly different ways each time. Given the size of the corpus, I decided it would be wiser to remove the clutter by hand than to write a one-time program that automated it. With a huge assist from Brandon Walsh, cleaning up the rest of the text with the Natural Language Toolkit (NLTK) was relatively straightforward. We wrote a small Python script that removed line numbers, then proceeded to write a script that would prep the clutter-free text files for text analysis, first by reading the text file as a list of lines (1), then by tokenizing that list of lines into a list of lists, where each sub-list is a list of the words that make up a line (2) . While this may seem kind of complicated, certain kinds of text analysis need the lines to be tokenized in this way—much of the work then involves getting the text to be the right kind of data type (list of words, list of lists, etc.) for a given kind of analysis. Because I’m interested in sentiment analysis, I also needed to make every word lowercase (3), remove punctuation (4), and remove spaces (5) . Having written out all these functions, we then made a new function that called on each of them one after the other, running through the pipeline of activities necessary for NLP (our notes-to-self included): Though it gets the job done, this code is clunky. It represents, in short, the first steps in my learning how NLP works. And while not the most elegant in terms of form or function, writing steps out in this way was conceptually clear to me as someone trying them for the first time. I also want to add that throughout much of this Brandon and I were practicing something called pair programming, with Brandon at the keyboard (or “driving”) and me observing, asking questions, and discussing different ways of doing things. In addition to being an exciting scholarly investigation, this project is also a learning experience for me, and our code-decision-making process often reflects that. But more on the intricacies of collaboration later. To recap, at this point I had a series of functions that, in a linear, step-by-step fashion, took my original text file and began to play with them in Python’s working memory: it took Amiri Baraka’s poetry as one data type (a giant string of words) and turned it into another (a tokenized list of lists), with some changes along the way (like lowercasing and getting rid of punctuation). What made this so clunky, however, stemmed in large part from how I had organized my tasks: I gave Python basically only one thing to think about and work with at a time. It would take my corpus, W, and turn it into X, which it would then turn into Y, and then Z, and so on. But if I wanted Python to remember X while it was working on Z, I had to write code to turn Z back into X—in short, a data-type nightmare. Which sounds pretty abstract, but presented all kinds of practical problems. For example, after having gotten all the way to Z—my lowercased, punctuation-free list of lists—I wanted to try a basic form of text analysis I had seen in an early chapter of the NLTK book (called stylistics) in which I compared the use of different modal verbs in the three books of Baraka’s poetry. The only way I knew how to do this was to run a frequency distribution on a giant list of words—which means I had to un -tokenize my nicely tokenized texts, basically jumping from Z back to W. So I wrote some clunky code that let me do so: Grappling with this problem, Brandon re-introduced me to something I had learned about before but never had to use—object-oriented programming. Rather than performing a linear series of functions on my text file, reorganizing my code along OOP lines let me treat this text file as an object with many attributes, any of which I could access at any time. If I wanted my file (or object) as a giant list of words to perform a frequency distribution, I needed only to call upon that particular aspect (or attribute) of my object. If I then wanted Python to think of it as a tokenized list of lists I could just call on that particular attribute rather than having to send it through a series of transformations. It’s as if my ability to manipulate a file gained a third dimension— instead of begin stuck going from X to Y to Z and then back to X, I had access to all three stages of my file simultaneously. In essence, what was once a one-way data-type conveyor belt now became a fully-staffed NLP laboratory. In another pair programming session, we started to shift my more linear code to an object-oriented approach. What we came up with definitely needs refactoring (in my TODO list) and can certainly be improved (i.e., not overwriting a variable multiple times), but again, in the spirit of showing my learning process, I wanted to share a visual of this early version that marked my beginning to grapple with OOP for the first time: Finally “getting” object-oriented programming conceptually was truly a programming awakening for me, even if my initial attempts need some improvement—it hadn’t really made sense as an approach until I was faced with the problems it helps address. So we have the poems in all their fiery intensity, as well as the beginnings of actually using sentiment analysis as another way of thinking through them. As it currently stands, Brandon and I have started using TextBlob to perform some basic tasks—more on that soon. If you have any questions or want to follow along, my GitHub project repository can be found here ."},{"id":"2017-11-30-my-experience-leading-a-workshop-on-text-analysis-at-washington-and-lee-university","title":"My Experience Leading a Workshop on Text Analysis at Washington and Lee University","author":"sarah-mceleney","date":"2017-11-30 05:15:22 -0500","categories":null,"url":"my-experience-leading-a-workshop-on-text-analysis-at-washington-and-lee-university","layout":"post","content":"[Sarah went to Washington and Lee University to give a workshop in Prof. Mackenzie Brooks’s DH 102: Data in the Humanities course through a Mellon-funded collaboration with WLUDH. More information about this initiative can be found  here, and this piece is crossposted to the WLUDH blog .] As a graduate student participating in the University of Virginia and Washington &amp; Lee University digital humanities collaboration, during the fall 2017 I led a guest workshop on text analysis in Mackenzie Brooks’ course DH 102: Data in the Humanities.  This workshop was an exploration of approaches to text analysis in the digital humanities, which concurrently introduced students to basic programming concepts.  For humanities students and scholars, the question of how to begin to conduct text analysis can be tricky because platforms do exist that allow one to perform basic text analyses without any programming knowledge.  However, the ability to write one’s own scripts for text analysis purposes allows for the fine-tuning and tailoring of one’s work in highly-individualized ways that goes beyond the capabilities of popular tools like Voyant. Additionally, the existence of a multitude of Python libraries allows for numerous approaches for understanding the subtleties of a given text of a corpus of them.  As the possibilities and directions for text analysis that Python enables are countless, the goal of this workshop was to introduce students to basic programming concepts in Python through the completion of simple text analysis tasks. At the start of workshop, we discussed how humanities scholars have used text analysis techniques to create some groundbreaking research, such as Matthew Jockers’ research into the language of bestselling novels, as well as the different ways that text analysis can be approached, briefly looking the online text analysis tool, Voyant. For this workshop students downloaded Python3 and used the simple text editor that is automatically installed with it, IDLE.  This way we didn’t have to spend time downloading multiple programs.  While IDLE is rather barebones, its functionality as a text editor is fine for learning the basics of Python, especially if one doesn’t want to install other software.  From here, by using a script provided to the students, we explored the concepts of variables, lists, functions, loops, and conditional statements, and their syntax in Python.  Using these concepts, we were able to track the frequency of chosen words throughout different sections of a story read by the script. The workshop then delved into a discussion of libraries and how work can be enhanced and made to better suit one’s needs by using specific Python libraries.  As the focus of the workshop was on text analysis, the Python library that we looked at was NLTK (Natural Language Toolkit), which has a vast variety of functions that aid in natural language processing work, such as word_tokenize() and sent_tokenize(), which break up a text into individual parts, as words or sentences, respectively.  The NLTK function FreqDist() simplifies the task of getting a count of all the individual words in a text, which we had done with Python alone in the prior script before working with NLTK.  The inclusion of NLTK in the workshop was meant to briefly show students how important and useful libraries can be when working with Python. While only so much can be covered over the course of a single workshop, the premise of the workshop was to show students that you can do some very interesting things with text analysis with basic Python knowledge, and to dive into Python programming headfirst while learning about general concepts fundamental to programming.  As digital humanities methods for humanities research are becoming more and more common, working with Python’s capability for natural language processing is a useful tool for humanists, and in an introductory class, the goal of my workshop was to spark students’ interest and curiosity and provide a stepping stone for learning more, and at the end of the workshop, further resources for students to turn to in learning more about Python and text analysis were discussed."},{"id":"2017-12-13-call-for-spring-2018-makerspace-technologist-applications","title":"Call for Spring 2018 Makerspace Technologist Applications","author":"laura-miller","date":"2017-12-13 12:29:28 -0500","categories":["Announcements","Job Announcements","Makerspace"],"url":"call-for-spring-2018-makerspace-technologist-applications","layout":"post","content":"Are you a UVA graduate student or upper-level undergraduate in the humanities? Come join our team as a Makerspace technologist! Our Makerspace is designed to foster experimentation with 3D printing, modeling, and digitization, physical computing (e.g. Arduino, wearables), virtual reality, and more. For humanists, it is a good way to learn more about experimental and digital humanities by exploring new uses for digital technologies in fields that do not traditionally integrate them. No prior experience with electronics or 3D printing is needed. Successful candidates will be trained on these tools and will in turn pass on their training to disciplinarily diverse students, faculty, and staff interested in using them for fun, teaching, and research. We also strongly encourage technologists to work on their own personal projects and to develop expertise based on their own scholarly interests. An important aspect of Maker culture is apprenticeship and supporting makers in their pursuit of professional experience. We are looking for motivated individuals who are capable of working independently and value the opportunity to engage with and support a growing community. Benefits of the job may include: access to expertise and mentoring in your field of interest, opportunities for collaboration and publication, use of equipment and tools, and ability to shape Scholars’ Lab workshops and programming. Candidates should be able to work up to 10 hours per week. Applications should consist of a cover letter discussing their interest in working in the Scholars’ Lab, any experience or interest in participating in a maker space, and any previous experience with public service or assisting others in using technology. Please send inquiries and applications to scholarslab@virginia.edu . Multiple openings are available for Spring semester, and review of applicants is ongoing until filled."},{"id":"2017-12-14-fellowship-calls-and-grad-student-professional-development","title":"Fellowship Calls and Grad Student Professional Development","author":"brandon-walsh","date":"2017-12-14 04:42:42 -0500","categories":["Announcements","Digital Humanities","Geospatial and Temporal","Grad Student Research","Makerspace","Technical Training"],"url":"fellowship-calls-and-grad-student-professional-development","layout":"post","content":"I want to share several developments from the grad programs side of the Lab this semester. It’s been a busy fall, and I’m pleased with all the work the team has put into our programs! For one, the CFPs for two of our fellowship programs are now live. The Praxis Program, which will welcome its eighth cohort next year, will have a deadline of February 15th for applications from PhD students at UVA. This flagship program is in many ways the core of our graduate community, and we’re very excited that it continues to thrive. I am also very pleased to announce that the Digital Humanities Prototyping fellowships, piloted this past year with a cohort of four students, will continue next year with its own application deadline of February 15th. Open to PhD and MA students at UVA, these fellowships are meant to shore up our support of students in the intermediate years of their graduate work, to provide collaborative projects a space in our fellowship portfolio, and to give young scholars a chance to craft a spark that might catch further down the line with applications for further funding here or elsewhere. Please tell your students and colleagues! I always strongly encourage students to get in touch with me if they are planning to apply - that way they will be on our radar for other opportunities down the line regardless of how this particular application shakes out. Along with our newly restructured DH Fellows program, these three fellowship programs provide support and experience for more stages of the graduate student timeline than was previously possible. In addition to the fellowship announcements, I also wanted to draw attention to a revamping of what was formerly known as the “graduate fellowships” page. Our programs have grown a lot since this page was last revised, and the new “graduate fellowships and opportunities” page now better represents the wealth of offerings in the Scholars’ Lab. This new, catch-all page offers a space where students can see all of our opportunities beyond our annual fellowship programs. We regularly employ graduate students as Makerspace Technologists to assist in 3D printing and experimental computing in our makerspace (and we just released a call with multiple openings for spring 2018 !). Cultural Heritage Informatics Interns each semester work with Will and Arin to 3D scan, process, and print artifacts all while getting course credit. Chris and Drew regularly work with student GIS Technicians who assist in the uploading of GIS datasets and creating applications on our GIS portal, all while getting valuable experience in spatial humanities. And, finally, a Mellon-funded collaboration with Washington and Lee University allows us to send students to their campus to give workshops on digital humanities to undergraduate courses. The amount of experience required for all these opportunities is quite variable, so be sure to read closely - in many cases we are more than happy to have you learn on the job. We’ve been doing all these things for quite a while, but hopefully now students can find easier access to information about our programs and how to get involved. Finally, I’m especially pleased to share that we have a new section in this page on professional development for graduate students . The Scholars’ Lab programs give students valuable experiences and training, but we’ve also historically gone further than these official offerings. As UVA students apply to alt-ac and DH careers, we regularly give advice on the whole process, from finding a job to producing materials to interviewing. These offerings have long been ad hoc and by request, but I worried over the last several months that some potential students might get left out of such arrangements. A student might not know, for example, that we’d be willing to mock interview them in the happy event that they’re invited to campus for that digital humanities developer position. Or a student putting together their first job talk for a post-doc in digital humanities might not realize that we’re happy to lend a friendly ear and also share our own job talks. This section is not perfect, and it by no means represents the sum of what any program can do to support graduate students. If you see something missing, drop me a line to let me know. But hopefully the statement of services there will serve as nice counterpoint to the values that we lay out in our group charter ; hopefully the page’s presence will help someone find their way to us who might not otherwise have done so. After all, tacit assumptions about how others perceive our services can lead to people falling through the cracks, feeling like they’re going through a job search alone. Best that we be explicit, and best that we match our values with public statements of what we will do to back them up. So in short - we’re here for you. If you’re part of the UVA community and looking for help with your DH or alt-ac job search, swing on by and let me know how we can help!"},{"id":"2018-01-12-spring-2018-uva-library-gis-workshop-series","title":"Spring 2018 UVa Library GIS Workshop Series","author":"chris-gist","date":"2018-01-12 04:52:37 -0500","categories":["Geospatial and Temporal","Research and Development","Technical Training"],"url":"spring-2018-uva-library-gis-workshop-series","layout":"post","content":"All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be held on Tuesdays from 10AM to 11AM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up! February 6th Making Your First Map with ArcGIS Here’s your chance to get started with geographic information systems software in a friendly, jargon-free environment.  This workshop introduces the skills you need to make your own maps.  Along the way you’ll get a taste of Earth’s most popular GIS software (ArcGIS) and a gentle introduction to cartography. You’ll leave with your own cartographic masterpieces and tips for learning more in your pursuit of mappiness at UVa. ** February 13th ArcGIS Online: Introduction With ArcGIS Online, you can use and create maps and scenes, access ready-to-use maps, layers and analytics, publish data as web layers, collaborate and share, access maps from any device, make maps with your Microsoft Excel data, and customize the ArcGIS Online website. February 20th ArcGIS Online: Spatial Analysis ArcGIS Online now has spatial analysis tools that can be easier to use than similar desktop GIS tools.  Come learn how to use the simple yet powerful analysis tools available through ArcGIS Online.** February 27th ArcGIS Online: Story Maps Story Maps are templates that allow authors to give context to their ArcGIS Online maps.  Whether telling a story, giving a tour or comparing historic maps, Esri Story Maps are easy-to-use applications that create polished presentations. March 13th ArcGIS Online: Data Collection Whether you are crowd sourcing spatial data or performing survey work, having applications that automatically record location and upload data directly to a mapping application is incredibly useful. ** March 20th What’s New with ArcGIS Pro The handwriting is on the wall. ArcGIS Pro will be replacing ArcMap as the desktop GIS in the near future.  Come learn about the changes and quirks of ArcGIS Pro from an ArcMap user prospective. March 27th Introduction to QGIS ArcGIS isn’t the only game in town.  The best and most popular open source GIS application is QGIS.  It runs on most platforms and does some things better than ArcGIS.  Come learn more about another tool in the GIS toolbox."},{"id":"2018-02-07-all-about-the-archive-guest-teaching-at-washington-and-lee","title":"All About the Archive: Guest Teaching at Washington and Lee","author":"lauren-reynolds","date":"2018-02-07 07:51:48 -0500","categories":null,"url":"all-about-the-archive-guest-teaching-at-washington-and-lee","layout":"post","content":"In this post Lauren Reynolds, a former PhD student in Spanish and Makerspace Technologist, describes her work with Professor Andrea LePage’s course at Washington &amp; Lee. This work is supported by an ASC grant expanding collaboration between Washington &amp; Lee and the Scholars’ Lab and supplemented by W&amp;L’s Mellon-funded grant to support digital humanities in the classroom. Cross-posted to the WLUDH blog . I was invited to guest lecture for Professor Andrea LePage’s undergraduate course, Contemporary Latinx and Chicanx Art . After discussing possible topics for the workshop, Professor LePage and I decided on the topic of “Archive as Protest.” This topic overlapped with my research on cultural memory in US Latinx texts and presented me with the opportunity to learn more about digital archives. As I developed the plan for the workshop, I organized the lesson around questions surrounding digital archives, preserving cultural memory, and cataloguing a variety of experiences. These are very broad questions, so I outlined two goals for the class: First, I wanted the students to begin to think about information storage in the broadest sense. Then we would narrow down the idea of seemingly endless information to a conversation about cataloguing and metadata. Second, I aimed for our discussion of cultural creation and preservation to help the students understand one way in which preserving information through archives can have a positive social impact. After introductions, we began the lecture with a brief discussion of Jorge Luis Borges’ short story La biblioteca de babel . This text gave me the opportunity to sneak a bit of Latin American literature into the course and provided an entry point for talking about information storage. So, we began with questions about Borges’ conception of an infinite library: Why do you think some people say that Borges “discovered” the internet decades before it was invented? What similarities do you see between the infinite library and the internet? What are some differences? How is a library organized? Is the internet organized? What possibilities/challenges do a universe of information pose? Next, we zoomed in to a more focused discussion of archives, their purposes, and how the internet has changed the preservation and accessibility of information. We talked about documenting history from many perspectives and, in small groups, the students reflected on the following quote from Daniel Mutibwa: “The overarching argument is that local, alternative, bottom-up approaches to telling (hi)stories and re-enacting the past not only effectively take on a socio-political dimension directed at challenging dominant, hegemonic, institutional narratives and versions of the past, but – in doing so – they also offer new and refreshingly different ways of understanding, representing, remembering, and rediscovering the past meaningfully in ways that local communities and regions can relate with.”[1] The students began to connect this quote to their own interests as we discussed the possibilities of digital archives. We specifically looked at the Hurricane Katrina collection to talk about the pros and cons of bottom-up archives: http://hurricanearchive.org/collections We noted how such archives allow for individual stories to be shared and how they can become part of a community’s healing process after a tragedy. This digital archive also prompted interest in logistical questions, such how stories are collected, saved, and mapped in the creation of an online archive. Specifically, the students were asked to think about: Development: How to choose what to include, authenticity Retrieval and Collection Reaching the Community: Supporting Research, Learning, and Teaching Reference Information and Providing Access   Our last activity presented the opportunity to learn about different types of metadata and its role in cataloguing. We discussed social media presences as types of personal, living archives and how hashtags such as #TBT, #breakfast, and #gooddog can be seen as a means of organizing Instagram posts. In pairs, the students were then given three photos of different US Latinx artworks and asked to assign categories to each photo. They thought about specificity and accessibility: how to make the photos both accessible in broad searches, but easily found for specific inquiries. Each pair shared their selected words with a another group. After comparing their different hashtags and debating which labels were the most useful, each group came up with a definitive set of categories. We then talked about the different “data sets” created in class, noting the benefits and possible drawbacks of each one. The class concluded with small group discussions of overarching questions: Difficulties posed by the fact that technology is always changing How to establish trust between archive curators and communities Library neutrality, the library’s role in community engagement, and the line between memorial and protest Advantages and disadvantages of allowing anonymous submissions Oral Histories: Who determines what questions are asked? How are these interviews and all texts edited and by who? Can “alternative” truths be abused to represent dangerous falsehoods? How do we preserve horrific histories? Do we reproduce offensive terms? With the time remaining, the students talked about whichever question interested them most in their work and, more broadly, in their lives. [1] Mutibwa, Daniel H. “Memory, Storytelling and the Digital Archive: Revitalizing Community and Regional Identities In the Virtual Age.”  International Journal of Media &amp; Cultural Politics, vol. 12, no. 1, 2016, pp. 7-26."},{"id":"2018-02-21-transcription-is-complicated","title":"Transcription Is Complicated","author":"ethan-reed","date":"2018-02-21 11:15:10 -0500","categories":null,"url":"transcription-is-complicated","layout":"post","content":"In a recent PMLA issue on digital methods, Johanna Drucker concludes her article “Why Distant Reading Isn’t” by claiming that distant reading’s literalness makes it the closest form of reading imaginable. What distant reading lacks is distance. That distance is critical; it is the space between the literal text and the virtual text, between the inscriptional, notational surface and the rhetorical, cognitive effect that produces a text. (633) In other words, when an algorithm “reads” a corpus by scouring it for patterns of one kind or another, it doesn’t transform the text the way that a human reader does. It can get so “close” because it reads without the powerful and dynamic cognitive filters through which human readers conjure, out of the written word, literary worlds. For Drucker, closing the gap between “reader” and text in this way is one of the things that makes distant reading “the closest form of reading imaginable.” But, crucially, human decisions shape how a program closes that gap in the first place. As Drucker argues elsewhere in the article, “modeling and paramaterization”—decisions made by scholars and programmers as to what a program will look for and, therefore, be able to find—not only “shape the terms by which a text is analyzed to produce quantitative data,” but are also “rendered almost invisible by the forms in which results are expressed” (632). These before-the-fact decisions, then, are what allow an algorithm to read from such a close range—ignoring the “rhetorical, cognitive effect that produces a text,” they engage with “the inscriptional, notational surface” according to a set of pre-established instructions to produce results of one form or another. In this sense, some might argue that the “distance” distant reading “lacks” is the gap in which literature happens: the unpredictable, unwieldy interpretive space in which a reader transforms text on a page or screen into a living work of art. Transcription As I assemble my corpus of poetry from the Black Arts Movement, I’ve grown more interested in this gap between “inscriptional, notational surface” and “rhetorical, cognitive effect.” In the past three weeks I have transcribed approximately twenty books of poetry. This is, in many ways, the kind of “reading” that we expect a machine to be good at: tedious and time-consuming, sure, but also mechanical, even mindless—something lacking that human “distance” Drucker describes above. When it comes to transcription, however, the devil is in the details. And anyone familiar with using OCR software to transcribe text from images knows that machines still struggle to get all the details right. After scanning pages into images and processing them with a program like ABBYY FineReader, the resulting text files are often garbled with mistakes—errors that require a human reviewer to identify, compare with the original, and correct by hand. Though an extremely useful piece of software, a program can’t be all things to all people, and I found this especially true for experimental texts like the poetry in my corpus that employ unusual indentation, spacing, punctuation, capitalization, and non-traditional spellings. But I already knew that ABBYY FineReader would have trouble transcribing text from images from my corpus. That’s one of the reasons I decided to transcribe them by hand in the first place. What I didn’t anticipate was how much trouble I—a presumably well-trained human reader—would have transcribing text from physical documents into a text editor. This being the case even when my documents were fully intact and the text completely legible. Over the course of the past few weeks, I found that this hairs-breadth, closest-form-of-reading-imaginable reading—the kind that seems to go no further than inscriptional surface—is also a complex task requiring creativity, imagination, and resourcefulness. Moreover, rather than being a mindless or merely mechanical task, the transcription of these texts frequently presented thorny decisions that demanded my judgement as a reader, scholar, and programmer. Arriving at these decisions often required not only a knowledge of digital methods, but also of bibliographical methods, questions of poetic form, and more practical project management skills. Spacing Take, for example, lines from “a/coltrane/poem,” the final poem from Sonia Sanchez’s 1970 collection We A BaddDDD People (and a poem that got me listening to Coltrane’s music while transcribing):          (soft       rise up blk/people.  rise up blk/people          chant)   RISE.  &amp;  BE.  what u can.                          MUST BE.BE.BE.BE.BE.BE.BE.BE-E-E-E-E-                                                            BE-E-E-E-E-E-                                         yeh. john coltrane.          my favorite things is u. Like many of the poems from We A BaddDDD People, “a/coltrane/poem” makes dramatic use of indentation, punctuation, the spaces between words, and the spaces between lines. Even transcribing these lines to be published here on the Scholars’ Lab WordPress site, however, raises a number of technical and practical issues. For example, there is no easy way to produce this kind of whitespace in HTML. When web browsers parse the whitespace in poetry—indentation, tabs, etc.—they more or less get rid of it. While investigating the poetry of Mina Loy, Andrew Pilsch argues in his chapter in Reading Modernism with Machines that “the nature of HTML resists—even prevents —the easy introduction of…typographic experimentation” (245)—something he discusses earlier on his blog . Like Pilsch, I ended up having to make use of the “&amp;nbsp” space —something Pilsch discusses more in-depth—to shoehorn spaces into the poem so it would appear correctly, I hope, in web browsers. This means that, in HTML, the above section of poetry looks like this: [Editor’s note - in migrating this older post to the new slab.org web design the careful use of nonbreaking space characters got entirely stripped out and had to be recreated again.] In other words, a complete mess. But before trying to print parts of this poem in HTML through WordPress, at an even more basic level I had to get it into a text editor, a process which also raised a number of questions requiring practical decisions. As I type out the above lines into Atom, I have to ask: how many spaces should separate the words that seem to be a stage direction on the left— “(soft / … / chant)”—from the words on the right? In an ideal world, I would have access to all materials used by Dudley Randall’s Broadside Press to publish this 1970 edition, as well as publication materials from all subsequent editions. Comparing these various documents, I would be able to get a better sense of the typographical materials and units of measurement used to represent Sanchez’s poem on paper. This would provide me with a more holistic sense of how to represent Sanchez’s poem in my text editor. However, given constraints on my time and resources as a Ph.D. student, as well as the size of my corpus, deciding how deep I want to dig in the archive to answer such questions requires serious consideration. Moreover, as far as I can tell, while there were printings of this edition of We A BaddDDD People as late as 1973, there were no other new editions of the work—so the edition I have is the only one I have to work with. So when faced with the question—how many spaces should separate these words in a text file?—I looked at how far a space gets me in relation to other characters, gauged this against the kinds of spaces in poems elsewhere in the book, and made an educated guess: three after “(soft”, and one after “chant)”. The same goes for the space between “&amp; BE.”, which is slightly larger than the gaps separating most other words. I’m not sure exactly how much larger this gap is, so I make another educated guess, giving it two spaces instead of one. In a multiple-page poem defined by such visual experimentation, however, trying to measure and align every word, space, and line break so that the text in my text editor resembles the text on the page—even roughly—is a real challenge. In some cases, given the functionalities of the editor I’m working with, this challenge becomes an impasse. Even in the example above: the space separating the line “yeh. john coltrane.” from the preceding line—“BE-E-E-E-E-E-”—matches the size of other line breaks within stanzas in this volume. But the space separating this line from its succeeding line—“my favorite things is u.”—is both larger than line breaks within stanzas and smaller than breaks indicating new stanzas. While transcribing, I normally represent adjacent lines in a poem with adjacent lines in my text editor; I represent stanza breaks with an empty line. How do I represent in my text editor a line break that is effectively 1.5 times the size of a normal line break? Without reworking my entire spacing system across all of my poems, I can’t—so I decided to transcribe them as adjacent lines despite the clearly visible difference on the page. Textual Scholarship The nature of these challenges would come as no surprise to scholars—like Drucker—interested in textual study, bibliographical study, and scholarly editing. Having had the great fortune of taking a seminar here at UVA on textual criticism and scholarly editing with David Vander Meulen, a course at the Rare Book School on the book in the American industrial era with Michael Winship, as well as many thoughtful conversations with friend, colleague, and textual scholar James Ascher, I’ve had the opportunity to adopt many of these methodological lenses as my own. These lenses help us to ask questions like: what exactly, is a literary work? Is Sanchez’s We A BaddDDD People the words printed in ink on the pages of the physical book I’m holding? If there are discrepancies between this book and later editions, how do we reconcile them? And, more relevant to my current project, how does the digital copy of this work in my text editor differ from the bound copy held at UVA’s library from which I’m making my transcription? In considering these questions, I find helpful the vocabulary used by textual scholar G.T. Tanselle that distinguishes between document, text, and work . To offer a greatly reduced shorthand for Tanselle’s nuanced thinking on these distinctions: there are texts of works and there are texts of documents . Texts of documents refer to the words, markings, or inscriptions on a physical object that is completely unique though it may seem to be identical to other artifacts. Texts of works, on the other hand, are slightly more complicated—they consider the words as instructions for performing that intangible thing that is a verbal literary work in the minds of readers. Though they may seem abstract, conceptual distinctions such as these have emerged from some of the most concrete, hands-on, rubber-meets-the-road scholarship in literary thought—for example, the kind of thinking that goes into examining multiple versions of a work (like King Lear ) so as to produce a single scholarly edition. A distinction like Tanselle’s between texts of documents and texts of works offers a guiding light for scholar down in the often bewildering weeds of a given archive. As Tanselle argues in “Textual Criticism and Deconstruction,” The distinction between the texts of documents (handwritten or printed, private or published) and the texts of works is basic to textual criticism. The effort to “reconstruct” or “establish” the texts of works presupposes that the texts we find in documents cannot automatically be equated with the texts of the works that those documents claim to be conveying. (1) In other words, scholars must exercise a great deal of judgement as they try to reconcile meaningful—and sometimes extremely significant—discrepancies between versions of a given physical text as found in physical documents in their efforts to determine the text of the work itself. The role that “intentions” play in all this— as in the words that were meant to be put down—and how best to account for the mediating forces and actors at work in the publication of a book, is a point of debate in textual scholarship, often dependent on the kinds of research questions one hopes to investigate (for more reading here, see D F. McKenzie’s Bibliography and the Sociology of Texts, Jerome McGann’s The Textual Condition, and Tanselle’s “Textual Criticism and Literary Sociology” ). And as many scholars have argued, these conceptual distinctions central to textual criticism and thought extend to digital artifacts as well—see, for example, Matthew Kirschenbaum’s “.txtual condition.” Scholarship such as this helps me to think through how a hand-typed .txt file of We A BaddDDD People relates to a physical codex made of paper and ink. Stanza Breaks Again, part of the purpose of this post is to expand on just how complicated transcription can be when it comes to performing text analysis on a literary corpus. Moreover, I’m hoping to think through how these practices are bound up with traditional bibliographical lines of inquiry. In short, I’m hoping here to offer further examples of how reading a literary text at extremely close range—Drucker’s “inscriptional, notational surface”—involves all kinds of human thought and judgement. Even if this thought and judgement are hidden in things we might take for granted—like the distinction between thinking of the book I’m holding as being Sonia Sanchez’s We A BaddDDD People, as opposed to a unique physical document inscribed with a text that intends to convey We A BaddDDD People . So I want to offer a couple more examples of typographical concerns that came up during my transcription process. Unlike extra spaces between words in a line, these issues also more directly impact the kinds of results my analysis aims to produce, as they impact what “counts” as a line or stanza in my model. The first has to do with stanza breaks. In my day-to-day reading practice, identifying a stanza break usually feels straightforward: lines grouped together in a poem, probably separated by white space. Digging a little deeper, The Princeton Encyclopedia of Poetry &amp; Poetics begins its entry by defining a stanza as “a unit of poetic lines organized by a specific principle or set of principles” (1358). Likewise, The Oxford Dictionary of Literary Terms defines a stanza first and foremost as A group of verse lines forming a section of a poem and sharing the same structure as all or some of the other sections of the same poem, in terms of the lengths of its lines, its metre, and usually its rhyme scheme. In printed poems, stanzas are separated by spaces. While this definition doesn’t help us much with something like Sanchez’s “a/coltrane/poem”—a poem that more or less flies in the face of traditional stanzaic form—it does seem like it would help us if we wanted to make a “stanza” a parameter in our analytical models, or even in figuring out how best to separate lines and stanzas in our text files. But even in more traditionally stanzaic poems—of which there are many in my corpus—deciding what “counts” as a stanza can get messy. Something as simple as page breaks, for instance, can wreak havoc in making such decisions. This is particularly the case when only one edition of a work exists, and one doesn’t have access to original manuscripts. Consider, for example, a poem titled “Malcolm Spoke/ who listened?” from Haki R. Madhubuti’s 1969 collection Don’t Cry, Scream, published with Broadside Press. The poem is stanzaic, and distinguishes stanzas with what seem to me like normal breaks. These groupings, however, have no regular rhyme scheme, no regular use of capitalization, no regular number of lines, no tight thematic or narrative structure (i.e. a point of view that alternates from stanza to stanza), and no regular pattern in punctuation (i.e. some stanzas conclude with no punctuation while some conclude with a period). And, crucially, the poem extends partway onto a second page. These are the two groups of lines on either side of the page break: animals come in all colors. dark meat will roast as fast as whi-te meat [PAGE BREAK] especially in the unitedstatesofamerica’s new self-cleaning ovens. For a few reasons, I decided to transcribe these two sections as a single stanza. First, at a more visual, design level, the poem has no other stanzas as short as two lines. The book as a whole, in fact, has very few two-line stanzas, and while there are a few single unattached lines, they usually come right at the end of a poem. In comparison with the rest of the poem and the other poems in the collection, then, it seemed more likely to be a larger stanza than not. More convincingly, however, my feeling that these two chunks are one unit comes from the poem itself—the group of lines above seems, to me, to develop a coherent line of poetic thought. The first two lines introduce the metaphor of meat of “all colors” roasting, and the following line (after the page break) intensifies this imagery by locating this metaphor in the United States and its “new /self-cleaning ovens.” The lines after the page break make most grammatical and metaphorical sense when taken as part and parcel of the lines prior to the page break. This is not to say that other poems in this volume don’t break up grammatical expressions across stanzas—they definitely do. Other poems in this volume also develop specific metaphors or images over the course of several stanzas. But with this poem in particular, stanzas seem to be doing something else. Each has a kind of conceptual focus—they stand alongside one another as evenly-weighted, coherent units of expression. For example, the stanza preceding the one quoted above is as follows: the double-breasted hipster has been replaced with a dashiki wearing rip-off who went to city college majoring in physical education. This stanza develops, from line to line, a description of—and stance towards—this “dashiki wearing rip-off” who replaces the “double-breasted hipster.” Each line builds on the last, slowly unfolding different aspects of how one figure “has been replaced” with another: the speaker discloses a skeptical attitude towards these figures, identified by what they wear, where they went to school, and what they studied. Like the stanza with the page break, this group of lines seems to me to develop a coherent line of thought that doesn’t spill over into subsequent stanzas. Understanding these stanzas in light of the poem as a whole, then, aligns with this reading: the rhythm of the poem as it moves from stanza to stanza seems to emerge from a feeling of moving from one idea to the next—and, for me as a reader, breaking this group of lines at the page break into two different stanzas feels like it disrupts that rhythm. It could certainly be argued that the group of lines with the page break was meant to be two stanzas specifically so as to disrupt the rhythm of this stanzaic form—that such a disruption is vital to the poem’s meaning. But, as is the case with scholarly editing, I had to make a judgement call to proceed with my project. So I considered everything I knew, tried to find out more if possible, and made the best decision I could given what I had in front of me. Runovers One last example. Lines of poetry can get very long. Sometimes, lines get too long for the physical documents on which they’re inscribed. During an enlightening conversation with Jahan Ramazani on this and many other issues addressed in this post, he gave me the example of editing The Norton Anthology of Modern and Contemporary Poetry and having to print and number the extremely long lines of Allen Ginsberg’s “Howl.” Central to this decision-making process was considering standard practice on what the Chicago Manual of Style calls “Long lines and runovers in poetry.” The CMS defines runovers as “the remainder of lines too long to appear as a single line,” which are “usually indented one em from the line above.” In other words, when lines get too long—as in Ginsberg’s poetry, or Walt Whitman’s—a hanging indent about an em-dash in length tells the reader that the line was too long for the book. The entry concludes, however, by indicating that it might not always be so clear when an indentation is a runover and when it’s a new line: Runover lines, although indented, should be distinct from new lines deliberately indented by the poet … Generally, a unique and uniform indent for runovers will be enough to accomplish this. As we’ve seen already just in this post, much of poetry in my corpus rebels against traditional poetic form, including standard indentation and spacing practices. Determining whether or not a group of words is one or two lines, however, is extremely important for my project. The “line” is the basic unit I’ve been asking sentiment analysis tools in TextBlob and NLTK to evaluate for sentiment. In short: what counts as a line really matters, and ambiguities surrounding runovers could very well add up to have a significant impact on the results of my analyses. An excellent example of this appears a few pages earlier in Madhubuti’s Don’t Cry, Scream, in a poem titled “Gwendolyn Brooks.” The poem is available online through the Poetry Foundation, and it appears in my physical copy as it does on this website, indentations and all. Halfway through the poem there is a distinct sequence, over a dozen lines long, that lists a series of portmanteaus describing different kinds of “black”—from “360degreesblack” to “blackisbeautifulblack” and “i justdiscoveredblack.” Over the course of this sequence, there are three indented lines, each one-word long, that interrupt the otherwise steady stream of images. At first bluff, these lines struck me as runovers. The list-like nature of the lines felt like they lent themselves to running a little long—as we see with a poet like Whitman, once a list starts, it can just keep going and going. Moreover, no thematic or poetic reason jumped out at me as to why someone might indent these words as opposed to any others. Of course, there is the possibility that such indentations were completely on purpose, and are part of a project to disrupt and transform any resonance with someone like Whitman and the canon he represents. Sitting in front of my computer, a little bleary-eyed from all the transcribing, I honestly wasn’t sure. So I began looking for other appearances of the poem. The version published by the Poetry Foundation complicated my initial thought that these one-word indented lines were runovers. Jahan Ramazani also suggested that, given the importance of anthologies to the Black Arts Movement, even if a book has no later editions, individual poems therein might appear somewhere in a collection. Such a realization, however, presents another fork in the road of my research. As a researcher committed to being as thoughtful and thorough as possible as I work with the poetry from a revolutionary art movement, I am delighted to know that I still might be able to pursue questions that I thought would remain unanswered (i.e., “is this a runover line or two separate lines?”). As a researcher with limited resources, however, I have to decide whether or not pursuing these questions will be the best use of my time and energy in this particular project. There are a lot of anthologies containing poetry from the Black Arts Movement out there, so I have to weigh the time it would take to locate and look through them all for instances of those poems from my ~20 book corpus that may have runover lines, against the potential impact it would have on the results of the analyses I hope to perform. As it currently stands, I’ve made a note of this particular ambiguity and plan to reassess what I should do with it and others like it after assembling the rest of the corpus. Final Thoughts As this post has hopefully shown, transcribing texts from book to screen can get very tricky. More than a simple act of mechanical reproduction, it can stump us with questions about literary works that seem to have no discernible answers. From one moment to the next, it can demand a working knowledge of bibliographical methods; digital methods; aesthetic form; and how to manage a project’s resources. And—as Drucker above argues regarding text analysis more generally—navigating these questions requires rigorous human judgement every step of the way. Even in situations where the practicalities of project management and the realities of our textual archive make this judgement feel all-too-fallible. There are other, important aspects of this human judgement which I haven’t had time to think through as much as I would like to have in this post. For example, digging deeper into those questions explored by Andrew Pilsch mentioned above that investigate the challenging ways in which web browsers are designed to parse the whitespace in poetry in HTML. Or, how the default parameters of the basic tokenizing packages in NLTK throw away whitespace—the idea that the programmers behind these text analysis technologies view their standard use as most likely to focus on text, not the spaces between text. Very long story short: transcription is complicated! And I hope this post has done something to foreground some of those invisible, behind-the-scenes decisions that—like modeling and parameterization—give shape to the results a text analysis project produces."},{"id":"2018-02-28-digital-humanities-dissertation-starter-kit","title":"Starter kit for considering a DH dissertation","author":"amanda-visconti","date":"2018-02-28 01:30:07 -0500","categories":["Digital Humanities"],"url":"digital-humanities-dissertation-starter-kit","layout":"post","content":"Cross-posted on the Modern Languages Association (MLA) Committee on Info Tech blog . Successful non-traditional dissertations include a comic book ( Nick Sousanis ), a hip-hop album ( A.D. Carson ), code and design without written chapters ( me ), and the use of digital formats and methods such as a Tumblog counter-narrative ( Jade E. Davis ) or topic modeling ( Lisa Rhody ). Are you curious about using digital methods or forms to pursue your dissertation research questions? Or maybe dissertational gate-tenders (advisors, mentors, departments) have you seeking successful examples of DH as part of the dissertation. Wherever you’re coming from, here’s a short selection of readings to get you started exploring digital humanities as doctoral scholarship: Humanities Commons hosts around 70 dissertations ; you can search for “digital humanities” to whittle these down to around 10 DHy theses such as Jason Rhody’s Game Fiction and Alex Gil’s Migrant Textuality . DH discussions under the aegis of scholarly organizations like MLA or AHA spend a good portion of time on concerns around evaluation and promotion relevant to the dissertation process. Resources you might check out on Humanities Commons include the Digital Humanists Group and events like the Online Forum on DH and Medicine recently posted on the Medical Humanities site. #RemixtheDiss Models ( hosted by the Futures Initiative at the Graduate Center CUNY and HASTAC ) A collaboratively built GoogleDoc with information on a variety of dissertations using new formats and methods, including digital humanities approaches. Info includes each dissertation’s university and department, tools and media used, status of the dissertation (possibly outdated), and contact addresses for getting on touch with these dissertators. Our Praxis Program ( hosted by the Scholars’ Lab at the University of Virginia Library ) Now in its seventh year, the Praxis Program offers 7 examples of what a small team of (usually pre-ABD) humanities graduate students new to DH can learn, research, and build over the course of two semesters. If you’re a UVA grad student, check out our full offerings for DHy graduate fellowships as well as our variety of informal services for graduate students . DH Guidelines for Dissertations and Tenure Zotero Library ( 46 items ) Tenure and promotion cases can be models for DH dissertations, with the same issues of evaluation, effort, packaging, and translation at stake. HASTAC ( the Humanities, Arts, Science, and Technology Alliance and Collaboratory ) A good community to read what other students in similar fields and stages of the degree to yours are thinking about, and for becoming comfortable with scholarly blogging. See especially those posts tagged as digital humanities or digital dissertations . Some of the materials I created before or during my completed DH dissertation might also be relevant: My PhD Exams reading list, including an introductory argument for the items on the list and two sets of materials: broad digital humanities readings, and textual scholarship with a focus on digital scholarly editing. Among other benefits, making DH a focus of your exams reading can help you understand your exam committee’s (as a potential dissertation committee) attitudes toward DH. A Zotero library of some of the list’s readings is here . My dissertation itself is a website: Dr.AmandaVisconti.com . The focus of the dissertation was the design, building, and analysis of a participatory digital edition, InfiniteUlysses.com . I blogged throughout the dissertation process. I particularly recommend the linked posts on defense advice, the actual defense talk, evaluating a DH dissertation, thinking through diversity and inclusion in your dissertation scholarship, planning out the many pieces of a dissertation project, and choosing the best format (chapters? something else?) for your scholarship ."},{"id":"2018-04-02-writing-in-public-on-purpose-at-washington-lee-university","title":"Writing in Public (on Purpose) at Washington & Lee University","author":"catherine-addington","date":"2018-04-02 12:48:37 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"writing-in-public-on-purpose-at-washington-lee-university","layout":"post","content":"Makerspace Technologist Catherine Addington went to Washington and Lee University to give a workshop in digital humanities through a Mellon-funded collaboration with the WLUDH . More information about this initiative can be found  here . Her post is  cross-listed on the W&amp;L blog . I am  a graduate student in Spanish, a freelance writer, a newsletter creator, a former full-time media professional, a prolific blogger, a website manager, and an incessant tweeter . As such, I felt particularly excited to be invited to give a guest lecture in Prof. Sydney Bufkin’s course, Writing in Public, as part of the Scholars’ Lab collaboration with Washington &amp; Lee University. In order to give students direct experience with public writing themselves, Prof. Bufkin had encouraged them to purchase their own domain name, set up their own website, and try out a Twitter account. I visited during the last week of classes to describe how I had used these tools in my own career, and help students envision how they could use them after the course draws to a close. Prof. Bufkin and I had three main goals for the lecture: Model both personal and professional relationships with an individual digital presence for students. Inform students about public writing careers and the publishing process, particularly as a freelancer. Lead students in brainstorming plans for their own domain name and Twitter account. The text of my talk follows. The presentation slides are also available here . Writing in Public (on Purpose) aka how to grow up on the Internet and reverse-engineer that into something professional Thanks to all of you and to Professor Bufkin for having me. I’m currently a graduate student in Spanish at the University of Virginia, but I’m also a longtime public writer, onetime professional journalist, and a part-time website manager. I’ve called this talk “Writing in Public (on Purpose)”, but the subtitle is what I really want to focus on: how to grow up on the Internet and reverse-engineer that into something professional. Because that may be the one thing in which I am a bona fide expert. So today I’m going to talk to you about my own relationship with public writing, and hopefully help you to work out what you’d like yours to be. But I’d like to start with a short exercise: please Google yourself. Don’t worry, I’m not going to ask for results. But raise your hand if you found something positive, something you’re proud of. Now raise your hand if you found something negative, or something you feel is kind of silly. Now raise your hand if you found nothing in particular. You can see for yourselves: most likely, you’re going to have an Internet presence of some kind. It’s just a question of how much control you have over it. Today I want to share with you what getting control over that looked like for me, and what it might look like for you. First, I plan to discuss my own experience with balancing personal and professional public writing. Then, I’ll describe what public writing is like as a career. Finally, I’ll turn it over to you all to workshop how you can use public writing for your own goals, and specifically what you plan to do with your domain name once this class ends. I’ll start with my own experience. My first website was a fandom-heavy Tumblr, and an angsty teenage diary. I didn’t think of this as public writing at the time. My friends and I all used Tumblr as a social network, not a blogging platform. It was all Champions League highlights and emo song lyrics. But because someone felt the need to run my account through the Wayback Machine, this early iteration of my Tumblr blog, and the various URLs it took on later, are preserved for posterity. That’s actually still my website, but now it’s a portfolio and blog, too. If you go far back enough in the archives, you’ll still find plenty of fandom and angst. In fact, you’ll still find plenty of both. What’s changed is my intention and self-awareness about it being a public-facing platform. Similarly, my Twitter also started out as fan enthusiasm. Here you can see me waxing poetic about FC Barcelona’s former goalkeeper. Now, Twitter has become my professional network too. Here you can see two examples of my writing being shared and commented on by other writers. (But of course, I still tweet plenty about soccer.) My point is: you know how sometimes a friend will comment on an old profile picture on Facebook so that it’s at the top of everyone’s news feed and suddenly all your college friends know what you looked like when you were 14 and a mess? Well, having had a blog and a public Twitter since I was in high school is like that, but in public, and your employer is invited. I have more embarrassing tweets and angsty tumblr posts out there than I could ever take back. That may not be the case for you, but if it is, I have some advice. If you, too, are growing up on the Internet: Be able to laugh at yourself (and forgive yourself, when you need to). Everyone has had a phase they wish everyone else would forget. Nobody cares about your embarrassing memories as much as you do. The Internet has not fundamentally changed this, we just have to laugh at ourselves a little louder. Personality is an asset. People actually do not want the sum total of your human existence to be a soulless LinkedIn page. I mean, you might need a LinkedIn if that’s common practice in your field, but just remember that people like people, not brands. I once interviewed a potential intern because she worked on a blueberry farm. Of course, she actually got hired because she was qualified and seemed like someone we’d enjoy working with, plus there are real professional skills from working on a blueberry farm—talk about initiative and teamwork! But being able to actually stand out from a pile of papers is a good thing. More importantly, there’s more to life than work, and you are going to want to spend it as a person, not an idea. Kill it with excellence. This is my main strategy, to be honest. You can’t always take things back, but you can often drown them out. The Internet is great for making a lot of noise. People are welcome to dig up my nonsense tweets but they’ll have to pass by a fair amount of intelligent conversation to get there too. Now let’s put that into practice and talk about how to professionalize your digital presence. I used two main tools for this, and they’re the same ones you use in this class: a personal website and a Twitter account. As I see it, the personal website has three main functions: place, process, and product . Place : the personal website as business card. Many professionals have a website that includes a short bio, links to their social media, and ways to get in contact. Some include a print or digital version of their resume as well. Process : the personal website as blog. Sharing “process” writing looks different for everyone. It can be a reflection on your study abroad travels, a step-by-step description of your latest experiment and results, a discussion of the readings you did for class, or even just stray observations from your day that you find interesting. Personally, I develop ideas best in conversation, and find that blogging is a good way to get feedback. I also use my newsletter, Cartas de América, for this purpose—that’s where I talk to friends and readers about my academic research. Product : the personal website as portfolio. Sharing “finished products” will also look different for everyone. I have two of these: a “ writing &amp; media ” tab for published writing and podcast episodes, which I link at the bottom of my resume; and a “ teaching portfolio ” with example lesson plans and a statement of teaching philosophy. For you, it might be a multimedia gallery with art or design samples, a list of relevant coursework and job experience, a web store, or photos of events you’ve planned or projects you’ve collaborated on. As for Twitter, making it not just my personal thoughtstream but also my professional network was a complicated process I’m not going to get into here, but I will say this. Twitter is a great place to get mentorship, directly or indirectly. Follow people who have your dream job, and watch how they use it. Classroom instruction will often prepare you with the existing knowledge in a field, but online conversation can inform you about what those same experts are working on next. Twitter is a good way to plug into conversations you might not have access to in your daily life as a student. Moreover, it gives you the opportunity to talk with professionals who are where you want to be—and lets you get your name in front of potential future employers, too. I’m not saying that it’s essential for you to have a personal website and a Twitter to be successful in life (though they help a lot if you want to work in media). I’m just saying it worked for me. You might need different tools depending on your own goals. But I do think that over time, digital literacy is becoming less “impressive” and more “expected.” Even if all this isn’t a “requirement” for your field, it may be an opportunity. Now that we have an idea of how to balance a personal and professional online presence, I’d like to share with you what I’ve done with mine and talk briefly about public writing as a career. I want to emphasize that I only worked one year as a full time journalist—and actually I spent most of that time producing a magazine, not writing for one. Most of my writing has been published as a side gig while a student, both undergraduate and graduate. This is called “freelancing”, and it paid my rent for several months this past year. (And if you’ve got friends in Charlottesville you know that’s saying something!) I want to highlight that up front to remind you that you’re already in the so-called “real world,” and what you think of as hobbies (like extracurriculars, clubs, fundraising, volunteering, events) are giving you real professional experience that you can and should frame as such. Freelance writing is a fairly straightforward process: pitch, edit, and publish. A “pitch” is just media lingo for a proposal: briefly explain what you want to write, why you’re the one to write it, and why this particular editor or publisher is a good fit. Each magazine or outlet has their own process for taking pitches, and usually they’ll post that on their website. But I usually contact individual editors—often people I’ve met through Twitter. In fact, most of my pitches have started as conversations or DMs on Twitter. The next step, once a piece is accepted, is to discuss expectations with the editor. I always ask for a deadline, a word count, and an idea of my compensation amount and procedures right up front. Some places pay upon publication, others upon acceptance of your piece. I want to pause here to encourage you to resist our collective social allergy to talking about money. This is especially crucial for freelancers, because plenty of places won’t pay you if you don’t explicitly ask to be paid. Young writers are particularly easy to scare with the concept of “writing for exposure.” But writing is work like any other, and you should be paid for it. I only give stuff away on my own terms (i.e. a blog)! This applies not just to writing, but to any professional field—do not be afraid to get and give straightforward money advice. After writing and submitting, a piece will go through rounds of editing and fact-checking. This usually happens on the magazine’s end, though sometimes a writer will be contacted with fact-checking questions. That’s why it’s important to carefully reference sources in a draft, as though writing academically, even if it’s a totally different medium. Once the piece is published, celebrate, because that’s awesome. And if you haven’t already, go get paid: invoice the publisher for the agreed-upon amount and keep a record for tax purposes. There are great online templates for invoicing freelance work of any kind, writing or otherwise. Meanwhile, I always make sure to save a PDF copy of my published work, since URLs aren’t stable. I also update my social media, website, and resume with any new pieces that I’ve published (which we call acquiring “clips”). The last step is crucial: engaging readers productively. That adverb is doing a lot of work. It means I don’t read the comments unless I’m prepared to see cruelty, but I do try to engage in interesting conversations that arise on Twitter in response to the piece. After all, what’s the point of public writing if you don’t actually want to talk to or hear from the public? I want to share two examples of this process from my own work, that I think exemplify varying degrees of success. Here’s the first one: one time at a work event in DC, I met an editor for The Spectator, which is a conservative British magazine with a significant following. I really like a lot of their arts and culture content, but I don’t personally share the magazine’s politics, so I wasn’t sure if it was a good idea to pitch the editor. But I also thought it would be a good opportunity to share my own perspective—and of course to get my name in an old, fancy magazine. So I pitched the editor on something I knew would get his attention: the then- (and still-) collapsing political and economic situation in Venezuela. Everyone likes a story about their political enemy doing poorly, and Latin America happens to be my academic specialty. The article was accepted, and the result was…interesting. As you can see here, the headline that the editors put on my article was exactly as I expected: look how terrible socialism is. But in my article, I argue that authoritarianism, not socialism, is to blame. My instinct was to take a self-aggrandizing lesson from this: well, when you’re a public writer, you have to be prepared for the public to see whatever they want to see. I think the real lesson is one in humility: you need to own your work. If you want to pick a fight—which I clearly wanted to here—you need to actually do that, instead of trying to sneak in your viewpoint. It’s a matter of respect for your audience. A more successful example of writing to a specific audience is this article I wrote for my former employer, The American Conservative . On the left, you have my undergraduate thesis, which you will not be surprised to hear was about soccer (specifically in Argentina). On the right, you have an article I wrote during the summer of 2014, when the World Cup was on. In the article, I used my thesis research to argue that Americans denouncing soccer as European and therefore socialist are playing into a long tradition of denouncing and then adopting foreign customs. Because that’s exactly what happened in Argentina—they denounced soccer as an English import but over time it became the popular obsession it is today. This article was more successful because I wrote with my audience in mind: they would likely be aware of the ongoing World Cup, and may have seen other conservative publications’ negative reactions to it. I took that established knowledge as the point of departure, and added more context. Instead of trying to sneakily insult my audience’s opinion (like I did in The Spectator ), I directly engaged and responded to events that my audience would be familiar with. Most of my public writing career consists of work like this, taking scholarly research and turning it into public writing—and taking work about Latin America and making it accessible for an English-speaking audience. Now that I’ve shared my own experience with public writing, it’s time to turn it over to you. But I doubt that all of you are in the very specific market of writing for niche magazines about soccer, saints, and Latin American politics, so my roadmap will be of limited use. So I want to start with the basic premise of project management: identify your goals first, and then pick the tools best suited to execute it. So before we talk about that domain name that you bought, let’s brainstorm. What are your goals for your digital presence? Let’s take five minutes to take notes individually on the following questions: What do you want people to know about you? When someone mentions you in conversation, what do you hope it’s in reference to? What do you hope people associate with you? Whom do you imagine as your target audience—the people reading your website? Family, friends, coworkers, industry experts, potential employers? Now, let’s look at some examples of what other young professionals have done with their personal websites. These are all people I know in real life, and few of them are professional web designers, so I want you to keep in mind that all of this was done with free or cheap tools to which you also have access. As you look through a site or two, think about these questions: Based on this person’s website, what do you know about them? What are you likely to associate with them? Whom do you think is this person’s intended audience? How does their website reach that audience? What do you like and dislike about the website? How could it be improved? (Students noted that Jorge Ariel Escobar used a visual-heavy website to showcase his photography, which matched his apparent goal of marketing his skills. They also reflected on Matthew and Maggie Loftus ’s use of their more text-heavy website to keep a blog about their medical practice in a way that was accessible to non-medical professionals, perhaps for a more personal audience since they describe living and working far from home. Finally, we discussed Eric L. Silver ’s website as an example of taking a colorful personal tone, even while aiming at a professional audience. After all, I pointed out, if you’re a podcast producer and writer, you want people to spend hours listening to your voice or reading your words. It doesn’t make any sense to excise your personality if that’s exactly what you’re selling!) Now that you have an idea of your own goals and a few examples to work with, let’s make an action plan for your domain name. Think about these questions: What elements of the example sites you looked at, if any, would help you achieve the goals you laid out in your brainstorm session? Think about the personal website as process (blog), product (portfolio), and place (business card). Which of those three elements would be useful to you, if any? I’ll leave you with a few recommendations for further reading and research pertinent to the topic. If you have any questions, feel free to ask now or get in touch ."},{"id":"2018-04-11-announcing-2018-2019-fellows","title":"Announcing 2018-2019 Fellows!","author":"brandon-walsh","date":"2018-04-11 11:47:34 -0400","categories":["Grad Student Research"],"url":"announcing-2018-2019-fellows","layout":"post","content":"We are thrilled to announce the 2018-2019 Scholar’s Lab fellows for the Praxis Program, the Digital Humanities Prototyping Fellowship, and the Graduate Fellowship in the Digital Humanities. We are welcoming 11 fellows from 6 disciplines from the arts, humanities, and social sciences and the School of Architecture. Our graduate fellows are joining a robust and vibrant community of past fellows! Praxis Program We are delighted to welcome 6 diverse disciplinary team members to the 8th year of the Praxis Program, our flagship introduction to digital humanities by way of collaborative, project-based pedagogy: Catherine Addington (Spanish) Zhiqiu Jiang (Urban and Environmental Planning, Constructed Environment) Eleanore Neumann (Art and Architectural History) Mathilda Shepherd (Spanish) Chloe Wells (Art and Architectural History) Chris Whitehead (History) Look forward to more details about the Praxis Program’s new project in the fall! Digital Humanities Prototyping Fellowship This year we will welcome 3 students to our short-term fellowship dedicated to boosting nascent digital humanities research in a flexible, collaborative environment. Alyssa Collins (English, Fall 2018) Christian Howard (English, Fall 2018) Sarah McEleney (Slavic Languages and Literature, Summer 2018) Alyssa and Christian will collaborate on a scraping and archiving project working with twitter data as it pertains to the relationship between literature and social justice. Sarah will be working on a project using machine learning to examine social realism in Soviet-era literature. These students will work with our R&amp;D team for a single semester to rapidly prototype their project ideas and position themselves for future work in digital humanities. Look for more from them this coming year! Graduate Fellows in the Digital Humanities Finally, we are looking forward to working with Kelli Shermeyer and Sean Tennant, our 2018-2019 Graduate Fellows in the Digital Humanities. Kelli Shermeyer’s (English) dissertation is titled  “Less-than-Human” Tragedy?: Ecologies of Suffering in Contemporary Tragic Drama . Sean Tennant’s (Art and Architectural History) dissertation is titled  Domestic Spaces in the Roman West: Architectural Adaptation in Gaul, Britannia, and Germania . These fellows will work with our team throughout the year on substantial research projects related to their dissertations. We are looking forward to working with all these fantastic students in the coming year!"},{"id":"2018-04-12-augmenting-an-iconic-structure-the-rotunda","title":"Augmenting an Iconic Structure: The Rotunda","author":"christian-howard","date":"2018-04-12 07:20:35 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"augmenting-an-iconic-structure-the-rotunda","layout":"post","content":"The Praxis team has been hard at work developing our project, and we’re now beginning to unite the various aspects of UVA Reveal, including our work in Unity, our work on linked data, and our work on a website. Part of this unification has involved crafting stories about the university: what do we see, and what remains hidden? How can we visually represent spaces of the university, especially spaces that have been erased or largely forgotten? Is it as simple as using augmented reality to display images that are otherwise not there, or is there another way that we can utilize the space? The Foundations of the Rotunda &amp; the University I’ve been thinking about space particularly in relation to the Rotunda, which stands at the heart of the University of Virginia. The Rotunda has a rather troubled history. In 1821, Jefferson designed the Rotunda and proposed his design to the Board of Visitors. Construction began in 1822 and was completed in 1826, though the steps leading up to the South Portico were not built until 1832. On October 27, 1895, the Rotunda caught fire, most likely due to faulty electrical wiring (which was installed in 1888). Unable to subdue the fire, students and faculty rushed to save books and other objects – including the heavy statue of Jefferson inside the Rotunda – from the flames. In 1896, the Rotunda was redesigned by Stanford White. About the changes White made to Jefferson’s original design, the UVA Rotunda Preservation Committee has written: “Jefferson had designed the Rotunda with three floors, and Stanford White changed it to two floors, making the Dome Room much bigger. White sought to merge Jefferson’s design with the University’s needs, which still included using the Rotunda largely as a library. He also designed east and west wings on the north side of the building, to match the south wings, and he connected the wings with colonnades. He designed the Rotunda with central heating and a mechanized ventilation system.  While the central heat was retained, the ventilation system was scrapped because of cost” (see http://rotunda.virginia.edu/history ). The Rotunda was rededicated in 1898. The Rotunda has been restored several times since the fire, including in 1976 for the bicentennial celebration. The most recent restoration work began in 2012 and was completed in 2016. Partly because of its iconic stature, the Rotunda has been host to numerous events and countless visitors. Notable among these are Gertrude Stein, Queen Elizabeth II, and Michelle Obama. To see images (primarily from UVA’s Special Collections) showing some of these famous events and visitors, please view the video on our website. Yet if the Rotunda has served as the public face of a public university, this same university was built and run by the labor of enslaved African Americans from 1817 until 1865. According to Brendan Wolfe, “Most of the university’s first enslaved laborers were rented from local landowners and worked alongside whites and free blacks in performing all the tasks associated with building what the school’s founder, Thomas Jefferson, called the Academical Village. In March 1825, the first students arrived and African Americans transitioned to working in the pavilions, hotels, and the Rotunda; maintaining classrooms, laboratories, and the library; ringing the bell; and serving the daily needs of students and faculty.” Most enslaved laborers at the University of Virginia were rented from their owners for a set period of time. Wolfe records that “At the height of building, in 1820, the university paid $1,099.08 in hiring fees. In 1821, hired slaves cost the university $1,133.73, while in 1822 that figure dropped to $866.64, and in 1825, as construction was being completed, it fell to $681. On average, the board of visitors paid owners $60 per slave per year, although the particular amounts depended on enslaved men’s ages, physical conditions, and skills.” If this were not enough, there has been a history of institutional racism at the University of Virginia. Prior to the Civil War, many students and even faculty were outspoken about their support for slavery. In 1850, for instance, students founded the Southern Rights’ Association of the University of Virginia. The Southern Rights’ Association drafted a constitution and published 2,500 copies of a pamphlet titled “The Address of the Southern Rights’ Association of the University of Virginia.” This pamphlet proclaimed the association as “united for the purpose of promoting, as far as we may be able, unity of sentiment and concert of action in the slave holding States against our aggressors.” Numerous professors also promoted pro-slavery ideology. For instance, Professor Basil L. Gildersleeve, who was a professor of Greek and Hebrew at the University of Virginia from 1856 until 1873, published an essay titled “Miscegenation” in the Richmond  Examiner  on April 18, 1864. In this essay, Gildersleeve argued for the purity of the “white race” and warned against race mixing. He writes: “A jealousy natural to our English blood and fostered by our peculiar system, has prevented the intrusion of mongrels [i.e., individuals of mixed race], even of the third and fourth generations, into the society and the privileges of the white race. In no other part of the world, in which the two races [i.e., “white and black races”] have existed, side by side, has this exclusion been so absolute; and it is to this watchful care, which is so natural, that we keep it up unconsciously, and which is so much to our interest that we take no credit to ourselves for it—it is to this watchful care that we owe the supremacy of the white man on the continent, and that we look down so proudly on the mixed population of Mexico and the twenty-two cross-breeds of Lima.” Representing History, Augmenting Spaces Given this complex history, the Praxis team has been faced with a number of challenges, which I’ll pose here as questions: What are our ethical responsibilities as researchers and scholars when it comes to augmenting spaces at the University of Virginia? How do we represent the many stories of the university without privileging any single one? How do we celebrate the successes of the university without downplaying the atrocious foundations upon which it was built? Conversely, how do we honor the lives of those enslaved African Americans who made the university what it is today without slipping into maudlin reflections about the university’s past? We’re far from satisfactorily answering these questions, but I think it safe to say that our reflections have been fruitful nonetheless. That is, given our attempts to balance these stories, we’ve decided to utilize space as a natural organizer of time and history. Let me give you an example by describing our augmentation of the Rotunda. We chose to augment two different views of the Rotunda: the first view is that of the front side (the side facing University Avenue), and the second view is of the plaque dedicated to the men and women who helped build the Rotunda and the Academic Village. By augmenting these two spaces, we hope to contrast the public-facing and the “hidden” parts of the university, if you will. The side facing University Avenue stands as the public face of the University, and our augmentation thus showcases the famous people who have visited UVA. Yet this view is juxtaposed with the plaque to “both free and enslaved” men and women who helped build the university. Physically, this plaque is placed beneath the portico of the Rotunda, embedded into the brick walkway and stepped on by students and visitors alike. This plaque is therefore ideally placed for our augmentation of the “hidden stories” surrounding the construction of the university. These augmentations will include images held in the UVA Special Collections (such as historical images of the Rotunda and photographs of former enslaved African Americans at UVA), as well as short video and audio clips that highlight relevant information about the enslaved laborers who helped build the rotunda and UVA’s involvement with the Confederacy. In this manner, our augmentations attempt to spatially reconstruct not only the stories surrounding the university, but also the historical attention to those stories. Current Efforts to Reform It would be remiss of me to not mention the current efforts of the university to address its troubled history. In 2013, Dr. Marcus Martin, Vice President and Chief Officer for Diversity and Equity, proposed that a commission be formed to further explore the topic. The result was the “The President’s Commission on Slavery and the University” ( http://slavery.virginia.edu/ ), which includes the creation of a Memorial to Enslaved Laborers (estimated to be completed in Spring 2019) and the establishment of the annual Cornerstone Summer Institute for high school students that “encourages critical thinking while students learn about both the University’s past and the modern-day legacies of slavery” ( http://slavery.virginia.edu/cornerstone-summer-institute/ ). The Commission has also created a short documentary about slavery at the University (“Unearthed and Understood”). [embed]https://youtu.be/d_997dhrOtM[/embed] Founded in 1969, the Black Student Alliance (BSA) at UVA seeks to “represent the social and political concerns of the African-American student body, and to establish a more perfect union between the African-American, multi-cultural, and University communities” ( https://www.bsaatuva.com/ ). The BSA has been instrumental in implementing changes at the University, and following the August 11th riots, the BSA successfully advocated for the removal of a plaque on the Rotunda that “commemorated members of the University who died serving the Confederacy” (“An In-Depth Look at BSA Demands”). The BSA continues to play a fundamental role in promoting racial awareness at the University. Faculty efforts include a multimedia initiative, Black Fire at UVA ( https://blackfireuva.com ), which “document[s] the struggle for social justice and racial equality at the University of Virginia” through photos, videos, and audio interviews. Black Fire is sponsored by UVA’s Office of the Vice Provost of the Arts and organized by Professors Kevin Jerome Everson of the Department of Art and Claudrena N. Harold of the Carter G. Woodson Institute for African-American and African Studies and the Corcoran Department of History. The Demographics Research at the University of Virginia has created a “Racial Dot Map” that visualizes the “geographic distribution, population density, and racial diversity of the American people in every neighborhood in the entire country” using information from the 2010 US Census. This map thus displays how segregation is still endemic to the United States and shows how much more work needs to be done. Image 6: A screenshot of “The Racial Dot Map” that visually displays how neighborhoods are segregated in Charlottesville For more information, please visit our project website, reveal.scholarslab.org . Works Consulted and Further Reading “ The Address of the Southern Rights’ Association, of the University of Virginia, to the Young Men of the South. ” Broadside 1851 .V8, Special Collections, University of Virginia, Charlottesville, Va. Black Fire at UVA . Organized by Professors Kevin Jerome Everson and Claudrena N. Harold. Sponsored by UVA’s Office of the Vice Provost of the Arts. 2018. Web https://blackfireuva.com/ . Black Student Alliance Movement . 2018. Web https://www.bsaatuva.com/ . Cable, Dustin. “The Racial Dot Map.” Demographics Research Group at the University of Virginia . July 2013. Web https://demographics.coopercenter.org/racial-dot-map/ . Clemmons, Anna Katherine. “Period Pieces: New Book, Exhibits Showcase More of Those Things that Tell UVA’s History.” The University of Virginia Magazine . 2018. Web http://uvamagazine.org/articles/period_pieces . Gildersleeve, Basil L. “Miscegenation,” Examiner . April 18, 1864. Web https://www.encyclopediavirginia.org/_Miscegenation_by_Basil_L_Gildersleeve_April_18_1864 . Gravely, Alexis, et al. “An In-Depth Look at the BSA Demands.” The Cavalier Daily . August 31, 2017. Web http://www.cavalierdaily.com/article/2017/08/an-in-depth-look-at-the-bsa-demands . “Meigs Rotunda Plan (1859).” Jefferson’s University . Web http://juel.iath.virginia.edu/node/250 . Montes-Bradley, Eduardo. “Unearthed &amp; Understood: Slavery an the University of Virginia.” YouTube, Rector and Visitors of the University of Virginia. 21 Apr. 2017. Web http://www.youtube.com/watch?v=d_997dhrOtM . “President’s Commission on Slavery and the University.” University of Virginia . 2013. Web http://slavery.virginia.edu/ . “The Rotunda at the University of Virginia.” University of Virginia . 2018. Web https://rotunda.virginia.edu/ . “The Rotunda: History.” University of Virginia . 2018. Web http://rotunda.virginia.edu/history . “Rotunda Reborn: Videos about the Historic Renovation.” Virginia Magazine . Fall 2016. Web http://digital.uvamagazine.org/articles/rotunda-reborn/videos.php . Svrluga, Susan. “UVA Acknowledges Its Slave History.” The Washington Post . June 24, 2015. Web https://www.washingtonpost.com/news/grade-point/wp/2015/06/24/u-va-acknowledges-its-slave-history/?utm_term=.281948bf7601 . “Teaching with Historic Places: Visual Evidence.” National Park Service . Web https://www.nps.gov/nr/twhp/wwwlps/lessons/92uva/92visual2.htm . Wolfe, Brendan. “Slavery at the University of Virginia.” Encyclopedia of Virginia . Virginia Foundation for the Humanities, 2 Feb. 2016. Web https://www.encyclopediavirginia.org/Slavery_at_the_University_of_Virginia . Wolfe, Brendan. “Henry Martin (1826–1915).”  Encyclopedia Virginia . Virginia Foundation for the Humanities, 15 May. 2017. Web https://www.encyclopediavirginia.org/Martin_Henry_1826-1915#start_entry ."},{"id":"2018-04-24-poems-with-pattern-and-vader-part-1-quincy-troupe","title":"Poems with Pattern and VADER, Part 1: Quincy Troupe","author":"ethan-reed","date":"2018-04-24 10:55:31 -0400","categories":null,"url":"poems-with-pattern-and-vader-part-1-quincy-troupe","layout":"post","content":"(This post is part of a two-post series—I ended up having too much to say about the two poems I looked at with VADER and Pattern, so I split it up. Second half can be found here !) Quincy Troupe’s “Come Sing a Song”—the 11-line poem that opens his 1972 collection Embryo Poems, 1967-1971 —welcomes the reader with a series of invitations that are also requests. Apostrophizing in the imperative, the speaker begins with an appeal (“Come sing a song, Black Man”) and goes on to make similar appeals in almost every subsequent line. For example, the final three lines of the poem: sing jazz, rock, or, R &amp; B, sing a song Black Man, sing a “bad” freedom song A first reading of this poem might see it as an invitation for black life to be newly acknowledged, recognized, and celebrated. More specifically, the speaker grounds this “singing” in black history, particularly black music, asking elsewhere in the poem that “a blues,” “a blackblues song,” and “a work song” be sung. In this sense, we might see in this poem a desire that this recognition and celebration of black life be sung by black voices with an ear for black audiences. In the context of the Black Arts Movement, this reading makes intuitive sense: many of Troupe’s contemporaries regularly invoked and entered into dialogue with black music (listen, for example, to Jayne Cortez’s performance of her poem “How Long Has Trane Been Gone” ; or to Sonia Sanchez as she discusses her first public reading of “a/coltrane/poem” ). And many writers of the BAM sought explicitly to make art that spoke first and foremost to black communities—see, as two examples among many, Addison Gayle, Jr.’s extensive introduction to the edited collection The Black Aesthetic (1972), in which he argues that “today’s black artist … has given up the futile practice of speaking to whites, and has begun to speak to his brothers” (xxi). Or Haki Madhubuti’s (then Don L. Lee) 1968 article in Black World / Negro Digest, where he claims that “Black poets write out of a concept of art for people’s sake and not art for art’s sake. … The black poet is writing to black people and not to whites” (28). This reading is also in keeping with the general scholarly consensus on Troupe’s work. The first sentence of his entry in the academic reference series Black Literature Criticism describes Troupe as “an acclaimed African American author whose jazz-inflected poems explore political and personal themes and celebrate the contributions of black artists, writers, musicians, and athletes” (310). With all this in mind, it makes sense that, in opening the collection, Troupe’s “Come Sing a Song” feels almost like a kind of invocation, asking black voices to sing songs celebrating black life. PatternAnalyzer, the default sentiment implementation in TextBlob (which makes use of the Pattern sentiment classifier), considers “Come Sing a Song” to be the single most negative poem in the entire corpus. As one might gather from my reading above, I disagree strongly with Pattern’s judgment in this case. In a corpus of poetry containing direct attacks, extreme invective, and explicit takedowns of individuals, groups, and institutions, I did not find this poem to contain an exceptional amount of negative sentiment. On the contrary, I found “Come Sing a Song” to be positive and celebratory. So: Pattern’s and my reading of this poem do not stack up. This program, with features designed to evaluate sentiment in text, is, to my mind, clearly missing something with regards to evaluating the sentiment in “Come Sing a Song.” That said, however wrong I find Pattern’s understanding of this poem to be, I don’t find this wrongness to be particularly bizarre or bewildering. Pattern is, after all, just following instructions—making programmatic decisions about how much positive or negative sentiment is in a given snippet of text according to rules given to it by humans. As they say in their article, the humans that built Pattern intended it to be “a Python package” that “provides general cross-domain functionality” across “web mining, natural language processing, machine learning and network analysis, with a focus on ease-of-use,” a goal at which it is very successful. They did not intend Pattern to be a thoughtful or savvy reader of modern American poetry. Knowing this, how are we to make sense of Pattern’s analysis of Quincy Troupe’s poem? Using Pattern and VADER to Read BAM Poetry In this post, I hope to make sense of disagreements like this—between a sentiment classifier like Pattern and a trained human reader like myself—with an eye for the larger use context of sentiment analysis, and my use of it in the study of poetry. I’ll do so by talking about two sentiment classifiers—Pattern (via TextBlob) and VADER —and some initial results of using these programs to analyze my corpus of texts. Because of the scale of my analysis—26 books of poetry—these results are, for the most part, exploratory. I haven’t used these tools to try to make any general claims about this incredibly diverse body of poetry. Rather, my analyses have tried to incorporate insights from these methods into existing scholarly conversations, while also confirming how fraught these methodologies can be when used to analyze poems that regularly tie formal experimentation to an explicitly political quest for racial justice. Another note: while I won’t delve into it in this post, it is important to acknowledge that many of these poets were highly attuned to decontextualized reading practices at work during the period, particularly those that aimed to undermine their quests for justice: government surveillance programs, active FBI counterintelligence operations, and a larger cultural climate fearful of radical thought. In this sense, the use of distanced computational techniques like Pattern and VADER might be seen as a troubling echo of the interpretive practices employed by FBI agents (a group one scholar describes as “ghostreaders”) involved in J. Edgar Hoover’s COINTELPRO, a 1956-1971 FBI program designed, in Hoover’s own words, to “expose, disrupt, misdirect, discredit, or otherwise neutralize the activities of black nationalist, hate-type organizations and groupings, their leadership, spokesmen, membership, and supporters” (3). This program systematically targeted groups and individuals fighting against racial injustice, including operations against targets within the Black Panther Party as well as Martin Luther King, Jr. In these operations the FBI often illegally and intentionally violated the rights of those targeted: in its final report, the Senate Church Committee formed to investigate oversight in government intelligence activities declared, among other things, that “[i]ntelligence agencies have undermined the constitutional rights of citizens.” In short: my goal here has been to explore the possibility of using these computational tools in a way that pursues questions, problems, and lines of inquiry centered around black thought and experience, including longstanding concerns and topics of interest in BAM scholarship. So if I want to know more about what natural language processing techniques can show us about things BAM scholars have already identified as noteworthy about this poetry—i.e., strategically heightened affects—I also need to take a hard look at what Pattern or VADER do on a line by line basis. This includes questioning those biases and assumptions these programs bring into their evaluations. In this two-part post, I’ll do so by comparing results from Pattern and VADER in the analysis of two poems: first, Troupe’s “Come Sing a Song,” and second, Nikki Giovanni’s “The True Import of Present Dialogue, Black vs. Negro” (from her 1968 Black Feeling, Black Talk ). I have chosen these two poems in particular because each, in one way or another, came to the fore over the course of my exploratory use of computational methods. For example: though both sentiment classifiers consider Troupe’s Embryo Poems as a whole to be somewhere in the middle of my corpus in terms of sentiment, Pattern thinks “Come Sing a Song” has more negative sentiment than any other individual poem. Likewise, VADER considers Giovanni’s “The True Import” to have the most negative sentiment in the corpus, but Pattern disagrees to the point of assigning the poem a positive score. In short: even when I disagree with the findings of these programs, computational methods have helped guide me, in an exploratory fashion, to poems or groups of poems which I have then re-read, thought through, and analyzed using more conventional literary methods (i.e., historical contextualization, close reading, consideration of relevant scholarship, etc.). What follows is a small window into the first stages this process, showing how my thinking has played out in two examples. Troupe’s “Come Sing a Song” Which brings us back to Troupe’s poem. To recap: Pattern considers “Come Sing a Song” to be the most negative poem in my corpus. I do not. In the opening paragraphs of this post, I discussed how I as a human reader thought through the positive, celebratory affective dimensions of this poem, looking to historical context, BAM scholarship, and so on. So how, exactly, does Pattern go about evaluating “Come Sing a Song” for its sentiment? Because it is a lexicon-based classifier, Pattern’s sentiment analysis of a poem boils down to checking each word in a given snippet of text (in this case a line) against a dictionary of words it already knows to be “positive” or “negative” (rated on a scale of 1.0 to -1.0). Pattern’s dictionary of words with sentiment scores draws from another lexical database called WordNet, and is available to peruse in its entirety on GitHub . Roughly speaking, after scoring each line based on the values of words as found in its dictionary, Pattern weighs the polarity scores of each line to produce the score for the entire poem. (Side note: TextBlob’s PatternAnalyzer might be modifying how this “weighting” process unfolds, but not from what I can gather looking at their GitHub page and their sentiment lexicon —they write in their documentation that PatternAnalyzer is a “[s]entiment analyzer that uses the same implementation as the pattern library.”) Either way, this final score represents the sentiment value of the poem. In the case of “Come Sing a Song,” this is -0.156. So where does Pattern’s dictionary of positive and negative adjectives come from? As they explain on their website, Pattern’s classifier learned which adjectives were positive or negative based on the kinds of adjectives that appeared in positive and negative product reviews. This training process represents a relatively standard workflow in machine learning: in broad strokes, a corpus of text is marked up by hand (in this case as either positive or negative); a program then “trains” or “learns” to identify positive or negative text by seeing lots of examples of each and generalizing rules that will help it to make accurate predictions in the future; the classifier is then tested or validated by being asked to evaluate the sentiment of texts the creators already know to be positive or negative (also usually marked up by teams of humans). After much tweaking, once the predictive results are acceptably accurate, creators often make their classifiers available for use online, where people, like me, use it for all kinds of things outside the scope of the product’s original intentions. Now, you may be wondering, why on Earth would someone use a classifier that was trained on product reviews (rather than on poetic corpora) to evaluate something as rhetorically complex as poetry? For a number of mostly pragmatic reasons: perhaps most significantly, creating one of these sentiment lexicons from scratch is an extremely time-consuming process. It takes entire teams of people and lots of resources. My goal in this project is not to develop a sentiment classifier that works on experimental poetry in English. Rather, it’s to see what existing classifiers can show us about a specific corpus of poetry—not just Pattern and VADER, though those are the only ones I will discuss today. So, we know now that Pattern’s sentiment analysis features were not designed to evaluate the sentiment in Troupe’s “Come Sing a Song.” And, having used them to evaluate this poem, the results seem to confirm this. Consider the lines of the poem I’ve discussed above and their corresponding sentiment scores in Pattern (rounded to the third decimal point): 1 Come sing a song, Black Man, … -0.167 … 9 sing jazz, rock, or, R &amp; B, … 0.000 … 11 sing a “bad” freedom song … -0.700 Pattern assigns the six-word snippet of line 9 a score of 0.000 because none of these words appear in its sentiment lexicon. It assigns line 11 a score of -0.700 because, of the three definitions of “bad” Pattern knows, each sense of the adjective has a score of roughly -0.7 (with some variation in the averaging due to the “confidence” of the score—accounting, I’m guessing, for variation in the original human markup). Pattern’s evaluation of line 1 as -0.167 presents a more troubling problem. The only adjective in this snippet that appears in Pattern’s dictionary is the word “black.” Looking at the code, Pattern knows three meanings to this adjective: “of or belonging to a racial group having dark skin especially of sub-Saharan African origin”, polarity = 0.0 “extremely dark”, polarity = -0.4 “being of the achromatic color of maximum darkness,” polarity = -0.1 Two of these meanings have a negative polarity (sentiment score) associated with them. As far as I can tell, because Pattern has no idea which sense of the word is being used here, it averages the polarity scores of the three senses to assign a sentiment to the line: -0.167. This means that, yes, whenever Pattern sees the word “black” in my corpus, it assigns the word a negative sentiment value. This is bad. Even acknowledging that a given tool can’t do all things in all contexts, this is bad. Technology has a long history of reinforcing racism and racist power structures in America; if Pattern’s out-of-the-box sentiment analysis capabilities read the word “black” as expressing negative sentiment—even if the one “sense” of the word referring to race in its dictionary is neutral—that is a huge problem. Moreover, in a project examining poetry from the Black Arts Movement, this particular look “under the hood” renders Pattern’s findings not only extremely troubling, but practically incoherent. If Pattern assigns a sentiment score of -0.167 to the line “I am a black woman” from Mari Evans’s poem of the same name—which it does—it’s hard to see the tool as anything but disturbingly biased in terms of race and sentiment. What’s more, this problem only became visible to me because I stumbled across it, stopping to look more closely at what felt like weird results. Nothing I could find in Pattern’s (or TextBlob’s) documentation explained how these word-by-word judgment calls would be made—i.e., that it would basically average the scores of different senses of a word in evaluating its sentiment. The discovery came from experimentation on a word- or sentence- level scale—a scale that is often beneath the scope of larger computational projects—as well as careful digging through documentation dispersed over multiple webpages, published articles, and commented code . This isn’t any particular fault of Pattern’s, but rather indicative of the way that even accessible products designed to have “a focus on ease-of-use” have elements that can feel blackboxed —that the details are in there somewhere, even if implicitly in the inner workings of the code itself, but can be hard to find. But because I’m working at the scale that I am and have purposefully spent time in the technical weeds, this particular bias was clear as day. VADER’s Conflicting Feelings on “Come Sing a Song” Fortunately, Pattern is not the only sentiment classifier available for projects like mine. VADER (short for “Valence Aware Dictionary for sEntiment Reasoning”), is described by its creators as a “parsimonious rule-based model for sentiment analysis of social media text.” Like Pattern, VADER uses a sentiment lexicon (or dictionary). Unlike Pattern, VADER has been trained specifically with an eye for the “sentiment-oriented language of social media text, which is often expressed using emoticons, slang, or abbreviated text such as acronyms and initialisms” (9). Moreover, VADER was designed to incorporate context for these words: “grammatical and syntactical conventions that humans use when expressing or emphasizing sentiment intensity ” (1)—recognizing, in other words, the difference between “good” and “very good,” or “it was great” and “it was great!!!!”. VADER’s sentiment lexicon is available here, which includes the final weighted score of each item along with the original scoring from ten Amazon Mechanical Turk raters (on a scale of -4 to 4) that went into each evaluation. With regards to the sentiment in “Come Sing a Song,” VADER seems closer to the mark. The three lines it sees as having negative sentiment make more sense to me. Each includes either the phrase “Blind Joe Death” or “prison chain gang,” both of which feel more endowed with negative feelings and associations (“death,” “prison,” and “blind” all have negative scores in VADER’s lexicon). What’s most interesting, however, is VADER’s valuation of the final line: “sing a ‘bad’ freedom song,” which it scores as slightly positive. Pattern considers this final line to be the most negative in the poem, as “bad” is the only word in the line that Pattern has in its dictionary. While VADER also has “bad” scored as negative (-2.5, with the max being -4), VADER has “freedom” score as positive (3.2, with the max being 4). In other words, with regards to intensity prior to grammatical context, VADER weighs “freedom” as being more positive than “bad” is negative. On its own, this math isn’t that interesting. What’s interesting is that, in having to weigh these values against one another in this final line, we see VADER’s classifier struggling with the layered meanings of Troupe’s words. The classifier has conflicting feelings. A human reader, of course, would also be doing this, and with much, much more nuance. Just looking at the original punctuation, we can see that Troupe has marked off the word “bad” by putting it in quotation marks—a sign that something special might be going on with this word and how it is being used. My mind goes first to “bad” as it appears in the title of Sonia Sanchez’s 1970 collection We a BaddDDD People, which I discussed in my previous post . This “bad” doesn’t read so much as negative as it does “dangerously good,” to quote from William J. Maxwell’s work on African American literature and the FBI (289). In this sense, singing a “‘bad’ freedom song” feels again like an invitation to celebrate —in this case, the “dangerously good” work of black individuals in the struggle for freedom in America, past and present. But VADER isn’t reading for things like this. Even with regards to the quotes surrounding the word “bad,” as I discussed in a previous post, my program removes the punctuation and capital letters in this line for tokenization purposes before it even makes it to VADER. Instead, VADER is just going on the surface meanings—the denotation of these words rather than their potential connotations. But while VADER knows nothing about history, freedom, singing, or that the word “bad” might actually mean “good,” we can see the classifier in its own way trying to sort out the layers of meaning in this line—that whatever is in this snippet of text might be both negative and positive at the same time, or in different ways. For me, this instance of conflicting feeling represents an excellent jumping off point for the larger questions of how feeling, affect, and sentiment might be operating in a poem or group of poems: the cues and signals that VADER struggles with but human readers can take almost for granted; the biases that classifiers bring to evaluating feelings versus those of a human reader; the way individual words carry affective weight both in and in spite of their context. But that’s all for now—the second half of this post, which dives into VADER’s reading of Nikki Giovanni’s “The True Import” and offers more general reflections on this research process as a whole, will follow shortly!"},{"id":"2018-04-26-poems-with-pattern-and-vader-part-2-nikki-giovanni","title":"Poems with Pattern and VADER, Part 2: Nikki Giovanni","author":"ethan-reed","date":"2018-04-26 05:38:23 -0400","categories":null,"url":"poems-with-pattern-and-vader-part-2-nikki-giovanni","layout":"post","content":"(This post is part of a two-post series—I ended up having too much to say about the poems I looked at with VADER and Pattern, so I split it up. First half can be found here !) Nikki Giovanni’s “ The True Import of the Present Dialogue, Black vs. Negro ” is probably one of her most famous poems. According to Howard Rambsy’s excellent recent book on the larger literary scene of what he calls “the Black Arts enterprise,” this poem is also “among Giovanni’s most anthologized pieces,” with Giovanni herself being “a fixture in anthologies of African American verse” (72). More than just a fixture in anthologies, Giovanni was at the time “undoubtedly one of the most popular” of the new black poets, according to Melba Joyce Boyd in her book on Dudley Randall and the Broadside Press (175). This poem is also famed for being exemplary of what Rambsy calls “black arts discourse,” which he describes as “characterized by expressions of militant nationalist sensibilities, direct appeals to African American audiences, critiques of antiblack racism, and affirmations of cultural heritage” (10). According to Rambsy, “The True Import” in particular holds an “aggressive approach to liberation,” similar to other poems in “utilize[ing] violent and nationalist rhetoric to encourage [a] presumably black audience to liberate their minds from the hegemony of whiteness” (10). Giovanni’s entry in the Black Literature Criticism reference series seconds this, describing the poem as “typical of her early work: a call to black Americans to destroy the whites who oppress them as well as the blacks whose passivity and compliance contribute to their own oppression” (881). Likewise, The Oxford Companion to African American Literature names this early “well-known” poem as one “that led to Giovanni’s identification as an angry, militant poet” (317). In other words, many see “The True Import” as an angry poem—a poem that is, in fact, famous for its anger. This assessment feels in keeping with a first reading. To offer a straightforward gloss, “The True Import” explores the difference between being an African American locked into white supremacist ideology (being a “negro”), versus being an African American who has liberated themselves from white thought and come into their own (being “Black”). With regards to then-contemporary conversations regarding these terms and their significance, Haki Madhubuti’s “Toward a Black Aesthetic” and Sarah Webster Fabio’s “Who Speaks Negro? Who Is Black?”—both of which appear in the 1968 September-October issue of Black World / Negro Digest —are informative and insightful. Madhubuti writes that, unlike the “black man (or woman)” who is “positive of [their] identity,” “the Negro is a filthy invention” that “didn’t come into existence until about 1620”— an “imitation white” (27). Fabio, meanwhile, writes that “Negro is a psychological, sociological, and economic fabrication to justify the status quo in America” (33). With regards to Giovanni’s “The True Import,” this poem explores the role that violent liberation plays in this difference between “negro” and “Black.” But a summary like this doesn’t get across what makes the poem so “angry,” which has more to do with the poem’s style—the texture of its explicitly violent diction (“Can you splatter their brains in the street”), its point-blank, repetitive questions (the phrase “Can you kill” appears 13 times in the 51 line poem), and its rapid-fire tempo (most lines are only a few words). Moreover, “killing” plays a central role in this poem’s idea of liberation: killing white men (“Can you piss on a blond head / Can you cut it off”) as well as killing the consciousness within that has internalized oppressive white thought (to kill part of your “mind / And free your black hands to / strangle”). There is, of course, more to this poem than just anger. Cheryl Clarke in her 2004 book on female poets in the BAM notes a tension in its concluding line—“Learn to be Black men”—in that it addresses “Black men” specifically, as opposed to black people more generally. Clarke suggests this “erasure of black women” might have to do with a desire “to project the urgency for unity and solidarity, to focus on the possibilities for sameness” within the movement (53). In his 1971 Dynamite Voices, Haki Madhubuti (then Don L. Lee) notes Giovanni’s references to Vietnam in the poem (“We kill in Viet Nam / for them”) as a concern that “Black men have been sent out of the United States to kill other ‘colored’ peoples of the world when the real enemy is here” (68). In short: scholars and critics have had a lot to say about “The True Import” from a variety of perspectives. Here, however, I am interested in the poem’s purported “anger.” VADER and Pattern’s Analyses So what do our sentiment classifiers think? Pattern is so off the mark again that it isn’t worth going into very deeply. As explained in my previous post, Pattern only looks at adjectives. Of the poem’s 51 lines, it assigns a neutral score of zero to 43 of them, because the lines lack adjectives Pattern has in its lexicon. Lines like “can you shoot straight and / fire for good measure” both have positive scores because of the adjectives “straight” and “good.” The only “negative” words Pattern knows in this poem are “down” and “black,” meaning that the final line—perhaps the most hopeful, affirmational moment of the poem to which the intensity of the prior lines builds—has a score of -0.167, because “Learn to be black men” has the adjective “black” in it, which Pattern, as discussed in more detail in my previous post, scores as negative. Pattern gives the poem a neutral score (0.02), but for all the wrong reasons, some of which are quite troubling. With regards to VADER, however, I was surprised to find that the classifier’s results are very much in keeping with what critics have said. Critics consider “The True Import” to be one of the most significant examples of a certain type of angry, militant, even aggressive poem; having evaluated each of its lines, VADER considers it to be the single most negative poem in the 26-book corpus. That is to say that, in a sense, critics and VADER actually agree about something: that Giovanni’s “The True Import” is a poem that, on the surface, has an exceptional amount of negative sentiment compared with its contemporaries. I add the caveat of “on the surface” because, as mentioned above, a lot is going on in this poem that might complicate our understanding of its angry, revolutionary rhetoric—something that scholars, critics, and other readers of Giovanni’s poetry note but that VADER does not. VADER doesn’t know the meaning or significance of any of the words it analyzes. It just knows sentiment scores for strings of letters like “kill,” “poison,” and “die.” Having been designed to analyze social media text, VADER is (unlike Pattern) also equipped to deal with slang like “piss” as well as the racial expletives used throughout the poem, which it counts as having negative sentiment. But because it doesn’t know anything but sentiment scores for these words, VADER misses what William Harris (in his chapter in Mari Evans’s edited volume Black Women Writers ) calls the “complex connotations” of certain racial expletives and the speaker’s strategic use of them “to suggest the consciousness that wants to conform to white standards,” and, subsequently, the idea that ‘killing’ this part of the mind will “transform consciousness” (221). In short: VADER finds sentiment, but has nothing more to add with regards to interpreting its potential significance. Brandishing Anger As a more informed reader of Giovanni’s poetry, I do have more to add. For example, I would argue that the role of “negative sentiment” as it appears in “The True Import” goes beneath and beyond the immediate, denotative, and affective impact of individual words or lines and actually relates deeply to the poem’s structure, genre, and social purpose. In her essay “Black Poetry—Where It’s At” from a 1969 issue of Black World / Negro Digest, BAM writer and poet Carolyn M. Rodgers details “several broad categories” or types of poems in then-contemporary black poetry. One such type is signifying poetry, in this case referring to the black vernacular tradition of signifying. Scholars like Henry Louis Gates Jr. have explored the history and significance of signifying more in depth, which Gates describes in his 1988 work The Signifying Monkey as both the commonplace practice of “engag[ing] in rhetorical games” (68) as well as a more general “theory of criticism that is inscribed within the black vernacular tradition and that in turn informs the shape of the Afro-American literary tradition” (14). But rather than jump straight to Gates’ now famous definitions and theorizations of this tradition, I want first to stick with Rodgers’ formulations of it in her essay. Citing Giovanni, Don L. Lee, Sonia Sanchez, and “the master of it all” Amiri Baraka as having written these kinds of poems, Rodgers describes signifying as “a way of saying the truth that hurts with a laugh … a love/hate exercise in exorcising one’s hostilities” (14-15). She also notes, however, that signifying “is very often a bloody knife job,” and because it “often contains such a broad base of truth it has been known to cause—in fact, is famous for causing—a fight or a death. It can get too down, too real, so true and personal it uncovers too much” (15-16). While acknowledging its long history in black vernacular traditions, Rodgers also emphasizes signifying’s fresh significance and potentially productive social purpose as a poetic genre: From a literary point of view, it is a significant, exciting aspect of today’s poetry. I know, and you know, that we have always signified. On the corners, in the poolrooms, the playgrounds, anywhere and everywhere we have had the opportunity. … However, to my knowledge, no group of Black writers has ever used it as a poetic technique as much as today’s writers. It is done with polish. … Too much signifying can be negative, I think; however, most of today’s poets are very conscious of how important positive vibrations are, and few have carried signification to an extreme. In the main, it is being used, for constructive destruction. (14) Just with these brief descriptions, we can already see an intense ambivalence between what might be seen as “positive” or “negative” in a type of poem that engages in this rhetorical practice. They are “love/hate exercise[s]”; they speak truths “that hurt,” but do so “with a laugh”; they are an opportunity for “exorcising one’s hostilities”. In writing such a poem, poets must demonstrate restraint and moderation so as not to carry “signification to an extreme”: they must strike a balance because “positive vibrations” are important and “[t]oo much signifying ” can be “negative”. As Rodgers summarizes at the end, a good signifying poem destroys, but in a constructive way. It is “a bloody knife job,” but one that can have a productive social purpose. Rodgers’ article offers, I feel, an extremely productive lens through which to view the “anger” in Giovanni’s poem. Other scholars would seem to agree—Cheryl Clarke, cited above, notes that, along with the poem’s “harsh repetition,” “violent rhetoric and images,” and “its castigation of white people and black people,” the poem has a “dozens-like resonance” through which it “engages in the politics of conversion by rebuke” (60)—the dozens being what Gates calls “an especially compelling subset of Signifyin(g)” (90). Rebuke, castigation, conversion, “constructive deconstruction,” speaking “the truth that hurts”—rather than just an expression of rage or militant feeling, this poem uses “anger” in complicated, socially-minded ways. This poem isn’t just angry—it wields anger. And by brandishing anger in this way, the poem strategically applies a specific set of affects to a specific set of issues with an eye for inciting change. On the surface, what the poem declares to be “The True Import of the Present Dialogue, Black vs. Negro” seems to boil down to a militant voice asking the question: “Can you kill.” Beneath the surface, however, this voice uses negative sentiment (including the repetition of questions like “Can you kill”) to urge, push, and even shove the reader into crucial—if painful—awareness: to realize the life-or-death stakes of racial injustice, as well as the different kinds of violence that oppressive racial ideologies can inflict. This perspective changes not only the “message” of the poem, but how we read the seemingly negative sentiment in individual lines. For example, the poem’s final lines make two demands: that the reader “Learn to kill” their own internalized oppression, which, the poem implies, will allow them to “Learn to be Black men.” Rather than read this first command as just another instance of the poem’s persistently violent rhetoric, we might better see it as a transitional line or hinge—one half of a closing couplet that uses two imperatives to channel a backlog of violent rhetoric into something constructive (Rodgers’ “constructive deconstruction”). In this sense, this hinged couplet makes a sudden shift from “negative” sentiment into intense recognition—a kind of poetic anagnorisis, or what The Oxford Dictionary of Literary Terms defines as “the turning point in a drama at which a character … recognizes the true state of affairs, having previously been in error or ignorance.” Painful knowledge, in short. An important insight acquired through what might have also been “a bloody knife job.” Surface Sentiment and Positive Negativity To reframe this with an eye for this project: this poem, and signifying poems Rodgers describes more generally, can make use of a positive negativity . Which is basically VADER’s worst nightmare. Sentiment-laden language deployed with this depth of rhetorical nuance, figurative complexity, and vision of broader social purpose is exactly the kind of thing VADER’s sentiment classifier cannot pick up on. VADER was, however, able to pick up on and offer an explicit line of reasoning for why the poem seems so negative—something that many others have noted since its original publication. While mechanically averaging the affective weight of words and phrases according to a sentiment lexicon built to evaluate social media text isn’t exactly how a reader today would go about getting a first impression of this poem, this gloss of the poem’s surface ended up being an indicator that something was indeed going on in terms of the poem’s use of heightened affects. This disjoint—between the “negative sentiment” on the surface of this poem, and the significance and meaning of this sentiment with regards to the poem as a whole—is one of the things I find interesting about “The True Import,” and one of the things that VADER’s analysis helps me to see. This may seem like a sharp left turn, but I happen to be reading I. A. Richards’s Practical Criticism (1929) and found strange echoes between his experiment (in which he documents and evaluates a corpus of undergraduate responses to anonymous poems in an effort to develop new critical techniques) and certain aspects of my own current project. For example, Richards’ is greatly interested in the way that his students jumped to conclusions about poems based on initial impressions. This is particularly the case when it comes to the connections between a word’s “sense” and its “feeling”—almost exactly the parameters that a sentiment classifier like Pattern’s or VADER’s invite us to consider. He writes: “Words, as we all recognise, are as ambiguous in their feeling as in their sense; but, though we can track down their equivocations of sense to some extent, we are comparatively helpless with their ambiguities of feeling. We only know that words are chameleon-like in their feeling, governed in an irregular fashion by their surroundings” (203). In short, Richards is confident that we can figure out the “sense” of a word in a poem, but less sure about pinning down its “feeling” given how complicated poetic contexts can be. Moreover, Richards is clear to distinguish between a word’s feeling-as-it-exists-in-the-poem in contrast to its more general affective connotations, basically the “external” affective baggage a word might drag into a given gloss or interpretation. He writes …we are concerned, firstly, with the feeling actually aroused by the word in the poem, not with the feelings the word might have in other contexts, or the feeling it generally has, or the feeling it “ought to have,” though these may have with advantage be remembered, for a word’s feeling is often determined in part by its senses in other contexts. … Is the pull [of the word’s feeling] exerted by context … sufficient to overcome what may be described as the normal separate feeling of the questionable word? Can this pull bring it in, as an item either in accordance or in due contrast to the rest? Or does the word resist, stay outside, or wrench the rest of the poem into crudity and or confusion? (201-203) Richards is, in short, trying to think through the kind of “feeling” a word has as used in a poem in contrast to its “normal separate feeling.” More specifically, he is interested in the competing “pull” these two loci of feeling exert on a given reader. For Richards, while a poem often makes strategic use of a word’s “senses in other context,” it is a kind of failure for a poem to lose this gravitational contest of feeling—for a word’s feeling to “resist, stay outside,” and thus “wrench the rest of the poem into crudity and confusion.” Though I disagree with Richards’ conclusions on this, I find the distinction to be a useful one: a word’s “normal separate feeling” is exactly the kind of sentiment that VADER was designed to evaluate. But where Richards’ views this as a distraction external to a poem and capable of casting it “into crudity and confusion,” I (almost 90 years later) view it as a consideration that can be more central to poem’s aesthetic and affective practice. As explored above, Giovanni’s “The True Import” makes explicit use of “apparent” feelings—those affective associations floating nearest to the surface of a word—as well as a word’s more complicated affective connections, using a combination of both to urge the reader, with great rhetorical nuance, towards a specific kind of understanding. With all this in mind, I view the ability to read for this kind of surface sentiment as extremely valuable. In the case of these sentiment classifiers, VADER in my mind reads the poem the way that someone unfamiliar with the history of the Black Arts Movement might—a surface reading more attuned to the immediate affective impact of words (based on their “normal separate feeling”) than to their affective import as shaped by poetic, literary, social, and political contexts. I imagine that if this poem were assigned in an undergraduate seminar without any introduction, it might ruffle some feathers. VADER, by highlighting the intensity of this poem’s negative sentiment according to the words and phrases it uses—without the literary and historical context of their use—both anticipates this potential discomfort and helps us to see one reason why a reader might respond in such a way (certain words can spark certain feelings regardless of context). But ruffling feathers is, of course, part and parcel of many BAM poets’ aesthetic practice. And this disjoint between a surface anger and a poetic form that leverages “negative sentiment” to address other issues requires an interested and informed human reader to identify, untangle, and make sense of. Exploratory Computational Analysis In closing, I want to note another role that VADER played in addition to its perspective in the readings offered in the preceding paragraphs: when I was first reading Giovanni’s Black Feeling, Black Talk, this poem stood out to me for its particularly charged language and affective stance. But I was reading a lot of poems in a lot of books—hundreds of them—and at the time of reading had yet to learn about the privileged place that “The True Import” has in anthologies, criticism, and scholarship. As a researcher, I only learned about all that after having decided to look more closely at the poem as opposed to others, which I did because of its prominence in VADER’s analysis. Twenty-six books of poetry may not seem like that many when compared with other computational projects, but for someone with limited time and resources (i.e., most researchers), VADER’s suggestion that “this poem might be particularly interesting!” led me immediately to a text that ended up being extremely relevant to my initial research questions, even if VADER thought it was interesting for different reasons than I eventually would. That’s pretty awesome. And something that, as far as I can tell, is an exciting, relatively unexplored use for sentiment analysis in literary study. While sentiment classifiers can’t explain why a poem, line, or word matters (or even what it means), they have proven so far to be an intellectually productive way for me to explore the many texts in my literary corpus—particularly when pursuing research questions that I already know matter to the scholars, critics, and readers of that corpus."},{"id":"2018-05-08-teaching-black-arts-poetry-and-computational-methods","title":"Teaching Black Arts Poetry and Computational Methods","author":"ethan-reed","date":"2018-05-08 07:02:08 -0400","categories":null,"url":"teaching-black-arts-poetry-and-computational-methods","layout":"post","content":"Enjoy this guest post by Ethan Reed, a 2017-2018 graduate fellow as well as a Ph.D candidate in English Literature at the University of Virginia. He went to W&amp;L to give a workshop in Prof. Lesley Wheeler’s ENGL 295_02: African-American Poetry course through a Mellon-funded collaboration with the Scholars’ Lab at UVA. More information about this initiative can be found  here . This post is cross-listed on the WLUDH blog . An introductory note: this post offers a rough sketch of the planning that went into, and the ideas that emerged from, a three hour seminar on African American poetry I visited last week taught by Professor Lesley Wheeler at Washington &amp; Lee. As such, it’s pretty long. Feel free to skim or jump around to those sections you’re most interested in! They are (by heading): (1) a brief note on the occasion of me visiting the seminar, (2) how I went about contextualizing Black Arts poetry in an undergraduate seminar setting, (3) insights that emerged from our conversation on Amiri Baraka’s poetry, and (4) how I went about running a brief workshop on machine learning and computational approaches to cultural objects. Enjoy! Plus a huge thank you and shout out to Professor Lesley Wheeler, Mackenzie Brooks, and everyone else at Washington &amp; Lee as well as the Scholars’ Lab that made my visit possible! (1) The Visit Last week I had the pleasure of participating in Prof. Lesley Wheeler’s seminar, English 295: African-American Poetry, at Washington &amp; Lee University through a collaboration between the Scholars’ Lab at UVA and W&amp;L. As a part of W&amp;L’s intensive spring semester, this class session clocked in at a solid three hours: we discussed Black Arts poetry, Amiri Baraka in the 1960s, Amiri Baraka in the 2000s, and how to make sense of provocative, overtly political poetry. As a PhD candidate in English Literature at UVA, and current Graduate Fellow at the Scholars’ Lab, I study Black Arts poetry. My current project involves using natural language processing techniques like sentiment analysis to analyze and interpret texts from a corpus of Black Arts poetry collections (Baraka included). In particular, I’m interested in digging into how feeling, affect, and sentiment happen in a poem—and how this happening might be coded in terms of race and gender. Part of this includes interrogating the limits of these distanced, potentially decontextualizing computational techniques to think through BAM poetry, and how these methods might best be used to pursue questions, problems, and lines of inquiry centered around black thought and experience. After initial planning conversations with Prof. Wheeler, we decided it would be most valuable if we combined me presenting on my work with a hands-on workshop on the digital methods I’m using, as well as a more general discussion of how all this might change how we read poems (like those assigned for the class). For structure, we settled on me starting with a brief intro to the Black Arts Movement, followed by a conversation on the assigned readings, then a mini presentation / discussion on my own research that included a full-on participatory workshop on the principles of machine learning, sentiment analysis, and how a sentiment classifier works. For my presentations and the workshop, I spoke with slides rather than reading from a written out paper. But I still thought it worth sharing how I went about introducing the BAM, conducting a machine learning workshop, and presenting on my own research. So in this post, I’ll give a rough paraphrase of these presentations in addition to insights that emerged from the conversations we had as a class around these issues. As I say in the introductory note—this is a lot! So feel free to jump around. (2) Contextualizing Black Arts Poetry After introducing myself to the class, I talked through a handful of slides that I felt offered an engaging introduction to how writers within the Black Arts Movement conceptualized themselves and articulated their artistic, social, and political goals. These are most of the quotations I shared, along with rough paraphrases of how I glossed them with the class: From Larry Neal, “The Black Arts Movement,” The Drama Review, vol. 12, no. 4 (1968): “The Black Arts Movement is radically opposed to any concept of the artist that alienates him from his community. Black Art is the aesthetic and spiritual sister of the Black Power concept. As such, it envisions an art that speaks directly to the needs and aspirations of Black America. In order to perform this task, the Black Arts Movement proposes a radical reordering of the western cultural aesthetic. It proposes a separate symbolism, mythology, critique, and iconology.” To give an example of what this “radical reordering of the western cultural aesthetic” might look like, I shared an image of Sonia Sanchez’s “a/coltrane/poem,” from her 1970 We A BaddDDD People—a poem that, even just glancing at a page, is clearly pursuing a new, radical kind of typography to match its radical aesthetic. (For more on this poem’s indentation, spacing, punctuation, capitalization, and non-traditional spellings, check out my post on transcribing these texts into text editors!). Moreover, this “separate symbolism, mythology, critique, and iconology” included dedicating poems to African American figures like John Coltrane. Sanchez’s “a/coltrane/poem,” then, is one of many Black Arts era poems to take the musician as its main subject matter. Another example of this would be those poems dedicated to Malcolm X—like the 1967 anthology For Malcolm: Poems on the Life and the Death of Malcolm X, edited by Dudley Randall and Margaret G. Burroughs. From Gwendolyn Brooks’ intro to Don’t Cry, Scream by Haki R. Madhubuti [then Don L. Lee] (1969): “[Lee] is well-acquainted with ‘elegant’ literature (what hasn’t he read?) but, while certainly respecting the advantages and influence of good workmanship, he is not interested in supplying the needs of the English Departments at Harvard and Oxford nor the editors of Partisan Review … He speaks to blacks hungry for what they themselves refer to as “ real poetry.” … Don Lee has no patience with black writers who do not direct their blackness toward black audiences.” From Haki R. Madhubuti’s Preface to Don’t Cry, Scream (1969): “What u will be reading is blackpoetry. Blackpoetry is written for/to/about &amp; around the lives/spiritactions/humanism &amp; total existence of blackpeople. … Blackpoetry in its purest form is diametrically opposed to whi-te poetry. Whereas, blackpoets deal in the concrete rather than the abstract (concrete: art for people’s sake; black language or Afro-american language in contrast to standard English, c.). Blackpoetry moves to define &amp; legitimize blackpeople’s reality ( that which is real to us).” I felt these quotations showed how Black Arts writers made the social and political mission behind their work as explicit as possible—that many not only sought to make art that spoke first and foremost to black communities (“Blackpoetry is written for/to/about &amp; around the lives/spiritactions/humanism &amp; total existence of blackpeople”), but felt that the need for this art was urgent, even a kind of imperative (“Lee has no patience with black writers who do not direct their blackness toward black audiences”). Seeing this quotation from Madhubuti’s preface, one of Prof. Wheeler’s students asked a question about why Madhubuti turns the phrases “black poetry” and “black people” into one word—“blackpoetry” and “blackpeople.” I think this is a great question, and my response was mostly that it could mean any number of things depending on context—but that what matters is that we, as readers, pay careful attention to this kind of aesthetic practice (one that includes typographic, linguistic, and conceptual experimentation) and ask questions inquiring into how they might tie into that work’s meaning and potential social practice. From Amiri Baraka, “An Explanation of the Work.” Black Magic: Sabotage, Target Study, Black Art: Poetry 1961-1967 (1969): “Sabotage meant I had come to see the superstructure of filth Americans call their way of life, and wanted to see it fall. To sabotage it, I thought maybe by talking bad and getting high, layin out on they whole chorus. But Target Study is trying to really study, like bomber crews do the soon to be destroyed cities. Less passive now, less uselessly ‘literary.’” Baraka’s poetry was the main subject of Tuesday’s class, so I wanted to give an example of how Baraka himself introduced his work from the 60s in 1969. More specifically, I shared an example of the provocative rhetoric that Baraka has become famous for—what one scholar describes as “a lifetime of saying the unsayable.” Prof. Wheeler’s students had already read several of Baraka’s poems for this class—“ Preface to a Twenty Volume Suicide Note,” selections from “Hymn to Lanie Poo,” “ A Short Speech to My Friends,” “ Three Modes of History and Culture,” “ Black Art,” “Black Bourgeoisie,” “Clay,” and “ Somebody Blew Up America .” So they had experience with the violence, militancy, and anger behind some of his more incendiary verse. But this quotation, I felt, helped to show how this intensity existed outside of his verse as well—that Baraka would use plain prose in a preface to point his poems like weapons at that “superstructure of filth Americans call their way of life” which “he wanted to see … fall.” The main idea being: it’s not just other people that read Baraka’s poems and interpreted them as militant. Baraka himself framed them as such in introductions to his work. From Helen Vendler, “ Are These the Poems to Remember? ” The New York Review of Books, November 24, 2011: “Rita Dove, a recent poet laureate (1993-1995), has decided, in her new anthology of poetry of the past century, to shift the balance, introducing more black poets and giving them significant amounts of space, in some cases more space than is given to better-known authors. … Dove is at pains to include angry outbursts as well as artistically ambitious meditations.” “Dove must realize that the new ‘literary standards’ behind this example of Baraka’s verse [“Black Art”] don’t immediately declare themselves. Printing something in short lines doesn’t make the writer a poet; it only makes him a person with a book of short lines. … If one wants evidence of black anger against ‘whitie’ and ‘jewladies’ and ‘mulatto bitches,’ here it is. But a theme is not enough to make a poem.” From Rita Dove, “ Defending an Anthology,” The New York Review of Books, December 22, 2011: “It is astounding to me how utterly Vendler misreads my critical assessment of the Black Arts Movement, construing my straightforward account of their defiant manifesto as endorsement of their tactics … [she] focuses on that handy whipping boy, Amiri Baraka, plucking passages from his historically seminal poem “Black Art” in which he denigrated Jews, thereby slyly, even creepily implying that I might have similar anti-Semitic tendencies. … I would not have believed Vendler capable of throwing such cheap dirt, and no defense is necessary against these dishonorable tactics except the desire to shield my reputation from the kind of slanderous slime that sticks although it bears no truth.” These quotations come from an exchange in The New York Review of Books between Helen Vendler, A. Kingsley Porter University Professor at Harvard, and Rita Dove, United States Poet Laureate, Pulitzer Prize winner, and Commonwealth Professor of English at the University of Virginia. The anthology in question is The Penguin Anthology of 20th Century American Poetry, edited by Rita Dove and published in 2011. The issue in question is whether or not poetry by poets like Baraka should appear in it. I give all these details to emphasize that this controversy, who was having it, and where they were having it was—and is—a big deal. To summarize, Vendler argues that poets like Baraka—whose poetry she characterizes as “angry outbursts,” lacking “literary standards,” and not so much poetry as “a book of short lines”—should not appear in such an anthology at the expense of poets (to cite the examples she gives) like Wallace Stevens and James Merrill. Dove, on the other hand, argues that the “defiant manifesto” of the Black Arts poets matters for our understanding of 20th century American poetry, and deserves to be represented in an anthology claiming to cover that historical and geographical ground. I included these quotations and this controversy in my introduction to show that, almost fifty years later, Baraka’s poetry still pisses people off. As Dove says in an interview in which she discusses the controversy, this response to certain aspects of Baraka’s poetry makes sense: “No question about it: Amiri Baraka’s ‘Black Art’ is highly problematic in a social sense, a rant with racist, Antisemitic and sexual elements. There’s nothing in this poem I would agree with on a social level … And yet it’s not only a seminal poem of the Black Arts Movement, important for understanding the shock engendered when such indiscriminate rage was thrust into the public, but it is also … a poem that pushes language to despairing extremes and ultimately cracks it open.” Her final comments here, I feel point to another set of problems that professionals, to this day, have with Baraka’s work—a question of whether or not the angry, often militant provocation Black Arts poets made to “literary standards” (what counts as poetry, what aesthetic practices are legitimate, how poems should point themselves toward social or political issues, and so on) should be taken seriously. For Prof. Wheeler’s students, I asked them to consider that if these poems are ruffling feathers in the 2010s, imagine the waves they made in the 1960s during the civil rights movement. From Ishmael Reed, a 1995 interview quoted in Kalamu ya Salaam, “Black Arts Movement,” in The Oxford Companion to African American Literature (1997): “I think what Black Arts did was inspire a whole lot of Black people to write. Moreover, there would be no multi-culturalism movement without Black Arts. Latinos, Asian Americans, and others all say they began writing as a result of the example of the 1960s. Blacks gave the example that you don’t have to assimilate. You could do your own thing, get into your own background, your own history, your own tradition and your own culture. I think the challenge is for cultural sovereignty and Black Arts struck a blow for that.” This quotation from The Oxford Companion to African American Literature entry on Black Arts (also available here ), I feel, offers a nice birds-eye view on the legacy of the movement. It’s also coming at a slight historical remove—a few decades—and from someone who published poetry during the period but that The Oxford Companion describes as “neither a movement apologist nor advocate” (70). Reed’s connection of Black Arts with multi-culturalism and the idea of “get[ting] into your own background, your own history, your own tradition and your own culture” is a useful one—I think it helps to offer some context on the ripple effects that these artists and their work had on the future of American artistic production. It’s easy to forget and difficult to imagine, I think, how much a political or artistic scene (and the culture associated with it) can change in just a few decades. In the context of a seminar with a broad historical range, hearing Reed reflect on the impact of BAM poetry in this way hopefully helps to bring this historical difference to light. (3) Class Discussion of Baraka’s Poetry After this introduction, we turned to the assigned poems by Baraka (“ Preface to a Twenty Volume Suicide Note,” selections from “Hymn to Lanie Poo,” “ A Short Speech to My Friends,” “ Three Modes of History and Culture,” “ Black Art,” “Black Bourgeoisie,” “Clay,” and “ Somebody Blew Up America ”). Students began by sharing “resonant words” with the whole class—quotations from the readings that resonated with them—after which everyone broke into smaller groups to discuss their choices and why those words in particular stood out. After several minutes, these small groups opened up to the class and kicked off our discussion. The topics, themes, problems, and passages discussed in the time that followed was genuinely exciting—I had never taught Black Arts poetry before. Though I could anticipate how students might respond to some of these literary texts, I wasn’t sure exactly what to expect. Long story short: I was blown away by the thoughts Prof. Wheeler’s students brought to these texts. In this space here, I want to focus in particular on some of the insights of our discussion of “ Somebody Blew Up America,” which many students chose as the subject of their “resonant words” and seemed more generally to spark a great deal of interest for discussion. To offer some backstory, “Somebody Blew Up America” is one of Baraka’s most infamous poems. Written in the months following 9/11, the poem is a cacophonous, insistent, even hyperbolic interrogation of the “Who” behind various systems of oppression throughout history and across the planet. In discussing the poem’s role “in the business of defining and disrupting what can be said,” one scholar notes the poem’s rhetorical force draws in part from “its torrid mixture of factual, ambiguous, humorous, grotesque, suggestive, and intentionally provocative content” (275). Another scholar describes its “arresting diatribes against the evils of imperialism and the attendant evils of racism” as part of what makes it “an angry poem, perfectly consistent with Baraka’s traditional ‘angry’ persona, fashioned as a response to historical acts of violence caused by imperialist and racist thinking” (464). The poem became infamous primarily, however, after its performance at the Geraldine Dodge Poetry Festival in September 2002. In particular, critics of the poem cited several lines in which it suggests (by way of asking an unanswered question) that the Israeli government had foreknowledge of the 9/11 attacks. Many charged Baraka with anti-Semitism and called for his removal as poet laureate of New Jersey. Amidst this public outcry, Baraka published a defense of his poem in Counterpunch, titled “ The ADL Smear Campaign Against Me ” (2002). In the end, unable to remove him from his post as poet laureate, the New Jersey State Senate abolished the post altogether rather than continue to have Baraka fill the position. One scholar cites this controversy as a powerful example of how difficult it is “to read such [political] poetry in an unbiased, informed, appreciative way and how to stay attuned to its aesthetic quality without compromising its ideological potential. In other words, the challenge is how to read overtly political poetry as poetry” (463). In short: having a conversation about Baraka’s “Somebody Blew Up America” means having a conversation about a whole host of thorny political, social, and artistic issues. The poem is, in a word, provocative—it provokes intense response from its readers. But in the case of Prof. Wheeler’s class, the poem provoked an incredible discussion, parts of which I wanted to highlight here. The poem’s directness in naming names stood out for many students—from President George Bush to former Secretary of State Condoleezza Rice to U.S. Supreme Court Associate Justice Clarence Thomas, the poem names the names of those in positions of power as it questions their roles in various systems of oppression. In addition to naming names of the powerful or oppressive, the poem also speaks the names of those killed or harmed while resisting systems of oppression or fighting for social change: from Medgar Evers and Fred Hampton to Rosa Luxemburg and Karl Liebknecht. Many students commented on this intensive allusiveness, how the poem pointedly reaches outside of itself and into the world of politics and history. For example, a few noted that the website I had linked for the poem (genius.com, where it is available to read for free) included annotations for many of the poem’s historical references—a textual apparatus several students found extremely helpful in parsing the poem’s external layers of meaning. Prof. Wheeler also noted that the many references in “Somebody Blew Up America” provide it with intellectual heft, as well as evidence of Baraka’s own erudition. Drawing from an earlier discussion on the more canonical literary tradition that Black Arts poets hoped to break from, we also discussed the poem’s resonances with another extremely allusive work (complete with textual annotations): T. S. Eliot’s The Waste Land . But where Eliot invokes Greek myths, Dante Alighieri, and the New Testament, Baraka names current presidents, secretaries of state, and assassinated civil rights leaders. In this sense, we discussed how “Somebody Blew Up America” mobilizes an entirely different audience of readers and for a different purpose, drawing as it does from an entirely different constellation of names, images, myths, and histories. Students also pointed to the radical rhetorical forms these references took, appearing as they did in rapid fire lists that spanned immense historical and geographical ground in the space of a few clauses. The form of the “list” came particularly into focus after watching a performance of “Somebody Blew Up America” by Baraka in 2009. Almost instantly, students noted a shift in the tone of the poem from its life on the page versus in performance by Baraka with a saxophonist accompanist. The way references unfolded in lists felt qualitatively different: what seemed slower, weightier, and even more solemn on the page felt faster, lighter, or even breezier in performance. Listening to Baraka, there’s no time to satisfy the itch to “get” references by pausing to look them up—as one student noted, Baraka wasn’t waiting around for his listeners: you either got it this time or you didn’t. (Near the end of the performance, Baraka even signals to the saxophonist to speed up, building momentum as he nears the poem’s close). But rather than feeling overwhelmed, the need to “keep up” with Baraka in performance seemed to change into something else. Drawing from my own listening experience, it felt almost like the particular pacing and affective artistry of Baraka’s performance elaborated on these allusion-rich lists which were themselves shorthand for broader historical perspectives—the idea that, even if you missed the references this time, Baraka’s performance could help, for now, to fill in some of the gaps and to keep you on the same page. (4) Machine Learning Workshop &amp; My Project After a brief break following this discussion of Baraka’s poetry, I introduced the class to my project, “Measured Unrest in the Poetry of the Black Arts Movement.” I discussed how I assembled my corpus, how I’m manipulating it to be able to process it with computers, and how I interpret and analyze my results (which I share some of here and here ). But before I did any of that, I introduced two concepts central to my research: machine learning and sentiment analysis. In this part of the post, I want to offer a sketch of what that introduction looked like. Natural language processing, machine learning, sentiment analysis—while extremely important concepts in certain fields, I don’t expect folks in a poetry seminar to have much familiarity with them. So when brainstorming ways to teach these concepts, it felt important for this intro to be hands-on —that is, engaging the class in a participatory way rather than me lecturing—and, for lack of a better word, fun —that is, making concepts that might feel “over someone’s head” instead feel intuitive, exciting, and immediately accessible. With all this in mind, I decided to make this intro a collective class workshop. My goal was to share the principles behind machine learning processes so that something like sentiment analysis—and what makes it possible—starts to make more sense. So in this workshop, we as a class acted as the “algorithm” in a supervised learning experiment. Our goal was to create a classifier that would guess whether or not a movie is a western based on its movie poster. This took the form of us looking at a whole bunch of movie posters we knew to be westerns and finding patterns in these posters—in this case, identifiable features they had in common. We then tried to generalize from these patterns and guess as a class whether or not a new movie poster was a western. Shout out here to Brandon Walsh, who has used the predictability of movie posters in similar workshops ! I have burgled a number of his ideas, with a few key differences—namely that I wanted to push our classifier to the point where its limits became clear and its pre-decided biases or assumptions came to the fore. We began with our “training data,” several movie posters we already knew to be from westerns. This included classics— Stagecoach, The Man Who Shot Liberty Valance, The Searchers, High Noon —as well as some later films— A Fistful of Dollars, For a Few Dollars More, Unforgiven, and so on. Our ongoing informal discussion of what patterns we saw in these posters was a lot of fun—one of my goals!—and had the lighter tone I was hoping for. We decided our “algorithm” saw four features in western movie posters: (1) man with hat, (2) man with gun, (3) horses, (4) desert / prairie. Some other features like “damsel in distress” came up, which lead to productive discussions on the differences between what a computer would be good at identifying vs. a human—i.e., that certain features might require levels of interpretive nuance that an algorithm wouldn’t be capable of. More on this in a moment. With our “classifier” trained, we turned it loose on our “test data”—another set of movie posters, this time with the titles/credits redacted—to see how well it performed. Ideally, the classifier in question would have no foreknowledge of these new inputs (the test data), and would simply perform its classification by rote according to how it was trained (in this case, looking for our four features—hat, gun, horse, dessert/prairie). We would then be able to tell roughly the accuracy of our classifier and tweak it as necessary. In reality, however, our “classifier” was also a room full of undergraduates already extremely familiar with cinematic genre conventions by virtue of their years of lived experience in a world with movies. So we did our best to “suspend” our far more nuanced human understanding of these movie posters and tried to act instead on our far less nuanced computational decision-making. This is, after all, part of the basic trade-off that computational approaches offer—purchasing the power of automated decision-making and the ability to deal with large corpora at the cost of nuance and complexity. So, more posters. We started with several our classifier found easy: The Magnificent Seven, Butch Cassidy and the Sundance Kid, Once Upon a Time in the West, to name a few. Each has hats, guns, horses, and deserts/prairies galore. We then tried more contemporary examples (i.e. The Hateful Eight _and _Django Unchained ) that, while straightforward in terms of how our classifier interpreted their movie posters, started to become more complicated in whether or not they “counted” as westerns. But my goal with this workshop wasn’t just to give students experience with using a classifier. I also wanted to give them experience with how a computer thinks . This includes the limitations of computational approaches, as well as the ways that initial human biases and assumptions affect seemingly objective algorithmic “results.” So the ambiguous cases of these more modern films started nudging us in that direction. Fortunately for this example of movie posters for westerns, I teach a course on “the western” at the University of Virginia and have some experience with the history, adaptability, and flexible boundaries of this genre (it’s as if I’d been orchestrating it all along!). So I threw a few curveballs: posters for films like Hud, Brokeback Mountain, There Will Be Blood, and_ The Revenant_—movies that take place in the American west, have themes, imagery, and plots in keeping with more traditional westerns, but also revise or explode the genre in one way or another. This, I tried to emphasize, is where the rubber meets the road in computational analysis. And where all sorts of human, subjective gray areas show up in what may have felt like an objective, algorithm-driven process. For example, we as human readers might recognize that Leonardo DiCaprio’s snow-crusted beard, long greasy hair, and weathered fur coat on the poster of The Revenant positions the film in a generic tradition defined by rugged individualist heroes and revenge-seeking antiheros struggling against the backdrop of an unforgiving frontier landscape. As thoughtful, human readers, we can also have a conversation about the history of white characters adopting a certain image of Native American culture as they find their place in this “unfamiliar” landscape—an act of appropriation often used to paint their relationship with American soil as non-colonial and “more authentic” as compared with greedy colonialist “bad guys” that they spend the film fighting against, often “on behalf” of Native American communities (more on this in a moment). But an algorithmic classifier cannot have this conversation. It looks at the input and determines its output according to its training as fast as it possibly can. Pushing this example a little further, I put up the poster for Kevin Costner’s Dances With Wolves . The narrative this film presents—of an alienated white American man being accepted into a Lakota community, then intuitively becoming more skilled at practices associated with this community than its actual members and “inheriting” their culture before they seemingly “vanish”—shows up a lot in stories about the American west (for more on how appropriative, insidious, and ultimately damaging these kinds of narratives are, see a chapter from Louis Owens’ _Mixedblood Messages _ titled “Apocalypse at the Two-Socks Hop: Dancing with the Vanishing American”). After showing the movie poster for Dances With Wolves —which our “algorithm” did not think was a western—I put up the poster for James Cameron’s 2009 Avatar, a film with an almost identical narrative structure, though instead of the American west it takes place on an alien planet named Pandora, and instead of a Lakota community, the people indigenous to Pandora are a blue alien species called the Na’vi. Judging by its poster, this film looks nothing like a western. Even watching it in theaters, it would be possible to miss ties the film has with a genre that mythologizes the conquest of the American frontier. Talking about the film critically, however, these connections become clear. So as we tried to decide how our classifier related to these posters, a number of difficult questions arose—can a movie set in space be a western? What even is a western—what does it mean for a film to be part of, or responding to, an established genre? Is it something we can discern from a movie poster in the first place? The answers to these questions—central to the kind of results that a classifier developed through machine learning can provide—are also human decisions, all the way down, made while the classifier is being trained. Now that the limits and interpretive dangers of computational approaches had taken concrete form, I transitioned to my own use of computational approaches in my research: how I use sentiment analysis (using a classifier to evaluate a given snippet of text for different kinds of sentiment, i.e. “positive” or “negative”)—in my analysis of Black Arts poetry like those that we had spent most of the three hour seminar discussing. Similar questions applied to this research as well: what does it mean for a poem to be “positive” or “negative”? How does a snippet of text “have sentiment” in the first place? How do computational approaches stack up with more traditional ways of reading poetic language for sentiment, feeling, emotion, or affect? By the time we finished this workshop and I shared how it applied to my own research, we had run out of class-time. Which means, unfortunately, I had to cut an activity I had planned using Prism to “computationally read” Baraka’s poem “Black Art” for political and artistic practice as a class. There’s always more to do and discuss, but what we did discuss felt like an enormous success—by the end of this workshop students seemed to have a strong handle on the principle ideas behind machine learning, as well as how a classifier might be used to analyze different aspects of artistic objects. That, combined with the seriously insightful conversations we’d been having all afternoon, felt like a huge win! So thanks again to Professor Wheeler, Mackenzie Brooks, and everyone else at W&amp;L as well as at the Scholars’ Lab who organized it all and made my whole visit possible."},{"id":"2018-05-11-hack-your-pants-video","title":"Hack your pants - video","author":"ammon-shepherd","date":"2018-05-11 12:15:48 -0400","categories":["Makerspace"],"url":"hack-your-pants-video","layout":"post","content":"Here’s a quick video to show you how to make your pants pockets bigger!"},{"id":"2018-08-21-fall-2018-uva-library-gis-workshop-series","title":"Fall 2018 UVa Library GIS Workshop Series","author":"chris-gist","date":"2018-08-21 09:03:30 -0400","categories":["Geospatial and Temporal","Technical Training"],"url":"fall-2018-uva-library-gis-workshop-series","layout":"post","content":"All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be taught on Wednesdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up! September 19th Making Your First Map with ArcGIS Here’s your chance to get started with geographic information systems software in a friendly, jargon-free environment.  This workshop introduces the skills you need to make your own maps.  Along the way you’ll get a taste of Earth’s most popular GIS software (ArcGIS) and a gentle introduction to cartography. You’ll leave with your own cartographic masterpieces and tips for learning more in your pursuit of mappiness at UVa. September 26th Georeferencing a Map - Putting Old maps and Aerial Photos on Your Map Would you like to see historic map overlaid on modern aerial photography?  Do you need to extract features of a map for use in GIS?  Georeferencing is the first step.  We will show you how to take a scan of a paper map and align in it in ArcGIS. October 3rd Getting Your Data on a Map Do you have a list of Lat/Lon coordinates or addresses you would like to see on a map?  We will show you how to do just that.  Through ArcGIS’s Add XY data tool and Geocoding (address matching), it is easy to take your tabular lists and generate points on a map. October 10th Points on Your Map: Street Addresses and More Spatial Things Do you have a list of street addresses crying out to be mapped?  Have a list of zip codes or census tracts you wish to associate with other data?  We’ll start with addresses and other things spatial and end with points on a map, ready for visualization and analysis. October 17th Taking Control of Your Spatial Data: Editing in ArcGIS Until we perfect that magic “extract all those lines from this paper map” button we’re stuck using editor tools to get that job done.  If you’re lucky, someone else has done the work to create your points, lines, and polygons but maybe they need your magic touch to make them better.  This session shows you how to create and modify vector features in ArcMap, the world’s most popular geographic information systems software.  We’ll explore tools to create new points, lines, and polygons and to edit existing datasets.  At version 10, ArcMap’s editor was revamped introducing new templates, but we’ll keep calm and carry on. October 24th Easy Demographics Need to make a quick demographic map or religious adherence?  This workshop will show you how easily navigate Social Explorer.  This powerful online application makes it easy to create maps with contemporary and historic census data and religious information. October 31st Introduction to ArcGIS Online With ArcGIS Online, you can use and create maps and scenes, access ready-to-use maps, layers and analytics, publish data as web layers, collaborate and share, access maps from any device, make maps with your Microsoft Excel data, customize the ArcGIS Online website, and view status reports. November 7th Introduction to ArcGIS Pro The handwriting is on the wall.  ArcGIS Pro will be replacing ArcMap as the desktop GIS in the near future.  Come learn what’s the same and what’s new, and how this powerful new tool incorporates seamlessly with ArcGIS Online and ESRI’s cloud-based mapping tools."},{"id":"2018-09-04-fall-2018-virtual-reality-workshop-series","title":"Fall 2018 Virtual Reality Workshop Series","author":"laura-miller","date":"2018-09-04 15:10:30 -0400","categories":["Geospatial and Temporal","Technical Training"],"url":"fall-2018-virtual-reality-workshop-series","layout":"post","content":"Oct. 2 VR Workshop: Creating Virtual M useums (6 seats available; register ) This workshop will explore creating virtual museums using the free game engine Unity3D and found assets from the web. Participants will be guided through the initial stages of museum creation using stills, videos, audio and 3D models and will culminate with exporting projects for the web and for VR. No experience necessary. Location: Clemons Library, 3rd Fl, Robertson Media Center Immersive Space Oct. 9 VR Workshop: VR Painting, Modeling and Graffiti (6 seats available; register ) This workshop will allow users to try their hand at virtual modeling, painting and graffiti using titles such as Google Blocks, Tiltbrush, Kingspray Graffiti and MasterpieceVR. No experience necessary. Location: Clemons Library, 3rd Fl, Robertson Media Center Immersive Space Oct. 16 VR Workshop: VR Storyboarding, Animation &amp; Puppetry (6 seats available; register ) This workshop will explore creating animated 2D and 3D videos using puppets and characters in virtual environments. Participants will explore creating simple scenes with software such as Tvori, Mindshow and Flipboard Studio. No experience necessary. **Location: ** Clemons Library, 3rd Fl, Robertson Media Center Immersive Space Oct. 23 VR Workshop: Photogrammetry - Capturing Your World (12 seats available; register ) This workshop will be an overview of software and techniques used to convert a series of photos to a 3D model and dataset. No experience required. **Location: ** Clemons Library, 3rd Fl, Robertson Media Center VizLab Oct. 30  VR Workshop: Augmented Reality Tools (12 seats available; register ) This workshop will explore augmented reality game creation using ARIS Field Day and Unity3D. No experience necessary. Location: Clemons Library, 3rd Fl, Robertson Media Center VizLab Nov. 6 VR Workshop: Visual Scripting with Unity3D: Playmaker (12 seats available; register ) This session will involve an overview of the Unity3D asset Playmaker which allows users to create interactive content without coding by using logic flow diagrams. Basic familiarity with Unity3D is a bonus but not required. **Location: ** Clemons Library, 3rd Fl, Robertson Media Center VizLab Nov. 13 VR Workshop: Virtual Music Creation (6 seats available; register ) Try your hand at creating music in virtual reality using tools such as Exa: The Infinite Instrument, Soundstage and Lyra. No experience necessary. Location: Clemons Library, 3rd Fl, Robertson Media Center Immersive Space"},{"id":"2018-09-05-fall-2018-maker-code-workshop-series","title":"Fall 2018 Maker & Code Workshop Series","author":"laura-miller","date":"2018-09-05 10:41:30 -0400","categories":["Makerspace","Technical Training"],"url":"fall-2018-maker-code-workshop-series","layout":"post","content":"Makerspace Workshop: Programming Electronics on the Raspberry Pi In this workshop we’ll learn the basics of using your Raspberry Pi to control electronic circuits. No experience with electronics or Raspberry Pi required for this workshop. Due to limitations of hardware, this workshop only has four open spots. Thursday, October 11, 1-2pm** (Alderman 423) Reserve your seat . Wednesday, October 31, 12-1pm** (Alderman 423 Reserve your seat . Code Workshop: Build a Website for Beginners - The Very Basics of HTML and CSS This workshop will walk you through creating a very basic personal website with programs you already have on your laptop computer. If you don’t have a laptop, we can supply one for you. This is the wading pool of website building, no experience necessary. Monday, September 10, 3-4pm** (Alderman 423) Wednesday, October 3, 3-4pm** (Alderman 423) Thursday, October 25, 3-4pm (Alderman 423) Canceled: Wednesday, November 14, 3-4pm (Alderman 423) Monday, December 3, 1-2pm** (Alderman 423) ** Makerspace Workshop: Hack Your Pants - Make your Pockets Fit your Phone**\nAre you tired of your phone poking out of your pocket? Did you get a new Phablet and now you have nowhere to stick it? This workshop will show you how to take an old tshirt and make your pockets bigger. No experience necessary, but you will need to bring your pants and an old tshirt. Warning: you will be cutting your pants pockets, and there is a possibility that the pocket will not be to your liking in the end. Wednesday, October 17, 12-1pm (Alderman 423) Wednesday, November 7, 12-1pm (Alderman 423) ** Makerspace Workshop: Easy Electronics - Arduino for the Beginner**\nCome learn the basics of using the Arduino for fun or profit. No experience or equipment needed. Arduino kits and laptops provided to use in class, but you are welcome to bring your own. This workshop will go through the very basics of electricity, how to setup the Arduino, and building a first circuit; an LED nightlight. Wednesday, September 26, 12-1pm (Alderman 423) Thursday, October 18, 2-3pm (Alderman 423) Monday, November 5, 3-4pm (Alderman 421) ** Code Workshop: Basics of Version Control with Git**\nDo you have file names like: Paper.doc, Paper-revision1.doc, Paper-v2.doc, Paper-Final.doc, Paper-Final2.doc? Have you heard of version control, but not sure how to get it working for you? This workshop will give you an overview of how to use Git and GitHub to version your documents (word processing, coding document, or any type of files). This is a workshop for absolute beginners, so no experience is required. We will mostly be talking about version control from a theoretical standpoint, and won’t be installing or using git on your own computers. Thursday, September 20, 1-2pm (Alderman 421) Wednesday, October 10, 12-1pm (Alderman 423) Monday, October 29, 2-3pm (Alderman 423) Monday, November 19, 1-2pm (Alderman 421)"},{"id":"2018-09-13-building-ourselves-up-to-build-things","title":"Building Ourselves Up to Build Things","author":"catherine-addington","date":"2018-09-13 10:10:47 -0400","categories":["Grad Student Research"],"url":"building-ourselves-up-to-build-things","layout":"post","content":"I’m a Praxis fellow this year and a PhD student in Spanish. (You might also remember me as the one who really loves public writing ; you’re going to be reading a lot of blog posts from me this year.) And I’m a coder, apparently. Some background on that last bit. To start off our inaugural CodeLab session earlier this week, the Praxis fellows and the Scholars’ Lab staff went around the room answering a question that seems easy: what’s your technical background ? I said I basically didn’t have any, that I got myself into digital-ish positions by virtue of being the person in the room willing to click around the longest and figure it out. In part, that’s true—and patience is a skill that I don’t take for granted! But near the end of the session, as our fearless leaders Zoe and Shane officially proclaimed us coders (“so now you can stop worrying about whether or not you count yet”), I started to realize I hadn’t really given myself credit for what I bring to the table. I’m lucky enough not to have really faced that infamous beast known as “impostor syndrome” in my grad school career thus far, so I didn’t see it coming. Nevertheless, sitting there surrounded by actual developers who do this for a living…I was feeling it. (This despite having been working on a collaborative text-analysis project at UVa that I presented at a real live DH conference that real live DH people said was interesting!) In honor of the very official proclamation, I thought back on what I know how to do that I was scared to “count” as real digital knowledge and want to bring to this program going forward. Annoyed-user HTML : you know when you’re trying to make a thing on the internet, and the platform you’re using is free so you can’t complain, but the very simple thing you want isn’t built in so you groan and click “source” to examine the HTML only to see endless and &lt;div&gt; tags that are out to personally ruin your life?! I have now come to understand that the years of fandom-tumblr-theme-tweaking and tinyletter-wysiwyg-fighting that got me messing with HTML by trial and error were incredibly valuable. So maybe I can’t build a website from bottom-up (yet), but I can look at what’s gone before me and figure out how to build on it a little. Apparently that’s a good thing . I-guess-I’m-the-website-manager-now patience : you know when your boss asks you if you know how to do something and you lie yes and then you google it and then you’re the expert forever and ever amen? This is the story of how I spent way too much time on the back end of the inexplicably-custom CMS of the website of every organization I’ve ever worked or volunteered for, and now I have a pretty good idea of what Wordpress and Drupal do. So maybe I don’t have a profound understanding of these tools (yet), but I can translate between frustrated web developers and money-having executives who don’t know what they want. Encouraging idea-havers and tool-knowers to match their skillsets is a good value to bring to this program. Language-instructor enthusiasm : you know when you have to knock out your language requirement so you pick Spanish because whatever you had it in high school so hopefully it doesn’t completely ruin your first-year GPA? Yeah, I teach that. It’s pretty much my job to make group work on a subject of unknown interest into a productive experience for everyone involved. So maybe our project this year and everyone’s interest in it is unclear right now, but I’m in a good position to channel that willingness to get hype about anything—and to sow seeds of that excitement in others. Makerspace attitude : you know when you have to fix a 3D printer basically once a week because those things are the finickiest invention of all time? No? Just me? Well, working in the Makerspace last year, I got really comfortable with patient troubleshooting, trying new things, and most importantly, asking for help. That last one doesn’t come easy for me, but it’s going to be important heading into this year. So maybe I’m willing to play around—but I’m also committed to knowing my limits and taking advantage of the institutional knowledge and life experience of the experts around me. In the coming weeks, our Praxis cohort will be developing a charter to set down the values and goals that will guide us in our collaboration this year. I’m hoping that the skills and values I’ve uncovered in my individual reflection will prepare me to build up a similar confidence in my collaborators. In that spirit, we’ll start from a place of honesty and openness, valuing everything we bring to the table—whether they’re skills we see in ourselves or skills we see in each other."},{"id":"2018-09-24-job-opening-developer-outreach-testing-coordinator","title":"Job opening: Come advocate for our users!","author":"amanda-visconti","date":"2018-09-24 12:36:44 -0400","categories":["Announcements","Job Announcements"],"url":"job-opening-developer-outreach-testing-coordinator","layout":"post","content":"( As of 9/28/2018, we updated the length of the User Advocate position’s appointment to 18 months (with the possibility of 6 months’ renewal). This post has been edited significantly since its original posting to reflect this change. Contact jeremy@virginia.edu with any questions. ) We’re seeking an additional colleague for our R&amp;D team: Development and Testing Outreach Coordinator ! Come be our advocate for the users of our DH projects, working on documentation, design, development, and project management approaches to champion user needs. Apply now  (or search UVA Jobs for posting #0624083), or read on for more info. We welcome applications from women, people of color, LGBTQ, and others who are traditionally underrepresented in technical roles. In particular, we invite you to apply even if you do not currently consider yourself to be a tech person or programmer. We seek someone with the ability to collaborate and to expand their technical skill set in creative ways; we’re happy to discuss whether you’ve got the skills we need for this role. Summary Job title: Developer Outreach &amp; Testing Coordinator (“User Advocate” for short) Job type: Full-time (40 hours/week) with benefits; not a permanent position (contract length of 18 months, with possibility of additional 6 months) Salary: $60,000/year Location: Scholars’ Lab, Alderman Library, University of Virginia (Charlottesville, VA) Start date: **Any time between now and January 1, 2019\n**Benefits: include health/dental/vision insurance, conference travel funding, 20% self-initiated R&amp;D time Job description This is a non-permanent Developer Outreach &amp; Testing Coordinator role located in the University of Virginia Library ’s Scholars’ Lab. The primary purpose of this role is to apply working knowledge of design, development, and project management to advocate on the behalf of our various DH project user communities. You’ll do this through: creating technical documentation for both developers and non-technical users leading workshops or teaching modules on our projects refining existing documentation through user testing and community feedback responding to requests for technical support related to the grant projects via email, the project’s forum, and in-person consultation other support for our staff doing design and development work, such as organizing and commenting on existing code, designing and coding proofs of concept, and participating in code and design reviews Education requirements: Either a bachelor’s degree plus one year of work experience, or at least five years of work experience. We prefer (but do not require) a bachelor’s degree with relevance to the humanities, or graduate coursework with relevance to the humanities—but we’re truly open to considering other experiences as comparable to these. **Knowledge &amp; skills: **You do not need to possess all of these, as we’ll consider strength in some of these as compensating for lack of experience with other skills on the list. We’re seeking: Empathy and understanding for the technical needs of both developers and non-developers Experience instructing, mentoring, or otherwise assisting other people in learning to use technology Passion for interactive experiences across a variety of media (web, mobile) with a strong desire for innovation Comfort with complexity and ambiguity, and readiness to take on the challenges of the humanities and social sciences Understanding of user interface client-side technologies such as Javascript, HTML, and CSS; and PHP Experience running user testing and conducting accessibility testing Experience creating standard user experience deliverables, including site maps, personas and use cases, and wireframes and interactive prototypes Solid presentation and communication skills, especially around the ability to communicate the connections between technical and cultural scholarship, and the importance of design for building relationships and supporting communities About us The Scholars’ Lab is the UVA Library’s lab for experimental and digital research, teaching, and other forms of scholarship, open to all disciplines and anyone curious about getting their hands dirty. We’re proud to be part of the Library’s mission to provide equitable access to learning for everyone. We frequently collaborate with students, staff, and faculty on campus, as well as community members, and hold an international profile as a digital humanities and digital scholarship research center. The Scholars’ Lab currently consists of 14 staffers, plus an amazing cohort of student fellows, interns, and makerspace technicians. As our colleague, you’ll have exposure to a wide range of projects, tools, and topics, including spatial humanities, interface design, innovative pedagogy, data visualization, text analysis, digital archiving, 3D modeling, virtual reality and gaming, and other experimental humanities and library approaches. The Library and the Scholars’ Lab are committed to diversity, inclusion, and safe/brave spaces, and we have focused recent speaker series and practice on accessibility and social justice ( check out our team-authored charter for more on our values ). We welcome curious, critical, and compassionate professionals who are keenly interested in the overlaps between technology and the humanities (literature, history, art, cultural heritage, and related fields). Interested? Contact us or apply! Apply here  (or search UVA Jobs for posting #0624083). You can address your cover letter however you wish, e.g. “To whom it may concern” or “Dear search committee”. Please feel free to reach out with any questions—for yourself or a friend—by emailing jeremy@virginia.edu or tweeting @scholarslab. In particular, we’re very happy to talk with anyone who’s interested, but not sure whether they have the required technical background or how this non-permanent role might fit into their career plans or personal life. All job discussions and the search process will be treated as confidential to the search committee. We understand that there are many reasons for not wanting other folks (including your references) to know you’re curious about other jobs, and we will ask your permission before contacting any references."},{"id":"2018-09-27-i-o-reading-writing-as-a-digital-humanist","title":"I/O: Reading & writing as a digital humanist","author":["amanda-visconti","ammon-shepherd","brandon-walsh"],"date":"2018-09-27 07:58:06 -0400","categories":["Digital Humanities"],"url":"i-o-reading-writing-as-a-digital-humanist","layout":"post","content":"In which we consider our different practices of reading and writing. Tell us a bit about yourselves. AS: My name is Ammon Shepherd. I’m a Digital Humanities Developer and I help out in the Makerspace. I work in fits and spurts on different projects, and my reading kind of reflects that. BW: I’m Brandon Walsh, Head of Graduate Programs in the Scholars’ Lab. I listen to a lot of audiobooks and podcasts, and this lean on a variety of different formats has formed a lot of my reading habits. AV: I’m Amanda Visconti ; I co-direct the Scholars’ Lab. I like making websites about books, and reading books about websites. How has your reading style changed over the years? AS: While researching and reading for my dissertation in history, much of my reading was very traditional; books, journal articles, scans and images of archival documents, and loads of printed copies of archive papers. After doing archive research, I would typically go through each document and take notes as I went on the things that I felt would fit into the different chapters. Reading books and articles was a bit different. With books and articles, I was usually trying to find information for specific aspects of the narrative or argument I was making. I would skim through the text looking for points to reference. My reading now is much less academic, at least for the traditional field of history, or the humanities paradigm in general. My day to day work involves more problem solving and research of technical aspects and computer programming. I read a lot of blog posts, support forums, and software documentation in order to solve the problems. I’m usually reading for a quick fix to a specific problem. Instead of books and journals, I’m reading personal blog posts, Stack Overflow questions and answers ( https://stackoverflow.com/ ), documentation for ancient ( http://cocoon.apache.org/ ) and modern ( https://nodejs.org/en/docs/ ) software, and tutorials for constructing electronic projects ( https://www.raspberrypi.org/ ).   BW: I remember reading for my PhD comps as a massive shift in my reading practices. During that time, I read more long-form pieces - novels, essays, books - than I have ever read before or since. It really burnt me out on sustained reading in a way that took several years to recover from. I’ve been slowly rebuilding the ability to commit to sustained long-form reading in the years since then, and it is only in the past several years that I’ve been able to make any headway, certainly in terms of pleasure reading but also in terms of reading for work. One of the main changes recently has been to the context in which my (work) reading is happening - I have far less time to do it. Particularly in a 9-5 office job, one of the challenges is recognizing that reading for work is, well, work, and not some distracting cheat on my other responsibilities. This is as much an internal pressure that I apply to myself as anything else. One strategy for encouraging that reading actually happens is to block the time off on my calendar for it (a strategy I think I adopted after Ryan Cordell ’s example). Doing this feels like an indulgence in between meetings, but it makes me better at my job. And the flip side of this is that I work hard to try and limit work reading to work hours and read other things outside of the office. It’s a difficult balance to strike, particularly when a novel or article might be both for pleasure and relevant to my work. But I think it’s important at least to recognize that thinking and reading can be work. And the time and energy we give to them should be factored into conversations about sustainable work practices. AV: Like Brandon, reading for literature comps ( here’s my reading list and rationale! ) changed my relationship to reading, but in an ultimately good way—I pay a lot more attention to whether I’m enjoying and challenged and learning from something I’m reading now, and am better at just stopping reading when those things aren’t happening. There was never enough time to read, during the first three years of my PhD program—I always was racing to get through the pages, and didn’t have enough time to reflect, or to move through readings non-linearly. I worried about being questioned about some random concept on page 180, rather than getting a sense of some of a book’s arguments and what I thought about them. And I felt like I needed to Know Things, that I needed to have read X thinkers on Y authors to be a functional scholar. (My mentors in grad school were amazing, and to their credit none of these pressures came from them. Rather, I was seeing how other students were working and figuring out what being a scholar meant to me. I remember Melanie Kill recommending I use exams prep to figure out how I want to manage the reading part of being a scholar, before starting the first big project of the dissertation. Her advice rewrote my initial floundering with how to usefully ingest a lot of texts into something expected and welcomed.) I basically don’t read any books or print journal articles anymore. I’ve gotten off most (all?) listservs. I’m not any less of a scholar for delimiting my reading to other formats. Folks who do all or some of their reading via these formats are real scholars, too. How, when, and where you read are personal choices. There is a lot of writing and discussing out there, no one can read more of it, and it’s completely fine and intellectually responsible to delimit the formats and topics you’re going to read. Scholarship doesn’t come from restricting the ways you take in new ideas; scholarship is challenging yourself to think hard and share that thinking so others can learn from and build on it (a re-articulation of Mark Sample’s great “ When does service become scholarship? ”). And I guess I do extrapolate some best practices from that definition of scholarship, like representative and accurate citation being necessary for scholarship to work (e.g. see Sara Ahmed ’s articulation of citation as a black feminist practice). You can’t share your thinking well if you don’t say whose work you’re building on and how, and contribute to the community’s knowledge of other thinkers to learn from in addition to your thoughts. How do your reading inputs connect with writing/outputs? BW: When I worked on my dissertation I had a good cycle on input becoming output. Each morning I would read and mark down quotes that were relevant, and then in the afternoon I would write, directly integrating the new material I had just read. For the most part, things I read were all immediately instrumentalized. Of course, that approach really only works when you’re working in a sustained way on a big writing project on a topic. It’s harder to maintain that system when you’re just reading whatever happens to come through Twitter on a regular basis and when you’re also balancing all of that with the actual working realities that pull your attention in a thousand other directions. Now the material I take in doesn’t directly turn around in the same way. I still read, and things get absorbed. But these readings are often on a number of different topics and less likely to immediately and directly get piped right back out (perhaps because I’m not doing as much academic writing these days). In some ways, the day-to-day experiences of my job actually wind up being more directly and consistently piped into my writing as context for my thinking. Which is to say - reading and citation are only one way of accumulating evidence for a piece of writing. Lived experience, with enough reflection is another. And that’s something I constantly try to teach my students, who tend to be most familiar with how to work with written resources in their scholarship. For them, thinking critically about the lived process of their scholarly work is often more alien, and it’s useful to help them reflect on it and learn to connect it to their reading and writing. AV: My reading nowadays largely consists of: blog posts, tweets and Twitter conversations, attending conference and event talks (or reading transcripts afterward), talking/Slacking/emailing with people. For Slack, we’ve got an active Scholars’ Lab internal Slack for our staff and students, I love the Documenting the Now Slack for generous and smart thinking about the ethics of archives (particularly archiving and doing research with social media), and the DH Slack (which I’ve been posting to less lately, but still enjoying reading). Sometimes the blog posts or tweet link to digital projects or online journal articles that I read, but the “next” books on my work to-read list have been the same ones for what feels like a year, and I’m not getting to them. I’ve moved away from bookmarking stuff that I think I ought to read, and am only bookmarking things I think I’m likely to read soon, and that will impact my work (change or improve how I think or talk or listen). Having a huge backlog of “should reads” can make me feel behind, or like I’m slogging through processing text for reasons other than my own. I’d rather not have too much stuff on the backburner, and take longer thinking about the stuff I do read. Sometimes it’s physically small pieces of reading that have a large impact on my writing, like this tweet by Élika Ortega, that rewrites the popular focus on “What is DH?” to instead ask “What can DH be?” That tweet—and her work living that shift—plus Stewart Varner’s building on Élika’s thinking, all encouraged me to take more personal agency in my scholarly community. Rather than just imagining what I’d like DH to become as a field, I started looking at how the tiny snowflake of my scholarly choices contributes to what the field becomes. For example, I’d thought I was okay with my dissertation creating yet another edition of a canon white male author, and I don’t think I’d argue the same today. That is not a comment on what anyone else should study—I just found that a shift in my thinking about the field of DH accompanied a shift in scholarly topics and formats. This is all to say that something as un-book-like as a tweet can cause a profound shift in someone’s scholarly career, and influence various kinds of future scholarship (including, in this instance, blogs posts, research projects, and keynotes). Élika’s tweet (arising from the #WhatIfDH2016 etc. hashtags) made a huge impact, but Twitter’s been significant for my scholarship in subtler ways as well. I’ve learned to be a better scholar and person from Twitter exposing me to new ideas, or ideas I didn’t take seriously enough. It is wrong that I’m able to get so much from a platform that is actively unsafe for many of my peers to use. Some of my more recent scholarship has been a reaction to this: what features and feeling would an ideal academic virtual community have; what good virtual academic communities already exist, and whether there’s even a need for alternative technical platforms to assist with addressing social issues; and how we can better structure the work of community-building and moderation that helps such communities thrive (particular interests: more equitable distribution of the labor and stress of moderation, and recognition and reward for this work as scholarship). AS: While doing traditional academic research, the output was foremost a dissertation, secondly a hopeful journal publication, and finally a blog post and website, because in the end, that was the only way the research was going to be accessible. The dissertation and journal pieces were not accepted, but the website was self published, and read by hundreds. There was always the sense that there was a larger audience interested in the topic I was researching, and a dissertation or journal for academia was not the right fit. So the website contains the text of the dissertation ( https://nazitunnels.org/dissertation ), and a blog ( https://nazitunnels.org/ ) about the process of research and writing. Nowadays, I don’t do a whole lot of writing for others to see. The websites and code that I write are the output, and most of that is unseen. The output can also be in the form of a physical object. One recent project produced a hand-held mp3 player with 3-inch monitor, built from a Raspberry Pi computer and lots of knobs, switches and other electronic components. The process of creation was only made possible through the many tutorials found on the World Wide Web. Another form of output is writing, but it is documentation of the processes to create objects and websites. These writings aren’t found in journals and blogs, but in GitHub or GitLab repositories ( https://gitlab.com/scholars-lab/womensbios ). Rarely, I’ll post some musings and findings on my personal blog, but they are almost exclusively technical now ( https://mossiso.com/2018/09/18/what-is-tput-sgr0-doin-in-my-bash-prompt.html ). What do you wish you did differently? What do you want to try? AV: I’d like to get back to blogging a couple of times a month. I’ve got a large backlog of drafts, including a series on designing DH infrastructure from my previous role at Purdue—I’d like to post those, and to continue to make infrastructural documents and decisions from my role in the Scholars’ Lab public, like the work of structuring our new job openings equitably and drafting job ads to match ( one example, another example ). I also want to experiment further with my blogging voice. I’m interested in making public the pieces of scholarship that get “published” less often, whether that’s things like hiring that we don’t share to the detriment of the diversity of our community (e.g. folks sharing the text of their successful job talks— Lee Skallerup Bessette, Celeste Tuong Vy Sharpe, Chris Bourg, and Brandon Walsh, as well as my Scholars’ Lab and previous Purdue job talk s), or what the daily scholarly practices look like that eventually result in a book or website (e.g. the software they use to make daily progress on writing a book ), or some small technical thing I figured out (e.g. automating screenshots of my liked tweets for personal Morale Boosting; post coming… soon?). And I’m proud of my Programming Historian lesson, which I tried to write with kindness and thorough explanation of non-obvious technical stuff. But I also love to read digital writing that is thoughtful in a different way about language and structure. I think I’ve had “write something that makes me feel excited about scholarship the way Bethany Nowviskie ’s blog posts make me feel” as a goal for years now. Brandon recently shared some of his writing that made me similarly really excited about work, and reminded me that I want to try similarly structuring some of my own thoughts. I think more attention to language and structure could help me here, but I’m also realizing that a difference between many of my blog posts and these digital essays I admire, is that they argue in a generative way much of my writing doesn’t. Some things I’d like to experiment with emulating and building on: Bethany’s lyrical force, Brandon’s clarity, Aimée Morrison ’s life-giving affirmation of scholars’ brains and bodies, and April Hathcock ’s courage and concision . BW: I always really enjoy blogging, and I wish I did it more regularly. At present I mostly work from a “blog whenever I feel like it or have a talk to share” mode, but I think the result of that is that the public sharing of ideas can often end up on the backburner. I’d like to get back into a habit of thinking out loud and on the page as a regular part of my work routine. I regularly encourage students to blog their process and to think about the blog as a place where still-forming reflections on any number of process-related things can have a home. But I’ve become something of a hypocrite in that I go months without posting anything and wait around until I feel like I have something to say. I’d like to walk the walk more and get back to regularly producing smaller nuggets of text. Like any muscle, creativity is one that needs to be nurtured and grown, and I think blog writing gets harder the less I do it. Kathleen Fitzpatrick, Erin Rose Glass, and Jim McGrath are models for me in this regard that I’d like to emulate more. Their commitment to public thought, interrogating and developing new modes for representing writing, and their  provocations about the kinds of possible subjects worth talking about are all deeply inspiring. AS: I would like to have more time to analyse and view the things I’m reading objectively. If I’m writing code or building electronics as a collaborative effort on someone else’s project, I would like to take time and read the scholarship behind the project and contribute to the more academic side of the scholarship. Often when working on a digital humanities website (usually in the mode of fixing or archiving the project), I will spend time reading content. I think I’ll take time to reflect on those readings and write up a short blog post about the “research” I just spontaneously did. Perhaps, if I’m feeling extra outgoing, I’ll find a way to contribute directly to the project. What technologies do you use to facilitate your reading and writing? AV: I blog in a GoogleDoc (for collaborative stuff like this post), or using a Markdown editor (Typora) or text editor (BBedit, for when I really need to focus and not mess with formatting). I moved my personal blog ( LiteratureGeek.com ) from WordPress to a Jekyll-generated static site hosted on GitHub Pages, so sometimes I write directly on the GitHub website. I read and write tweets using Tweetbot (on my iPhone) or Tweetdeck (from my laptop). In the past, I’ve used WriteRoom (for distraction-free exams presentation writing), and bookmarking tools including Pocket, Instapaper, and Pinboard to save things to read later. Currently, I save links from Twitter to Chrome bookmarks, and try to open those tabs at work once a week to read or discard them. BW: I’ve found Pocket to be very useful for managing the incoming stream of blog posts that I want to get to. Pocket is a great way to collect lots of different pieces and then package them into something that I can work with offline. So, in addition to general capturing, before I take flights I’ll often go through and find a bunch of blog posts on a particular topic. I store them all in Pocket, and then I can use my phone to read around a lot of stuff during travel. Reading one blog post might sometimes feel like procrastinating, but reading twenty posts in one sitting while unable to do much else is a great way to feel productive. I also make a lot of use of audiobooks, podcasts, youtube lectures turned into sound recordings using youtube-dl, or organizations like devdh.org that post their materials in an audio format. This is all a holdover from when I had a long-ish commute at my previous place of work, and I was just trying to make the long drive feel like less of a time sink. For a lot of people this form of reading might seem like sacrilege - I’ve often heard people say that too much gets lost when trying to take a text in through your ears. But I like how D.E. Wittkower writes about the phenomenology of audiobooks. In short, it’s not that the one format is better or worse. You certainly lose things in particular formats, but you also gain. You get something useful out of each experience. All of this is to say that I make a lot of use of technologies that let me get at reading in other ways and other contexts. AS: My most used technology while reading is a search engine, usually Google. Most of my writing is done in a terminal, using a text editing program called Vim. I will also use Visual Studio Code ( https://code.visualstudio.com/ ) for writing code. Where do you read? What do you read? (authors, forums/platforms, formats, topics…?) BW: In terms of what I read, I sometimes like to use blogs as an opportunity to do a deep dive into one person as a thinker. Blogs are interesting in this way - they give a serial reading of a single person or group’s thoughts over an extended period of time. You could theoretically grab every book or article a person has ever read and sit down to read them all start to finish, but that requires a lot more logistical work and a lot more time. So before flights I’ll sometimes go through a blog and scoop every piece of public writing from a particular DH person and move through their documented thoughts. I’ve done this for Kathleen Fitzpatrick, Bethany Nowviskie, John Unsworth, and Stephen Ramsay . More generally, I tend to get a lot of ideas for my reading through Twitter. Since I have lists of particular people based around particular topics I often get plenty to take a look at based on those thinkers I am interested in. But lately the most inspiring reading I’ve done has come from Cathy Davidson and Katina Rogers . AS: Almost 100% of my reading is done on the computer nowadays. There are always printed books that cover some of the technology and programming languages that I use, but these are often out of date just after they are published. Documentation for a given project is usually only available on their website. I don’t usually follow a specific author, platform or topic, except perhaps for the following blogs and news outlets: https://hackernoon.com/, https://hackaday.com/, https://slashdot.org/ . AV: I’ve been meaning to gather a list of frequent authors/places I read online, but have exhausted too much energy on my other answers to do that well right now! ¯\\_(ツ)_/¯ In addition to more professionally related reading by folks like my colleagues at the Scholars’ Lab, Bethany Nowviskie, Brandon Walsh, Aimée Morrison and the other Hook &amp; Eye bloggers, April Hathcock, and Chris Bourg, I follow folks on Twitter who do creative, feminist tech work, many of whom also publish great zines . Sarah Werner does great scholarly blogging and tweeting, and I think I’ve also enjoyed basically every fiction book she’s recommended on Twitter. Our thanks to colleague Ronda Grizzle for helping us frame this piece in a generative way!"},{"id":"2018-09-28-a-toolkit-the-public-domain","title":"A Toolkit [...?] the Public Domain","author":"mathilda-shepard","date":"2018-09-28 09:19:49 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"a-toolkit-the-public-domain","layout":"post","content":"January 1, 2019 is Public Domain Day . This means that all copyrighted works published in the year 1923 will suddenly become available for anyone to publish, distribute, or reuse in their own derivative creations. To seize the moment, our Praxis cohort will spend the next nine months constructing a “public domain toolkit.” Since the project is still in the conceptual phase when all doors are open and no one knows exactly where it’s headed, now seems like a good time to reflect on what it means to work […?] the public domain. You can see that I’m struggling with the prepositions. Our work will most likely derive  from the 1923 materials and their metadata, and it will certainly feed back  in to the public domain as a publicly-available resource. But what would a project for the public domain look like? Who is “the public”? And should we be working with the public as we move forward? So far I have used the term “public domain” to mean the negative space outside of intellectual property laws; works in this zone are owned by the public, as James Boyle explains in his excellent book . But “domain” does not always refer to a property relationship. The Oxford English Dictionary indicates that it can also signify “realm,” “sphere of activity,” or “a sphere of thought or action.” These other meanings make it possible to recast “public domain” as “public sphere” and consider how our project might interface with the public’s “sphere of thought or action.” Insights from the public humanities might help us think through how we want our work as digital humanists to engage with this field. I’m hardly the first person to propose a public-digital humanities mash-up. It’s something we’ve discussed as a team and tried to reflect in our cohort charter (coming soon!) Wendy Hsu and Shelia Brennan have theorized the public dimensions of digital humanities, and Will Fenton directly calls on digital humanists to “own their role as public humanists.” From their writings, I have gleaned that practicing public digital humanities entails working  with and for people outside of the academy. It also involves identifying who the public is, and seeing them as “real people” rather than as an  “unidentified other” (Brennan). Making a project available online is not enough for it to be public digital humanities; scholars must consider the community’s needs in order to design meaningful tools and accessible user interfaces. If the target audience lacks reliable internet connectivity, uses outdated machinery or has some other unique technical situation, it might make sense to take a minimal computing approach. Circling back to our embryonic public domain toolkit, if we want to get texts off the bookshelf and into people’s lives in meaningful ways, we need to a). decide who our intended public(s) is (are), and b). consult members of these communities to determine how we can develop a constructive intervention together. As Steven Lubar asserts in his Seven Rules for Public Humanists, “It’s not ‘We’re from the university, and we’re here to help,’ but ‘What are you doing already, and how can we participate? How can we be useful?” Rather than guess how public domain works might be useful to people and imposing a socially-engaged project from our perch at the University, we can think about practicing what Wendy Hsu calls “collective ideation” by collaborating with community members from the beginning. At the very least, we can be mindful about defining who we are trying to reach and researching the best ways to design tools for their benefit. It’s exciting to be at this stage in the process, when all doors are open and we can really think about what we’re doing, how we’re doing it, and why. I look forward to delving into the public dimensions of our project as it takes shape over the next few months. Stay tuned!"},{"id":"2018-09-28-text-mining-and-digital-humanities","title":"Text Mining and Digital Humanities","author":"cho-jiang","date":"2018-09-28 06:33:20 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"text-mining-and-digital-humanities","layout":"post","content":"I’m proud to join Praxis 18-19 cohort and pleased to begin blogging here. Nowadays, tremendous amounts of textual data are available, the volume and variety of data have exceeded the capacity of manual analysis. Computationally-driven text analysis, as a helpful tool for examining elements such as word frequencies, co-occurrence, and ‘topics’ of large corpus data, has been used in a variety of disciplines. It helps to connect humanist scholars to linguists, social scientists, computer scientists, and statisticians. There is a massive amount of resources, code libraries, services, and APIs out there to help with textual information analysis, while coding skill definitely is a plus for learning text mining methodologies. Thanks to the opportunities that Praxis has created, I have started learning programming (Python) and fundamentals of natural language processing – mainly through code sessions in Scholars’ Lab complemented with online resources like Lynda.com and DataCamp. This training in Python (or other programming languages) can expand my imagination about what is possible and help to perform text analysis, but as a beginner in writing python programs (or programming), it is a very creative (and challenging) activity to me. When I am stuck in coding, people in Scholars’ lab are very helpful and supportive. I am grateful that I have been working with such a great team and receiving kindness, patience and support during my phd path. Digital Humanities is a growing and promising area of research, and text mining in DH is a popular topic these days. We have well-developed digital history and digital literary studies. The advances in new technologies such as machine learning or deep reinforcement learning algorithms are expected to allow DH scholars to perform mass digitization of textual resources and automated text analysis. I would like to end on a more personal note, I look forward to learning more practical skills and exploring capabilities of text mining in DH research."},{"id":"2018-10-01-call-for-digital-humanities-fellows-applications-2019-2020","title":"Call for Digital Humanities Fellows Applications – 2019-2020","author":"brandon-walsh","date":"2018-10-01 08:31:58 -0400","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"call-for-digital-humanities-fellows-applications-2019-2020","layout":"post","content":"We are now accepting applications for the 2019-2020 DH Fellows Cohort! Applications are due  Friday, November 30th . The Digital Humanities Fellowship supports advanced doctoral students doing innovative work in the digital humanities at the University of Virginia. The Scholars’ Lab offers Grad Fellows advice and assistance with the creation and analysis of digital content, as well as consultation on intellectual property issues and best practices in digital scholarship and DH software development. Fellows join our vibrant community, have a voice in intellectual programming for the Scholars’ Lab, make use of our dedicated grad office, and participate in one formal colloquium at the Library per fellowship year. As such, students are expected to be in residence on Grounds for the duration of the fellowship. Supported by the Jeffrey C. Walker Library Fund for Technology in the Humanities, the Matthew &amp; Nancy Walker Library Fund, and a challenge grant from the National Endowment for the Humanities, the highly competitive Graduate Fellowship in Digital Humanities is designed to advance the humanities and provide emerging digital scholars with an opportunity for growth. The award provides living support in the amount of $20,000 for the academic year, as well as full remission of tuition and University fees and the student health insurance premium for single-person coverage. Living support includes wages for a half-time graduate teaching assistantship in each semester.  A graduate instructorship, particularly one with a digital humanities inflection, may be substituted for the GTA appointment based on availability within the fellow’s department. Applicants interested in such an option should indicate as such in their application and discuss the possibility in advance with  Brandon Walsh . See past fellowship winners  on our People page . The call for applicants is issued annually in the fall semester. Eligibility, Conditions, and Requirements Applicants must be ABD, having completed all course requirements and been admitted to candidacy for the doctorate in the humanities, social sciences or the arts at the University of Virginia. The fellowship is provided to students who have exhausted the financial support offered to them upon admission. As such, students will typically apply during their fifth year of study or beyond for a sixth year of support.* Applicants are expected to have digital humanities experience, though this background could take a variety of forms. Experience can include formal fellowships like the  Praxis Program,  but it could also include work on a collaborative digital project, comfort with programing and code management, public scholarship, or critical engagement with digital tools. Applicants must be enrolled full time in the year for which they are applying. A faculty advisor must review and approve the scholarly content of the proposal. How to Apply A complete application package will include the following materials, all of which should be emailed directly to  Brandon Walsh : a cover letter, addressed to the selection committee, containing: a summary of the applicant’s plan for use of digital technologies in his or her dissertation research; a summary of the applicant’s experience with digital projects; and a description of UVa library digital resources (content or expertise) that are relevant to the proposed project; a  Graduate Fellowship Application Form; a dissertation abstract; 2-3 letters of nomination and support, at least one being from the applicant’s dissertation director who can attest to the project’s scholarly rigor and integration within the dissertation; and your availability for interviews on December 5th from 10:30AM-12:00PM or 1:00-2:00PM . We’re aiming for a quicker process this year by announcing those interview times in advance, though we can work out alternatives if scheduling difficulties arise. If you are unavailable then, please suggest other times on the 5th or 6th of December. Questions about Grad Fellowships and the application process should be directed to  Brandon Walsh . Applicants concerned about their eligibility, for whatever reason, are strongly encouraged to write as well. Please note that, per University policy, a student who has undertaken affiliate status and ceased to enroll full time is not eligible to resume full-time enrollment or hold a graduate teaching assistantship.  Because GTA appointments are a component of the DH Fellowship, students who have already undertaken affiliate status are not eligible to be considered for this award."},{"id":"2018-10-01-twitterature-mining-twitter-data","title":"Twitterature: Mining Twitter Data","author":"christian-howard","date":"2018-10-01 09:43:26 -0400","categories":["Digital Humanities","Geospatial and Temporal","Grad Student Research","Visualization and Data Mining"],"url":"twitterature-mining-twitter-data","layout":"post","content":"Hello again, everybody! I’m back this semester as a DH Prototyping Fellow, and together, Alyssa Collins and I are working on a project titled “Twitterature: Methods and Metadata.” Specifically, we’re hoping to develop a simple way of using Twitter data for literary research. The project is still in its early stages, but we’ve been collecting a lot of data and are now beginning to visualize it (I’m particularly interested in the geolocation of tweets, so I’m trying out a few mapping options). In this post, I want to layout our methods for collecting Twitter data. Okay, Alyssa and I have been using a python based Twitter scraping script, which we modified to search Twitter without any time limitations (the official Twitter search function is limited to tweets of the past two weeks). So, to run the Twitter scraping script, I entered the following in my command line: python3 TwitterScraper.py. This command then prompted for the search term and the dates within which I wanted to run my search. For this post, I ran the search term #twitterature (and no, the python scraper has no problem handling hashtags as part of the search query!). After entering the necessary information, the command would create both a txt and a csv file with the results of my search. Above: Screen-shot of the python Twitter scraper Given Twitter’s strict regulations on data usage, the csv files created from my Twitter mining list only a limited amount of information about the tweet, while the txt files just contain the Tweet IDs (a distinct, identifying number that is assigned to each Tweet) that matched my search query. Above: Screen-shot of Tweet IDs Fortunately, each of these Tweet IDs contains all the data collected from each Tweet; this data is simply encoded. The twarc tools developed by Documenting the Now ( DocNow ) were instrumental in de-coding this data. That is, DocNow has developed a tool called “Hydrate” that, when applied to a set of Tweet IDs, parses and reveals all of the information associated with each particular Tweet ID. To execute the Hydrate tool, I entered the following into the command line: Twarc hydrate [tweet_ids.txt] &gt; [tweets.jsonl]. In order to avoid confusion, I named each of the jsonl files according to the search term and the dates of the particular query. Above: Screen-shot of tweet hydration For whatever reason, the Twitter scraper occasionally replicated Tweet IDs. As such, I applied the to eliminated redundancies. In particular, I used a twarc tool to “deduplicate” the Tweet IDs. The prompt that I entered into the command line was deduplicate.py [tweets.jsonl] &gt; [deduped_tweets.jsonl]. Above: Screen-shot of the deduping process Once the Tweets were hydrated and deduped, I was able to play with different Twarc possibilities, including geolocation. For the Tweets in which geolocation was available (an option that must be activated by the Twitter user), I was able to output geojson files. Because the geo coordinates associated with Tweets are often not the most exact (sometimes the coordinates are accurate down to a few meters, whereas sometimes the coordinates simply specify the country in which the Tweet originated), I chose to use centroids rather than bounding boxes to indicate Tweet location. In the case where a large area is specified by the geo-coordinates, centroids use the center of the bounding box as approximate location. The command to produce geojson files with centroid-approximated locations was geojson.py [deduped_tweets.jsonl] –centroid &gt; [tweets.geojson]. Above: GeoJSON file with bounding boxes Above: GeoJSON file with centroids As I mentioned above, for this post I looked specifically at the uses of #twitterature per month from January 2011 through February 2018. Using the method described above, I was able to track the changes in usage over time as well as visualize the locations where #twitterature was tweeted in a given year (and remember, the geolocation data is limited to the tweets in which the user chose to activate geolocation services). Above: Graph of usage of #twitterature over time Above: #Twitterature for 2011 Above: #Twitterature for 2012 Above: #Twitterature for 2013 Above: #Twitterature for 2014 Above: #Twitterature for 2015 Above: #Twitterature for 2016 Above: #Twitterature for 2017 As the above data indicates, #twitterature became an increasingly global term between 2011 and 2017, with a noticeable contraction in 2015 (which corresponds to the dip in usage of the hashtag during that same year). It should, of course, be noted that this hashtag is used in primarily English and French speaking countries, and no occurrences of #twitterature are recorded in South America, Russia, China, or other East Asian countries. Despite the prevalence of English in India, #twitterature likewise fails to make an appearance in this country. This dearth may be due to the lack of geospatial data that is available; alternatively, it may indicate that there are other, more popular ways of referring to literature published or disseminated via Twitter in these countries. As we continue working on this project, we’ll look into such alternative references as well as different mapping possibilities to display the data. So that’s the basic idea! You can download the python scraper (TwitterScraper.py) from our GitHub page: github.com/CHoward345/Twitterature-Methods-and-Metadata . For the DocNow tools, visit: github.com/DocNow/twarc . We’ll be posting more about the visualization process and our results soon! And if you want the short version, here you go:"},{"id":"2018-10-27-teaching-transcription-and-secretly-metaphysics","title":"Teaching Transcription (and Secretly Metaphysics)","author":"catherine-addington","date":"2018-10-27 18:05:41 -0400","categories":["Grad Student Research"],"url":"teaching-transcription-and-secretly-metaphysics","layout":"post","content":"As part of the Praxis program’s unit on pedagogy, each member of the cohort has developed a low-tech workshop on a digital humanities topic. Mine focuses on print-to-digital transcription, and the materials are freely available here: lesson plan and slides . Below, I share some reflections on how I came to this topic and what my goals for implementation are. I came to the academy from a journalism background, and though I’ve done plenty of public writing, I found my more menial tasks at the magazine to be a surprising source of intellectual stimulation. Skills like copy-editing, fact-checking, layout, design, and transcription—yes, I have typed up some faxed typewritten pages in my day—do not constitute the creative structural work considered properly “editorial,” but they prepared me well for literary scholarship, the only other field to which I could think to apply these hyper-close reading skills. As I spent all day every day attentive to kerning, spelling, font, and facts, I was trained to switch fluidly between multiple modes of reading. I operated at the level of the character during the work day, took breaks to observe the sentence on Twitter, and refreshed myself with the paragraphs and pages of library books on my commute home. Since beginning my graduate program, I’ve found myself drawn to editing as a form of scholarship. My projects reflect this: I am collaborating on the Multepal edition of the Popol Wuj (developed in Allison Bigelow and Rafael Alvarado’s digital humanities course ) and working on a digital edition of Bartolomé de las Casas’ confessionary (developed in Jim Ambuske and Loren Moulds’ digital history course ). Meanwhile, I study colonial Spanish writers who edited indigenous and Catholic religious texts to address alternative worldviews. Before coming to UVa, I had no idea that textual scholarship was a thing, let alone that I had been engaged with it for years. It was only through this coursework and these projects that I have been able to understand my research interests as legitimate intellectual activity, with theoretical stakes and everything. I’d like to pay that forward. With that in mind, I designed a workshop that is nominally about transcription but secretly about metaphysics. Go big or go home, right? I am considering different implementation possibilities for this workshop. One possibility is that I give it “in-house” to my fellow Praxis participants. It’s a good time for it, since our cohort is shifting into our project phase with the broad objective of developing a toolkit to celebrate the long-awaited release of materials from 1923 into the public domain. That means we are getting excited about remediation—taking works from one medium and transferring them into another. We’re excited to perhaps do that ourselves, but even more so to create a toolkit that allows others to do so independently. My main goal is to speak with an audience interested in remediating these newly released works, and to give them a chance to pause and consider the intellectual work involved in that process. I would hope to empower workshop participants to consider remediation as intellectually legitimate work, whether their interventions into public domain works are as minimal as a diplomatic transcription or as transformative as creative remixing. With that in mind, I am considering implementing the workshop with a wider audience at the library in conjunction with Public Domain Day 2019. If you’re interested in bringing this workshop to an audience near you, contact me personally or download the materials here ."},{"id":"2018-10-30-string-theory-or-lets-explore-social-networks-with-string","title":"String Theory, or: Let's Explore Social Networks with String!","author":"chris-whitehead","date":"2018-10-30 03:53:28 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"string-theory-or-lets-explore-social-networks-with-string","layout":"post","content":"For the last few weeks, the Praxis Fellows have been workshopping workshops designed to render the digital humanities methods we use in our own research accessible (and useful) to undergraduates. If that sounds meta, it’s because it is! But the process has also helped distill some interesting digital humanities methods into their fundamental concepts. In this blog post, I will share the outline of an analog workshop to help convey the basics of social network analysis to undergraduates. By the end of the workshop, the seminar room should look something like this: Photo from “Sherlock Holmes: A Game of Shadows” (2011) And students should look more or less like this: Photo from “It’s Always Sunny in Philadelphia” Because, friends, we’re examining social network analysis. With string! But first, some background and caveats. I am not an expert on social network theory. I have included some links at the bottom of the page for those interested in getting into the nuts and bolts of social network analysis. As a PhD candidate in the history of Native peoples, I never expected to incorporate digital humanities methods into my own research. However, as my dissertation came to focus on the kinship ties binding (and separating) various Native and European peoples living in seventeenth- and eighteenth- century New France, New England, and New York, my research has benefited from social network analysis. As I detail below, one of this workshop’s goals is to encourage students to think about what sorts of research questions social network analysis can help them answer. Goal and Scope By the end of this workshop, students will understand the concepts of social networks and social network analysis: A social network comprises all the connections ( edges ) between all the actors ( nodes ) in a community We conduct  social network analysis   to understand the patterns underlying the network Specifically, students will learn the concepts of the following analyses:   Network Density : In concept, the denser the network, the more people who are connected to each other. Undergrads would be too young for the reference, but the social network of the bar in “Cheers” would be about as dense as they come, because everyone knows everyone. As an actual calculation, network density = connections/possible connections. Betweenness Centrality : This is a key measure of influential nodes within the network. Rather than merely finding the person with the most number of connections, nodes with high betweenness centrality are those that act as the shortest connection point between two other nodes. Modularity : Networks can consist of linked sub-networks. Depending on the research question a student seeks to answer, they might want to understand why sub-networks form and the attributes of those nodes linking them together in the larger network. Students will also reflect on whether social network analysis is appropriate to help them achieve their research goals. Setting, Scale, Materials, and Adaptability The ideal setting for this workshop would be an undergraduate seminar of 15-20 students who meet around a conference table. As detailed below (and hinted at above), we will map connections between students using yarn/string. Things might start getting unruly with more than 20 students. And by gathering around a conference table, each student could see the entire network captured through the yarn connections. To conduct this workshop, the presenter will need the following: String (cut into various lengths two to ten feet long) Yarn (cut into various lengths two to ten feet long) Rope (cut into various lengths two to ten feet long) Masking or duct tape (Make sure the string, yarn, and rope each have a different thickness that is obvious to the naked eye.) I envision the following workshop taking between 60-75 minutes to complete. However, it is possible to adapt the workshop to fill more or less time by the number of examples pursued for each analysis. Furthermore, the presenter could augment the final part of the workshop, “Applications,” with examples from their own work. Photo from playrightmeow.com Now…Time to Play with Yarn Begin by having students construct the social network. Place the masking/duct tape and various lengths of string, yarn, and rope in the center of the conference table and give students a vague instruction such as, “use these materials to map the social network in this seminar.” Proceeding with only this vague instruction, students will hopefully grapple with some early stumbling blocks: What constitutes a connection between two people? Are all connections equal? Hey, Sharon is out sick today…should we somehow include her in this analysis, or proceed without her? What’s the deal with the various thicknesses of the materials (string, yarn, rope)? If the students proceed without asking these questions, inquire what implicit assumptions their actions follow. After a few minutes of the students identifying and working through these problems without any guidance from the instructor, interject and have a group discussion to standardize  edge weights . In other words, determine as a class the criteria for weak (string), medium (yarn), and strong (rope) connections with people. Allow flexibility for how students want to define strength of ties. But some ideas are: weak ties exist between people who have had one-on-one conversations before and who might say a quick “hello” if they passed each other in the dining hall; medium ties exist between people who spend time together socially or have texted each other during the last two weeks; strong ties exist between people who consider each other trusted friends. Spend a few minutes reflecting on why agreeing on these criteria might prove vital in future analysis. Then, instruct the students to use the standardized edge weights to map the social network. (Remember to encourage them to use the tape to secure the connections.) With the social network taped down to the table, ask students about their preliminary observations. Does the social network look like what they would expect? If not, what is surprising? Does seeing the web of string, yarn, and ropes raise any questions for the students? Allow some flexibility to raise questions and discuss potential ideas before moving on to the analysis segment of the workshop. Social Network Analysis Ask the students if they think the social network is  dense . What might we mean by that term? What observations lead them to their conclusion about the network’s density? After some initial reflections from the students, prompt them to a more exact definition of network density. How many connections are there in network? How many connections were possible? Use these to calculate the density. Ask the students whether that number clarifies things, makes them more confusing, or is essentially arbitrary. Ask students what, as a concept, density tells us about the network. Why might this be a useful analysis to run on a network? Next, have students take a few minutes to identify all those people in the network with whom they have NO connection. Give them the following scenario: you need the phone number for each person with whom you have no connection. Using only your existing network, how would you get those phone numbers? (Note: don’t actually collect phone numbers.) Have students write down the path they would follow to retrieve each number. As a group, discuss how students went about finding the path to the missing nodes. Why did they select the nodes they did? What quality or qualities did those nodes have that others lacked? Students likely implicitly pursued the shortest path to the nodes with whom they weren’t connected. They likely also found node(s) connected by the strongest edge weight. Make explicit these internal judgments that students made. Define  betweenness centrality  (see above). As a group, see if there are a few people who have high betweenness centrality. Discuss why this might be an important metric for researchers to collect. What sorts of research questions could it help answer? How does betweenness centrality differ from having the most connections? As a group, see if there are subnetworks that form. What do these subnetworks look like? What judgments do we need to make to call something a subnetwork? What additional information would we need to collect to understand the patterns underlying the formation of these subnetworks? Define  modularity   (see above). Discuss why this might be an important metric for researchers to collect. What sorts of research questions could it help answer? Reflections Ask students what reflections they have about conducting a social network analysis on the “community” of their seminar. Did the analysis reveal anything about the social dynamics of the class that they had not thought about before? Discuss the sorts of research questions that students have that social network analysis might help them answer. Applications and What’s Next Time permitting, demonstrate how social network analysis helps your own work. Show the concepts of density, betweenness centrality, and modularity in action - and why those analyses are useful in answering a research question. Ideally, the instructor’s research uses social network analysis software like Gephi. Connecting the concepts discussed in class to the powerful suite of tools in this software provides a preview of the opportunities available to students interested in learning more about social network analysis. Further Reading For a helpful and accessible overview of how historians can use social network analysis, see: Wetherell, Charles. “Historical Social Network Analysis.” International Review of Social History 43, no. S6 (December 1998): 125–44. I drew from Wetherell’s clear definitions of social networks in the “Goal and Scope” section of this workshop. For an insightful and thought-provoking application of historical social network analysis, see: Kane, Maeve. “For Wagrassero’s Wife’s Son: Colonialism and the Structure of Indigenous Women’s Social Connections, 1690–1730.” Journal of Early American History 7, no. 2 (July 21, 2017): 89–114. I drew from Kane’s explanations of various metrics of social network analysis in the “Goal and Scope” section of this workshop. If students are interested in learning more about how to use digital tools to continue developing their social network analysis skills, refer them to the programming historian tutorials on the subject. I recommend beginning with  From Hermeneutics to Data Networks: Data Extraction and Network Visualization of Historical Sources  and Exploring and Analyzing Network Data with Python . How can I improve this workshop? This workshop is a work in progress. I have tried distilling down the concepts that I have found helpful in my own research to touch on some of the basics of social network analysis. But I’m sure that there are ways to improve this workshop. If you have any thoughts, comments, feedback, ideas, please let me know by posting in the comments section."},{"id":"2018-10-31-starting-off-on-the-right-foot-part-one","title":"Starting off on the Right Foot (Part One)","author":"kelli-shermeyer","date":"2018-10-31 06:51:20 -0400","categories":["Grad Student Research"],"url":"starting-off-on-the-right-foot-part-one","layout":"post","content":"This post is the first in a series that contextualizes my current project, Digital Skriker, within a larger tradition of studying and archiving stage movement. My project explores both the theoretical cruxes and archival possibilities enabled by robust and increasingly accessible motion capture and virtual reality technologies using Caryl Churchill’s play, The Skriker (1994) as a case study. I’m interested not only in how these technologies might change the way we think about documenting stage movement and gesture, but also how they may be used to create modes of (posthuman?) performance. Nothing is more revealing than movement,” wrote Martha Graham, legendary choreographer and pioneer of modern dance. “The body says what words cannot.” Her view that dance could reveal human interiority is shared among many practitioners in the performing arts, as is reflected in the many innovative projects (both analog and digital) that attempt to find better or more useful ways to “read” or archive movement. My work in this field builds on my experience as a theatre director. I once attended a directing workshop that introduced me to the arcane art of composing stage pictures. The facilitator had participants observe volunteers move in two different ways. First, a small group entered the stage area from upstage left and moved to downstage right (Diagram 1). The group then repeated the movement, this time starting from upstage right and moving downstage left (Diagram 2). The facilitator then asked the audience which movement path seemed more threatening and the group, virtually unanimously, replied that watching characters move from upstage left to downstage right (Diagram 1), or moving laterally from the audience’s right to left, conveyed more menace. How strange that something as simple as watching someone move from right to left would be interpreted so similarly by a group of theatre practitioners! This logic—that some areas of the stage and some movements across its surface have connotations—is not unique to contemporary theatre, either. We may also look to acting manuals from the eighteenth and nineteenth centuries that align certain movements and postures with certain emotions, Bacon’s A Manual of Gesture which relates hand gestures to public oratory, or the choreography of classical ballet for examples of embodied semiotics. Furthermore, a 2012 study at Cleveland State University that looked at lateral movement in film provides data to support the idea that characters moving from right to left had a “negative affect,” making audiences feel more anxious and uncomfortable. The researchers at Cleveland State made some conjectures as to why their participants, like those in my directing workshop, were more comfortable with left-to-right lateral movement. They suggested that perhaps the dominance of right-handed people has made the left-to-right pan vastly more common in film. Audiences, they suggest, may divide the screen into “good” and “evil” sides according to the pervasiveness of Christianity in Western Culture; it is far better for one to sit at the right hand of God than the left. I wonder, too, whether or not our perception of lateral movement is conditioned by the way that we read. In English, moving our eyes from left to right would be more natural than moving from right to left. Our interpretations of human movement seem, like our other interpretations, to be culturally and historically contingent. Further examples of this can be found in the ways that some theatre- and film-makers also use directionality to categorize certain movement as “strong” and “weak.” For example, in Understanding Movies, Louis Gianelli posits that characters perceived to be moving in an upward direction (due to the way that camera pans) seem stronger or more dominant, while downward movement appears weak or subservient. Theatre directing is full of these kinds of strange adages as well, and directors often speak of composition as filmmakers describe the mis-en-scene, using lines, levels, and spacing to create emphasis. This is all to say explicitly something that choreographers, actors, and directors understand intimately: movement is an essential part of theatrical expression. We find meaning in and interpret it, as we do the social gestures we make and observe in our everyday lives. In Digital Skriker, I’m mulling over the implications of motion capture technology on the documentation (and potential re-creation) of performance. The Cleveland State study provides empirical evidence that confirms existing wisdom about film composition; I wonder to what extent motion capture can do the same for theatre? In the next post in this series, I will discuss the limitations of thinking about movement and composition in the terms I’ve outlined and explore other systems for documenting movement and digital projects that play with stage movement, choreography, and motion capture. Stay tuned, friends!"},{"id":"2018-11-06-sounding-scholarship-a-workshop-on-making-your-research-sing","title":"Sounding Scholarship: A Workshop on Making Your Research Sing","author":"emily-mellen","date":"2018-11-06 10:47:51 -0500","categories":["Grad Student Research"],"url":"sounding-scholarship-a-workshop-on-making-your-research-sing","layout":"post","content":"Hello! My name is Emily and I’m part of the 2018-19 Praxis cohort. This month, we’ve been thinking about pedagogy and specifically about how to translate our research interests into teachable workshops. If (fellow Praxis member) Catherine’s workshop is secretly about metaphysics, mine might be secretly about sound studies. The workshop revolves around the central question of sound studies, “What can we get from sound that we can’t get from other things?” However, while the discipline of sound studies looks at using sound as subject and evidence, this workshop seeks to encourage practitioners to explore what sound can do for presenting or interpreting research. The workshop is divided into three main parts. The first is largely interactive and its intention is to demonstrate the ways in which sound can convey information differently (and sometimes more effectively or efficiently) than a written text. The second section demonstrates ways in which sound can be used to present research, by showing a series of examples. The third concludes by asking participants to contemplate how these ideas can be used in their own work, as well as other possible methods of using sound in scholarship. This is intended as a stand-alone workshop with an approximate duration of 90 minutes, but elements could easily be taken away or expanded to fit a shorter or longer time period. I would also like to try it as the introductory part of a workshop series or course which would delve deeper into teaching the actual tools used in the examples and allow participants to create their own projects. Required Materials: 2 pieces of blank paper for each participant Writing utensils Printed copies of sound text (see below) for each participant Sound recordings and a way to play them (see below) Simple musical instruments: maracas, toy piano, kazoo, small drums, etc. (optional) Ideally, this workshop will take place in a space with multiple computers that participants can use in part II. Alternatively, participants should be asked to bring laptops or similar technology. Printed copies of information about tools/project examples (just in case) Part I: Why Use Sound? First, participants will be played a sound recording or given musical instruments and allowed a few minutes to play with them. Then, I will ask them to describe the sounds they heard. Possible Prompting Questions: How many sounds were there? How would you describe each sound? How long did they last, individually and collectively? How loud was the sound? Did it change over time? Were there softer and louder sounds within the collective sound? Did the combination of sounds change how you heard individual parts? Did the sounds remind you of anything? Did they evoke an emotion? Then, they will be given a few minutes (and paper and writing utensils) to write about the sound. At the end, I will ask them how it felt to talk and to write about the sounds. Did they notice any differences about the possibilities in talking and in writing? Were there any struggles? Next, they will be given a short text example describing sound. This text can come from anywhere, the only requirements are that it attempts to describe and make sense of a series of sounds and that the sounds that it describes can be reproduced for the group (probably via a recording, but other methods could be considered as well). Texts about a musical piece accompanied by recordings of the piece can be helpful here, but other ideas could include descriptions of political speeches, movie or radio dialogues, or natural phenomena (birdsong or lava flowing, for example). After reading, the participants will be played a recording (or live demonstration) of the described sounds. Then, they will be asked: Did you have a different experience reading about the sound than listening to it? How? What might be the benefits and disadvantages of using each of these ways of incorporating sound into your work? How might the fact that the sound was recorded vs. live affect your perception of it? At this point, it should be possible to enter into a more conceptual conversation about the possibilities of sound. Participants can be asked to imagine ways in which one might use sound in addition to or instead of written text. Some helpful questions to stimulate conversation could include: What sounds do or could scholars talk about? Would it be helpful to hear these sounds? Why? What are the benefits and disadvantages of incorporating sound into text? What are the possible advantages and disadvantages of presenting research entirely through sound? (some interesting points here include cost, portability, and accessibility for oral languages and dialects) What are some ways that scholars could use sound in presenting their work to the public? Part II: How to Use Sound, A Few Examples This section will focus on examples of work in different formats that include sound, as well as tools that can be used to replicate these kinds of presentation. Participants can be asked to split into groups and each group will focus on one example or tool. They can either use provided computers or bring their own technology. After 10 minutes or so to familiarize themselves with the projects, each group will report back, responding to the following prompts: Summarize the tool/project(s) that you looked at What are some possible benefits to this approach? What are some disadvantages? Do you know of any related projects? How could you use this technology in other projects or disciplines? This list is in flux and should be adjusted to respond to the interests of workshop participants, but examples might include: Sound in Online Publishing: Soundcite, The Ballad of Geeshie and Elvie, and Treasured Island . Soundcite is a tool which allows writers to overlay text with audio, so that with the click of a button they can listen as they read. The Ballad of Geeshie and Elvie shows how this can be useful with musical examples and Treasured Island uses it to seamlessly incorporate examples of Tangier Island’s unique accent and dialect as they are discussed. Sound in PDFs: Adobe Buttons . Similar to Soundcite, Adobe Buttons allows you to create buttons which can activate audio and video incorporated into text PDFs. I don’t currently have an example for this, but I suspect it would be great for using sound in course papers or in articles for downloadable journals. Podcasts in Scholarship: Sounding Out! Podcasts ( website / iTunes ), Chart Chat, and Ottoman History Podcast ( website / Soundcloud ) Workshop participants are, of course, likely to be familiar with the concept of a podcast, but these examples demonstrate ways in which podcasts can be incorporated into academic scholarship. The first is a series of podcasts designed by the sound studies website Sounding Out!, in which scholars write brief and insightful work on sound in the form of blog posts. The podcasts are similar to the blog posts, but of course, allow for a different incorporation of sound and different listening possibilities. Chart Chat is a weekly podcast run by two UVA grad students working on popular music. In Chart Chat, they respond to the weekly changes in the American and British popular music charts. For them, this is a format for long-term consistent analysis and a place to work out initial ideas and for listeners, it is a place to learn a little more background and analysis of weekly chart trends. The Ottoman history podcast puts scholars from a variety of institutions and disciplines who work on Ottoman history in conversation. Each week they delve into a different issue broadly related to this topic. Making Podcasts: Garageband and Audacity Garageband and Audacity are two tools that students and other aspiring podcast makers can use. Garage Band is available on all Mac computers and is fairly intuitive to use. Audacity is a free download and has a less friendly interface, but is a little more flexible and allows for more fine tuning. Sound and Interactive Websites in Scholarship: The Roaring ‘Twenties and Lynching in America The Roaring Twenties is a research project about sound in New York City towards the end of the 1920s. The website includes an interactive map showing the location and contents of various noise complaints and of playable newsreels. Lynching in America is, as the name implies, a website about the history and impact of lynching in the United States. Along with interactive maps, a film, and other resources, the website incorporates a section of oral histories taken by people affected by lynching. I am torn about including this project, as it is undeniably important, but also a painful topic that requires a depth of attention that just looking at its use of sound might not give it. Right now, my idea would be to describe it and show the first page, but not delve into the sound examples themselves. Sound and Mapping: London Sound Survey Among other thing, the London Sound Survey features an interactive map of London, broken into a grid in which each square features a series of recording taken from different locations within that area. This project imagines a different way of conceptualizing and remembering the city as it changes over time. Part III: Reflections and New Ideas Finally, participants will be asked to consider new ways of using sound and how this might be useful in their own research. This topic can be presented in conversation or as a written activity, or both. At the end, participants will also be given an opportunity to write down their emails and receive access to the Google Doc containing all the resources listed above, as well as to the workshop slides, if they so desire. An Important Note : It is important to remember that recorded sound is subject to copyright and that before considering using sound in their published work, scholars should consider the necessary protocol to get access to their sounds. If they are creating their own sounds (voice recordings, interviews, etc.), they should remember that these too can be copyrighted. If they do not want a copyright, it is a good idea to consider taking steps to mark their sounds as open access, so that they cannot be copyrighted by others in the future. How Can I Improve This Workshop? This workshop has been designed based on my own interests and experiences and, undoubtedly, there are many other aspects of incorporating sound that I do not cover here. Please comment below or write me at em6ks@virginia.edu with any thoughts, corrections, or ideas for further resources!"},{"id":"2018-11-26-call-for-applicants-dhsi-2019-tuition-fellowships","title":"Call for Applicants: DHSI 2019 Tuition Fellowships","author":"elizabeth-mitchell","date":"2018-11-26 07:00:00 -0500","categories":["Digital Humanities","Announcements"],"url":"call-for-applicants-dhsi-2019-tuition-fellowships","layout":"post","content":"Want to learn more about digital humanities skills, methods, and inquiry? The Digital Humanities Summer Institute (DHSI) at the University of Victoria has a tradition of transformative training. The University of Virginia, as a sponsoring institution of DHSI, provides 5 tuition-free fellowships to attend a Digital Humanities Summer Institute course or workshop during the summer of 2019. Students, staff, non-TT faculty, and those without access to research travel funds are especially encouraged to apply. The fellowships entirely cover the cost of course tuition for one course, using a code at the time of registration (rather than reimbursement afterward). These fellowships do not cover travel (to Victoria, B.C.), meals, or lodging, so applicants should be prepared to fund these out of pocket or by locating additional funding sources on your own. Visit dhsi.org for descriptions of the available courses. Applications are due ASAP. Selection, organized by the Scholars’ Lab, will begin on December 10th and continue on a rolling basis. To apply, please email dhsi-fellowships@virginia.edu with the following: - UVA affiliation &amp; status (e.g. 3rd year PhD in art history, staff job title in Library/department) - Availability for June 3-7 (courses), June 10-14 (courses), and/or June 8-9 (workshops) - The course/workshop you’d like to take (and 1-2 other courses of interest, in case a course cap is reached) - A short, specific statement (300-500 words) about your background, professional/academic interests, and why you would like to attend DHSI - Whether you’ve identified funding (personal or from another funding source) to cover travel, lodging, and meal costs. Questions and applications can be sent to  dhsi-fellowships@virginia.edu ."},{"id":"2018-12-13-call-for-2019-2020-praxis-fellows","title":"Call For 2019-2020 Praxis Fellows!","author":"brandon-walsh","date":"2018-12-13 08:40:17 -0500","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"call-for-2019-2020-praxis-fellows","layout":"post","content":"Calling all UVA grad students! There’s snow on the ground, but it’s never too early to start thinking about spring. Or about what you’ll be doing next year! Consider spending some time with us. The call for applications for next year’s Praxis cohort is now open! Applications are due February 28th, 2019 . The Praxis Program is a radical re-imagining of the annual teaching and training we offer in the Scholars’ Lab. This fellowship supports a team of six University of Virginia PhD students from a variety of disciplines, who work collaboratively on a shared digital humanities project. Under the guidance of Scholars’ Lab faculty and staff, Praxis fellows conceive, develop, publish, and promote a digital project over the course of an academic year. Praxis is a unique and well-known training program in the international digital humanities community. Our fellows blog about their experiences and develop increased facility with project management, collaboration, and the public humanities, even as they tackle (most for the first time, and with the mentorship of our faculty and staff) new programming languages, tools, and digital methods. Praxis aims to prepare fellows with digital methodologies to apply both to the fellowship project and their future research. Our first two cohorts designed and built  Prism, a digital tool for crowd-sourced humanities interpretation, visualization, and textual analysis. Our third and fourth cohorts re-imagined Ivanhoe, a WordPress theme enabling collaborative criticism through roleplay. Our fifth cohort explored sonification of humanities data with the project Clockwork . More recently, cohorts have worked on Dash-Amerikan, a social media ecology of the Kardashian family], and UVA Reveal, an augmented reality project that layers contextual information on contested public spaces on UVA’s campus, and a Public Domain Toolkit for documenting the affects of legal changes on our ability to teach and research with free culture. Beginning as a 2011-2013 pilot project supported by a grant from the Andrew W. Mellon Foundation to UVa Library’s Scholarly Communication Institute, the Praxis Program is now generously supported by UVa Library and GSAS . The Praxis Program is a core module of PHD+, a university-wide initiative to prepare PhD students across all disciplines for long-term career success. The work Praxis Fellows undertake over the course of their fellowship year may be submitted in partial fulfillment of the practicum requirement for UVA’s Graduate Certificate in Digital Humanities. The Praxis fellowship replaces recipients’ teaching responsibilities for the academic year. Fellows are expected to devote 10 hours per week in the Scholars’ Lab. Fellows join our vibrant community, have a voice in intellectual programming for the Scholars’ Lab, and can make use of a dedicated graduate space in the Lab offices. Eligibility All University of Virginia doctoral students working within or committed to humanities disciplines are eligible to apply. We particularly encourage applications from women, LGBT students, and people of color, and will be working to put together an interdisciplinary and intellectually diverse team. Applicants must be enrolled full time in the year for which they are applying. Applicants must still be drawing upon their regular funding packages as part of their doctoral program. I.e. students will typically be in years 2-5 of their program during the year the fellowship will be held. N.b. - Praxis students are not expected to come in with particular technical training or experiences - we cover that over the course of the fellowship year! Prior experience with digital technology is only one part of an application and should not keep anyone from applying. Everyone brings something different to the team, and your strengths in critical thinking about media, collaboration, project development, and more could be great ways for an application to shine. Concerned students are encouraged to reach out to Brandon Walsh, our Head of Graduate Programs, to discuss their backgrounds or eligibility. How to Apply The application process for Praxis is simple! You apply individually, and we assemble the team, through a process that includes group interviews and input from peers. To start, we only ask for a letter of intent. The letter should include: the applicant’s research interests; summary of the applicant’s plan for use of digital technologies in your research; summary of what skills, interests, methods the applicant will bring to the Praxis Program; summary of what the applicant hopes to gain as a Praxis Fellow; and your availability on the days we’ve identified for group interviews March 18th from 11:00 to 12:00 and March 19th from 9:00 to 11:30. We’re aiming for a quicker process this year by announcing those group interview times in advance, though they may be subject to change if scheduling difficulties arise. Questions about Praxis Fellowships and the application process as well as completed application materials should be directed to Brandon Walsh . We hope that you’ll consider applying. Please circulate broadly to all of your friends, colleagues, and students."},{"id":"2019-01-09-parents-fund-award","title":"Parents Fund Award for AR/VR Development","author":"brandon-walsh","date":"2019-01-09 07:45:39 -0500","categories":["Announcements","Digital Humanities"],"url":"parents-fund-award","layout":"post","content":"I am pleased to announce that the Scholars’ Lab and UVA Library have been awarded $6,000 by the UVA Parents’ Fund to increase capacity for augmented and virtual reality teaching and research on grounds. The grant will support a small cohort of students this calendar year as they work with us to create public, novice-friendly documentation on how we apply these technologies to learning and research. If time allows and given student interest, they will also co-teach a workshop series on augmented reality for the university and Charlottesville communities and develop cultural heritage 3D data for preservation in our institutional repository. The hope is that this work will increase our capacity to support interest in these technologies, make our own lessons available to a broad public, offer professionally legible work for the students working to develop the project with us, and establish these same students as a community of experts who can help pay their experiences forward to others. The project is lead by Scholars’ Lab staff Brandon Walsh, Arin Bennett, and Will Rourk. We are very grateful to the Parents’ Fund for their generous support of the Lab and of the Library!"},{"id":"2019-01-09-starting-out-on-the-right-foot-(part-two)","title":"Starting Out On The Right Foot (Part Two)","author":"kelli-shermeyer","date":"2019-01-09 03:30:50 -0500","categories":["Grad Student Research","Digital Humanities"],"url":"starting-out-on-the-right-foot-(part-two)","layout":"post","content":"This post is the second in a series that contextualizes my current project, Digital Skriker, within a larger tradition of studying and archiving stage movement. My project explores both the theoretical cruxes and archival possibilities enabled by robust and increasingly accessible motion capture and virtual reality technologies using Caryl Churchill’s play, The Skriker (1994) as a case study. I’m interested not only in how these technologies might change the way we think about documenting stage movement and gesture, but also how they may be used to create modes of (posthuman?) performance. In my previous post, I outlined some of the prevailing wisdom about stage and film composition as it relates to they way that audiences make meaning from what they see before them. Movement patterns are coded as winning or losing, strong or weak, good or evil in ways that, to me at least, indicate the ways that our interpretations of human movement are culturally and historically contingent, but also become strangely naturalized. It’s here we reach what I find to be one of the most productive tensions in performance studies and theatre-making: any claims that theatre (or art, more broadly) can portray certain universal [emotional] realities collide with our general commitment to historical contingency and cultural specificity. I’m reminded of Richard Schechner’s chapter on “Magnitudes of Performance” in Performance Theory which uses Paul Eckman and Ray Birdwhitsell’s work to suggest that there is a universal language of basic emotions that exists outside of verbal language, on top of which culturally specific kinemes (tiny bits of physical language) are built. Larger movement patterns, like the ones I discussed in the previous post are perhaps best understood as aggregates of these kinemes: sentences or paragraphs of physical language. The idea that movement can (or should) be treated like a language is contested, but also foundational to many of attempts at codifying movement, either through analog means such as Labanotation, or digitally in something like Bill Forsythe’s Synchronous Objects . Movement-as-language falls prey to the same kinds of ideologies and biases as we find embedded in verbal discourse. One blog I looked at for this post describes stage movement in a variety of binaries: “strong, winning attitudes” that are aggressive, concise, dominate, and independent, vs. “weak, struggling or failing attitudes” which are uncertain, evasive, reliant, or indecisive. It’s not hard to see the way that these categories of strong or weak also engage gendered stereotypes and speak to a particular set of values. We see aggression and directness and think “strength”, whereas caution, hesitance, and reliance on others—also practices of those of us in thought-disciplines—signify weakness. If we lived in a society that values vulnerability, would we find the three-quarter backwards position—the classically coy positioning of fashion models and femme fatales—fundamentally weak? If our society were less hierarchal and more accepting of circuity and recursivity would we find backward movement defensive, or just moving forward in a new way? It may be an obvious point, but the common wisdom surrounding movement in stage also assumes a particular kind of performance space—a proscenium stage where the audience only views the performance from directly in front of the stage. These common principles about stage composition are complicated when a performance occurs in a thrust, stadium, round, or immersive setting, as it so often does in contemporary theatre. As we find new forms of spatial expression, so our modes of interpreting also adapt, and, I imagine, conventional compositional wisdom shifts as well. What if we could archive and track these kinds of changes in compositional style across time? Among other things, “Digital Skriker” plays with the capabilities of motion capture technology to translate human movement into another kind of stable, “readable” data. What are the implications of more empirical studies of movement? How might the technologies we use to capture and record human movement expand both creative and archival possibilities for theatre-makers and educators? What is the relationship between one’s movement data and their performance (insofar as it is intellectual property). In the final post of this introductory series, I will look at several key theatre-based projects in the digital humanities and outline what I hope “Digital Skriker” will contribute to an ongoing exploration of how we experience and interpret movement on stage."},{"id":"2019-01-10-what-is-praxis-working-on","title":"What Is Praxis Working On?","author":"brandon-walsh","date":"2019-01-10 10:44:01 -0500","categories":["Example category","Digital Humanities","Grad Student Research"],"url":"what-is-praxis-working-on","layout":"post","content":"Crossposted to my blog Well, that’s a wrap. The fall semester is done, and 2019 is getting started. Actually, when I first started drafting this post the previous spring semester had just finished wrapping as well. This post has been sitting in my drafts folder since spring of 2018, so I wanted to try to push this out before another semester flew by. At the end of the spring 2018 semester, the Praxis students presented reveal.scholarslab.org to an utterly packed house, which meant that the first full cycle of Scholars’ Lab student programs under my watch had taken place. It feels like I just got here, but, as I like to mark milestones, it felt worth reflecting on a few lessons from the last year (and then some). In addition, I’ve found myself often describing a few basic theoretical and practical tensions at the heart of our fellowship programs to anyone who is interested in what we do, so it seems worth writing down my thoughts on them in a wider, more public forum. This post, then, will aim to do both of those things by offering a bit of a glimpse into the history of one of our programs and the thinking behind it. In particular, I’ll talk about the line we try to walk between student-centered learning and project-centered learning. I’ll be talking about what Praxis works on each year and how we go about picking that project. Because, frankly, the questions about our programs that I’ve gotten most often over the course of the last year are “what are they?” and “what are they working on?” Some background and practical considerations. The Praxis Program is just one of our fellowship programs each year, but, as it serves more students than our programs, I think it’s safe to say that it takes up the most mental energy of everyone in the lab. Each year, we take six humanities students from various departments around the University (this year we have our first Architecture School student!) and offer them a soup-to-nuts introduction to digital humanities work. The fellowship asks for about ten hours of attention from the students each week in the form of a weekly meeting or two and collaborative work in a number of forms. The students are funded to participate in it by way of reducing their teaching obligations for the year - the students get time to learn and grow together as a result of the reduced course load. You can read more about the program on the project website, the DH abstract for the poster the team presented when the project first launched, or in this introductory post by Bethany Nowviskie . The program draws together several strands of pedagogical theory, a sampling of which might include Paulo Freire’s Pedagogy of the Oppressed, the UCLA Student Collaborator’s Bill of Rights, or many of the writings of bell hooks. We’ve published a pair of charters outlining the pedagogical obligations we feel towards our students, both in the Praxis program and in our programs more generally . (These charters didn’t exist when I first drafted this post in the spring of 2018. That’s one benefit of waiting seven months to finalize a post!) Check those documents out for general statements about our belief in public, process-driven pedagogy. But those statements won’t illuminate much about the day-to-day of what life is like for our students in our programs. Nor will they necessarily illuminate much about the logistics of how we carry out a fellowship year. So I thought I would share a little more about those specifics - what our students work on and how they get there. After all, the theory informs the doing, and the writing of those documents helped shape our programs this year. One thing I’ve been thinking a lot about this year is the primary vehicle by which we facilitate the learning process in the Praxis program - the project. I usually put it this way: Praxis is a year-long fellowship that has a distinctive shift in feel between the fall and spring semesters. In the fall, there is still a fair amount of the Scholars’ Lab staff in the front of the class leading seminars or giving workshops to the students, as you might expect from a class where they will be learning a lot of digital methods. But we also share a range of strategies and techniques that aim to give a high-level look at all the different components of digital work - intellectual property, project scoping and management, collaboration planning, and way more (you can check out the common curriculum here ). Most importantly, we help them think about what they’ll be doing together during the year: we help them design and scope a project that they’ll be working on in the spring, when we shift to much more of a hands-on, side-by-side, lab approach to the learning experience. While we still think a lot in the spring, our day-to-day work is mostly taken up working together to make the thing happen. We try to give the students power over the process all year long, but this is really evident in the spring when we put them in charge as much as possible. Folks interested in Praxis often ask what the students are working on because, at the end of the day, people know that we want them to build something . But how they get to it has changed significantly over time. In another post by Nowviskie, she mentions that “It’s just too much to ask that students new to digital humanities work invent a meaningful project from whole cloth on Day 1 of the program — especially one that, we hope, will make a meaningful intervention in the current scene of DH research and practice.” I think it’s worth sharing a little Praxis history here. (I should say at the outset of this discussion that, while I was a student in the second cohort of the program, I was never really privy to the pedagogical conversations that drove the fellowship. So take all of these observations from the outside with a grain of salt, except for the last year or so where I am happy to take any blame.) The compromise in the first few years was to give the students a piece of vaporware or a project already in progress and, within those restrictions, ask the students to make this kernel of an idea their own and to develop their own intellectual contribution based on it. The first Praxis cohort developed Prism, a digital version of a vaporware experiment in collaborative textual analysis and visualization developed by Nowviskie and SpecLab. The second cohort extended Prism, adding in new features based on their own interests and priorities, with the goal of taking the project from a proof of concept to a tool that was more open and usable. Accordingly, Prism has seen significant uptake, particularly in K-12 classrooms as a pedagogical tool for teaching a variety of reading and interpretive skills. In its third year, the program took a similar approach to developing a project - the staff tasked the Praxis cohort with developing a new version of Ivanhoe, another older Scholars’ Lab project that was an exercise in collaborative interpretation and literary intervention, that would run as a WordPress theme. The following year, the fourth cohort of students redesigned Ivanhoe further to allow multimedia contributions. So, in one sense, the first four years of the Praxis program were heavily directed - the core project was given, but the intellectual framing, stakes, and intervention itself were left to the students. Because of this, the students were saved the difficult task of making a from-scratch contribution to a field they were just learning for the first time. The students still maintained ownership of the project, but the initial guidance meant that the projects were able to go quite far for student projects. Each project ran on a two-year cycle, which meant that one cohort was tasked with the heavy lifting of developing a first pass at the digital project while the next would inherit the intellectual weight of conversations they hadn’t participated in. Each cohort got different glimpses of the lifecycle of a digital project. The next three years were even more student-directed. The students still were given wide latitude over the kinds of skills they took away and the nature of the project that developed at the end, but different productive constraints were used to guide the work of the group. The staff stepped back a bit more in years five and six of the program and did not explicitly give a project to the students. Rather than being given an explicit project, the students were given an idea or topic. In each case, the students were told to think about the concept of time and develop a project that would intervene in our understanding of it. Naturally, the slightly more free-form approach meant that the students had the leeway to take wildly different approaches. The fifth cohort developed ClockWork, which used sonification to grapple with the monetary measurements of time. The sixth cohort took time as encouragement to think about social media as the best representation of the now, and these investigations led them to think about the social media ecologies of the Kardashian family . In the seventh year, my first in charge of the program, we stepped back even more, leaving off the time prompt for the students. We came in with a light plan but really followed student interests where they wanted to go, giving broad freedom over the direction of the program and the project. The result was UVA Reveal: Augmenting the University, a project that used augmented reality to layer contextual information onto public spaces to challenge public narratives of contested spaces. Ironically, even though time wasn’t the students’ focus, the project still deeply engaged in questions of the past, the present, and the stories we tell ourselves about both. For the eighth, current year of the program we’ve instituted some significant changes to things that I will write more about in the future. But relevant to this discussion about freedom vs. constraint is that we returned somewhat to the roots of the program, providing the students with a fairly well defined kernel of an idea to make their own and implement in their own way. Students this year are working with the library’s Director of Information Policy to develop a project that engages with the ramifications of January 1st, 2019, when new works entered the public domain for the first time since the passage of the 1998 Sonny Bono Copyright Extension Act. I can go on about how excited I am for it, but I’d prefer for the students to frame the intervention themselves when they’re ready to do so. Keep an eye out for those insights! Therefore, in answer to the question “What is Praxis working on?” I’d say: the students have worked on many different types of things over the years. I think, however, that people ask what we’re engaged with presently not just to get a sense of the final product, but also in an attempt to better understand our process. Because no matter how public and transparent we try to be, the program is always happening for a small group of students behind doors that, if not closed, are cracked only slightly (we’re a library after all). In that case, I have a slightly more boring answer to the question that I think really gets at the heart of the program. What is Praxis working on? We’re working on it. The truth is we could have the students work on pretty much anything. The discussions would change, and the interventions into the scholarly conversations might be more or less incisive. But the built project is only part of the equation for us. And as the Scholars’ Lab charter states, the development of our people is at least as important as the projects or products themselves. The thing the students work on is less important than what that object can do in the service of the larger group. Our real aim is to build up the students as confident, conversant scholar-practitioners of digital method. Praxis can be difficult to describe because it changes every year, and that’s by design. The content of the project changes, sure, and the students necessarily change. But the staff also changes - the lab has had a lot of turnover since Praxis began. I’m now the third or fourth person to take charge of Praxis, and we all have different ways of running things and conceptualizing the program. Rather than think of the program as a search for the perfect approach to project-based pedagogy, in the last year I’ve really come to see its evolving nature as a strength. When I’m asked what is Praxis working on, I’m led to think about how we’re constantly trying to reshape our fundamental assumptions, re-evaluating the process, and making changes based on the needs of our students. The staff are always working on the fellowship program itself. Because process is everything. How we have students approach the work can really set the students up for success for failure-even more than the nature of the project itself. In the Scholars’ Lab, we’re committed to putting the students in charge of their education. But doing so can take a lot of different shapes, and Praxis has led to a lot of questions for me in the last year - questions that have been going round in my head all year and that have drifted outwards in this post. What are our limits at the lab in how design the student experience? How free can we be? How constrained? I’ve taken to jokingly describing the Praxis program as a riff on the tagline of the old MTV show Real World - “seven strangers picked to live in a house.” Except, in our case, we have six strangers picked to work on a project together. Are we setting them up for drama worthy of reality TV or for transformative learning experiences? What is Praxis working on right now? We’re working on being better. More on that soon!"},{"id":"2019-01-15-gis-workshops-spring-2019","title":"Announcing our GIS Workshop Series for Spring 2019","author":"amanda-visconti","date":"2019-01-14 20:01:01 -0500","categories":["Digital Humanities","GIS"],"url":"gis-workshops-spring-2019","layout":"post","content":"All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be taught on Wednesdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up! January 23rd Introduction to ArcGIS Pro Here’s your chance to get started with geographic information systems software in a friendly, jargon-free environment.  This workshop introduces the skills you need to make your own maps.  Along the way, you’ll get a taste of the latest software suite from Earth’s most popular GIS company and a gentle introduction to cartography. You’ll leave with your own cartographic masterpieces and tips for learning more in your pursuit of mappiness. January 30th Georeferencing a Map: Putting Old maps and Aerial Photos on Your Map (ArcGIS Pro) Would you like to see historic map overlaid on modern aerial photography?  Do you need to extract features of a map for use in GIS?  Georeferencing is the first step.  We will show you how to take a scan of a paper map and align in it in ArcGIS. February 6th Getting Street Address on a Map (ArcGIS Pro) Do you have a list of street addresses crying out to be mapped?  Have a list of zip codes or census tracts you wish to associate with other data?  We’ll start with addresses and other things spatial and end with points on a map, ready for visualization and analysis. February 13th Creating and Editing Spatial Data (ArcGIS Pro) Until we perfect that magic “extract all those lines from this paper map” button we’re stuck using editor tools to get that job done.  If you’re lucky, someone else has done the work to create your points, lines, and polygons but maybe they need your magic touch to make them better.  This session shows you how to create and modify vector features in ArcGIS Pro.  We’ll explore tools to create new points, lines, and polygons and to edit existing datasets.  February 20th Introduction to ArcGIS Online With ArcGIS Online, you can use and create maps and scenes, access ready-to-use maps, layers and analytics, publish data as web layers, collaborate and share, access maps from any device, make maps with your Microsoft Excel data, and customize the ArcGIS Online website. February 27th Using ArcGIS Online Story Maps Story Maps are templates that allow authors to give context to their ArcGIS Online maps.  Whether telling a story, giving a tour or comparing historic maps, Esri Story Maps are easy-to-use applications that create polished presentations. March 6th Using ArcGIS Online Data Collection Tools Whether you are crowd sourcing spatial data or performing survey work, having applications that automatically record location and upload data directly to a mapping application is incredibly useful. March 20th Using ArcGIS Online Web AppBuilder Would you like to make a custom online mapping application without have to code?  Us too.  ArcGIS Web AppBuilder allows developers to use drag and drop tools to create responsive mapping applications."},{"id":"2019-01-15-site-relaunch","title":"Website relaunch!","author":"amanda-visconti","date":"2019-01-15","categories":["Digital Humanities"],"url":"site-relaunch","layout":"post","content":"We’re happy to announce that our Lab website has a new look and feel, all aimed at making Scholars’ Lab more accessible and the site more useful. The website ScholarsLab.org represents the collective effort of our staff and collaborators, past and present. It’s existed in multiple forms: most recently, as a WordPress site and now, as of this blog post, as a Jekyll-generated static website. Why renew the website? Our previous website was headed by the homepage text “collaborate, iterate, discuss”: We’re a community lab focused on experimental scholarship. We care most about the impact of scholarship on people, ranking positive human experience as the most important outcome of projects or other work. This means that failure and a journey of many iterations are not just frequent by-products of our work, but essential characteristics. For example, we periodically renew our lab charter to reflect our ongoing growth as scholars and professionals; as it was last updated in 2016, we’ll be renewing its text (and even, perhaps, its form) over the coming year. Before renewing our charter, though, we worked on synthesizing our many views of what the lab currently is into a coherent, concise mission–visible at the top of our new homepage. Having the website look “newer” wasn’t a priority for us; rather, we used this re-articulated mission to reimagine a site more aligned with our values… Why does the site look this way? We started thinking about a site renewal back in Summer 2017, with two rough goals: a general rethinking of the site to fit our current work, and a migration of platforms from WordPress to a Jekyll-generated static website. (You can see this article for more on what a jekyll-generated static site is, why you might want to use one, and how.) WordPress is a popular blogging platform with the advantage of a user-friendly administrative dashboard. Unfortunately, the benefits of WYSIWYG (what you see is what you get) blog post editors and easy media uploads come with significant costs of web developer time updating the site’s core, plugin, and theme code and keeping the site secure. Moving to a Jekyll-generated static site helped us think through applying minimal computing values to our main online platform. This move means making small content changes to the website (e.g. writing a new blog post) a little harder by requiring some steps we didn’t require before, but… we think these steps: 1. are within the grasp of anyone (regardless of technical experience), 2. ultimately enfranchise everyone to understand more and do more with our site than WordPress allowed, and 3. are in line with our teaching each of our Praxis cohorts the basic steps required to use git, GitHub, and markdown (i.e. by teaching these skills to our team and affiliates more broadly, we hope to also enfranchise folks to create their own websites with the same technologies). After much discussion and work, these initial goals yielded a more important goal: reimagining our website to more clearly focus on making the lab accessible to folks who wouldn’t otherwise already know about or feel welcome to join us. We already have some friendly and well-trodden paths to reaching the lab, such as through our work and fellowship CFPs, consultations, events, and open office hours; so we decided to aim the website at reaching folks who weren’t already coming into our space. Some ways we approached this goal:\n1. Encouraging site visitors to feel they’re in the right place, by updating textual content and photos to be more explicitly welcoming. For example, we reorganized our Makerspace and Spatial Tech equipment and software inventories into named categories; you might not know what a serger is, but where you’d probably just skim past the word before, you can now see that it’s part of a set of equipment and supplies aimed at making things with fabric. The hope is that by grouping resources like the serger into clearer categories, you might realize that our Makerspace has some resources and folks you can talk to about, for example, your interests in making stuff from textiles, historical costuming, and/or other fabric art approaches. 2. Making the info folks need to get more involved easy to find, once they feel comfortable with our resources being theirs. We wrote a FAQ anticipating questions folks often ask us, overhauled the structure of information on our site (e.g. menu order and wording, page titles, layout and order of information on a given page). We also created clear maps showing how to get to the specific locations within our somewhat large space (the west wing of the 4th floor of Alderman Library).\n4. Accurately capturing the work of all our current staff, including recent hires and folks welcomed into the lab during the library reorganization (who hadn’t had their work represented on the old site), to show the true breadth of disciplines and approaches we cover . We also reworked the metadata on each of our current and past Scholars’ Lab folks, so that we could more easily and accurately show what roles folks have held and who has contributed anything to a given project—again, more accurately showing how the Scholars’ Lab is itself a digital scholarship project, one built together by many people over the years.\n5. More clearly showing that we’re part of the UVA Library, so folks understand that we’re part of the Library’s mission to provide equitable access to knowledge and learning to everyone. This work includes an FAQ question, a section of the homepage with text and the UVA Library logo, and a new SLab “Library” page highlighting the work of Library colleagues that helps us do what we do, and that may feature stories about intralibrary collaborations in the future. Who made this? The 2017-2018 renewal of this website incorporates text, suggestions, and feedback from everyone on the Scholars’ Lab staff . Additionally, the following folks put in significant extra effort on the site’s design, coding, content, project management, and documentation: Katherine Donnally, Jeremy Boggs, Brandon Walsh, Ronda Grizzle, Laura Miller, Zoe LeBlanc, Ammon Shepherd, Shane Lin, Beth Mitchell, and Amanda Visconti. We want to especially highlight the leadership and labor of Katherine Donnally (Scholars’ Lab DH Designer 2017-2018) in realizing this site. Katherine’s thinking and work gave the renewal project a coherent focus around making the lab more accessible and supportive of folks who might not otherwise feel part of our community; in addition to design and development work, she created structures and documentation (e.g. goal and progress tracker, code commenting, analysis of the old website, articulation of the purpose of the new site) that were instrumental in both imagining the new site and seeing it to the finish line."},{"id":"2019-01-19-Spatial-VR-Aviation-Resources","title":"How to get started with VR flight simulation","author":"arin-bennett","date":"2019-01-18 20:00:00 -0500","categories":["Spatial"],"url":"Spatial-VR-Aviation-Resources","layout":"post","content":"If you’re looking to get into VR flight simulation, here are some starter tips on equipment and software to look into. VR headsets : \n- As for general availability Oculus and HTC Vive are still leading the pack - in my opinion Oculus is a bit more comfortable and HTC tends to have a bit better tracking. When seated in a cockpit however one tends to move around less so tracking is arguably less of an issue with this genre of software. These change every year and get slightly better with each iteration. HTC’s customer service is a nightmare so if your equipment will be getting lots of usage and/or breaking often, keep that in mind. Hardware - You’ll need a decent PC to run any of these programs. Flight simulators in particular are particularly resource-heavy. Make sure your CPU is at least an i7 or i9 (or equivalent), that you have as much RAM as you can afford and that you spend some money on a decent video card (the nVidia 2080ti model is the current leader in consumer video card performance.)\n- While you can use your headset’s VR motion controls in some of the apps, you’ll likely want a nice HOTAS setup (joystick, throttle, rudder pedals). Thrustmaster makes a few nice bits of hardware: http://www.thrustmaster.com/products/hotas-warthog\n- Putting it all together: at some point you’ll want to move from strapping all this gear on a desk to a more deliberate setup. I’ve got this VolairSim frame at home and like its flexibility.  http://www.volairsim.com/ (There are many similar products, this just happens to be the one that I chose.)\n- If you really want to get crazy you can look into motion platforms. They are identified by their “DOF” or degree-of-freedom. http://xsimulator.net is a community of folks who all either build their own or buy and maintain various motion platforms. It’s a good place to do preliminary research. This is one of the cheaper but more well-reviewed models: https://dofreality.com/\n- Tactile feedback / bass shakers - These devices will take the bass frequencies and resonate or shake the entire frame accordingly. This really helps the sense of immersion. https://thebuttkicker.com/ is the most popular model. You can also build your own with off-the-shelf resonators and an audio amplifier. VR compatible flight sim software: - X-Plane: https://store.steampowered.com/app/269950/XPlane_11/ This one is neat because you can (with a little manual tinkering) inject real satellite terrain so that the ground under you looks real. \n- FlyInside Flight Sim: This is a fork of the popular Microsoft Flight simulator product redesigned for VR. It’s probably the most targeted to VR environments and supports finger tracking with an optional Leap Motion device. This lets you actually reach out and flip the switches and turn the knobs in the cockpit. \n- Aerofly FS2: https://store.steampowered.com/app/434030/Aerofly_FS_2_Flight_Simulator/ - A nicely polished flight sim. Not the most feature packed but worth a visit. It’s my go-to when I want to quickly hop into a flight sim and don’t want to tinker as much.\n- DCS World https://store.steampowered.com/app/223750/DCS_World_Steam_Edition/ This sim focuses on military aircraft, targeting systems etc. VR is supported.\n- RC Simulation training: Realflight 8 now supports VR headsets. This is great for learning how to pilot a RC plane or drone. You can buy their RC controller as well so that the controls feel accurate to the user. https://www.realflight.com/products/rf8/index.php?moreinfo=features"},{"id":"2019-01-22-starting-out-on-the-right-foot-part-three","title":"Starting Out On The Right Foot (Part Three)","author":"kelli-shermeyer","date":"2019-01-22 04:47:16 -0500","categories":["Grad Student Research","Digital Humanities"],"url":"starting-out-on-the-right-foot-part-three","layout":"post","content":"This post is the final in a series that contextualizes my current project, Digital Skriker, within a larger tradition of studying and archiving stage movement. My project explores both the theoretical cruxes and archival possibilities enabled by robust and increasingly accessible motion capture and virtual reality technologies using Caryl Churchill’s play, The Skriker (1994) as a case study. I’m interested not only in how these technologies might change the way we think about documenting stage movement and gesture, but also how they may be used to create modes of (posthuman?) performance. It’s hard to overstate the coolness of choreographer Bill Forsythe’s Synchronous Objects, a collaborative project with the Advanced Computer Center for the Arts and Design at Ohio State University. The project creates a variety of visualizations—scores, maps, graphics, animations—that reproduce in various ways Forsythe’s dynamic piece One Flat Thing. Forsythe sees choreography as “an organization practice that employs fundamental creative strategies relevant to many other domains” and researchers believe that the visual objects they create enable users to quickly grasp the ideas in the choreography and encourage new interpretations of the dance. In effect, finding new visualizations for visual movement enable new forms of meaning. This project was highly influential to my earliest thinking on what has now become Digital Skriker. I was intrigued by how Forsythe and the research team chose to transform the movement into a readable score and indicate the ways that certain dancers’ movement cues certain others—processes and language familiar to both music and stagecraft. What if every Forsythe piece had an accompanying score? Would access to those visualizations change the ways we characterized his choreographic style? Would he become known for certain movement chords? Does the data we gain from these visualizations give us any more purchase on Forsythe’s style than our expert observation? (I’m not so sure…) In my observation, digital projects that engage with movement can be said to loosely engage one or more of the following aims: documenting, interpreting, and creating. As one might image, there’s significant overlap between these three categories (as documenting often involves some sort of interpretation and creating also relies on knowledge of past practices, etc.). Motion Bank creates an archive of dancers’ movements to serve as choreographic resources for other creators. As in Synchronous Objects, movement data is processed into beautiful digital renderings. Other notable projects that I’ve run into include the 2009-2010 Inside Movement Knowledge which aimed to develop an installation, documentation method, and prototype course book to enhance dance studies in higher education and The Digital Intermedia Collaborative Platform at Berkeley, which allows for collaborative investigation of movement and bodily expression. But of course, records of movement can exist elsewhere besides the digital space. I’ve greatly benefited from the experience of my colleague Barry Houlihan, archivist and drama specialist at NUI Galway, who I have spoken with about how one might guide researchers using collections like the Abbey Theatre Archives to recreate performances with prompt scripts when video records are absent. Archival materials like prompt books and stage manager notes are vital resources for those of us interested in the blocking of significant historical performances. (The Dance Heritage Coalition also offers this guide that seems relevant for those interested in theatre archives, too.) Other technologies, like the Simulated Environment for Theatre (SET) project, enable users to either translate the movement data they gather from analog sources or create their own blocking into visualizations. This kind of work is immensely helpful, particularly, I suspect, for drama students who may not always be used to thinking about who’s on stage at a given moment—a line dramatically changes in delivery, for example, if it’s said with no one on stage, or in the midst of a crowd. Projects like SET blur the line between documentation and creation, offering possibilities for both, much like the motion capture technology that I’m also engaging in Digital Skriker. Motion capture data, in its rawest form, is a pretty stable form of information about movement. Though it can be mapped onto avatars for a variety of visualizations, the core of the data is just coordinates tracing the change in position over time of various limbs. If I printed out the mocap data, I could lose access to all of my fancy visualization products and still be able to draw some kind of conclusion about the movement represented—how fast or slow are movement changes occurring? How fluid or angular are the transformations? It’s a solid form of documentation. However, because mocap plays well with a variety of animators, theatre practitioners have also started using it to experiment with live performance. Recently the Royal Shakespeare Company partnered with Intel and Imaginarium (a production company co-founded by Andy Serkis) to stage a production of The Tempest featuring a partially digitally-rendered Ariel played by Mark Quartley, who embodied all of the character’s turns towards magic with the help of what one reviewer called “digital wizardry” . The production marked an innovative research collaboration between industry and artists at a scale and requiring an amount of resources inaccessible to most theatre companies. However, the RSC’s commitment to preserving what Julie Taymor has called the “double event”—the co-presence of actor and avatar—strikes me as a model for using technology in performance that preserves the status of the human actor and live human interaction which is central to our conceptions of the theatrical experience. I love this model for incorporating technology into performance, though my work in Digital Skriker imagines another possibility by experimenting with pre-recorded motion as an element of live performance. Like SET or Motion Bank, my work plays with both documentation and creation, though, I’m increasingly interested in the latter. The avatar I have rigged with my actor’s movement is a layered documentation of both my directing and the mover’s creativity. The final avatar is a combination of that creative collaboration, my (limited) technical prowess, and the computing power of Unity. So yes, in some ways motion capture does enable a certain kind of record to be kept about a mover’s performance—but even that record does not exist in a vacuum. The resulting animation is from its’ earliest steps, co-created between both the humans involved and technological actors—leading us perhaps on a form of posthuman performance. More on that later."},{"id":"2019-01-31-archiving-dh","title":"Archiving DH Part 1 - The Problem","author":"ammon-shepherd","date":"2019-01-31 03:29:13 -0500","categories":["Digital Humanities"],"url":"archiving-dh-part-one","layout":"post","content":"The Problem I have been working in the field of Digital Humanities for about 13 years now.\nMost of my time has been on the technical side: setting up the infrastructure\nto support DH projects, building the DH projects, maintaining the DH projects.\nActually, most of my time has been with that last one: maintaining DH projects. There is a huge problem in the DH world and it has to do with lack of\nforesight, poor institutional support, and the ever changing face of\ntechnology. To see how this problem emerged and what we can do about it, I\ncreated this really cool website, interactive map, database, 3D model, VR\nenvironment, and leveraged AI to text mine a million scholarly articles about\nDH. That paragraph above is the epitome of what’s wrong with DH projects. A grand\nidea, a lot of work to create something; and then stop. That’s all the thought\nthat goes into it. To be fair, over the past 13 years I have seen the DH field\nbecome more and more aware of the technical challenges involved in bringing\nforth the great new project for the benefit of the field and general humanity.\nMore attribution is going to the developers who make “the thing.” But that is\nstill not quite enough. It is still just a 2-5 year plan. What about 100 years\nfrom now? This is one reason why books are never going away; because books last a\nreally, really long time. The book, as a vessel for the continuation of\ninformation, has a really long shelf life (pun intended). Anything digital\ndoes not. And there-in lies the crux of the problem. For all the good intentions and the\ngreat resources created by DH projects, they won’t last. Let’s just get it out\nthere right now. That wonderful new project you just spent years and thousands\non, will be out of date, obsolete, and unavailable in 2, 3, or 10 years.\nWhereas that book you published will be in the hands of learners for the next\n100 years. At least, that’s the current state of DH projects. Why? Mainly because the\ninfrastructure for supporting DH projects in the long term is not in place (or\neven invented). Past and current DH projects are built on a shaky foundation of\never changing technology. Unless this issue is addressed head on, all DH\nprojects are doomed to oblivion. In the coming months, I will be exploring more about the problems and possible\nsolutions facing the longevity and usefulness of DH projects. What can we do at\nthe outset to make projects outlast the technology? Can we rely on current\ntechnology to exist in 10, 30, 50 years? A difficult question, since modern\ncomputers have only been around for about 55 years. What solutions are out\nthere already for making DH projects “archive ready” from the get go? Why So, one may ask, is all of this work to maintain a DH project really worth the\neffort? I guess this all depends on your interpretation of DH projects as\nscholarship. If it is scholarship, then it should be maintained and kept\naccessible just as much as the dominant information vessel, the book. We\ntake for granted the infrastructure and resources that are involved with\ncreating a book. The global book publishing industry is valued by one estimate\nat $143 Billion\nUSD .\nThat’s a lot of infrastructure behind the book. Libraries are the maintainers and repositories of scholarship that has reached\nbook form, then why not too for DH scholarship? My argument in the end, will be\nthat libraries will need to step in as the infrastructure to support long term\nmaintenance and “storage” of DH projects."},{"id":"2019-01-31-automating-webpage-tweet-screencaptures","title":"Automating Webpage & Tweet Screencaptures","author":"amanda-visconti","date":"2019-01-30 20:01:01 -0500","categories":["Tutorial","Digital Humanities"],"url":"automating-webpage-tweet-screencaptures","layout":"post","content":"Do you have a list of webpage URLs for which you’d like screencaps (aka screenshots)? ( Know this tutorial fits your needs? Jump to the instructions . ) Maybe you study social media: you can capture all the data about a tweet using Documenting the Now’s tools and get a spreadsheet listing all of a hashtag’s tweets with text, file names for any image attachments, publication date, RT/like counts… But maybe you’d also like to have screencaps of the webpage per each tweet as they appeared at a given time: \n- keep an offline folder of all your faved tweets? (before time passes and some of them are deleted) - record how tweets looked on Twitter.com at a given point in time, e.g. the avatars of folks who interacted with the tweet (using the avatar images current—at least as of the time of screencap—since people change these over time), or what metadata was viewable to the casual observer at a given time, as with this example tweet screencap: Having screencaps of tweets can be useful for Twitter receipts, conducting a user study where you want to gauge folk’s reactions to seeing a tweet as it looked on Twitter.com (without needing to be online), or keep a folder of motivating tweets to refer to when you need encouragement. This post will help you also automate taking screencaps from a list of URLs, by pointing you two tutorials plus some advice on combining these. This tutorial is written from a Mac user’s perspective. My uses cases I recently had two needs for a workflow automating screencaps: 1. Screencapping faved tweets (aka motivation) : I used to “like” tweets where folks said useful things about my work, for later encouragement when I needed it. I wanted to switch to just screencapping these tweets as I saw them, saving the “like” function to indicate interest in others’ tweets, but that meant I need a way to get all those previous likes as screencaps. This workflow gave me a folder of over 2k screencapped tweets! (Now I just need to make a bot to show me a random one each time I log in…) Figuring out how many of our posts were amplified by a DH aggregator : For an external review of DH at UVA, we’re drafting a report on Scholars’ Lab as a DH unit. As a small part of this, I wanted to include the number of times SLab staff or students have had a blog post highlighted by Digital Humanities Now . Searching the DH Now with a variety of search terms (e.g. Praxis, Neatline, variants of “Scholars’ Lab”, names of former staff) turned up a bunch of posts, but without opening each result in a browser to skim that post, I couldn’t be sure that these were all actually either SLab-authored or citing SLab work (vs. e.g. happening to use the word “praxis”). By automating screencaps of all the links my search turned up, instead of opening each link in a browser, I could use Preview to bulk-open screencaps and glance over the top of each webpage, seeing more quickly whether a post was related to SLab or not. Credits This is an extremely light repackaging of two sets of advice, for which I am grateful: Ryan Bauman’s ( twitter.com/ryanfb ) tweets on how to get a list of URLs for your faved tweets and Joshua Johnson’s 2011 post “AppleScript: Automatically Create Screenshots from a List of Websites” . You may also wish to explore Documenting the Now ’s tools for ethical preservation and research with social media, including the Tweet Catalogue to locate datasets of tweets using a given hashtag, and the Hydrator tool for turning those datasets back into a list of tweets and their metadata. How to automate screencapping 1. You’ll need a text document (plain text / .txt) with each URL you want to be screencapped listed on a separate line: If you want to create a list of just your faved tweets, see this tweet by Ryan Bauman tweet to get just the liked tweets from your Twitter &gt; Settings &gt; “Your Twitter Data” download. If you have a bunch of bookmarked URLs you want to screenshot (possibly in one bookmarks subfolder?), you can export your whole set of bookmarks from the browser, then edit the resulting HTML file (using search+replace or regular expressions; sorry, that’s a whole separate lengthy tutorial, but Doug Knox’s tutorial here ’s a good place to get started understanding regular expressions) to strip it to just a list of the URLs you want to screencap. 2. Next, we’ll use Joshua Johnson’s 2011 post “AppleScript: Automatically Create Screenshots from a List of Websites” to actually take the screencaps. We’re going to use the Mac “Script Editor” tool to tell our computer to take that list of URLs, open each in Safari, take a screencap of that screen, and save it. Open the Script Editor app (Applications &gt; Utilities &gt; Script Editor). 3. Grab the text from this page, which puts Joshua Johnson’s script in an easy-to-find place (to understand what the script does, read his post where he walks you through adding and adjusting the different pieces). Paste the text into the top text field: If you have a slow internet connection or want to screencap pages on websites that load slowly, you may wish to find and adjust the piece of the script that says “delay 5”, to a larger number than 5 (this controls the time the script waits between directing safari to visit a given URL, and taking a screencap of whatever is then currently on the Safari screen). 4. Open the Applications &gt; Safari browser, and drag its screen to be as large as possible. Depending on what level of zoomed-in/zoomed-out you usually use the Safari browser at, you may wish to adjust for the purpose of taking the screencaps; the screencaps will only capture what’s showing in the window you’ve opened (not stuff you’d need to scroll down to see). 5. Open the Applications &gt; TextEdit app (not a different text editor, unless you want to figure out how to edit the script’s tell application \"TextEdit\" how to point to a different application). Paste in your list of URLs. If you have other TextEdit windows/tabs, make sure that the one with the URL list is the topmost/frontmost. 6. You’re ready to take screencaps! You’ll want to do this when you’re okay not having access to your computer for a while, because the screenshots will be of whatever is at the front of your screen. If you step away from your computer, the script knows to leave a Safari browser window open at the front of your screen, take a screencap, then move to the next webpage in your list and screencap that. If you hide the Safari window or have other stuff open over it, you won’t get the screencaps you want. (I’m sure there’s a way to make this not the case, but I didn’t need to figure that out for my use cases.) Back on the Script Editor, press the “red dot” record icon to begin automatically taking a screencap of each of the URLs in your list: When the script finishes running, you’ll have screencaps of all the URLs!"},{"id":"2019-02-01-welcome-beth-mitchell-community-advocate","title":"Welcome to our new Community Advocate, Beth Mitchell!","author":"amanda-visconti","date":"2019-01-31 20:01:01 -0500","categories":["Announcement","Digital Humanities"],"url":"welcome-beth-mitchell-community-advocate","layout":"post","content":"We’re pleased to welcome our new Community Advocate, Beth Mitchell, after a competitive national search . Beth brings with her a terrific skillset both technical and personal, with expertise around evaluating and offering improvements toward DH accessibility and usability. She has a strong technical background in JavaScript (particularly in data visualization) and Omeka/Neatline, and also has experience teaching both of these to undergraduate students. As Community Advocate, Beth advocates for the users of our DH projects, working on documentation, design, development, and project management approaches to champion user needs. She is a Ph.D. Candidate in Architectural History, and previously worked as Scholars’ Lab Community Project Manager and as a research assistant on the NEH-funded Neatline project. Beth’s work will include advocating and building for users of our NEH-funded Neatline project, LYRASIS-funded work developing tools for campuses to respond with digital collecting to community crises (PI Kara McClurken), and work with the Documenting the Now project (ethical design for social media research and preservation) co-PI—UVA Media Studies Professor Meredith Clark—bringing DocNow tools and ethical social media research to the classroom and community. Beth’s activities will include: - creating technical documentation for both developers and non-technical users - leading workshops and teaching modules on our projects - refining existing documentation through user testing and community feedback - responding to requests for technical support related to our grant projects\n- other support for our staff doing design and development work, such as organizing and commenting on existing code, designing and coding proofs of concept, and participating in code and design reviews Welcome, Beth!"},{"id":"2019-02-14-theatricality-of-motion-capture","title":"Some Reflections on the Theatricality of Motion Capture","author":"kelli-shermeyer","date":"2019-02-13 20:00:00 -0500","categories":["Digital Humanities","Research and Development","Grad Student Research"],"url":"theatricality-of-motion-capture","layout":"post","content":"How is working with motion capture like staging a play? As I’ve been working on my fellowship project in the Scholars’ Lab, I’ve also been directing a play for our local community theatre. Engaging in these projects simultaneously has led me to reflect on the ways that collecting motion data has much in common with the process of making theatre. Not only are both Digital Skriker and the play I’m directing collaborative ventures, but they contend with similar sets of limitations that must be accepted or overcome, responded to with both creativity and candor. It strikes me as useful to articulate some of the overlaps in these processes, imagining the way that working in the digital humanities can support artistic work, as well as scholarship. Conceptual Work After we scheduled our first official movement recording session, I acted as a dramaturg, providing my mover (the person’s whose movement we were capturing) with concepts, context, and images that might give her a sense of the characters that her movements would ultimately help illustrate. What kind of choreography might we use to capture the movement of fantastic characters such as a bogle or the Green Lady? To this end I assembled a sheet of images, ideas, and context, like I would have had I been serving as a dramaturg for a play. Though I was not going to strictly choreograph this movement, I thought it would be a wasted opportunity not to use the project as a chance to think about how the relationship between humans and nonhumans might be expressed through movement (and posthuman theory is currently one of my primary intellectual and artistic interests).  As I knew I wanted to explore how my mover would interpret nonhuman movement, I intentionally left space for interpretation and variation. Casting There are no qualifications to be a mover—everyone gestures and moves as part of their everyday embodied experience (and in fact our different bodies and capacities condition the kinds of shapes we can make, adding depth and diversity a wide human archive of movement). In theory, anyone could serve as a mover for the purpose of motion capture. But as I was ideally going to engage in a creative, reciprocal process and offer adjustment and direction, I decided it would be helpful if I shared some movement vocabulary with my mover. I asked one of the friends with whom I practice yoga to allow us to capture her motion data; we have a shared language for bodily awareness, breath control, and embodied shapes. Choosing another mover—a football player, another graduate student, a classically trained dancer, a person on crutches or in a wheelchair—would have dramatically changed the range of motion we could capture, just as casting an actor in a play puts creative limits on the character that will emerge in performance. Direction I realized I approached our motion capture session the same way that I approach directing: I see my role as the editor of another artist’s choices. I told our mover about the characters and let her play around and then gave her feedback on how she might move or which movements she might repeat on subsequent takes. This is how I prefer to work because I think it shows the most respect for the creativity of one’s collaborators. When I served as my own mover, I’m sure my dancing would have benefited from the feedback of another director. This aspect of the process drove home the point that even capturing motion data to import into another artwork is a fundamental part of the artistic process itself. Costume Typically, costumes are constructed for the actor after they are cast in a role, but it’s worth meditating on the ways that the “costume” we wear for motion capture conditions the kinds of data we’re able to capture. We started by experimenting with UVA’s Animazoo suit, which I imagine could be the uniform for a futuristic motorcycle gang. It’s also skintight. While generously sized and featuring Velcro adjusters, it certainly would not be able to fit every mover. I find this to be a significant drawback to that technology’s ability to contribute to my creative process as it puts a strict limit on size and shape of bodies that I can use in my AR-enabled performances. The VR sensors we’ve been using recently are much more accessible in that way; however, without a range of sizes in Velcro bands to attach the sensors, I’m still limited to a narrower range of bodies than I’d like. A technology that does not truly open up representational possibilities in performance is not worth the effort, in my view. Costumes also dramatically condition the kind of movement we can capture by restricting the body in certain ways. For example, if I’m using the VR sensors which require a headset to be plugged in, my ability to complete quick turns and jumps become a safety hazard. This too, is not unlike some theatre productions—even the most meticulously and generously constructed period costumes still affect actors’ abilities to move around in all the ways they are used to. This limitation can be turned into an opportunity for creativity and depth, by enabling actors to experience the bodily constraints their characters might have faced in their historical periods. Though not a period piece, the show I’m directing now requires a lot of athletic movement from the characters and we’ve chosen the sizing and inseams of the costumes based on what will accommodate free and comfortable movement for all of our actors. That’s all to say that I think the affect of “costuming” on movement is one of the strongest points of convergence between capturing movement data and staging a play. Set and Audience We’ve experimented with two “sets” for our motion capture, both taking up aboutthe same square footage on fluffy rugs in the RMC in Clemons library. The soft texture of the floor encourages us to jump by suggesting a soft landing. The high visibility of this site also affects the kind of movement we might be comfortable doing. My first mover felt more comfortable dancing with the VR headset on, which enabled us to locate her in a virtual space so she could feel as though she was moving around in an environment that would be appropriate for the characters she was portraying, while also ensuring she would be completely unaware of any students glancing at what we were doing.  For her motion capture session, we also covered our station with dividers to provide more privacy. Audience, or perceived audience, conditions our movement. I’m not a shy mover, so when I was in the suit, I happily jumped, twirled, acted like Golem, and performed vinyasas. I did think, however, that perhaps it would not be a good idea to move in a way that could be interpreted as “inappropriate” for an academic space, even if that would have been the appropriate choice for the characters whose motion I was capturing. Sound Perhaps this is a no-brainer, but music affects movement—this was true in our motion capture sessions as it is on stage. I prepared a playlist for our first motion capture session that had songs of various genres and tempos in the hopes that it would support the mover in providing a variety of sequences. When I was in the suit, I moved in silence, which was admittedly kind of weird. As in theatre productions, sound can support the actors and reinforce the tone of the action. I’m interested in how adding sound in “post” will affect our perception of the movement sequence’s we’ve recorded—I suspect they will read differently, depending on the kind of sound we use to accompany them. Trust Capturing motion happens at close quarters and requires trust among all parties present. As the work is so focused on the body and on the body’s interaction with technology, movers often need the assistance of others in putting on and adjusting the equipment and attention is paid to their bodies and its movements. It’s worth acknowledging that this is a vulnerable part of the process that requires trust and explicit consent from all collaborators, just as it does when one is working with sensitive material in a play script. The foundation of fruitful artistic collaborations is trust, in all disciplines. Process I still consider myself a Unity newbie, and so when I’ve been constructing scenes from the motion capture, I’m struck by just how limited I am, at least aesthetically. As I’m relying on standard free assets, I’m not yet able to precisely realize an artistic vision to imagine The Skriker in a way that I think is “finished” enough to warrant an audience. But maybe that’s okay, at least for now. Local, nonprofit theaters serve vital roles in forging community and allowing amateur creators an artistic home; their budgets and spaces often place limitations on the kinds of designs they can execute. But this does not mean that messy work does not serve a purpose or contribute to the life of the community. From my perspective, DH work and theatre-making share a core value: the knowledge created in the process is valuable and meaningful."},{"id":"2019-02-21-dh-dissertation-to-director","title":"DH Dissertation to Director: Notes connecting my student and staff experiences","author":"amanda-visconti","date":"2019-02-21","categories":["Digital Humanities"],"url":"dh-dissertation-to-director","layout":"post","content":"A colleague recently asked me about how I draw on my DH dissertation experience ( dr.amandavisconti.com ) in my work co-directing the Scholars’ Lab. Below, I share the rough notes I typed up in response about the first couple such dissertation-director connections that came to mind: Digital humanities &lt; experimental humanities Getting to do an unusually shaped dissertation meant doing a lot of meta-dissertational work analyzing and synthesizing precedents for making as scholarship, dissertations that didn’t produce written chapters, and experimental scholarly methods and formats more generally. I not only needed to prove that such work could reach the goals of a dissertation, but that my research design choices also fit tmy particular research questions. It was profoundly useful to work through what I thought was the best research design for my questions, and then also find the language to convince others of this as well. I do not want to add more barriers to the dissertation process for anyone, but it does seem mildly unfortunate to me that most humanities dissertation creators aren’t encouraged and supported to explore multiple research aproaches and argue for why their project format is the best fit for their research questions, rather than treating written chapters as the only choice. The Scholars’ Lab has historical prominence as a DH center, but we also offer strong expertise in other areas, such as GIS/spatial reasoning and cultural heritage informatics. Some of my work as a Lab director has been collaborating with my colleagues to update our understanding of what’s most important to us, articulate that in a concise and memorable way, and relaunching our Lab website (ScholarsLab.org) to better communicate that updated mission. I think my dissertation experience helped me do a better job of hearing and synthesizing our staff’s visions for the Lab: rather than forcing everything to fit under the rubric of DH, we’ve identified a shared lab commitment to questioning assumptions around the methods and publication formats of scholarship, to cultivating a readiness to experiment and create new approaches where existing ones don’t best fit our research questions. It’s the experimental mindset of DH that drives us and connects our team members’ many skillsets—as you can see in partially or wholly analog work like our Makerspace and campus community design work—rather than something particular to the Digital. Passing it on I have been extremely lucky, from the MITH and UMD English Department smart and caring mentorship that allowed me to realize my unusual dissertation, to getting to work at the Scholars’ Lab, where the amazing staff have long, successfully made it their mission to model and nurture a generous and human-centered digital humanities. My dissertation experience was an extremely lucky one: I had incredible mentors as my dissertation committee (Matt Kirschenbaum as advisor; Kari Kraus, Neil Fraistat, Melanie Kill, Brian Richardson). They met with me throughout the dissertation process to give me feedback and help us make sure that, for a very non-traditionally shaped dissertation, we were all on the same page as to expectations for the final outcome. They all had this unusual combination of innovative scholarship mixed with humanity and generosity. I’ve heard people say that either your advisor is nice to you but doesn’t push your work forward, or they’re mean (how is this not just a thing, but a fairly common one?) but make your work better. Every interaction I had with my committee was a kind one, and yet they constantly challenged me to do better, with their full faith, and both the support and space necessary to figuring out what scholarship is to me. Having mentors modeling this approach to scholarship made me determined to pass on the impact of their gift to other folks I encounter, as best I can. To work so that the luck and privilege that allowed me to do my dissertation might become standard resources for anyone. To put my heart into Scholars’ Lab’s goal of training the next generation of scholars to be the scholarly community we want to exist, and to take responsibility for my tiny snowflake in the storm of what DH is (e.g. think through the impact of my doing my dissertation around a canon work by a white, male, Anglo, cis author; and change my scholarly trajectory to respond to the problems that investigation raised for me—please note that I no longer agree with that blog post, but haven’t yet written up how my thinking has evolved since then…). Scholarship is people I read Mark Sample’s excellent “When does service become scholarship?” while dissertating, and I’ve paraphrased his argument into my preferred definition of “scholarship”: one part critical thinking + one part communicating the results of that thinking Critical thinking can be accomplished through many methods, including research, writing, coding, building, designing, experimenting, and play. Communicating critical thinking can take the form of traditional publishing or other forms of “making public” such as tweeting and blogging, teaching, sharing open source code, or any other way for your intellectual community to learn from, critique, evaluate, and/or build on your work. Decompressing this definition: \n1. Anyone trying to think well and share their work is part of the scholarly conversation 2. Making research just half of scholarship rather than synonymous brings the scholarly community into focus as the goal of our research work—with the implicit belief that the work of care, respect, mentoring, amplifying of your community members is not just a “nice” characteristic taking second place to the completely broken concepts of “rigor” or “brilliance”, not just “eulogy worthy”, but definitionally a necessary part of being a scholar 3. I don’t have time for a definition of scholarship that doesn’t recognize the brilliant contributions of project managers, teachers, students, librarians, fanfic writers, book club members, more folks than I can list: all as much a part of building intellectual communities’ knowledge and abilities as academic faculty. 4. A scholarship that fits with the Scholars’ Lab’s unofficial moto of “people over projects”: focusing on community knowledge and health as the goal, and research as the means, rather than the other way around. Documentation of scholarship’s process is scholarship I spent around six months of my dissertation building something on an Islandora-based framework, and then ended up not using those months of my server and code work in the final digital edition I produced. But because I documented throughout why I was making certain technical and design choices, how the technical work developed my critical thinking, I had that research to share with my intellectual community, and I wasn’t six months behind on my dissertation. In my work today, I encourage others to share their notes, design choices, and other documentation as they go; it’s better for the community, and I think it’s also better for scholars new to a method to know they can start giving back to the community right away (e.g. What do you know now that you didn’t understand a week ago? Write for that person)."},{"id":"2019-02-27-moderation-scholarly-online-communities","title":"A Moderate Proposal: Healthier Systems for Running Online Digital Humanities Communities","author":"amanda-visconti","date":"2019-02-27","categories":["Digital Humanities"],"url":"moderation-running-dh-online-communities","layout":"post","content":"Healthy, diverse online learning communities depend on the labor of community design: unseen and often stressful work such as moderation, shaping discussions, and encouraging positive community behavior. As more opportunities emerge for learning online as part of a virtual community, how can we: 1. recognize (credit, publicize, reward, treat as scholarly work) and 2. reinforce (design, automate, fairly distribute the labor, deal with mental health impacts) the labor of community design? As marginalized identities are most at risk for harm when interacting with others online, support for moderation is support for community inclusion. I write from my experiences moderating both my digital dissertation project Infinite Ulysses (a social platform for annotating/commenting on James Joyce’s novel) and the Digital Humanities Slack I started (a set of themed chat rooms with over 2,000 digital humanist members; co-moderated by Alan G. Pike, Sam Abrams, Alex Gil, Brandon Walsh, Ed Summers, Paige C. Morgan, Jeremy Boggs, Eleanor Dickson, Liz Rodrigues, and Erin Pappas). It’s been exhilarating using technology to collaborate with folks I don’t know: if you’ve taught DH, think of the students who respond to your lessons like “DH is exactly what I want to do, I just didn’t have a term for it before”, but expand to include folks from all over the world and more diverse walks of life. But it’s also been a significant source of anxiety, balancing my responsibility to the community with my desire to help folks learn to be more positive influences on that community. That stress was half of why I moved Infinite Ulysses from a Drupal site allowing commenting, to a static archived version without commenting (the other half was some serious thinking about how I could best contribute to the DH I want to see, and realizing that just for me personally, I need to move away from Joyce and Modernist studies to best accomplish that). I do want to note that the challenges of moderating online DH and other academic/professional communities are, at least for me, privileged through my ability to choose my level of work (or opt out completely) without hurting my livelihood or family; compared with the pain, lack of compensation, lack of respect, and more evils experienced by folks like those who moderate YoutTube and Facebook, working with smaller communities that have some norms (though inequitable) and professional consequences (though often used as a weapon against rather than a protector of folks with marginalized identities) are less pressing problems. Why I’m blogging today This post is largely drawn from a non-successful IMLS pre-proposal I PI’d (Spring 2018) for an NLG planning grant to explore, identify key challenges, and plan pilot community experimental approaches to aid the foundational labor of moderators underpinning online learning communities, titled “ A Moderate Proposal : Recognizing and Reinforcing Online Community Moderation to Benefit Diverse Learners” and co-designed with colleagues Jeremy Boggs, Katherine Donnally, Shane Lin, Laura Miller, and Brandon Walsh. (If you like the title pun, you may also appreciate that our proposal drafting GDoc was titled “DigitalHumanities.club”, after the domain that I own because it’s awesome, but that I was also using to explore Mastodon and Mattermost for better communities.) I finally got around to this post due to cool stuff two DHers are up to: 1. Scott Weingart’s Twitter thread on Casey Newton’s recent article . I’ve only skimmed Newton’s piece, but I’ve read and admired Sarah T. Robert’s more longstanding work in this area: * a Maclean’s article in Summer 2018, “Meet the people who scar themselves to clean up our social media networks” * an article in The Atlantic in Spring 2017, “Social Media’s Silent Filter” * And now a book! My SLab colleague Ammon Shepherd has kicked off a series of blog posts around the challenges of archiving DH work, with what promises to be a special focus on the technical and especially sysadmin work unlerying these challenges. You can read the first post in the series now, but Ammon’s also orchestrating a really cool system of inviting UVA Library colleagues to co-write the rest of the series with him, complete with clear documentation on how to take part and make the discussion better. Thanks to Scott and Ammon for being awesome scholars doing work collaboratively and in public! A moderate proposal Community design requires moderation and scaffolding . Moderation is both the work of controlling spam and intentional harassment in a community, as well as designing and implementing a code of conduct (e.g. working behind the scenes with a community member whose words impact others negatively, but who wants to become a better community member). Scaffolding is the work of maintaining and nurturing a community: suggesting conversation topics, encouraging community member involvement, amplifying marginalized voices, and otherwise enacting strategies to grow a healthy and active community. Creating the technical platform for an online community is relatively simple, given existing open-source versions of popular social media platforms. Rather, the challenge to improving moderation is a social project: habituating community members to behavior that reduces harassment, encourages real accountability to others in your community, distributes the stress of moderating, and recognizes/credits/rewards people who moderate and otherwise advance positive community design. I’ve focused on Twitter, Slack, and Mastodon as text-heavy platforms with public and frequent use by digital humanists, plus Civil Comments and the Mozilla Coral Project as technical solutions to online community challenges. I’m also interested in these past or current DHy communities and the scholars shaping them: DH Q&amp;A, Humanities Commons, DHthis, DH Commons, the DLF Forum, DH Slack, HASTAC, DHNow, the AADHUM Slack, and the Documenting the Now Slack. We’ve since updated our thinking and planning, but when I submitted our grant preproposal a year ago, we imagined the following activites and outcomes: Activities The project team would identify and invite 10 experts in online learning community-building, with diverse representation of gender, race, and other identities, from libraries, archives, museums, and academia. Our initial work draws on a symposium with these advisors to spark collaborations, identify key challenges, and plan out steps for piloting and assessing improvements to social systems of moderation: 1. Project team reviews literature and existing communities: Distribute Twitter and DH Slack surveys to gather anecdotes about good and bad online academic communities and their moderation systems. Use team’s existing text analysis skills to explore frequent vocabulary and sentiment in the discussions on a sample community, the Digital Humanities Slack (2k+ community members), if* DH Slack community decides this is an acceptable analysis (read more about the community’s decision to keep the Slack ephemeral via a non-paid Slack plan in this post ). Create an annotated bibliography plus pre-symposium reading list. Assign each advisory board member to read 3 articles and report useful takeaway ideas from these at the symposium. 2. 2-day symposium of 10 advisory board members with Scholars’ Lab staff and UVA Library colleagues: Project team presents results of the DH Slack message text analysis* as a grounding case study. Advisory board gives lightning talks summarizing their assigned readings. Goals set for the rest of the symposium. Facilitated design jam sessions, paper prototyping, breakout groups addressing specific challenges, such as 1: amplifying credit for moderators, making their labor institutionally legible for promotion, addressing gendering of work; 2: templates for difficult moderation conversations with community members, tech features that could support social needs such as shadow-banning, moderator self-care. 3. After symposium: Project team prepares report of best practices to support online learning community moderation, shares with advisory board for feedback, disseminates to public. Team outlines next steps, shares with advisory board for feedback. Probable steps include identifying both technical and social solutions to pilot, formally approaching communities to collaborate on testing practices (e.g. MLA/Humanities Commons, ACH, DLF), scheduling pilots and assessment, prototyping and testing ideas, and reporting back to public. We take the NEH “Off the Tracks” workshop resulting in recommendations for ethically crediting collaborators as a model for all of this work, and commit to thoroughly and accurately crediting all participants in these projects. Outcomes We will identify project staffers with expertise in ethical design practices, and charge them with holding our focus on improving online communities for marginalized identities, while also documenting and publishing the conversations and findings occurring through the planning phase to ensure others can build on our work. During this grant’s planning phase we will create: * An annotated bibliography and reading list * A symposium whitepaper covering the ideas, best practice suggestions, and conversations a diverse advisory board of experts generate during the symposium * A public conversation around crediting moderation labor and improving community moderation, connecting scholars interested in this work * A written plan outlining next steps for pilot projects designing, building, and assessing various technical and social features for their impact on online community moderation. Experimental approaches may include: * Strategies to protect users from one another, unwanted surveillance, or commercial interests * Documented design-driven approaches to building more ethical academic social media platforms * A code of conduct template in support of community design labor * Model a technical platform for online learning communities that is designed from the start with an ethic of care, with particular focus on making the system useful to those identities usually marginalized by commercial social media platforms * Text analysis results discussion from analysis of DH Slack community vocabulary and sentiment Interested? Know things? If anything in this post interests you (future collaboration?), overlaps with your current work, or could benefit from my reading and citing you or others, please let me know at visconti@virginia.edu. Thank you! Thanks to colleagues Jeremy Boggs, Katherine Donnally, Shane Lin, Laura Miller, and Brandon Walsh for co-designing of the grant proposal, and to Ammon Shepherd for helping me think about the technical sustainability aspects of better online moderation experiences."},{"id":"2019-02-27-workshop-on-reading-with-command-line","title":"Workshop On Reading With Command Line","author":"brandon-walsh","date":"2019-02-27 06:39:13 -0500","categories":["Digital Humanities"],"url":"workshop-on-reading-with-command-line","layout":"post","content":"Crossposted to my blog Alison Booth and I are co-teaching a graduate course this semester on Digital Literary Studies. As a part of the course, we’re having a series of technical workshops - command line, Python, text analysis, encoding, and markup. The scheduling worked out such that these workshops wound up being on Wednesdays, with the discussions of critical theory on digital humanities and literary method mostly on Mondays. That’s all fine and good, but I worried that neatly dividing the class out in that way would create a divide between the one day of the week when the students were actively discussing the material in the course and the other when they were mostly in hands-on workshops. Rather than having a hacking day and a yacking day, I wanted to see what I could do to create an environment for hack-yacking. So when I taught the first workshop on command line, I wanted to try something slightly different from what I normally do by making more room for discussion. I wanted to bring the Monday readings and conversations back into the Wednesday workshops. Teaching programming well is very difficult. An easy, low-hanging fruit way to get the material across to your students is to connect your laptop and type in front of the students, explaining your keystrokes as you go and asking them to mirror your actions. Another approach I’ve seen (and used) uses slides to do something similar - sharing code examples and interspersing theoretical concepts as you go. Both of those approaches are all to the good, but they feel a bit too much like lecturing to me. I was trained as a teacher of English literature, which means that my bread and butter is the seminar. My approach to the discussion seminar has always been to act as something of a void in the center of the class, asking questions that get the students to engage with the material and each other more than with me. Lecturing has its place, I’m sure, but it generally makes me uncomfortable and doesn’t reflect the kind of pedagogy I value. So when teaching this workshop on command line I wanted to push myself a little more to see how much discussion I could incorporate into a technical skills workshop. How could I use this experience to help the students begin to think about technology as something that they could engage critically with? as something that was not so alien from their normal work of thoughtful, critical discussion? My goals for the seventy-five minute session, then, were as follows: Introduce the concept of the graphical user interface (GUI) Introduce the concept of the command line Give practice with the command line Use the command line to do something that connected with the discussion we had been having elsewhere in the course The overriding concern for me was to communicate the technical skills the students needed while not making it a “watch what I type” workshop. I wanted discussion and engagement to be the main priority. To start off, I shared several different images of graphical user interfaces like this one. I began by simply asking the students to describe what they saw, framing the activity as continuous with their usual practices of analyzing prose, poetry, or photographs. In the same way that any piece of a text or image can be interpreted and described, so too can technology be subjected to the same analysis. This component of the workshop was inspired by Erin Rose Glass . When she visited the Scholars’ Lab last semester for a talk on surveillance capitalism, she asked the audience to close read the Microsoft Word interface, and I thought the move did a great job of defamiliarizing an everyday piece of technology and subjecting it to critique. In this case, I asked my students a series of questions: What kind of assumptions does the GUI make of its users? What kind of an experience does it want you to have? How does it structure that experience? What kind of audience does it have in mind? These provocations led to an interesting conversation about accessibility, design, and assumed expertise. I especially wanted the students to note how the GUI makes immediately apparent a range of options to the user. But those options are also limited. The interface sacrifices possibility for practicality, and it can never give you every possible option. This discussion set us up well for the next image I presented to them - a terminal window. Again, I paused for discussion, asking questions that directed the students to the lack of clearly visible options in the terminal and how this suggested an assumption that its users would have a certain level of expertise. For better or for worse, it assumes that you know what you’re doing. I could have just told the students all this, of course, but I wanted them to arrive at these ideas themselves. So I mostly framed the activity with questions and helped to direct discussion, just as I would in directing close reading of a passage in a novel. In some small way, I hoped the activity would show them that the close reading and analytical skills they developed in other English classes would still be applicable here, even though the objects of study might be different and even though they might feel a bit at sea. With some sense of what the command line was, the next phase of the workshop took the students through a variety of commands with the terminal. This component of the class felt the most like a typical introduction to the terminal in that I offered a command, executed it, and then we reflected on the results together. The commands we covered mostly focused on navigating the file system and adding or deleting files: cd ls pwd touch rm mkdir cp mv man Nothing too fancy or tricky. From a practical standpoint, the students needed to get the terminal under their fingers so that they’d be prepared to work with it for the rest of the course. As preparation for this piece, we had the students work through the Command Line Crash Course from Learn Python the Hard Way . So the workshop was not their first moment experiencing the terminal, and I could review these pieces quickly before trying to engage the students further. One easy way to try to bridge the gap between our discussions of readings and our workshop on the terminal was to drawn in one of the texts that they had read and discussed in a previous session. Superficially, this meant that the students would at least have familiar thematic material in front of them - I selected an essay for us to work with by Katherine Bode titled “ The Equivalence of ‘Close’ And ‘Distant’ Reading; Or, toward a New Object for Data-Rich Literary History .”\nThe students had been reading a cluster of essays about distant and close reading, and, for our last activity together, I wanted to defamiliarize the process of reading using the command line to further that discussion. The plan was to have our students use the terminal to “read” that article in a number of different ways. At each step along the way, we stopped to reflect on the kinds of reading enabled or inhibited by the commands. I put the text of the article in question up on my website ahead of time so that they could easily access it. First I had the students use the curl command to copy down the text of a URL onto their computer (note that I’ve since taken this text down): $ curl walshbr.com/materials/bode.txt The curl command spits the text of this page onto their screen, which lead to an interesting conversation about this as a type of reading. I asked the students to characterize the kind of reading it did or did not seem to enable. For one, they noted the lack of an ability to paginate through the text, which seemed to imply that the command was meant for readers that planned to take in the text all at once. As in, curl might be meant for machines using a text as data rather than a thing to be read in a more humanistic sense. To ensure that we all had the necessary materials, we grabbed that same text and sent it into a text file on our computer: $ curl walshbr.com/materials/bode.txt &gt; bode.txt This simple command allowed us to talk about how to get web texts and pages onto your computer, gave us occasion to discuss those situations in which you would want to do so, and began a discussion about the kinds of reading possible in different interfaces for reading texts. We then moved along to a couple of different commands for getting text out of a file and onto your screen using the command line. First, we got all of the text at once. $ cat bode.txt Second, we got a version that would allow us to proceed through the text page by page. $ less bode.txt These commands led to some good reflections on the embodied experience of paging through a physical book with your hands as opposed to using the space bar to do a similar activity. We also talked about the different kinds of reading that we might want to carry out, both in general and in digital literary studies. We don’t engage in texts the same way every time we use them, and these restrictive modes of reading foreground that. In one great moment, a student realized that he couldn’t get out of the interface created by the less command - “I can’t stop paging! I’m trapped in the text!” From there, we used the terminal to search for particular words (the word “reading” in this case): $ grep reading bode.txt And then we counted the number of times a particular word occurred in our text: $ grep reading bode.txt -c By searching for particular words relevant to the themes of the text, we had a good discussion about the point at which characters and words on a page become meaningful. In particular, the students noticed that, for a computer, capitalization matters in a way that it does not for a human reading a document. They inferred the differences between tokens and types, the difference between any particular occurrence of a sequence of characters and that word as a unique vocabulary unit. In order to really find out how often the term “reading” appeared in the text, the students realized they would need to normalize everything by converting it to lowercase. I’ve done workshops on the terminal in the past, but this was the first one where I tailored the commands we would be practicing to the subject-matter at hand. Besides the basics of how to interact with the file system, we really only worked with commands relevant to getting and examining textual data. The workshop helped the students come to an understanding that computers read quite literally, with no context or knowledge by default. Later on, the students would find these suspicions corroborated in a reading by Johanna Drucker on “Why Distant Reading Isn’t,” where she suggests that computational reading might be the closest reading of all. Using the terminal as a point of entry into those conversations meant that the students could find their own way to these concepts. In the future I’ll keep this sort of discipline-specific approach to teaching technical skills in mind. It worked well for making the material more immediately relevant and accessible to the students than it might have been otherwise."},{"id":"2019-02-28-connecting-linux-to-cavalier","title":"Connecting Linux To Cavalier","author":"ammon-shepherd","date":"2019-02-27 07:06:59 -0500","categories":["Makerspace"],"url":"connecting-linux-to-cavalier","layout":"post","content":"Connecting Linux (and Raspbian) to UVA’s “cavalier” Network: The following are instructions for connecting your Linux computer to the\n“cavalier” WiFi network at UVA. The steps are to be used at your own risk, and\nare offered without any guarantee of working or not totally destroying your\ncomputer and turning you into a goon (but we’re pretty sure that won’t happen). The steps were put together by Jasper Braun, a first-year Arts &amp; Sciences\nstudent. He deserves all the praise and credit for this article. He is willing\nto help troubleshoot, but be kind, he’s just a student, not a full-time support\nstaff. For Raspbian (regular Linux below): There is no gui network manager, so all entries must be done manually. On initial start-up (with the latest version of Raspbian), make sure you have set your Wifi country code if prompted. Get your personal .p12 certificate from https://standard.pki.virginia.edu/pkcs12/ and move it to the Pi with a Flash Drive. cd into the directory of that .p12 file in terminal. Convert it into a .pem file using openssl pkcs12 -in yourNetBadgeID.p12 -out yourNetBadgeID.pem Enter your netbadge password for all three prompts. No symbols will show as you type. Your .pem file may now be located at /home/pi/Desktop/yourNetBadgeID.pem. Make sure to note the location of your .pem file. Open terminal and type sudo nano /etc/network/interfaces Add the following text to this file iface wlan0 inet manual next line, wpa-roam /etc/wpa_supplicant/wpa_supplicant.conf It should look like this: iface wlan0 inet manual\n wpa-roam /etc/wpa_supplicant/wpa_supplicant.conf To save the file and quit nano, press ‘Control X’, Y, Enter In terminal again, type sudo nano /etc/wpa_supplicant/wpa_supplicant.conf Add the following to this file: ```\n network={\n     ssid=”cavalier”\n     eap=TLS\n     auth_alg=OPEN\n     ket_mgmt=WPA-EAP\n     identity=”yourNetBadgeID@Virginia.EDU”\n     ca_cert=”/home/pi/Desktop/yourNetBadgeID.pem”\n     client_cert=”/home/pi/Desktop/yourNetBadgeID.pem”\n     private_key=”/home/pi/Desktop/yourNetBadgeID.pem”\n     private_key_passwd=”yourNetbadgePass”\n } ``` Now, Control X, Y, Enter Find the wlan0 MAC address of your Pi from typing iwconfig wlan0 and getting the “ether” value. Register this at http://netreg.virginia.edu with your UVA account. Reboot the Pi and it should automatically connect to the ‘cavalier’ network. If an error shows in the Pi’s network monitor, type wpa_cli status in terminal to see if it is really connected or searching. For standard Linux distros with a gui network manager program: Follow all prompts and download your personal certificate as above. Convert it to a .pem as above. Open your network manager and connect to ‘cavalier’. The parameters are as follows: Network type is ‘WPA Enterprise’ Authentication is ‘TLS’ Identity is ‘yourNetBadgeID@Virgnia.EDU’ (note the caps) CA Cert and Client Cert and Private Key should be ‘yourNetBadgeID.pem’ Private Key Password is ‘yourNetBadgePassword’ Finally, find your wlan0 MAC address (Google for how, as dependent on version of Linux), login to netbage here http://netreg.virginia.edu, and register your device’s MAC address for the wlan0 interface. Final Remarks If you find any errors in this post, or would like to comment, please send an email to scholarslab@vriginia.edu ."},{"id":"2019-03-11-archiving-dh","title":"Archiving DH Part 2: The Problem in Detail","author":["Brandon Butler","ammon-shepherd","amanda-visconti","Lauren Work"],"date":"2019-03-11 11:33:18 -0400","categories":["Digital Humanities"],"url":"archiving-dh-part-2-the-problem-in-detail","layout":"post","content":"The Problem in Detail Trying to make sense of Digital Humanities projects and how to archive and preserve them is a lot like a jumble of puzzle pieces, and no picture to follow. The remaining posts in this series will look at these pieces and offer some ideas on how to put them together. This post will cover some (but definitely not all) of the problems inherent in Digital Humanities projects. These are problems that we have seen in the “wild.” These are just a few of the issues that programmers, IT people, Library people, and project leads face when they are planning their projects. Keeping these issues in mind from the start of a project will be helpful later on when question of archiving and preserving come along. Changing Technology The only three sure things in life are death, taxes, and change. Technology loves change. Technology IS change. In so many aspects, this is such a wonderful thing. Except when you try to take a snapshot of that technology for preservation purposes. Then the ever changing technology becomes the enemy. Several aspects of technology make it really difficult to maintain and archive your typical web-based DH project (really, any DH project). First, is the unending upgrades and versions through which technology advances. As soon as you begin a project using a specific technology stack (stack = all of the software, hardware, etc used to create a DH project), it is out of date the next month, or week, or day. For example, as the Systems Administrator at CHNM, I (Ammon) was in charge of updating our web servers. I wanted our developers to use the latest version of PHP for this new project they were working on. You may have heard of it? Omeka? The old server had PHP version 5.something.very.old. The big problem here was the breaking change that PHP introduced in version 5.3.something.not.as.old. This had something to do with what was called “magic quotes.” (from https://secure.php.net/manual/en/security.magicquotes.what.php ) The details of what “magic quotes” do is unimportant. What was very important, was that all of the projects (mostly websites) that were created before my time there (from 1994-2005) were written with “magic quotes” turned on. But this was determined to be a huge security risk, and as the notice above points out, that feature and ability was deprecated, and finally totally removed from PHP. To make matters worse, new software, like WordPress, were starting to require this newer version of PHP. So if I were to upgrade PHP so we could make new sites, then I would break all of the old sites. The solution in this case, was to purchase new servers to put the new sites on, and leave the old, dangerously insecure PHP on the old servers to host the old sites. So, not really a solution in the long term, but I did successfully punt the issue to the next guy (sorry Roberto). Insecure Technology The second problem with archiving DH projects occurs because the software and frameworks used to build DH projects are inherently insecure. There are always ways to break into software systems, and when this happens, it can wreak havoc on a project. WordPress, for example, has been the recipient of many a high security risk . The solution for this is vigilantly updating the software each time an update is made available. This is very costly in personnel time, and website up-time. At one point, I went so far as to write my own tool to update the 70+ WordPress installs I (Ammon) managed (this was before wp-cli existed in a usable form). As you can see, keeping software secure and up-to-date is a continual, long-term project. The opposite of “archived” and “preserved.” Unsupported Technology Another issue that DH projects face, is the dependence on software that is undependable. All software is written by people. People who are employed by a business or are writing the code out of the goodness of their heart, or some other reason. When businesses fail, interests wain, or time is scarce, then software projects languish. For a time, and arguably still, the greatest idea for text-based projects was to use XML. Several flavors of XML were created to address specific desires. TEI was developed to focus on prose, MEI and MusicXML for music, and BeerXML for the obvious https://en.wikipedia.org/wiki/List_of_XML_markup_languages . XML was taken even further and a programming language was built around it, and from there a software framework to build, store and deliver websites based on XML, Cocoon, http://cocoon.apache.org/ . Taken up by the Apache Software foundation as early as 1999, it seemed that Cocoon was a software that would enjoy a long life. A look at the developer email list for Cocoon tells the story of the rise, glory days, and fade of Cocoon better than words: Trendy Tech Software developers seem to be like small children (I, Ammon, am one, and I’m exactly like my own small children). Easily distracted by moving and shiny things. We all love our new technology, right? Software is the same. The old software goes out of favor as new techniques, technologies and abstractions are created. One of the neatest ways to see the change in software is to look at the programming languages of choice. Several companies hold an annual survey about programming language usage and other interesting facts about programmers. GitHub offers a good overview of who is popular on their platform. And StackOverflow offers a comprehensive survey of programmers, from favorite languages to how much time they spend outside. Once the bain of web developers (it was only for showing pop up ads and other trivial tricks), JavaScript has become the most popular programming language in the world. Ever changing technology stacks mean an increased knowledge base is required to get a project into a state for archiving. Each project that is different than the others means more work figuring out how that particular project can be archived. Knowledge Dissemination It should be noted here that the technological problems with planning and preserving DH projects are also closely bound to issues around human labor and sustainability, precarity of funding and capital as well as understanding and implementation of what “preservation” or “access” may mean to different groups across varying contexts and across the life of a project. It also concerns changes in scholarship and publication, as well as the implications of copyright, ownership, digital preservation, and fair use. As a community, we’ve been discussing, building on, and trying to frame out both the problem and the solutions around these issues for quite some time. Project Endings, from the University of Victoria, has compiled a great bibliography that demonstrates some of the DH and library/preservation community interest and scholarship around sustainability since the 1990s. We even took a strong run at attempting to address the issues around sustaining DH projects here at UVA over 15 years ago with the Sustaining Digital Scholarship project. It would seem that as a community over the years, we seem to have identified the broad patterns around the dependencies and cycles of DH projects fairly well. So why is this still such a difficult problem? The University of Pittsburgh was awarded an NEH grant to put together a sustainability roadmap for a well used, but aged website. One deliverable of the grant was to produce a template from their work to be used by others. “The final goal of the project was to create a digital sustainability roadmap for other developers and curators of digital projects to follow based on what we had discovered.… By taking your project team through the modules in each of the units, you will be able to produce an effective ongoing sustainability strategy, or to assess the effectiveness of your current strategy.” ( https://sites.haa.pitt.edu/sustainabilityroadmap/about/ ) As a preservationist (Lauren here, hello!) concerned with maintenance and access to materials and scholarship over time, I’ve seen this problem (and some potential solutions) play out from my perspective in ways that we’ll discuss more fully over the next two posts that touch on why this is still a difficult issue. But for now, for the problem in detail, how we try to present technical and administrative digital preservation pathways often circles around how we frame and receive (or don’t receive) answers to central questions like: What an  “active” versus “finished” or “archival” project looks like How we address ownership (including copyright and licensing), authorship, and responsibility, especially for collaborative projects What the Library or Scholars’ Lab role is in publishing When in the life cycle of scholarship ideas like preservation, sustainability, and portability are introduced How we frame discussions and expectations of authentic access and use over time for digital scholarship (aka nothing can be a website forever without maintenance) Plus many technological and resourcing questions that surround security, storage, labor, documentation, description, and more. ( MITH has some great blog posts about these topics as well) Humans underlie technology The human labor underlying technology is not easily sustainable; even when you have folks committed to the work, the kind and amount of labor required to maintain and update a project can shift away from what someone is willing or able to support, and external pressures (job changes, family, other projects, aging, retirement) also arise. I (Amanda, hi!) am particularly interested in how technical sustainability is impacted by the human labor of moderating online spaces, from my experiences moderating both my digital dissertation project Infinite Ulysses (a social platform for annotating/commenting on James Joyce’s novel) and the Digital Humanities Slack I started (a set of themed chat rooms with over 2,000 digital humanist members; co-moderated by Alan G. Pike, Sam Abrams, Alex Gil, Brandon Walsh, Ed Summers, Paige C. Morgan, Jeremy Boggs, Eleanor Dickson, Liz Rodrigues, and Erin Pappas). Moderation is both the work of controlling spam and intentional harassment in a community, as well as designing and implementing practices such as a code of conduct to both address the negative and encourage positive community behaviors. It’s been exhilarating using technology to collaborate with folks I don’t know and create online communities together with them: if you’ve taught DH in the classroom, think of the students who respond along the lines of “DH is exactly what I do or want to do, I just didn’t have a term for it before” but expand them to include folks from all over the world and more diverse walks of life. But this work has also been a significant source of anxiety, balancing my responsibility to the community with my desire to help folks learn to be more positive influences on that community, and needing to enforce the line after which a community member cannot healthfully continue as a community member. That stress was largely why I moved Infinite Ulysses from a Drupal site allowing commenting, to a static archived version without commenting. (Relatedly, I’ve just blogged on some of the work I’ve been doing to make online moderation of academic communities more sustainable!) Complex Ownership (and Missing Owners) DH projects are the kinds of things that copyright protects—they are works of original creative expression, including textual works, audiovisual works, databases, and the like. Preserving a DH project will require activities regulated by copyright—reproducing the project (to migrate it to new servers, e.g.), distributing, displaying, or performing it to the public (by publishing it online), and so on. So, an institution that wants to archive DH projects needs to navigate copyright law. I (Brandon, hi!) yield to no one on the scope of activity that libraries and archives can do without permission, thanks to both the general fair use right and the specific exceptions favoring libraries and archives. However, it never hurts to get permission when you can. A few things about DH projects can make getting permission a significant challenge. University policy (here’s UVA’s ) typically cedes ownership of scholarly IP to the creators, whether they are faculty or students. Copyright ownership in these projects can be quite complex given how many people contribute copyright-eligible work to them, and the nature of the academy means many (sometimes all) possible owners/authors may move on to different institutions (and even different careers) over time. Defining a project’s “authors” versus supporters and collaborators is also difficult. All of this makes obtaining permission difficult. (Solutions, which we will discuss at length in a future post, include securing permissions up front, ASAP, or better yet, assigning an open license to every aspect of the project so that anyone, including archives, will be free to preserve the work.) The following contributed to this article: \n- Brandon Butler - Ammon Shepherd\n- Amanda Visconti\n- Lauren Work"},{"id":"2019-03-27-Nailed-It","title":"Nailed It: A Progress Report","author":"kelli-shermeyer","date":"2019-03-26 21:00:00 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"Nailed-It","layout":"post","content":"Today, I just wanted to offer an informal and honest take on how my motion capture project is coming along, with respect to the focus on process and learning that is central to DH practice in the Scholars’ Lab. When I read about performances that use motion capture and augmented reality, they usually look like this: This still was taken from the Royal Shakespeare Company’s production of The Tempest, which partnered with Intel to use motion capture to stage Ariel’s magical transformations. Or take these stills from an Athens-based museum theater project, which experimented with real-time motion capture tech during a live theatrical performance that also employed computer generated scenery: Now compare this one of my current scenes: Have you ever seen NAILED IT on Netflix? Sometimes that’s how I feel about the work that I’m doing in Digital Skriker. My aim in this comparison it not entirely to be self-deprecating; in fact, one of the things I love about NAILED IT is how it celebrates the contestants’ efforts and everyone seems to have a really great time baking together. And in terms of my DH project this year, I feel like whatever the end product will actually look like, it’s worth acknowledging the different kinds of labor and learning that went into it. I’m writing this post in part because I’m anxious about how the project will turn out. I’m very confident when writing about the intellectual content of Digital Skriker, but much less so when I’m writing about the nuts and bolts of doing that work—the parts where I’m watching a bunch of YouTube tutorials, attempting to organize my files coherently, and endlessly trying to figure out how to get the motion capture hardware to register that my feet are actually oriented toes pointing forward. Since September, I’ve been learning how to use Unity; testing out several different programs and hardware for capturing and then manipulating motion data; researching previous uses of motion capture in theater performance; learning how to blog using Github and how to write about my process (still working on this one!); talking about my work with other potential collaborators; making artistic choices about the kinds of movement we captured; designing a stage setting in Unity; exploring how the resources (skills and money) that I have determine the kinds of assets I can use in Unity (which actually leads to artistic/aesthetic decisions that will affect how my work appears to an audience); and watching and re-watching a bunch of tutorials to figure out how to clean up my motion data and apply it to basic avatars. This is all doing DH. I wrote this post because I want to get better at sharing work that’s in progress, provisional, and messy. Sharing work at this stage – even if it’s just a screenshot – often feeds my imposter syndrome. I’m hoping that with time, presenting work in progress will inspire me to engage more with others about the directions my work may still take. Thanks for reading. P.S. My first several attempts to get these pictures to appear in the post didn’t work."},{"id":"2019-03-28-whats-a-pedagogy-and-how-do-i-find-mine","title":"What's a Pedagogy, and How Do I Find Mine?","author":"brandon-walsh","date":"2019-03-28 07:36:29 -0400","categories":["Digital Humanities"],"url":"whats-a-pedagogy-and-how-do-i-find-mine","layout":"post","content":"Crossposted to my blog As part of the Scholars’ Lab’s year of blogging, Amanda asked me to write a little about my pedagogy and things that have helped me find it. When someone asks me to do such a thing, I always think about a poll that Jesse Stommel put out on Twitter about pedagogical training in graduate programs. He asked: Dear higher education teachers, a poll. Answer below, reply with stories, and pass along. How much training in teaching or pedagogy was/is included in your graduate program? The majority of people responded to say their training amounted to “basically nothing.” I consider myself in the “basically nothing” category as well. My program did have a single semester course for pedagogical training, but for a variety of administrative reasons, it was possible to slip through the cracks and start teaching without any formal training. This is not to say there was no teaching support - we did have the help of a faculty mentor while serving as a teaching assistant, and we graduate students had each other. But that kind of catch-as-catch-can approach to pedagogical training left me with what felt like a very spotty teaching background and loads of imposter syndrome. So, in some ways, this post is a letter to myself from eight years ago. It talks about steps I’ve taken to develop a stronger sense of myself as a teacher. It’s worth noting at the outset here that teaching happens in many shapes and sizes - courses, workshops, weeklong institutes, one-on-one mentoring, etc. Similarly, your pedagogy itself manifests in a variety of different ways, but it should ideally inform every decision you make about yourself as a teacher, your students, and everything in between. It’s also worth noting that these steps are more truly about pedagogy generally - digital pedagogy is a related subject that overlaps with this one that merits its own discussion. But the steps below could certainly get you started thinking your way through a pedagogy in that field as well. Hopefully, what follows will help someone else work their way towards a more informed, aware, and critical sense of their own pedagogy. I’m still learning, and these are the steps I’m taking to do it. What’s a Pedagogy? When I put together my first syllabus, I had no idea where to start. I didn’t even really know what the genre looked like, despite having seen syllabi many times before. So I collected a bunch of example syllabi from friends and looked at what others had done. Using these pieces, I started assembling a Frankenstein’s monster of a syllabus that, when I looked down, was eight pages long and had pages of policies, rules, and readings. Do these sort of things inform your teaching? Sure. Are they your pedagogy? No…or not exactly. An important first step is recognizing something that Jim Seitz, UVA’s Director of Academic and Professional Writing, told me at a later date - your pedagogy is what arises out of your motivation and thinking behind every decision related to the classroom. Those rules and policies were not my pedagogy, but they were expressions of it . They reflected a variety of different ideas about who my students were, how I viewed the classroom, and how we would all be interacting with each other. Of course, when writing that initial syllabus, I didn’t know anything about all this - I was grabbing pieces that felt right without thinking too much about them. At the end of it all, I remember looking down at the resulting syllabus and thinking, “It sounds like I’m going to run this class on film musicals like it’s a police state.” So I deleted 90% of the rules and policies and started over. There was a pedagogical reason behind my doing this, but I didn’t really know it at the time. For me, discovering my own sense pedagogy has been about reclaiming agency over these sorts of impulses. I’ve learned what I care about, what I don’t, and places where I can express my vision of what teaching should be. How Do I Find Mine? The first thing to know about teaching is that if you’re asking that question you’re already moving in the right direction. These steps can help. Think about what you’re doing. You might not realize all the different components that go into the classroom, but every one of these could merit further examination. Everything from the way you dress to the way you interact with your students to whether you sit or stand while teaching can be things to think further about. Think about all the different elements that make up your teaching experience. Recognize which of the things you’re doing are choices. Most teaching situations bring with them a variety of constraints. Depending on your situation, you might not be able to adjust certain elements of the classroom. Of course, these sorts of restrictions can still be worth examining more closely - you might not always be constrained in this way, and it can be useful to have deeper thoughts about what you’re doing even if you can’t change it. Work from specific to abstract. Take a look at one thing you do and try to find the principle behind it. Given the entire world of possibility, what was it that would lead you to choose to do that one thing and not any others? And then take that justification and get even more abstract - why is it so important to you? To offer one example: when I run discussion I tend to contribute more questions than I do statements. As students respond to the questions, I give more. I do this because I’m genuinely interested in what the students have to say. And I think the underlying belief here is that students, even while learning, can make important contributions to complex discussions that are at least as important as anything I can offer. Observe others teach . Growing up, you probably spent a lot of time in classrooms as a student. But the irony is that, once it became clear that you would do some teaching, you probably didn’t get much of a chance to see how others do it. Education programs generally have student experiences designed to address this gap, but I think it’s the case that graduate students in the humanities don’t get enough opportunities to observe others. And you can’t rely on your memories of what good and bad teaching looked like - it’s fundamentally different to be a student than to be an outsider thinking about the logistics of how it all gets put together. So ask others if you can watch them teach. While some people might be self-conscious about the activity, you’ll get some people willing to let you sit in on a class. And while you’re watching, take notes about everything from the students to the teacher to the general classroom environment. Everything can be grist for your mill. Talk with others you respect about their teaching . This point can be taken in isolation, but it works best with observing someone teach. It often works well to get context for what goes on in a given classroom session. If the instructor has a lesson plan prepared in advance, ask them to send it to you (or at least a few sentences about what they’re planning to do). After the session, ask if you can follow up to talk about what you saw. Keep in mind that you’re asking the observee to do work for you. I sometimes offer to buy coffee to sweeten the deal if necessary, though this might not be financially possible for you. You might also offer to have them observe you in turn. When talking, it can often be a good entry point into the conversation to ask them how they thought the session went. And beyond observing individual classes, you should explore whether your institution (or one nearby if you don’t have one) has a group dedicated to teaching and pedagogy whose work and conversations you can tap into. Read and learn more . Depending on your own background, this fact might get obscured, but pedagogy is an area of study - and research - like any other in academia. There are countless scholars working to advance conversations about pedagogy, and beginning to read around in the field can help you to discover where you fit in. You might start off by reading people discussing teaching as it relates to your specific field to get a general sense, but strong pedagogy frequently transcends the disciplines in which it manifests. As with any discipline, you won’t agree with everything you read. If you’re reading this, odds are you are in some way interested in how we run our student programs in the Scholars’ Lab. A pedagogical starter pack for us might include Paulo Freire’s Pedagogy of the Oppressed, bell hooks’ Teaching to Transgress, Cathy Davidson’s The New Education, and L. Dee Fink’s Creating Significant Learning Experiences . As with any good lit review, I find it works well to read any given piece looking what I am going to read next. I’m always looking for my next citation. So as you’re going, keep an eye out for any compelling citations that might offer good opportunities for digging deeper into the conversation in the future. Where to Start . You might try a thought experiment: imagine that your supervisor has asked you to take on a new course/workshop/student. In short, you’ll be doing exactly what you currently do but with one crucial difference - they will dictate everything about your teaching. Everything will be decided for you and you will have few freedoms. But they recognize your strengths as a teacher and are willing to compromise - you can bring three things over from how you used to teach. Only three - pick the most important parts of what you do. Start with the three treasured components, find the beliefs that underlie them, and you’ll be well on your way to developing a stronger sense of your pedagogy. Hopefully the above activities will help someone reading this think more deeply about their teaching. The above suggestions are all born out of my own experiences with pedagogical training and growth - your mileage may vary. If you have had wildly different experiences I would love to hear more about them. My last suggestion is to write your pedagogy down . These writings might take many forms. The Scholars’ Lab has tried to express our pedagogy in the form of a charter . You might choose to blog about it or construct a generalized teaching statement, but, whatever form you choose, writing about your teaching will help you develop a better sense of it. And that labor is not for nothing - there are a number of great venues for writing about pedagogy, and doing so can be a professionally legible activity in its own right. The world needs more teachers, and it needs more good thinking about teaching. Contribute yours to the conversation."},{"id":"2019-03-29-sharing-unity-project","title":"Sharing a Unity Project","author":"ankita-chakrabarti","date":"2019-03-28 21:00:00 -0400","categories":["Digital Humanities","Research and Development"],"url":"sharing-unity-project","layout":"post","content":"Sharing a Unity Project Working collaboratively on a creative project not only makes the whole process a lot more fun but it also opens up space for ideas to bounce off of each other and potentially create something far more interesting! Here is an easy way in which you can build your own team of collaborators for a Unity project. To begin, make sure you have created a free Unity account and are logged in. The free account allows you to build a team of up to three collaborators Click on the Collab button on the Unity window of the project you want to share and then on “Start Now” You will be able to “Publish” your project at this point You can then create a team that will be able to access the project by clicking on the “Invite Teammate” icon at the bottom This will take you to the Unity online Dashboard where you can add members to your team Your teammates will be able to download the project once they have accepted the invitation Remember to publish and download changes as they occur - the Collab icon will change colors to alert you of possible changes that your teammates have made"},{"id":"2019-04-04-light-emitting-objects","title":"Illumination Through Light Emitting Objects","author":"ankita-chakrabarti","date":"2019-04-03 21:00:00 -0400","categories":["Digital Humanities","Research and Development"],"url":"light-emitting-objects","layout":"post","content":"Illumination Through Light Emitting Objects in Unity 3D This exercise will show how to make objects inside a room emit light and the basics of working with emissive material in Unity to create indoor illumination. For this exercise we are going to make the emissive objects be the only source of light within the room. Constructing a Room: Insert a Plane (3D Object) from the GameObject menu. Create five duplicates of the plane. Rotate them individually and position them to create four walls and ceiling for the room. You should have a cube that you can see into as you rotate your view around it. Putting all the planes together in one Empty GameObject, renamed “Room”, will help keep things organized. Creating the Light Sources: We are going to create four light emitting cubes within the room. Insert a Cube (3D Object) from the GameObject menu. Since the room is going to be illuminated from inside, we can delete or deactivate the Directional Light. The scene should look quite a bit darker now. In Assets, create a new Material. Materials define how the surface of an object should be rendered. Enable “Emission” in the Inspector and choose the color you like. You can also adjust the amount of emission. Then drag the Material from Assets on to the Cube. Create three duplicates of the Cube, and three different colored Materials. Adjust the size of the cubes and position them around the room. Note: Any object can be made to emit light within a scene by adding an emissive material to it. Baking a Lightmap: The room and the light emitting objects all need to be static (or non-moving) in order for the lighting to work. Select the objects and enable Static in the Inspector. This tells Unity that the objects, in this case the walls and the cubes, will not move during gameplay which allows Unity to begin “baking a lightmap”. This means that using the information provided, Unity will start to calculate and then construct the effects of the illumination into a “lightmap” (by tracking the paths of the light rays from the cubes) for the scene. We need to go back to the Materials Inspector for each cube and change the Global Illumination from “Real Time” to “Baked”. In Baked lighting the illumination is pre-calculated and does not change during gameplay. In order to tweak the lighting properties further we need to pull up the Lighting Settings. Since it is an indoor scene, there should not be any external or “environment” lighting. The only light should be from the light-emitters within the room. Hence, we need to change the Skybox Material to “None” and then change the Ambient Color to black (to indicate no ambient lighting). In the Lighting settings, deactivate Realtime Global Illumination and make sure Baked Global Illumination is enabled. In Lightmapping Settings you can make the following changes, Set the Lightmapper to “Progressive CPU” Increasing Lightmap Resolution and Lightmap Padding will create a more smoothly lit scene. This will also increase the Baking time. Lightmap Size can be reduced (in this case to 512) for a smaller scene which will reduce the file size without affecting the quality of light rendering. Deactivating “Compress Lightmap” solves the problem of light banding that can occur when light from an emitter fall on an object. This will increase file size. Enable “Ambient Occlusion” to intensify shadows cast by objects within the scene. Here it will intensify the shadows between the walls. Finally, your indoor lighting scene should look something like this! Sources: Lighting in Unity This is an excellent tutorial to understand how   lighting works in Unity and how to make your lighting look even better! StaticObjects Materials, Shaders and Textures Baked Lighting"},{"id":"2019-04-15-3d-content-vr","title":"3D Content for Virtual Reality","author":"abhishek-gupta","date":"2019-04-14 21:00:00 -0400","categories":["Digital Humanities"],"url":"3d-content-vr","layout":"post","content":"Designing the best virtual reality (VR) experience requires having the best 3-dimensional (3D) content that is suitable for your VR experience. Not all 3D content forms are equal, so it is important to distinguish between them. A helpful way to look at a project or experience is to first determine whether it seeks to create spatial content or whether it seeks to capture spatial content. Content Creation Creating 3D content in VR is particularly useful when the object you need is simple, geometric structures with specific spatial references. For example, one user might choose to create 3D content for their environment simply by establishing a geometric plane as the floor. Note here the difference between created content (i.e. the plane) and captured content which we will soon discuss. This process of user construction can be done repeatedly to create the desired experience. The two main methods discussed here are manual sketching and VR modeling. Manual sketching is the most traditional method, which uses software like Sketchup, Autodesk Maya, and Rhino 3D. These are standard computer-aided drawing (CAD) tools that content designers use to create objects such as 3D models, architectural drawings, gaming props and animations. To learn more about these types of programs, check out the following: Images and Informational Links Sketchup Maya Rhino 3D Similar to how the above tools are used on a traditional mouse and keyboard interface, modeling can now also be done in virtual reality. The concept is similar, except that the interface is all within virtual reality, and the controls are designed to have more intuitive controls and better spatial feedback. Examples of VR software that can do this include Google Blocks, Google Tilt Brush, and MasterpieceVR. All of these interfaces have immense functionality built around the two hand controllers, and MasterpieceVR even supports multi-user collaboration. Images and Informational Links Blocks Tilt Brush MasterpieceVR Content Capture As compared to creating one’s own content, capturing 3D content from the real world and importing it into VR is useful when the object is particularly intricate and requires preservation of the smallest details. In other words, this method allows you to make digital copies of the surface of 3D objects, complete with all the spatial and visual details. This can be done for smaller objects such as artifacts or larger objects such as landscapes or buildings. The method chosen depends on many factors including budget, environment, and the aims of the individual who is doing the capturing. The two main methods discussed here are photogrammetry and 3D scanning. Photogrammetry is a labor-intensive but simple way to create a 3D file of a real-world object. This involves taking numerous photos with a camera, which is then sent to software like Autodesk ReCap (stands for reality capture) to stitch together into a 3D mesh file. Other such programs include Photoscan, Zephyr3D, and Meshroom. The remarkable aspect is that the pictures can be taken from a high-resolution camera on any platform, including a DSLR, smartphone, or drone, which is why this option is viable for people who want an easy process. However, it is important to keep in mind the quality of the environment lighting and the reflectivity of the chosen object, since a dark area or a shiny object could ruin the algorithm that converts the photos to the final 3D model. Images and Informational Links How Photogrammetry Works Taking Good Pictures for Photogrammetry Finally, 3D scanning employs sophisticated tools that simplify the process substantially. These scanners use structured light, time-of-flight, or lasers to pinpoint the details on the object, and they come in all sizes that can measure the smallest objects up to the largest buildings. Platforms for 3D scanners can include handheld devices, revolving stands, and drones. These devices work on the principle of collecting millions of points on an object to form a point cloud that resembles the object when combined together, thus creating a point cloud or mesh file. To accomplish this, each scanning device usually comes with its own proprietary software. With this method, object and detail size can matter more, since the points’ spacing may not allow them to capture intricate details and/or small objects. Images and Informational Links How 3D Scanning Works University Hall 3D Scanning News Article Sources Sketchup: https://www.sketchup.com/sites/www.sketchup.com/files/su-opengraph.jpg Maya: http://cdn.studiodaily.com/wp-content/uploads/2019/01/maya_2019_cached-playback.png Rhino 3D: https://www.3dprinter.net/wp-content/uploads/2014/01/rhino-3d.jpg Blocks: https://roadtovrlive-5ea0.kxcdn.com/wp-content/uploads/2017/07/google-blocks.jpg Tilt Brush: https://i.ytimg.com/vi/TckqNdrdbgk/maxresdefault.jpg MasterpieceVR: https://i.ytimg.com/vi/bw1PoyHaCyE/maxresdefault.jpg Photogrammetry: https://qph.fs.quoracdn.net/main-qimg-3c963a747b0552885222bbf128df7e22 Tree Example: https://www.allegorithmic.com/sites/default/files/Image2.jpg 3D Scanning: https://proto3000.com/wp-content/uploads/2016/08/GoSCAN-50-450x500.jpg U-Hall Building: https://news.virginia.edu/sites/default/files/bezzo_uhall_scan_full_dome_inline.jpg"},{"id":"2019-04-15-announcing-2019-2020-fellows","title":"Announcing 2019-2020 Fellows!","author":"brandon-walsh","date":"2019-04-15 06:23:43 -0400","categories":["Grad Student Research","Announcements"],"url":"announcing-2019-2020-fellows","layout":"post","content":"We are thrilled to announce the 2019-2020 Scholar’s Lab fellows for the Praxis Program and the Graduate Fellowship in the Digital Humanities. We are welcoming 8 fellows from 4 disciplines from the arts, humanities, and social sciences and the School of Architecture. Our graduate fellows are joining a robust and vibrant community of past students! Praxis Program We are delighted to welcome 6 team members to the 9th year of the Praxis Program, our flagship introduction to digital humanities by way of collaborative, project-based pedagogy: Chloe Downe Welles, Art and Architectural History Janet Dunkelbarger, Art and Architectural History Conor Kenaston, History Natasha Roth-Rowland, History Nicholas Scott, History Lauren Van Nest, Art and Architectural History Look forward to more details about the Praxis Program’s new project in the fall! Graduate Fellows in the Digital Humanities Finally, we are looking forward to working with Jordan Buysse and Leigh Miller, our 2019-2020 Graduate Fellows in the Digital Humanities. Jordan Buysse’s (English, GSAS) dissertation is titled “The Word and the Bit: Information and 20th/21st Century Literature” Leigh Miller’s (Constructed Environment, School of Architecture) dissertation is titled “Being and Place in(between) Video Game Worlds” These fellows will work with our team throughout the year and over the summer on substantial research projects related to their dissertations. They join a vibrant community of students working in the lab in the coming year as Makerspace Technologists, Cultural Heritage Interns, GIS Technicians, and more. Special thanks to everyone who served on the application committees that selected these fantastic students. We are looking forward to working with all of them in the coming year!"},{"id":"2019-04-25-thirteen-oblique-strategies-for-digital-pedagogy","title":"Thirteen Oblique Strategies For Digital Pedagogy","author":"brandon-walsh","date":"2019-04-25 05:09:55 -0400","categories":["Digital Humanities"],"url":"thirteen-oblique-strategies-for-digital-pedagogy","layout":"post","content":"Crossposted to my blog When I sat down to write this post I had no ideas. That’s probably inevitable, given the year of blogging challenge that we’re undertaking in the Scholars’ Lab. The whole point is to write often and frequently, that there is value in a steady stream of thoughts rather than waiting for the perfect blog post, and that regular writing makes the whole thing easier. Still, all those good intentions didn’t help me as I struggled to put text to blank page. As I often do in those situations I got out a deck of cards and started playing. I’ve been obsessed with Oblique Strategies for years now. If you’re not familiar, Oblique Strategies is a deck of cards published by Brian Eno and Peter Schmidt that aims to offer short, pithy suggestions for getting around creative dilemmas. The idea behind them is that the serendipity of drawing a mysterious phrase from the deck will help disrupt any blocks you might have moving forward. I’ve got a stack of them that I keep on my desk, and it’s a comfort to know that I’ve always got a wrench to throw in the gears at any given time. This morning as I flipped through the deck for inspiration these were the cards that came up first: When is it for? Use an old idea Turn it upside down Once the search is in progress, something will be found Humanize something free of error A lot there! Surely, somewhere in there, I could find material for a successful blog post on digital pedagogy, the subject I’ve been trying to focus on with these regular posts. I did, and I could. But the activity interested me more: how could these cards - the idea of them more so than any one phrase on them - inform teaching within the field of DH more generally? Of course, I’m not the first one to think about how Oblique Strategies might apply to DH. Mark Sample’s keynote for the first annual Institute for Liberal Arts Digital Scholarship took up this topic. In “Your Mistake was a Vital Connection: Oblique Strategies for the Digital Humanities,” Sample does a fantastic job articulating the potential for the deck to inspire digital humanities research and pedagogy. Sample advocates not just using the deck as a means to an end - he suggests making serendipity the process and outcome itself. The deck can be quirky way to step over difficulty and get back to the serious business of doing work, but it can also offer a reconsideration of what the work could look like in the first place. Sample frames using Oblique Strategies in DH research and pedagogy as a question of embracing serendipity as a methodology. This seems to me to be right and correct, but his piece also makes me want to think further about how Oblique Strategies might offer ways for thinking through digital pedagogy in particular. Besides shuffling up any of those cards and thinking about any specific aphorism’s applicability to teaching in a DH context, what might a larger approach to digital pedagogy look like when informed by oblique strategies? Sample offers serendipity as a way into the question, but this is only one to think about the cards. After all, embracing chance as a methodology involves releasing something else - control. Take the way Sample offers the same question I just posed: “How might oblique strategies not only be another way to work in general, but specifically, another way to work with the digital scholarship and pedagogy we might otherwise more comfortably approach head-on, as Brian Eno put it.” This reading frames use of the cards as a question of directionality but also of power - rather than assert power, the cards seem to say, give yourself over to the whims of chance, let your creative faculties engage, and step in from the side. In order to play the game as invited by Oblique Strategies, you have to first accept you might not have all the answers. For me, this question of authority is the clearest link between the deck and how I might apply it in the DH classroom. For me, good digital humanities teaching is always about unsettling authority in the classroom. When digital humanists co-teach courses, they model how no one person has all the answers. When we encourage process and reflections on failure as important scholarly outputs, we encourage students to give themselves over to a more humane understanding of research. When we counsel students on imposter syndrome, we directly encounter the idea of the authority. There are more specific, personal implementations for me as well. When I run a class, I tend to ask more questions than I offer answers, and my ultimate goal is for the students to recognize that their own contributions to discussion can often be more valuable than mine. All of this makes me want to ask: how can Oblique Strategies encourage a more student-centered pedagogy? How could we use the ethos of the cards to disrupt vectors of power in the classroom and create a more meaningful, inclusive teaching environment? What about this would be specific to digital pedagogy? In the spirit of what I’ve described above, I won’t offer answers (yours would be as meaningful as mine). Towards the end of Sample’s blog post he shares together a playful tool for generating your own oblique strategies for digital humanities . I thought I might take a similar approach, offering a few elliptical statements on how we might approach digital pedagogy obliquely in the aphoristic style of Oblique Strategies . I had originally intended to give brief glosses about how they might be interpreted, but I think that gets away from the spirit of the thing. In a riff on Wallace Stevens’ “Thirteen Ways of Looking at a Blackbird,” I’ll give thirteen, lest any one appear to be more important than the others: Thirteen Oblique Strategies for Digital Pedagogy Create &lt;-&gt; consume Analog digital Interaction over interface Displace authority Questions Allow for gaps Process product People first Iterate Cultivate imposterdom Design Design Can you fail? Disrupt / connect Of course, maybe the best oblique strategy would be to hand the students a blank deck with some colored pencils and ask them to draw for you."},{"id":"2019-04-26-dh-center-staff-professional-research-and-development-time","title":"20% Time: Why we make self-initiated research & development part of the job","author":"amanda-visconti","date":"2019-04-26","categories":["Digital Humanities"],"url":"dh-center-staff-professional-research-and-development-time","layout":"post","content":"Crossposted to my blog . All Scholars’ Lab staff are encouraged to take 20% of their time to initiate research and professional development. In this post, we’ll share some of what we’ve been working on, and explain the rationale behind making this 20% time a formal part of our job descriptions. What is 20% time? Previously also referred to as “personal R&amp;D time”, we’ve recently shifted to saying “self-initiated R&amp;D time” to emphasize that this work is often collaborative, and always feeds back into the lab: what’s unique about 20% time is special leeway to experiment and fail, and formal support for staff to initiate and direct their own professional and scholarly development on the job. Making this a specific job expectation can help folks feel comfortable pursuing learning that isn’t immediately applicable to scholarship, but probably will be later on. For example, GIS Specialist Drew Macqueen’s “Arduino Chris” (GIF below, courtesy of Drew) used the problem of our office lights (which turn off when folks weren’t moving around much at their desks) to get some practice building electronics, which he can now apply to research consultations and collaborations as part of his spatial technologies toolkit: Above: GIF of Drew Macqueen’s “Arduino Chris”, using electronics and a photo of GIS Specialist Chris Gist to turn the office lights back on when they go off from no one moving around. Note that “R&amp;D time” (aka 20% time, personal R&amp;D time, self-initiated R&amp;D time) is distinct from the work of our Research &amp; Development (R&amp;D) unit, which focuses on programming and design. What kinds of work can it be used for? As with our other staff work, we hold our use of 20% time to the following 3 commitments (also outlined in our team charter ):\n1. At any moment, we should be able to relate our projects to the larger missions of the Scholars’ Lab, the Library, and the University as a whole.\n2. We should be able to articulate the interest and value of our individual and collaborative work.\n3. Our projects – including projects undertaken with UVa faculty and grads – are expected to enter public conversation and to be shared as broadly as possible . We have an expansive and evolving notion of valid “publication” events and public outcomes. We value open source code and open access information. How is self-initiated R&amp;D a good investment in our staff? One of our greatest strengths is our team of experts, folks with deep scholarly and professional training and experience across the disciplines. Every one of us embodies what I think of as full scholarship —internationally recognized research, combined with a passion for teaching, community care, and social justice that’s necessary for research to function as scholarship. Our department, the UVA Library, makes an unusually (perhaps uniquely, within DH?) strong investment in digital and experiment scholarship through hard-money (i.e. not grant-dependent) funding of 12 full-time employees in the Scholars’ Lab, plus 1 part-time employee (a full-time UVA staffer, with part of their FTE bought out from the English Department by the Library), and 1 full-time grant-funded employee. Given that Centernet (the international organization of DH centers) defines DH centers with 5+ FTEs as “large”, we are rich indeed in this most important of scholarly resources. Self-initiated R&amp;D time helps us make the best use of that staff investment. Formally committing to allowing on-the-job professional development is diffrent than requiring staff to make special proposals to their supervisors if they want to write a paper, learn a skill, or try something that may or may not pan out. Our staff need to stay at the forefront of experimental and digital scholarship, if they’re to do their best in nurturing and collaborating on the campus community’s research in these areas. Often, that means work that may or may not prove to have a scholarly application at UVA (but we wouldn’t know unless we tried). As Former Scholars’ Lab Director Bethany Nowviskie notes, “For software developers, who can command larger salaries outside the academy, [20% time is] a much-valued perk. For alt-ac staff, who trained as scholars, it is almost a psychological necessity” (Nowviskie, 2012, “Too Small to Fail” ). 20% time lets us hire and retain folks with strong backgrounds in research and self-initiated experimentation. If we require these skills in a job ad, why wouldn’t we want to structure the role to help staff continue strengthening such work once on the job? Our current staff includes 2 UVA general faculty members, 1 UVA full professor, 3 staff with humanities Ph.D.s, 5 additional staff with Ph.D. A.B.D.s, and 3 staff with terminal M.L.S.s, in addition to professional training and certification in areas such as project management, and teaching as UVA instructors of record. Formal R&amp;D time not only supports strong hiring and retention, it also helps our staff remain or become scholars themselves, so that they can bring that experience to their interactions with other faculty, staff, and student scholars on campus: “R&amp;D time is the thing that ensures that the Scholars’ Lab… can become an interesting intellectual community in its own right, and be perceived as such by our colleagues on the teaching faculty” (Nowviskie). Note that this is important for all our staff, not just those holding degrees or hired with explicit research work in their job description; this “helps the SLab along in our ambition to function as a team of equals—a family, with shared, long-term commitments to our mission together” (Nowviskie). How is self-initiated R&amp;D good for not only the Lab, but for the Library and UVA as well? Bethany Nowviskie wrote comprehensively and persuasively about the importance of 20% time, back in 2012 in “Too Small to Fail”, particularly in the section titled “Scholar-Practitioners”. I strongly recommend you read her piece if you’re considering asking for R&amp;D time, supporting colleagues in an R&amp;D time pilot, or have doubts about 20% time being a good choice. In addition to narrating the UVA DH history that led to the formation of the Lab, and the Lab’s rationale for investing heavily in grad students, Bethany covers: - A specific 20% time use that went on to impact many libraries: Project Blacklight (“solve[d] a serious problem for libraries dependent on expensive and inflexible vendor-provided catalog interfaces”) - How 20% time helps improve existing projects &amp; create new ones (e.g. our NEH-funded tool for telling stories in space and time, Neatline ) - Google’s popularization of 20% time in industry, where DH centers frequently lose technical staff - Scholars’ Lab’s self-initiated R&amp;D time has now been adopted at other DH centers and labs (e.g. Princeton) &amp; is cited as a best practice - Helps us retain a healthy, sustainable working environment: “I’ve watched too many digital humanities centers and projects balloon and then seem to lose their direction, getting caught up in soft-money scrambles and taking on work that is less than ideal for them, just to keep good staff employed” (Nowviskie) - As Bethany argues, it’s just good . We work to treat our staff as the experts they are, give them some guidelines but trust them to lead on what/when/where their best work happens, invest in them as humans who love to learn. Not because it’s good for the institution’s missions (although it very much is!), but because we need to take responsibility for making our institutions and communities better at caring for and celebrating people. Our implementation of 20% time directly supports the strategic direction illustrated in UVA Library’s “Library Entering Its Third Century” recommendations, particularly: “Deepen subject and methodological expertise across the Library by developing a culture of R&amp;D that strengthens our ability to contribute to the research, scholarly, and artistic enterprise of the University”. Show me some projects! In March 2018, we started what I hope can be an annual practice: getting the team together to eat pizza and share about the self-initiated R&amp;D we’re working on. It was lovely; we easily used up two hours asking questions and making suggestions about future work. (12 of our staff were able to attend, so that’s roughly ten minutes per person.) With the staff’s permission, I’m sharing what each of us was working on, to give you a picture of what 20% time can look like. (Please do note I may have lost nuances as a simultaneous note-taker and pizza-eater, and that these were only accurate as of March 2018—folks are working on different things now.) Ammon Shepherd (DH Developer): make art that’s tactilely responsive; thermal detonator watch with 12 LEDs to show the hour + access big data from somewhere Laura Miller (Head of Public Programs): Scholars’ Lab alumni network (map, viz) illustrating where our students go on to, showing the impact the lab has on careers and the field; Zoe made a couple Jupyter notebook demos to think through how this might look Zoe LeBlanc (DH Developer): for her app to store archival images, rewrote the logic and added auto-segmenting for paragraphs of text; work with computer vision scripts (OpenCV?), training dataset creation; was built in Django and AngularJS, she’s now rebuilding in React; statistics methodology by issue of magazines she’s studying; Cairo ’60s anti-colonial ideas change over time, so showing conceptual clustering changing over time; tagging images in her app (e.g. JFK images); interest in building her own OCR engine some day; Shane has possible data-testing set to use (cryptography journals) Brandon Walsh (Head of Student Programs): anxious pedagogies, thinking through how to expect + deal with frustration during classes, and how to apply this thinking with DH projects as well; text analysis cookbook from his frequently used code, with info on how/why cookbook snippets work; “how to learn text analysis” but with your own text Drew Macqueen (GIS Specialist): keeping a Notepad++ code journal; created the Arduino Chris (GIF above) for keeping the SLab office lights on and practice physical computing skills; learning about georeferencing 3D models with Will and Arin; CityEngine; wants to write a blog post e.g. on modifiable areal unit problem, why zip codes are bad locators; wants to add thermal sensor to Arduino Chris so it works better Jeremy Boggs (Head of R&amp;D): Thinking through distant reading of comic books, e.g. how Batman and the Joker in The Killing Joke become visually similar, John Walsh’s TEI for Comic Books, (image recognition of?) comic book text to study gender; using Italo Calvino’s writing towards a bibliography of books with their qualities ID’d being used as a framework for text analysis; interest in doing D&amp;D homebrew mashup with other content, e.g. Sarah and James’s characters from Samuel Johnson (http://genuineremains.jeremyboggs.net/) Shane Lin (Senior Developer): working on his history dissertation, including writing some code, studying Usenet for how/when cryptography entered into widespread political consciousness, when crypto went from mostly a math discussion to mostly a political one; exploring Usenet metadata e.g. to see whether messages are from people new to the discussion or not; turned OpenScan 2D sketch into 3D model Arin Bennett (3D Visualization Specialist): Working with Unity as part of his mentorship of the Praxis cohort; augmented reality (AR) mapping via the new Unity asset we purchased; working on a better AR interface for doing his trail running while using the headset, e.g. point of interest along his way, time countdown; AR walking tours; touching 3D print but seeing scan/photo version in headset Will Rourk (3D Data and Content Specialist): Modular synthesizers; making 3D-printed chocolate molds from any 3D model; writing and researching what CHI (cultural heritage informatics) as a field is today e.g. CLIR talk and paper, bibliography creation Chris Gist (GIS Specialist): learning new ARCGISonline tech, 3rd rebuild because the tech keeps changing and adding new products; tagging locations on a friend’s property; thinking about siloed research in regional cultural places and building a trail along the historic trail between them, creating new data layers to assist with pitch for new trail Amanda Visconti (Managing Director): Helping draft possible ACH code of conduct for all platforms/outside of conferences; turned my Drupal dissertation website into a static archival site; article revisions for Debates in DH and American Quarterly ; approaching HTML/CSS/web browser functionality as something one can have expertise in, e.g. figuring out what JS stuff I can hack CSS to do; considering writing a textual forgery LARP; building a pneumatic tube message system for the lab; Shane helped me 3D print my actual brain from a brain MRI; learned to use FabLab laser cutters; working as a Programming Historian editor Alison Booth (Academic Director): Collective Bibliographies of Women and proposing new NEH grant; creating Gothic Walking tour of Grounds with her class; thinking through a DH project extending understanding of the Anne Spencer House Challenges In practice, 20% time doesn’t happen exactly how we’d like. We’re busy staff, with many interests; because of our mentorship-not-service model, much of our daily work feels like, or is, what we’d personally choose to do R&amp;D on (fulfilling the original goal of making sure folks in service roles could continue to grow as scholars). The desire to use one’s 20% time is something each of our supervisors fully support, so challenges in making this time are never because management won’t let staff make time for this work; I don’t think we’ve ever needed to veto a project idea for not fitting our 3 guidelines. Known issues with 20% time include: Making the time for it . Blocking out time on our calendars has helped some with this (especially starting 1 term away, when calendars are empty enough you can e.g. grab the same time every week), but I think it’s less a matter of making specific hours, and more emphasizing to ourselves that this self-initiated work is as important as our work that more immediately impacts our community. We often don’t practice the same commitment to our self-initiated R&amp;D projects as we do for student and faculty collaborations. Some ideas for helping with this are a Slack channel to update colleagues on our progress and receive encouragement and help, and establishing writing/research partners. Another barrier is needing help from SLab colleagues to move forward with your self-initiated work, but we feel reluctant to ask for that help: knowing how busy everyone is, or feeling uncomfortable using resources for a personally-initiated project. Or folks actually don’t have time during fall and spring terms to help, and internal collaborations needs to wait for the summer. As a small step forward over this barrier, I’ve been trying to recognize the importance of these self-initiated projects by helping schedule meetings where relevant SLab staff can work together to advance a project. Setting a formal amount of time for staff research shifts the conversation from whether a project can happen, to how to best make it happen. Our colleagues in other Library units don’t have 20% time in their job descriptions, although a number do have language about professional development. Many of our library colleagues are very active researchers. I do see an explicit percentage of job time is different from knowing you can propose a self-initiated project to your supervisor, but they may or may not understand its application, and they may or may not support your making time for it during your work day. 20% is equivalent to one day a week, 10% is one day every two weeks; this doesn’t need to happen daily or weekly, and some staff find it best for them and their community to bank this time during fall and spring and use it during winter and summers. The Library supports each of its staff with at least $1k of professional development travel funding each year (and extra for the first 2 years on the job); UVA also provides university staff annual funds to take courses and attend conferences. The Library and UVA professional development funds can’t be used for books or equipment, which can be difficult for folks seeking to self-teach or to learn a hands-on skill like electronics. We’d like to both better communicate our work to our colleagues and the public, and also back up our colleagues (here and beyond UVA) when they’re articulating why they’d like to self-initiate research as a small but formal part of their jobs. In the last year, we’ve made steps toward these goals including getting pizza and sharing our R&amp;D work and frustrations among SLab staff; scheduling annual summer brownbags just for library colleagues, where we’ll update folks about each of our staffers’ work and discuss potential collaborations throughout the Library; and re-committing to publishing on any conference travel funded by SLab (either a blog post within 6 weeks of the event, or an internal brownbag updating Library colleagues, as a condition of future travel funding). We’ve also heard some questioning around staff using 20% time to work on dissertations, worded as “paying people to finish their dissertations”. That 20% time helps us hire such scholars and best serve the campus community is discussed above, but I’ll add that as long as dissertational work fits our 3 guidelines (ensuring work benefits the university, is made public), it’s actually particularly suited to 20% time: it’s likely got a clear timeline, mentor, and obvious application to that staffer’s professional growth. I agree with Jeremy Boggs’ argument that whether 20% time is used for work that is part of a dissertation is irrelevant; instead, we should be looking at whether it’s meeting those 3 guidelines, and think of it in terms of what work is being done. For example, Senior Developer and History ABD Shane Lin has used some of his 20% time to learn to scrape Usenet metadata to understand historical community development, sharpening skills around web scraping, text analysis, community research, and historical scholarship critical to his role in the lab (and incidentally, moving his dissertation forward as well). What do you think? If you have suggestions for improving our 20% time in policy or practice or questions, let us know! We’re also interested in your own experiences pursuing research and professional development, regardless of your job role; and what, if any, policy about research and professional development your institution offers. Special thanks to Bethany Nowviskie, whose arguments for 20% time still benefit many folks today, and whose excellent 2012 rationale for 20% time, “Too Small to Fail”, is quoted liberally in this post!"},{"id":"2019-04-30-archiving-dh-part-3-the-long-view","title":"Archiving DH Part 3: The Long View","author":["Brandon Butler","amanda-visconti","ammon-shepherd"],"date":"2019-04-30 06:53:47 -0400","categories":["Digital Humanities"],"url":"archiving-dh-part-3-the-long-view","layout":"post","content":"Photo by Mantas Hesthaven Begin with the End in Mind You don’t begin a journey without having a destination in mind. (Well, I guess you could. That would be a fun adventure. But for the purposes of our discussion, such a thing would be absurd.) Attributed to Stephen R. Covey, the phrase “begin with the end in mind” is a necessary way to think about any Digital Humanities project. When you have a clear sense of how the project should spend its life in the end (whatever that may be), then the project can be built from the beginning to fulfill that purpose. The final post in this series will look at what some of these “digital end-life” options might look like. This post focuses on some things that project owners, designers, developers, and users should be thinking about as they plan and create digital scholarship generally, and digital humanities projects, and web-based scholarship, more specifically. Here we look at the true costs of projects, establishing a data policy, defining ownership, documentation and resources for planning digital scholarship projects. Standing on Shoulders Many more brilliant minds than mine (Ammon) have written about this topic already. From the 2005 website and book “Digital History: A Guide to Gathering, Preserving, and Presenting the Past on the Web” by Dan Cohen and Roy Rosenzweig to last years (2018) book “The Theory and Craft of Digital Preservation” by Trevor Owens. (Disclaimer, I worked with Dan, Roy and Trevor at the Center for History and New Media at George Mason University, so I unabashedly hold up their offerings as great examples.) Another incredibly useful resource is the Research Data Curation Bibliography by Charles W. Bailey, Jr., which contains over 750 articles, books and technical reports relating to digital curation, including many on digital preservation. There are surely many, many more resources available. Someone oughta build a website and database… The Long View Photo by CEphoto, Uwe Aranas True Costs Supremely important for any project (not just creating Digital Humanities projects) is knowing with as much certainty as possible, what the project will cost. One wouldn’t agree to build a house without knowing how much it will cost. A funder doesn’t need to know how much each construction worker will be paid exactly, but the contractor does need to know how much the actual workers, materials and such are going to cost. And that’s one of the main points. As a DH project manager, developer, etc, we are more like a contractor than a funder. So we do need to know all of the hidden costs: costs of labor, and cost of resources, costs of sustainability, etc. So here are some the known and hidden costs that might occur in any given project, and that should be kept in mind when developing a road map for a project. Some of the hidden and known costs of DH projects may include: Resources you assume will be there Grant funding Department funding Personal funding In-kind resources Resources that you aren’t paying for directly Electricity Department or university provided hosting (servers) Technical support for servers, phones, computers, etc. Access to data, journals, databases, etc. Housekeeping for your office, building Staff and cost of living Developers Designers Documentation writers Project managers Testers Hosting or Server space local vs. contracted out ongoing costs Maintenance Updates Security patches Backup services Monitoring Domain name Yearly cost Moderators for digital collecting sites User advocates to monitor and answer support forums and contact email addresses Software Initial purchase costs Upgrade or subscription costs There are surely more. Just like each tasty meal you will eat this week is made from different ingredients, your specific project will have a unique blend of up-front and hidden costs, making your project the unique and wonderful gift to the world that it is. Deukhine - sauce with peanut butter and tamarind, By T.K. Naliaka, CC BY-SA 4.0 [ Source Link ] Data Policy Your project will most likely use data or generate data. You need to be clear up front on how you expect and desire others to be able to use, or not use, the data from your project. For a smaller, not institutionally supported DH project, a data policy can be a simple set of public commitments about the possible futures of your project. The following is a practical example of implementing a data policy for a small digital project. My (hey, this is Amanda!) Infinite Ulysses project was dissertational research supported by a single person, but it also had a wide and varied audience for such a small project (12k unique visitors during the first couple weeks of beta opening) and hosted user-created content (1k annotations). Its focus on interface design and user studies, and the use of social media encouraging testing of the site, were important in inviting use by enough users from both inside and outside academia to address my questions about the impact of such a shared space on literary reading and learning. On the other hand, the site looking nice and having some publicity probably conveyed a permanency for the site that I was hopeful for at the time, and had some real plans and resources for continuing post-PhD, but was ultimately the dissertation project of a single person contributing their own limited time, money, and moderation stress to running the site. I should have done better making sure folks were clear that there was one person behind the project and no presumption of ongoing hosting, for example for folks planning long term or repeat readings (likely, given Ulysses’ length, difficulty, and rewards for rereadings) or planning to demo or use the site in a class. I am happy that the project, from the start, did include a public document addressing both data privacy and data preservation plans for the project. For example, because the site invited and stored user annotations as they were reading a long and difficult novel, I had a responsibility to let folks know possible disasters (aka “I lose all your comments”) and what I was doing to prepare against those calamities. My public data policy stated: “This site has an automated daily backup that includes all user annotations and comments, as well as weekly server backups. In addition, the site is regularly replicated on a development and local server.” Critically, this public data policy included plans for if I needed to stop site interaction or take down the site for some reason. Even if your plan is for your project to exist in its current or an improved form ongoing, sharing a contingency plan from the start lets users make informed decisions about their activity with your project. My policy stated: “If the site ever needs to be shut down, go offline indefinitely, or be transferred to substantially different ownership, I’ll contact all users through the email address given on your user profile page with directions for downloading your content. Users will be given at least one month’s notice to export their content. Should such a situation ever occur, I’d prioritize keeping the site up but in static form (i.e. you can’t change or add to the annotations and comments anymore) so that it’s still available as a resource; users could then opt to use the Annotator.js browser plugin to continue the annotation of the text using an AnnotateIt account. If that ever becomes the case, I’ll post instructions to the front page of this site on how to continue using the site.” I did end up first shutting the site to new users, then to new annotations, and eventually migrating from Drupal to a static archived version of the site. I felt better about doing this given there was never an explicit promise to always run the site the way it currently ran (though see above caveats about site shininess and social media). My data policy also helped project users protect and own their labor through documented paths to export their own work for use elsewhere or personal preservation: “users should be able to export their content with the push of a button”, ideally, in multiple non-proprietary formats that support both reading (e.g. HTML or TXT) and data manipulation (e.g. JSON or CSV). To create a similar public data policy for a small DH project, you’ll want to ask yourself the following:\n1. What might people build off this? Think about ways your site/data might be treated as a feed, API, or permanent fixture that you might not hear about, e.g. if folks could be running Twitter bots off the presumption of continuous new content or activity on your website, or use in a classroom.\n2. What have people built on this? Uses you know about; on Infinite Ulysses, this meant users’ textual annotations, but also: the community they help build and constitute, and their reputation, scholarly or otherwise, in that community; any scholarship citing and/or depending on folks being able to look at or interact with your project as it currently appears. Ownership Who owns the project is also a very important decision to make at the beginning of the project life cycle. Agree up front on ownership and licensing issues and set out some permissions in advance: who are the authors of a project (as distinct from advisors, technical helpers, editors, well-wishers, etc.), who can decide its fate, who is responsible for fixing it when broken, who is liable for any legal issues that may arise. Decide in advance whether to give the Library permission to maintain/migrate/preserve/serve a project if/when it is no longer maintained by its authors/owners in the future. Decide on an appropriate copyright license. If so desired choose open license solutions that proactively grant the public permission to migrate/preserve/modify/update/fork projects when owners move on. Documentation, documentation, documentation Steve Balmer’s Developers mantra I wish I (Ammon, here) had an audience of DH developers (or more importantly, their supervisors and the project leads) where I could chant “Documentation! Documentation! Documentation!” until I had giant sweat spots on my shirt.  :) One of my biggest gripes with any DH project I’ve worked on is the lack of documentation. And for the record, I’m just as guilty as those I accuse. There seem to be about three phases to a project, and documentation is critical for each phase to go smoothly from one to the next: 1) development phase, 2) stable release, 3) retirement. If anything, documentation provides a history of the project. And as we all hope, some future researchers are going to study our project and want to know as much about it as possible. Let’s give those future scholars a wealth of documentation to work with. My plea is to build documentation writing into the process of creating the project. Give developers time and space (on the calendar and distraction free) to write about what they are creating. In my view, documentation is just as important as the project itself. Documentation and the project should actually be seen as the the same thing, they should be inseparable. Documentation is like one of three legs of a three legged stool. (I leave it to the reader to label the other two legs.) Image Source Development Phase Documentation during this phase is critical for developers actively working with the project. Ongoing documentation of the project is helpful for developers to collaborate and coordinate. Documentation during this phase provides the logic and reasoning behind coding and technology decisions, defining what files and resources are important to the project, any resources that are not used in the public side but required for the backend, or kept for storage or future ideas. But more importantly for the long view, actively documenting the process of creation provides information that is vital for those who need to put the project into an archived state. So many times I go into an ancient folder (anything older than 2005) and find many files that don’t seem to be used in the production version of the site. Because there is no documentation, I’m left to decide myself if they should be included in an archive version or left out. What I have found very helpful for projects is to include a File Structure section in the documentation. This section simply lays out all of the files that are required to make the project run and a short description of their function. An easy way to create the file structure (if you are using a Unix or GNU/Linux based server) is to use the tree command, like so: tree -L 1 This prints out the first level (-L 1) of files and directories located in the folder where you run the command. You may need to play with the level and other options to get the result your looking for. I find it is not necessary to list every single file and folder (like the application files and folders, or all of the resources like images and videos, files in archive directories, and such), but the main parent folders and a short description of the contents usually suffices. An example taken from the Collective Biography of Women project git repository README file is shown below. **Folder Structure**\n\nAll of these files are in the git repository (except .env which is created separately on local and production).\n\n```\n  ├── alldata.json        (data file, contains all of the bibliography information, in JSON format)\n  ├── data/               (data folder for the solr container)\n  ├── default.conf        (nginx config file with a change to redirect old URLs to new)\n  ├── Dockerfile          (File to create the nginx image, pulls in the default.conf and files from static-content)\n  ├── docker-compose.yml  (file used by docker, determines which docker settings and images to use)\n  ├── .env                (environment file)\n  ├── myconfig/           (folder containing the solr config files)\n  ├── README.md           (this file)\n  ├── static-content/     (folder containing the static html, css, js, image files. the cloned website, not under version control, but in the wb-static image as /usr/share/nginx/html/)\n  ├── web.xml             (solr config file that allows the javascript in the search.html file to access the solr server)\n```\n\nNote: The following files in the static-content/ folder were edited from the originally scraped static versions, or created new.\n\n- search.html (modified to connect with the solr docker container for search)\n- solrSearch.js (created new as the connection between the solr database and the search page; displays the results from solr to search.html)\n- style.css (adds styles for the loading animation) Stable Release While a project is out in the wild and working as planned, it is important to document the expectations of the project. Write out clearly what is the expected behaviour of the site. What should the results of searching look like? How was the site different than originally planned and written into the project or grant proposal? What were the big compromises that had to be made to get the project functional and live? What are the next steps for the project? Are there functionalities and steps that were left undone? Retirement This phase of the project should be seen similarly to the development phase. All decisions, changes, alterations, modifications, concessions, and choices should be documented. Specifically, note what functionality was lost during the archiving process and the reasons for that decision. Further Resources Here are a few more resources for planning Digital Humanities projects while keeping the long view in mind. If you know of other great resources, send an email to ammon@virginia.edu and I’ll add it to the list. https://jhupbooks.press.jhu.edu/title/theory-and-craft-digital-preservation https://sites.haa.pitt.edu/sustainabilityroadmap/ https://openpreservation.org/ https://dpconline.org/handbook https://rc.library.uta.edu/uta-ir/handle/10106/25646"},{"id":"2019-05-09-lessons-north-star","title":"Lessons Learned from Project North Star","author":"abhishek-gupta","date":"2019-05-08 21:00:00 -0400","categories":["Digital Humanities"],"url":"lessons-north-star","layout":"post","content":"Over the course of the spring semester of 2019, the Immersive Club at UVA built its own Project North Star headset. Project North Star, for those unfamiliar with it, is an open-source augmented reality (AR) headset based on the Leap Motion technology generally used for virtual reality (VR). Leap Motion is a company focused on finger tracking for VR and AR applications, and the device released from the company can be mounted onto the HTC Vive or Oculus Rift headsets without a complicated setup process. It can even be used as a standalone device connected to a computer, as shown below. Leap Motion Home Page Some inspiration for starting this project came after experiencing the Microsoft Hololens. Though it was a great first glimpse of an AR headset, the experience left much to be desired, including a larger screen area, wider field of view, and more responsive finger-tracking. From this, the club decided to take on the task of building an AR headset after reading about Project North Star. The North Star headset is a great improvement upon currently-available AR technology due to its open-source nature, larger screen area, wider field of view, and increased finger-tracking fidelity. In fact, the total costs for the entire project are approximately $300, while the Hololens costs an order of magnitude higher at $3000. The open-source aspect of it is noteworthy due to the high engagement of the community for providing feedback, support, and improvements. Unveiling North Star and North Star is Open-Source However, despite the low material cost, this project is heavily dependent upon access to 3D printers, along with other such makerspace facilities. Fortunately, the Immersive Club was able to utilize the 3D Printing Studio at Robertson Media Center in Clemons Library for fabricating the complex parts necessary for construction. More specifically, the Ultimaker printers with dual-extruder heads were particularly useful due to their ability to print with water-soluble support material. This allowed components with highly intricate geometries to be printed very easily. Additionally, the TAZ 6 printer at the 3D printing facility was used to bend a flat piece during assembly. This was done by heating up the build plate to 70 degrees Celsius, which is warmer than the traditional build plate temperature for 3D printing of approximately 60 degrees Celsius. Heating the plate to this temperature allowed us to put the flat piece on the heated plate, wait for some time, and then bend it to the desired specifications while the print was still hot and malleable. After the print cooled, the formerly-flat piece would now stick to its new curved shape that we altered it to. This technique was quite helpful in reducing filament used to create support structures and time spent printing the part, even if we weren’t confident in its effectiveness initially. Other university facilities that were involved in North Star’s production included Lacy Hall, the Mechanical Engineering building, the Makerspace at Scholars’ Lab in Alderman library, and the VR Space at Robertson Media Center in Clemons library. At Lacy Hall, we needed to cut two short pieces from a long metal bar, so the staff there helped us use the waterjet cutter to get our shorter metal bars. After cutting them at Lacy, it turns out that they didn’t have a tap size small enough to drill the screw holes through the bars. However, we reached out to the machinists at the Mechanical Engineering building, and we were able to get the bars tapped for our desired screw hole size and threaded for our screw type. We also needed to modify a special power-carrying USB cable by adding jumper pins to the stripped wire, which would then fit the slots on our circuit board. This was important because previous cables we tried didn’t have the capacity to carry the higher current needed for both of the displays together. Using the equipment at the Makerspace, we modified the cable and successfully fit it on the circuit board. Finally, the VR Space is where the club members met to assemble the headset and work through the calibration procedure. The PCs are already VR-capable, and we loaded the North Star demonstration in Unity so that all club members could work on it in a public facility. There were several aspects of the project that either could have gone better or went wrong entirely. Starting from the beginning, the 3D printing process seemed to be unnecessarily complicated, particularly when it instructed us to use Simplified3D, which is a paid software. We substituted Cura in place of that, which is a free software present on all PCs in the 3D Printing Studio. We also had a lot of combined experience with Cura, and thus we were confident that we could resolve issues faster if we used Cura rather than a software that none of us were familiar with. Additionally, the full assembly process was not documented as well as it could have been. Sometimes previously unknown part numbers would show up in the assembly documentation and it would be up to us to work backwards and determine how they fit into the overall assembly. There was also an issue where we put in a part upside down that then led to inverted and incorrect functionality. However, in hindsight, most of the parts fit together very intuitively, and there weren’t too many ways that we could have messed up the process. Regardless, Leap Motion should still make sure to update its documentation to resolve any ambiguities. Another issue with the assembly process is the placement of the ribbon cables for the screens on each side of the eyes. The screens are exactly alike, with the ribbon cable bending one way after it extends a little bit from the base. Unfortunately, this means one side of the headset has an awkward and potentially fragile placement of the ribbon cable. As seen in the image below, the ribbon cable on the left side of the headset folds in an unusual manner before it connects to the circuit board. Contrasted with the right side, the left side’s cable could be torn off the monitor very easily and is not tucked away neatly. Finally, after we finished the entire assembly process and tested out the demonstration software, the hand positions were way off, so we researched some calibration procedures. Though this seems to be the last major obstacle to a fully-functional North Star headset, it is a significantly difficult one to overcome. We are currently working through the manual calibration process, but so far it seems like a very complicated process for which exists little to no documentation. All in all, this project has been moderately successful for us, but there are a few things that we would have done differently. For people looking to do this themselves, it is definitely worth reading about all the resources that exist about construction, assembly, calibration, and development before proceeding with this project. This was a semester-long project for Immersive Club, but we could have also finished in half the time if we planned better schedules for purchases, 3D prints, and assembly. That said, anyone looking to use this for a class project should be able to get to this point even faster. To help with this, we could have split the work into two groups, where one worked on the electronics and the other on the headset assembly. Since the two are joined only at the end, there was no need for us to wait until the headset was finished before starting on the head mounting apparatus. Lastly, it would have been interesting to build the headset with the integrated Leap Motion sensor at the front, and for the next headset, the Immersive Club may consider that route to help organize components a little better. Overall, embarking on this journey has been extremely valuable for all club members involved, and we are glad that we found this North Star to guide us through our AR experience. UVA Resources 3D Printing Studio at Robertson Media Center in Clemons Library: The 3D Printing Studio is a facility for students to use 3D printers free-of-charge for academic projects or personal purposes. It has Makerbots (simple printers with equally simple interface), TAZ printers (higher-end printers that can print with flexible material), and Ultimakers (higher-end printers with dual extruders that can print with water-soluble support material). Student Experiential Center at Lacy Hall: Lacy Hall is a facility that helps students with experiential projects by providing the equipment and machinery needed to produce large mechanical components. Machine Shop at Mechanical Engineering building on Engineers’ Way: The Machine Shop also helps students with production of mechanical elements, but they work on smaller parts due to space constraints in the lab area. Makerspace at Scholars’ Lab in Alderman library: The Scholars’ Lab is a digital humanities and cultural heritage center at the UVA Alderman Library. The Makerspace is located within that area, and specializes in experimentation with technologies like 3D printing, sewing, physical computing, and augmented reality. VR Space at Robertson Media Center in Clemons library (image) :\nThe VR Space has two HTC Vive headsets, each with their own dedicated walkable area and VR-capable PC to create an immersive experience in virtual reality."},{"id":"classroom","title":"The Classroom","author":null,"date":null,"categories":null,"url":null,"layout":null,"content":"The Classroom is a flexible learning space shared by the Scholars’ Lab and other Library units, providing space for library-related instruction, workshops, invited speakers, and special events. Scholars’ Lab workshops and the speaker series are often held in this room. Please note that overhead air vents in middle of room mean a mic is required for folks in the back half of the room to hear presenters well. When not otherwise reserved, this room is available for use as open study space, with seating for 30 people at chairs and tables. Check the schedule on the door for times when the room is unavailable. <!--\n \t\n* **Size:** seating for 30 people\n \t\n* **Equipment:** 25 Dell laptops; ceiling-mounted projector; instructor workstation with desktop, audio, and video.\n \t\n* **Reservations:** This room is typically available only for library-related instruction and activities. Please contact [libevents@virginia.edu](mailto:libevents@virginia.edu) for more information.\n\n-->"},{"id":"common-room","title":"The Common Room","author":null,"date":null,"categories":null,"url":null,"layout":null,"content":"The Scholars’ Lab Common Room provides twenty computer workstations in both solo and small group/collaborative configurations. Sunny and open, it can be enjoyed as study space as well, with additional seating for about thirty-two people at tables and chairs and another fourteen at the laptop bar with a view of the patio outside. We hold regular Open Office Hours during fall and spring term at the discussion area at the south (main entrance) end of the room. Here is where you will also find our Makerspace . <!--\n  * **Size:** seating for 66\n \t\n  * **Equipment:** 12 Dell workstations with [advanced software](http://its.virginia.edu/labs/listEquipDetail.php?room_id=34&machine_group=1) (plus AbbyyFineReader and Adobe Creative Suite); flat bed scanners (2 large format, 4 with multi-sheet feed); color and black and white printers.\n \t\n  * **Reservations:** Typically not reservable, except for major library or university technology-related events. Contact [scholarslab@virginia.edu](mailto:scholarslab@virginia.edu) for more information.\n-->"},{"id":"makerspace","title":"Makerspace","author":null,"date":null,"categories":null,"url":null,"layout":null,"content":"The Scholars’ Lab Makerspace is a place for crafting, tinkering, and experimentation with technologies like desktop fabrication, physical computing, and augmented reality. Open to everyone —absolutely no prior experience needed!—we specialize in hands-on approaches to research questions in the humanities and arts.   <!--\n\n  * **Hours:** 1:00pm - 7:00pm, Monday through Thursday and 1:00pm - 5:00pm, Friday, or by appointment\n \t\n  * **Size:** seating for 8\n\n  * **Equipment:** Makerbot Replicator 2, Ultimaker 2, Lulzbot Taz 5, [Arduinos](https://www.sparkfun.com/products/12001), and a [variety](http://scholarslab.org/makerspace/) of other maker technologies\n\n  * **Reservations:**  Stop by to talk to one of our student consultants, attend our maker [workshops](http://www.scholarslab.org/events/), or contact us at [scholarslab@virginia.edu](mailto:scholarslab@virginia.edu) to schedule an appointment with Scholars’ Lab staff to discuss your planned project.\n\n-->"},{"id":"seminar-room","title":"The Seminar Room","author":null,"date":null,"categories":null,"url":null,"layout":null,"content":"The Scholars’ Lab seminar room is reserved space for library activities instruction, and when not in use as a classroom is available as open study space. Check the schedule on the door for times when the room is unavailable. <!--\n\n* **Size:** seating for 20 (seating around table limited to 14)\n\n* **Equipment:** ceiling-mounted projector; instructor station w/audio and video; 2 whiteboards.\n\n* **Reservations:** Throughout most of the year, this room is available for booking by members of the UVa community on a first-come first-served basis. Please contact [libevents@virginia.edu](mailto:libevents@virginia.edu) for more information.\n\n-->"},{"id":"3D-chi","title":"3D Cultural Heritage Informatics","author":null,"date":null,"categories":null,"url":"3d-chi","layout":"work","content":"3D cultural heritage informatics in collaboration with IATH. More info coming soon!"},{"id":"LhasaVR","title":"LhasaVR","author":null,"date":null,"categories":null,"url":"lhasavr","layout":"work","content":"The LhasaVR project represents UVA’s participation in the Mellon-funded Humanities Virtual Worlds Consortium. More info coming soon!"},{"id":"project-template","title":null,"author":null,"date":null,"categories":null,"url":"the-slug","layout":"work","content":"year options if not known:\ncurrent\n(before 2018) collaborators: \n  - name: TBD\n    role: Please contact us if you have information on past collaborators. These files also need a thumb &amp; banner image created &amp; placed in the appropriate location Include text recognizing all collaborators &amp; describing project research-category can be:\nInternal Projects &amp; Community Service\nCollaborations\nEvents &amp; Community Building\nPrototypes &amp; Proofs of Concept"},{"id":"augustcollecting","title":"Unite the Right Rally and Community Response","author":null,"date":null,"categories":null,"url":"augustcollecting","layout":"work","content":"Recognizing the significance of events on the weekend of August 12, the University of Virginia Library is building an archive of materials surrounding the “Unite the Right” rally and counter-protests. The Library is interested in personal digital submissions (including images, stories, audio, or video) from the events at UVA and in the Charlottesville area, so that they may make these materials available to students and researchers studying these events. Donors have the option to keep their name private if they wish. ( image courtesy of flickr user SCholewiak )"},{"id":"bibliographic-society-of-virginia","title":"Bibliographic Society of Virginia","author":null,"date":null,"categories":null,"url":"bibliographic-society-of-virginia","layout":"work","content":"The Bibliographic Society of the University of Virginia is a group founded to promote the interest of books, manuscripts, maps, printing, graphic arts, as well as bibliographic and textual criticism. The site contains digital publications, information about their print collections, as well as an archive of their journal, Studies in Bibliography ."},{"id":"collective-biographies-of-women","title":"Collective Biographies of Women","author":null,"date":null,"categories":null,"url":"collective-biographies-of-women","layout":"work","content":"Alison Booth’s Collective Biographies of Women began as a simple electronic text which, in collaboration with the Scholars’ Lab, blossomed into a rich instrument for the study of prosopography. Professor Booth has been selected as an  IATH and ACLS Fellow on the merit of this work."},{"id":"community-standards-3D-data-pres","title":"Community Standards for 3D Data Preservation","author":null,"date":null,"categories":null,"url":"community-standards-3d-data-pres","layout":"work","content":"More info coming soon!"},{"id":"cultural-heritage-dataverse-repository","title":"Cultural Heritage Dataverse Repository","author":null,"date":null,"categories":null,"url":"cultural-heritage-dataverse-repository","layout":"work","content":"More info coming soon!"},{"id":"design-pedagogy","title":"Design Pedagogy","author":null,"date":null,"categories":null,"url":"design-pedagogy","layout":"work","content":"A collaboration around Learning as Coordination: Postpedagogy and Design . In After Pedagogy Paul Lynch asks, “Is teaching still impossible?”—a question that echoes Geoffrey Sirc’s declaration a year earlier that “teaching writing is impossible” (508). These statements amplify a contemporary quandary facing teachers in writing studies: how has the originary and core imperative of writing studies—teaching—come to seem not just difficult but impossible to some of the most exciting thinkers of that very same discipline? Is teaching truly impossible, how did we get here, and how do we begin to move forward? We want to suggest that examining the relationships among postprocess, postpedagogy, and design can offer some answers."},{"id":"digital-yoknapatawpha","title":"Digital Yoknapatawpha","author":null,"date":null,"categories":null,"url":"digital-yoknapatawpha","layout":"work","content":"Currently led and hosted by IATH. More info coming soon!"},{"id":"equity-atlas","title":"Equity Atlas","author":null,"date":null,"categories":null,"url":"equity-atlas","layout":"work","content":"IMLS-funded project. More info coming soon!"},{"id":"faulkner-at-virginia-an-audio-archive","title":"Faulkner at Virginia - An Audio Archive","author":null,"date":null,"categories":null,"url":"faulkner-at-virginia-an-audio-archive","layout":"work","content":"Professor Stephen Railton, with assistance from the Scholars’ Lab, created the Faulkner at Virginia archive. Here you can listen in on William Faulkner’s sessions with audiences at the University of Virginia in 1957 and 1958, during his two terms as UVA’s first Writer-in-Residence."},{"id":"for-better-for-verse","title":"For Better for Verse","author":null,"date":null,"categories":null,"url":"for-better-for-verse","layout":"work","content":"A collaboration with UVA English professor Herbert Tucker, For Better for Verse tests users’ understanding of prosody, or poetic meter in English-language poetry. In the over 40 included poems, users can mark syllable stress, foot divisions, and even check answers. Intended primarily for teaching purposes, For Better for Verse allows students to explore poems of varying difficulty and complexity. The application embeds a core JSP, XSLT, and Javascript application within a Wordpress installation. The poems are encoded using the TEI P5 standards. Visit the site."},{"id":"geoloader","title":"Geoloader","author":null,"date":null,"categories":null,"url":"geoloader","layout":"work","content":"Geoloader is a suite of command-line tools that interact with the REST services of Geoserver and Geonetwork to make adding new media and metadata simple. Used in conjunction with Geoportal, this tools helps manage the flow of data to the discovery interface with support for Redis-based queing, constructing Solr indexes from ISO 19139-conformant metadata. This tool is available at Rubygems and the source code on our Github page. ( image courtesy of flickr user clickykbd )"},{"id":"gis-data","title":"GIS Data","author":null,"date":null,"categories":null,"url":"gis-data","layout":"work","content":"The Scholars’ Lab maintains a large set of GIS data that can be accessed from public computers in Alderman Library’s Scholars’ Lab and from public computers in the School of Architecture.  On those computers you will find a GIS Data link on the desktop that provides access to a long list of worldwide and US base datasets along with Virginia locality and University of Virginia datasets.  The base data includes government and census boundary layers, road network layers, etc. The Scholars’ Lab is in the process of making most of the Library’s GIS data available online at http://gis.lib.virginia.edu .  Your search results will include options to access the datasets in many standard formats. If you are looking for digital elevation models, orthoimagery, land use, etc. for the United States, we recommend the National Map: http://viewer.nationalmap.gov/viewer/ The best worldwide digital elevation models are ASTER datasets available for download from the USGS EarthExplorer: http://earthexplorer.usgs.gov/ ESRI makes many of their base layers available for download . Another good source for U.S. data is the National Atlas . Other data needs? Contact us ."},{"id":"jeffersons-notes-on-the-state-of-virginia","title":"Jefferson&#8217;s Notes on the State of Virginia","author":null,"date":null,"categories":null,"url":"jeffersons-notes-on-the-state-of-virginia","layout":"work","content":"Notes on the State of Virginia was created by Brad Pasanek and John O’Brien of the UVa English Department. The Notes project enables critical comparison and analysis of two versions of Thomas Jefferson’s Notes on the State of Virginia. The project includes page images and transcriptions of both Jefferson’s privately printed 1784 edition and the 1787 edition printed by London publisher John Stockdale."},{"id":"latvian-dainas","title":"Latvian Dainas","author":null,"date":null,"categories":null,"url":"latvian-dainas","layout":"work","content":"Latvian Dainas is an electronic edition of the 12 volumes of Latviešu tautas dziesmas, edited by Arveds Švābe, Kārlis Straubergs, and Edīte Hauzenberga-Šturma, originally published by Imanta, Copenhagen, 1952-1956. The electronic edition was edited by Maruta Lietiņa Ray. The collection chronicles the traditional Latvian form of music and poetry, which embody the cultural heritage Latvian society."},{"id":"makergrounds","title":"Makergrounds","author":null,"date":null,"categories":null,"url":"makergrounds","layout":"work","content":"More info coming soon!"},{"id":"mapping-afidna","title":"Mapping Afidna","author":null,"date":null,"categories":null,"url":"mapping-afidna","layout":"work","content":"More info coming soon!"},{"id":"mapping-holocaust-eastern-europe","title":"Mapping the Holocaust in Eastern Europe","author":null,"date":null,"categories":null,"url":"mapping-holocaust-eastern-europe","layout":"work","content":"More info coming soon!"},{"id":"mapping-the-catalogue-of-ships","title":"Mapping the Catalogue of Ships","author":null,"date":null,"categories":null,"url":"mapping-the-catalogue-of-ships","layout":"work","content":"The Scholars’ Lab is using geospatial technology to assist Classics professor Jenny Strauss Clay in testing her theories about the relation of ancient geography to mnemonic devices and poetic form. Least-cost path analysis in GIS, coupled with literary analysis of the nearly 190 place names mentioned in Homer’s Catalogue of Ships, also holds promise in helping to identify lost archaeological sites. Professor Clay argues that the Catalogue of Ships can be mapped as an itinerary, or more precisely, three itineraries that traverse most of Greece. The theoretical basis for the project is already complete. Clay’s recent book, Homer’s Trojan Theater (Cambridge University Press, 2011), argues that Homer was able to recite the Catalogue by creating a mental journey that used the mnemonic techniques involving loci or places, well known from ancient rhetorical writers. By envisioning a series of places, Homer could mentally walk – or sail – through Greece and produce a detailed catalogue. Our project will reproduce that journey by showing that the itinerary described follows the natural contours of Greek geography and the patterns of early Greek urban organization. Collaborators include Ben Jasnow and Courtney Evans, graduate students in Classics and past contributors to the Trojan Theater project. They are assisting with GIS analysis under the guidance of Chris Gist and Kelly Johnston of the Scholars’ Lab. Wayne Graham and other members of the Scholars’ Lab R&amp;D division are creating a presentational framework for our maps and text, and Jeremy Boggs is our lead designer. Bethany Nowviskie is guiding and coordinating the project."},{"id":"mlbs","title":"Mountain Lake Biological Station collaboration","author":null,"date":null,"categories":null,"url":"mlbs","layout":"work","content":"More info coming soon!"},{"id":"montpelier-archaeology","title":"Montpelier Archaeology","author":null,"date":null,"categories":null,"url":"montpelier-archaeology","layout":"work","content":"A collaboration with the University of Arkansas for 3D documentation of Montpelier archaeology. More info coming soon!"},{"id":"neatline","title":"Neatline","author":null,"date":null,"categories":null,"url":"neatline","layout":"work","content":"Neatline is a mix-and-match toolset for the creation of interlinked timelines and maps as interpretive expressions of the literary or historical content of archival collections. It allows scholars and archivists to build on standard descriptive metadata and georectified maps to produce rich, evocative – even theoretical – spatial and temporal visualizations of the textual content of catalogued letters, manuscripts, and artifacts. Neatline is therefore a geo-temporal framework for fruitful interchange among scholars and the stewards of primary resources. It builds on Omeka, OpenLayers, GeoServer, and SIMILE Timeline. Adam Soroka and Bethany Nowviskie conceived Neatline as a contribution to interpretive humanities scholarship in the visual vernacular.  Their initial work was funded by an NEH Digital Humanities Start-Up Grant, and the whole Scholars’ Lab team has taken the project forward with funding from the Library of Congress and in collaboration with the Omeka team at the Rosenzweig Center for History and New Media. David McClure is currently lead developer on Neatline. Learn more about our Neatline-related Omeka plugins, or visit the Neatline page ."},{"id":"omeka-plugins","title":"Omeka Plugins","author":null,"date":null,"categories":null,"url":"omeka-plugins","layout":"work","content":"Our work on Omeka is oriented toward adapting it for use in research and special collections libraries and with scholarly digital projects that build on library- or museum-managed archival resources. Our major Omeka project is Neatline, but we’ve also done recent work on UVA Special Collections exhibits and a project in collaboration with architectural historian Louis Nelson . We’ve developed the following plugins for the Omeka platform Bagit : Implements the BagIt specification for storing and transporting collections of files. FedoraConnector : FedoraConnector will make it possible to use objects from a Fedora Commons repository inside Omeka. This will permit users to comment on, annotate, and curate objects in the Fedora repository while using the simple, easy-to-learn Omeka interface. Neatline :  allows scholars, students, and curators to tell stories with maps and timelines. As a suite of add-on tools for Omeka, it opens new possibilities for hand-crafted, interactive spatial and temporal interpretation. NeatlineFeatures : NeatlineFeatures will allow users to visually manipulate geographic features with simple editors and combine features with material from NeatlineMaps to create even more powerful interactive maps. NeatlineMaps : NeatlineMaps connects the powerful open-source geospatial server GeoServer and Omeka. It permits users to ingest georeferenced images into Omeka and use them as parts of interactive maps. NeatlineTime : Allows users to create customized timelines of Omeka items and events, and provides the ability to browse an Omeka archive on a timeline. SolrSearch : Use the Solr search engine with Omeka. We’ve developed the following themes for the Omeka platform Astrolabe : a theme based on the design for neatline.org . Neatscape : a modern responsive theme design with support for Neatline views. We’ve also worked on some experimental plugins: EADImporter : EADImporter opens archival connections by allowing users to absorb Encoded Archival Description XML documents (the standard way for archives to describe their contents) into an Omeka repository. GenericXmlImporter : Import any arbitrary, flat XML data in to Omeka NeatlineWebService : Provides a public-facing facade for the Neatline plugin that makes it possible for public users to create accounts and use Neatline as a hosted web application. TeiDisplay : Render TEI files in HTML form that are attached to Omeka items. VRACoreElementSet : Bring the VRA Core Element Set in to Omeka"},{"id":"praxis-program","title":"Praxis Program","author":null,"date":null,"categories":null,"url":"praxis-program","layout":"work","content":"The Praxis Program is a project of the  Scholars’ Lab  at the University of Virginia Library. In its first two pilot years (2011-2013), it was generously funded by a grant from the Andrew W. Mellon Foundation to the Scholarly Communication Institute . Praxis is now fully supported by the University of Virginia Library. The Praxis Program funds a team of six University of Virginia graduate students from a variety of disciplines to apprentice with us each academic year. Under the guidance of Scholars’ Lab faculty and staff, they design and create a full-fledged digital humanities project or software tool. It’s a radical re-imagining of the annual teaching and training we offer, and is is meant to complement our work with Graduate Fellows in Digital Humanities, a program now in its fifth year at UVa Library. Recognizing that up-to-date methodological training is often absent or catch-as-catch-can for humanities graduate students, we see the early years of the Praxis Program as an opportunity to experiment with an action-oriented curriculum live and in public. We situate our program in a larger conversation about the changing demands of the humanities in a digital age. Praxis hopes to equip knowledge workers for emerging faculty positions and  alternative academic careers  at a moment in which new questions can be asked and new systems built. We’ll share our evolving curriculum and our staff and students alike will be  blogging about their experience . We invite you to watch as we see what it takes to produce thoughtful DH scholars who are comfortable designing effective user experiences, writing and working with open source code, engaging broad audiences, managing teams and budgets, and theorizing their work within the rich tradition of humanities computing."},{"id":"presidents-commission-on-slavery-and-the-university","title":"President&#8217;s Commission on Slavery and the University","author":null,"date":null,"categories":null,"url":"presidents-commission-on-slavery-and-the-university","layout":"work","content":"In 2013, UVa President Theresa Sullivan formed the President’s Commission on Slavery and the University . The Scholars’ Lab assisted the Commission by creating a web site so their explorations of the historical relationships between the University of Virginia, slavery, and enslaved people could be publicly shared. In addition, the Scholars’ Lab continues to consult on digital exhibits and projects related to the mission."},{"id":"project-blacklight","title":"Project Blacklight","author":null,"date":null,"categories":null,"url":"project-blacklight","layout":"work","content":"A free and open source ruby-on-rails based library discovery interface, recently highlighted in the Chronicle of Higher Education . Blacklight is a core component of the Hydra Project ."},{"id":"salem-witch-trials","title":"Salem Witch Trials","author":null,"date":null,"categories":null,"url":"salem-witch-trials","layout":"work","content":"The Salem Witch Trials Documentary Archive and Transcription Project consists of an electronic collection of primary source materials relating to the Salem witch trials of 1692 and a new transcription of the court records, including court records, contemporary books, and record books. Originally an IATH project, Salem Witch Trials has been supported by the Scholars’ Lab in recent years. The Documentary Archive is created under the supervision of Professor Benjamin C. Ray, University of Virginia. The Transcription project is supervised by Professor Bernard Rosenthal, University of Binghamton. Together with a team of scholars, Professor Rosenthal is undertaking a new transcription of the original court records, titled Records of the Salem Witch-Hunt, to be published by Cambridge University Press."},{"id":"slavery-images","title":"Slavery Images","author":null,"date":null,"categories":null,"url":"slavery-images","layout":"work","content":"More info coming soon!"},{"id":"solrmarc","title":"Solrmarc","author":null,"date":null,"categories":null,"url":"solrmarc","layout":"work","content":"A Java-based program for indexing MARC records into Solr indexes. This is the main indexing mechanism for both the Blacklight and Vufind projects. ( image courtesy of flickr user Roosh Inf3ktion )"},{"id":"spatial-search-and-delivery","title":"Spatial Humanities","author":null,"date":null,"categories":null,"url":"spatial-search-and-delivery","layout":"work","content":"In 2009 and 2010, with generous support from the National Endowment for the Humanities, the Scholars’ Lab hosted three tracks of an “Institute for Enabling Geospatial Scholarship.” An outcome of Institute, The Spatial Humanities website – which our participants dubbed not a clearinghouse but a “sharing-house” – was released in 2011 to serve as a community-driven resource for the spatial humanities. As of 2013, it continues to be updated. Community contributions are welcomed and encouraged ."},{"id":"speaking-in-code","title":"Speaking In Code","author":null,"date":null,"categories":null,"url":"speaking-in-code","layout":"work","content":"Speaking in Code was an NEH-funded symposium and summit that brought together 32 accomplished digital humanities software developers at the Scholars’ Lab for two days in November 2013. The goal was to give voice to things that are almost always tacitly expressed in our work: expert knowledge about the intellectual and interpretive dimensions of DH code-craft, and unspoken understandings about the relation of that work to ethics and inclusion, scholarly method, and humanities theory and critique. Together, we discussed, from developers’ own points of view, what is particular to the humanities and of scholarly significance in our work; examined particular hurdles or bars-to-entry, to people interested in DH software development; and devised and experimented with ways to bridge gaps in critical vocabulary and daily practice that can sometimes spring up between the creators of humanities platforms or tools and the scholars who depend upon and critique them. Our first meeting focused on creating inclusive, welcoming developer communities and ways to address the social implications of tacit knowledge exchange in digital humanities software development. The Speaking in Code web site remains as a place for publishing outcomes from the summit, sharing our original agenda, and fostering further conversation both online and face-to-face—including by making available a Codespeak Kit . Visit the Speaking in Code site ."},{"id":"takeback","title":"Take Back The Archive","author":null,"date":null,"categories":null,"url":"takeback","layout":"work","content":"Take Back the Archive was a direct response to the publication, in November 2014, of the article “A Rape on Campus” in Rolling Stone magazine, a searing account set at UVA. It was an effort by faculty, librarians, designers, developers, and students to record and interpret the outcry that followed the publication of the article, which five months later was retracted by the magazine. By that time, however, the archive had moved beyond its initial aims to embrace a bigger goal: documenting the history and culture of sexual violence at UVa over time, from the moment of its founding by Thomas Jefferson in the early nineteenth century to its current incarnation as one of the most prestigious public universities in the U.S.—a university marred, like most of its peer institutions, by a growing crisis of sexual violence. Faculty and librarians from the College of Arts and Sciences, the Scholars’ Lab, and Special Collections were joined by a succession of student assistants and interns, who did the primary research that produced the Collections now featured in the archive, and produced the Exhibits featured on the home page. They also shaped the archive, extending its interpretive strategies to encourage research at the intersections of race, class, and gender. We hope to add to the existing Collections and produce new Exhibits in the future, as the archive also seeks to replicate itself at other universities interested in documenting their own histories of sexual violence and assault. ( image courtesy of flickr user 10cuidados )"},{"id":"the-falmouth-project","title":"The Falmouth Project","author":null,"date":null,"categories":null,"url":"the-falmouth-project","layout":"work","content":"Louis P. Nelson of the UVa Department of Architectural History authored The Falmouth Project, an online geo-spatially accessible archive of information about the historic architecture of Falmouth, Jamaica. The archive includes information on the 764 buildings that fall within the boundaries of the historic district. Every building in the archive has a full PDF survey report that includes a summary of the building’s form and materials, a supposition about the building’s date of construction, an assessment of the building’s condition in 2008, and a photograph and approximate footprint."},{"id":"the-mind-is-a-metaphor","title":"The Mind is a Metaphor","author":null,"date":null,"categories":null,"url":"the-mind-is-a-metaphor","layout":"work","content":"The Mind is a Metaphor site is a collaboration with Brad Pasanek . The project collects “metaphors and root-images appealed to by the novelists, poets, dramatists, essayists, philosophers, belle-lettrists, preachers, and pamphleteers of the long eighteenth century.” The project took an existing database that was being manually edited through phpMyAdmin and reimagining it as a Ruby on Rails application with the primary search and browsing interfaces being provided through Solr and its faceting interface. By providing a faceted interface for the metaphors, researchers can quickly narrow search categories to quickly find examples of metaphors in specific literary periods, metaphor categories, genres, nationalities, political leanings of authors, religious views, as well as the gender of the writer. Technologies Used Ruby on Rails (with the  rsolr gem ) MySQL (backend data management) Solr (search and faceting interface) jQuery (user interface enhancements)"},{"id":"the-scholars-lab","title":"The Scholars' Lab","author":null,"date":null,"categories":null,"url":"the-scholars-lab","layout":"work","content":"The Scholars’ Lab is itself scholarship: an active exploration of experimental and digital approaches to knowledge, realized by experts who each combine theory and practice, and bring humanities and cultural heritage informatics expertise to address interdisciplinary research questions. As we discuss in our charter, we believe that research needs to be combined with human care and community to become scholarship. From putting our budget dollars behind our values, to working on projects documenting trauma on campus, to internal policies that support work-life balance, we’re working to both improve and document practices toward achieving a humane, ethical, and generative digital humanities community. In the future, we’ll collect on this page a list of blog posts, presentations, and other ways of making this scholarship public."},{"id":"the-university-of-virginia-art-museum-numismatic-collection","title":"The University of Virginia Art Museum Numismatic Collection","author":null,"date":null,"categories":null,"url":"the-university-of-virginia-art-museum-numismatic-collection","layout":"work","content":"The The University of Virginia Art Museum Numismatic Collection features nearly 600 coins of Greek and Roman origin. The coins were generally acquired in small lots that were purchased or donated from 1987-2001, but larger groups of coins belonging to English hoards were also acquired, including 51 from the Normanby Hoard and 302 from the Oliver’s Orchard Hoards. About 450 of the total number of coins are from the Roman Republic or Empire, providing a broad sample of coins from the late 3rd century B.C. to the late 3rd century A.D., particularly from the Crisis of the Third Century–including more than 100 coins from the breakaway Gallic Empire of A.D. 260-274. Many of the coins belonging to the U.Va. Art Museum are in poor condition, but digitization provides access to those coins that are too fragile to be handled by students and scholars of numismatics. In October 2007, funding was received from the University of Virginia Library to work in conjunction with the Art Museum and a Roman Numismatics class taught by Professor John Dobbins in the Classical Art and Archaeology program, and scanning of the coins in the Art Museum commenced shortly thereafter. The coins were scanned at 2000 DPI by Digitization Services, the department that creates and maintains new library content, providing a very fine quality image–one that provides superior detail than what can be seen with standard museum-issued magnifying glasses. Equally important to scanning the coins for preservation is to provide access to the images. The collection was described in Encoded Archival Description (EAD), with several coin-specific adaptations to describe physical attributes such as legends and iconography. This project appears to be unique in its application of EAD to numismatics. In addition to EAD’s capability of describing the physical attributes of each object in the collection, administrative history, essays, and index terms can be encoded in XML to create completely comprehensive metadata for those students and scholars of numismatics to use as a tool in their research. With the back end of the numismatic collection described completely in EAD by subject specialists, the Apache server applications Cocoon and Solr were adapted to create an aesthetically pleasing interface with the ability to conduct complex faceted queries to search, sort, and browse data in ways that are unique to numismatic websites.  The guidelines for adapting EAD to numismatics has so far been adopted by one other institution, and the site was selected as the ALA Digital Library of the Week in December 2008.  In 2009, John Dobbins became a fellow of the American Numismatic Society for the project’s contribution to the study of numismatics."},{"id":"thl-place-dictionary","title":"THL Place Dictionary","author":null,"date":null,"categories":null,"url":"thl-place-dictionary","layout":"work","content":"The “Places Portal” of the Tibetan and Himalayan Library stems from a long collaboration between the Scholars’ Lab and THL staff. “Places,” which includes a complex gazetteer, is the area of THL where spatial and geographical data is archived and presented, as well as where rich studies of specific places are published."}]